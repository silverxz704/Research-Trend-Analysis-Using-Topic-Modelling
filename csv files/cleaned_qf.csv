file_name,page_number,title,summary,page_data,published,pdata_clean
paper_qf_1.pdf,1,Quantum Game Theory in Finance,"  This is a short review of the background and recent development in quantum
game theory and its possible application in economics and finance. The
intersection of science and society is also discussed. The review is addressed
to non--specialists.
","arXiv:quant-ph/0406129v1  18 Jun 2004Quantum Game Theory in Finance
Edward W. Piotrowski
Institute of Theoretical Physics, University of Bia/suppress lysto k,
Lipowa 41, Pl 15424 Bia/suppress lystok, Poland
e-mail: ep@alpha.uwb.edu.pl
Jan S/suppress ladkowski
Institute of Physics, University of Silesia,
Uniwersytecka 4, Pl 40007 Katowice, Poland
e-mail: sladk@us.edu.pl
Abstract
This is a short review of the background and recent developme nt in
quantum game theory and its possible application in economi cs and
ﬁnance. The intersection of science and society is also disc ussed. The
review is addressed to non–specialists.
PACS Classiﬁcation : 02.50.Le, 03.67.Lx, 05.50.+q, 05.30.–d
Mathematics Subject Classiﬁcation : 81-02, 91-02, 91A40, 81S99
Keywords and phrases : quantum games, quantum strategies, quantum infor-
mation theory, quantum computations
1 Introduction
One hundred years ago, a single concept changed our view of the wo rld
forever: quantum theory was born [1]. Contemporary technology is based
on implementation of quantum phenomena as a result of this seminal id ea.
Regardless of the successes of quantum physics and the resulting quantum
technology social sciences persist in classical paradigm what in some aspects
1",2004-06-18T12:50:24Z, juquantum game tory nance edward ptrow institute toical psics  bia li pow pl bia poland jainstitute psics  sesia uni r sy tepl katowice poland abstra  t t ass le matmatics subje ass keywords introduone contemary regardless
paper_qf_1.pdf,2,Quantum Game Theory in Finance,"  This is a short review of the background and recent development in quantum
game theory and its possible application in economics and finance. The
intersection of science and society is also discussed. The review is addressed
to non--specialists.
","can be considered as an obstacle to uniﬁcation of science in the quan tum do-
main. Quantum theory is up to now the only scientiﬁc theory that req uires
the observer to take into consideration the usually neglected inﬂue nce of the
method of observation on the result of observation. Full and abso lutely objec-
tive information about the investigated phenomenon is impossible and this is
a fundamental principle of Nature and does not result from deﬁcien cy in our
technology or knowledge. Now, this situation is being changed in a dra matic
way. Fascinating results of quantum cryptography, that preced ed public key
cryptography [2] although not duly appreciated at its infancy, cau sed that
quantum information processing is currently expanding its domain. V arious
proposals of applying quantum–like models in social sciences and econ omics
has been put forward [3]-[8]. It seems that the numerous acquaint ed with
quantum theory physicists who have recently moved to ﬁnance can cause an
evolutionary change in the paradigm of methods of mathematical ﬁn ance. In
a quantum world we can explore plenty of parallel simultaneous evolut ions
of the system and a clever ﬁnal measurement may bring into existen ce as-
tonishing and classically inaccessible solutions [8]-[11]. The price we are t o
pay consists in securing perfect discretion to parallel evolution: an y attempt
(intended or not) at tracing the system inevitably destroys the de sirable
quantum eﬀects. Therefore we cannot expect that all quantum a spects can
be translated and explained in classical terms [12] (if such a reinterp reta-
tion was possible the balance could be easily redressed). Attention t o the
very physical aspects of information processing revealed new per spectives
of computation, cryptography and communication methods. In mo st of the
cases quantum description of the system provides advantages ov er the clas-
sical situation. One should be not surprised that game theory, the study of
(rational) decision making in conﬂict situations, has quantum counte rpart.
Indeed, games against nature [13] include those for which nature is quan-
tum mechanical. Does quantum theory oﬀer more subtle ways of play ing
games? Game theory considers strategies that are probabilistic mix tures of
pure strategies. Why cannot they be intertwined in a more complicat ed way,
for example interfered or entangled? Are there situations in which q uantum
theory can enlarge the set of possible strategies? Can quantum st rategies be
more successful than classical ones? All these questions have po sitive and
sometimes bewildering answers [14, 8]. There are genuine quantum ga mes,
that is games that can be deﬁned and played only in a sophisticated qu antum
environment. Some of these quantum games could be played only in ph ysical
laboratories but technological development can soon change this s ituation
2",2004-06-18T12:50:24Z,quantum full nature  fascinati it it trefore aentione ined dogame w are caall tre some
paper_qf_1.pdf,3,Quantum Game Theory in Finance,"  This is a short review of the background and recent development in quantum
game theory and its possible application in economics and finance. The
intersection of science and society is also discussed. The review is addressed
to non--specialists.
","(the most interesting examples emerge from cryptography). Som e classical
games can be redeﬁned so that quantum strategies can be adopte d [15]-[18].
This is ominous because someone can take the advantage of new (qu antum)
technology if we are not on alert [15, 8]. We should warn the reader th at
quantum games are games in the classical sense but to play a quantu m game
may involve sophisticated technology and therefore theoretical a nalysis of the
game requires knowledge of physical theories and phenomena nece ssary for
its implementation. This fact is often overlooked and quantum game t heory
is wrongly put in sort of opposition to (classical) game theory. Recen tly, in
a series of papers [6, 19, 20] the present authors described mar ket phenom-
ena in terms of quantum game theory. Agents adopting quantum st rategies
can make proﬁts that are beyond the range of classical markets. Quantum
approach shed new light on well known paradoxes [7, 21] and compu tational
complexity of economics [22, 23]. Besides the properties of Nature d iscovered
by human beings there is a whole universe of phenomena and appliance s cre-
ated by mankind. Therefore the question if present day markets r eveal any
(observable) quantum properties, although interesting, is secon dary to our
main problem of ﬁnding out if genuine quantum markets would ever com e
into existence. Quantum theory oﬀers a new paradigm that is able to pro-
duce a uniﬁed description of reality. This paper is organized as follows . First,
we present some basic ideas of quantum games. Then we describe qu antum
market games and review their attractive properties. Finally we pre sent our
personal view of the further development and possible applications of this
ﬁeld of research.
2 Quantum market games
As we have said in the Introduction, quantum game theory investiga tes con-
ﬂict situations involving quantum phenomena. Therefore it exploits t he for-
malism of quantum theory. In this formalism strategies are vectors (called
states) in some Hilbert space and can be interpreted as superposit ions of
trading decisions. Tactics and moves are performed by unitary tra nsfor-
mations on vectors in the Hilbert space (states). The idea behind us ing
quantum games is to explore the possibility of forming linear combinatio n
of amplitudes that are complex Hilbert space vectors (interferenc e, entangle-
ment [8]) whose squared absolute values give probabilities of players a ctions.
It is generally assumed that a physical observable (e.g. energy, po sition),
3",2004-06-18T12:50:24Z,som    rec e quantum besis nature trefore quantum  rst tnally quantum as introdutrefore bert taics gbert t gbert it
paper_qf_1.pdf,4,Quantum Game Theory in Finance,"  This is a short review of the background and recent development in quantum
game theory and its possible application in economics and finance. The
intersection of science and society is also discussed. The review is addressed
to non--specialists.
","deﬁned by the prescription for its measurement, is represented b y a linear
Hermitian operator. Any measurement of an observable produces with some
probability an eigenvalue of the operator representing the observ able. This
probability is given by the squared modulus of the coordinate corres ponding
to this eigenvalue in the spectral decomposition of the state vecto r describing
the system. This is often an advantage over classical probabilistic d escrip-
tion where one always deals directly with probabilities. The formalism ha s
potential applications outside physical laboratories [3]-[6]. But how t o de-
scribe complex games with say unlimited number of players or non–con stant
pay–oﬀs. There are several possible ways of accomplishing this tas k. We
have proposed a generalization of market games to the quantum do main in
Ref. [6]. In our approach spontaneous or institutionalized market t ransac-
tions are described in terms of projective operation acting on Hilber t spaces
of strategies of the traders. Quantum entanglement is necessar y (non–trivial
linear combinations of vectors–strategies have to be formed) to s trike the
balance of trade. This approach predicts the property of undividit y of atten-
tion of traders (no cloning theorem) and uniﬁes the English auction w ith the
Vickrey’s one attenuating the motivation properties of the later [2 4]. Quan-
tum strategies create unique opportunities for making proﬁts dur ing intervals
shorter than the characteristic thresholds for an eﬀective mark et (Brownian
motion) [24]. Although the eﬀective market hypothesis assumes imme diate
price reaction to new information concerning the market the inform ation ﬂow
rate is limited by physical laws such us the constancy of the speed of light.
Entanglement of states allows to apply quantum protocols of super –dense
coding [11] and get ahead of ”classical trader”. Besides, quantum version of
the famous Zeno eﬀect [11] controls the process of reaching the e quilibrium
state by the market. Quantum arbitrage based on such phenomen a seems
to be feasible. Interception of proﬁtable quantum strategies is fo rbidden by
the impossibility of cloning of quantum states. There are apparent a nalogies
with quantum thermodynamics that allow to interpret market equilibr ium
as a state with vanishing ﬁnancial risk ﬂow. Euphoria, panic or herd in -
stinct often cause violent changes of market prices. Such phenom ena can be
described by non–commutative quantum mechanics. A simple tactics that
maximize the trader’s proﬁt on an eﬀective market follows from the m odel:
accept proﬁts equal or greater than the one you have formerly achieved on
average [25].
We were led to these conclusions by consideration of the following fac ts:
4",2004-06-18T12:50:24Z,rmit iaany   t but tre  ref ihi ller quantum  eh pick rey quabrownish although entalement besis zero quantum intercepttre euphoria su
paper_qf_1.pdf,5,Quantum Game Theory in Finance,"  This is a short review of the background and recent development in quantum
game theory and its possible application in economics and finance. The
intersection of science and society is also discussed. The review is addressed
to non--specialists.
","•error theory: second moments of a random variable describe erro rs,
•H. Markowitz’s portfolio theory,
•L. Bachelier’s theory of options: the random variable q2+p2measures
joint risk for a stock buying–selling transaction ( and Merton & Scho les
works that gave them Nobel Prize in 1997).
We have deﬁned canonically conjugate Hermitian operators (obser vables) of
demandQkand supply Pkcorresponding to the variables qandpcharacter-
izing strategy of the k-th player. These operators act on the player’s strategy
states|ψ/an}bracketri}ht1that have two important representations /an}bracketle{tq|ψ/an}bracketri}ht(demand represen-
tation) and /an}bracketle{tp|ψ/an}bracketri}ht(supply representation) where qandpare logarithms of
prices at which the player is buying or selling, respectively [11, 26]. This led
us to the following deﬁnition of the observable that we call the risk inclination
operator [26]2:
H(Pk,Qk) :=(Pk−pk0)2
2m+mω2(Qk−qk0)2
2,
wherepk0:=k/angbracketleftψ|Pk|ψ/angbracketrightk
k/angbracketleftψ|ψ/angbracketrightk,qk0:=k/angbracketleftψ|Qk|ψ/angbracketrightk
k/angbracketleftψ|ψ/angbracketrightk,ω:=2π
θ.θdenotes the characteristic
time of transaction [25, 26] which is, roughly speaking, an average t ime spread
between two opposite moves of a player (e. g. buying and selling the s ame
commodity). The parameter m> 0 measures the risk asymmetry between
buying and selling positions. Analogies with quantum harmonic oscillator
allow for the following characterization of quantum market games. O ne can
introduce an analogue of the Planck constant, hE, that describes the minimal
inclination of the player to risk, [ Pk,Qk] =i
2πhE. As the lowest eigenvalue
of the positive deﬁnite operator His1
2hE
2πω,hEis equal to the product of
the lowest eigenvalue of H(Pk,Qk) and 2θ. 2θis in fact the minimal interval
1We use the standard Dirac notation. The symbol | /an}bracketri}htwith a letter ψin it denoting
a vector parameterized by ψis called a ket; the symbol /an}bracketle{t |with a letter in it is called a
bra. Actually a brais a dual vector to the corresponding ket. Therefore scalar products
of vectors take the form /an}bracketle{tφ|ψ/an}bracketri}ht(bracket) and the expectation value of an operator Ain
the state |ψ/an}bracketri}htis given by /an}bracketle{tψ|Aψ/an}bracketri}ht. A common abuse of this convention consist in denoting
the wave function ψ(p) as/an}bracketle{tp|ψ/an}bracketri}ht. (A wave functions is a vector in Hilbert space of square
integrable functions and one associates with the variable p an eigenv ector|p/an}bracketri}ht.)
2The reader that is familiar with the rudiments of quantum mechanics w ould certainly
notice that this operator is nothing else then the hamiltonian for qua ntum harmonic
oscillator.
5",2004-06-18T12:50:24Z,marwitz cac limortocho nobel prize  rmit iaand pk correspondi tse  pk pk pk t analogi pk as his is pk  dfrac t aually trefore bert t
paper_qf_1.pdf,6,Quantum Game Theory in Finance,"  This is a short review of the background and recent development in quantum
game theory and its possible application in economics and finance. The
intersection of science and society is also discussed. The review is addressed
to non--specialists.
","during which it makes sense to measure the proﬁt. In a general cas e the
operators Qkdo not commute because traders observe moves of other players
and often act accordingly. One big bid can inﬂuence the market at lea st in
a limited time spread. Therefore it is natural to apply the formalism of
noncommutative quantum mechanics where one considers
[xi,xk] =iΘik:=iΘǫik. (3)
The analysis of harmonic oscillator in more than one dimension [27] imply
that the parameter Θ modiﬁes the constant ℏE→/radicalbig
ℏ2
E+ Θ2and the eigen-
values ofH(Pk,Qk) accordingly. This has the natural interpretation that
moves performed by other players can diminish or increase one’s inclin ation
to take risk. Encouraged by that we asked the question Provided that an all–
purpose quantum computer is built, how would a market cleare d by a quantum
computer perform? To ﬁnd out we have to consider quantum games with un-
limited and changing number of players. A possible approach is as follow s.
If a game allows a great number of players in it is useful to consider it a s
a two–players game: the k-th trader against the Rest of the World (RW).
Any concrete algorithm Ashould allow for an eﬀective strategy of RW (for
a suﬃciently large number of players the single player strategy shou ld not
inﬂuence the form of the RW strategy). Let the real variable q
q:= lnc−E(lnc)
denotes the logarithm of the price at which the k-th player can buy the asset
Gshifted so that its expectation value in the state |ψ >kvanishes. The
expectation value of xis denoted by E(x). The variable p
p:=E(lnc)−lnc
describes the situation of a player who is supplying the asset Gaccording
to his strategy |ψ/an}bracketri}htk. Supplying Gcan be regarded as demanding $ at the
pricec−1in the 1Gunits and both deﬁnitions are equivalent. Note that we
have deﬁned qandpso that they do not depend on possible choices of the
units for Gand $. For simplicity we will use such units that E(lnc) = 0.
The strategies |ψ/an}bracketri}htkbelong to Hilbert spaces Hk. The state of the game
|Ψ/an}bracketri}htin:=/summationtext
k|ψ/an}bracketri}htkis a vector in the direct sum of Hilbert spaces of all players,
⊕kHk. We will deﬁne canonically conjugate hermitian operators of demand
Qkand supply Pkfor each Hilbert space Hkanalogously to their physical
6",2004-06-18T12:50:24Z,ido one trefore t pk  encouraged provid to  rest world any should  shted t t accordi supplyi caguits note and for t gbert hk t gbert hk  and pk for gbert hk analogous
paper_qf_1.pdf,7,Quantum Game Theory in Finance,"  This is a short review of the background and recent development in quantum
game theory and its possible application in economics and finance. The
intersection of science and society is also discussed. The review is addressed
to non--specialists.
","position and momentum counterparts. This can be justiﬁed in the fo llowing
way. Let exp( −p) be a deﬁnite price, where pis a proper value of the operator
Pk. Therefore, if one have already declared the will of selling exactly at the
price exp( −p) (the strategy given by the proper state |p/an}bracketri}htk) then it is pointless
to put forward any opposite oﬀer for the same transaction. The c apital ﬂows
resulting from an ensemble of simultaneous transactions correspo nd to the
physical process of measurement. A transaction consists in a tra nsition from
the state of traders strategies |Ψ/an}bracketri}htinto the one describing the capital ﬂow state
|Ψ/an}bracketri}htout:=Tσ|Ψ/an}bracketri}htin, whereTσ:=/summationtext
kd|q/an}bracketri}htkdkd/an}bracketle{tq|+/summationtext
ks|p/an}bracketri}htksks/an}bracketle{tp|is the projective
operator deﬁned by the division σof the set of traders {k}into two separate
subsets{k}={kd} ∪ {ks}, the ones buying at the price eqkdand the ones
selling at the price e−pksin the round of the transaction in question. The
role of the algorithm Ais to determine the division σof the market, the set
of price parameters {qkd,pks}and the values of capital ﬂows. The later are
settled by the distribution
/integraldisplaylnc
−∞|/an}bracketle{tq|ψ/an}bracketri}htk|2
k/an}bracketle{tψ|ψ/an}bracketri}htkdq
which is interpreted as the probability that the trader |ψ/an}bracketri}htkis willing to buy
the asset Gat the transaction price cor lower [25]. In an analogous way the
distribution/integraldisplayln1
c
−∞|/an}bracketle{tp|ψ/an}bracketri}htk|2
k/an}bracketle{tψ|ψ/an}bracketri}htkdp
gives the probability of selling Gby the trader |ψ/an}bracketri}htkat the price cor greater.
These probabilities are in fact conditional because they describe th e situation
after the division σis completed. If one considers the RW strategy it make
sense to declare its simultaneous demand and supply states becaus e for one
player RW is a buyer and for another it is a seller. To describe such situ ation
it is convenient to use the Wigner formalism3[28]. The pseudo–probability
W(p,q)dpdq on the phase space {(p,q)}known as the Wigner function is
3Actually, this approach consists in allowing pseudo–probabilities into c onsideration.
From the physical point of view this is questionable but for our aims its useful, c.f. the
discussion of the Giﬀen paradox [21].
7",2004-06-18T12:50:24Z,  pk trefore t t is t gat iby tse  to winner t winner aually from gi
paper_qf_1.pdf,8,Quantum Game Theory in Finance,"  This is a short review of the background and recent development in quantum
game theory and its possible application in economics and finance. The
intersection of science and society is also discussed. The review is addressed
to non--specialists.
","given by
W(p,q) :=h−1
E/integraldisplay∞
−∞eiℏ−1
Epx/an}bracketle{tq+x
2|ψ/an}bracketri}ht/an}bracketle{tψ|q−x
2/an}bracketri}ht
/an}bracketle{tψ|ψ/an}bracketri}htdx
=h−2
E/integraldisplay∞
−∞eiℏ−1
Eqx/an}bracketle{tp+x
2|ψ/an}bracketri}ht/an}bracketle{tψ|p−x
2/an}bracketri}ht
/an}bracketle{tψ|ψ/an}bracketri}htdx,
where the positive constant hE=2πℏEis the dimensionless economical coun-
terpart of the Planck constant. Recall that this measure is not po sitive
deﬁnite except for the cases presented below. In the general ca se the pseudo–
probability density of RW is a countable linear combination of Wigner fun c-
tions,ρ(p,q) =/summationtext
nwnWn(p,q),wn≥0,/summationtext
nwn= 1. The diagrams of the
integrals of the RW pseudo–probabilities (see Ref. [25])
Fd(lnc) :=/integraldisplaylnc
−∞ρ(p=const.,q )dq
(RW bids selling at exp ( −p))
and
Fs(lnc) :=/integraldisplayln1
c
−∞ρ(p,q=const. )dp
(RW bids buying at exp( q)) against the argument ln cmay be interpreted as
the dominant supply and demand curves in the Cournot convention, respec-
tively [25]. Note, that due to the lack of positive deﬁniteness of ρ,Fdand
Fsmay not be monotonic functions. Textbooks on economics give exam ples
of such departures from the law of supply (work supply) and law of d emand
(Giﬀen assets) [29]. We proposed to call an arbitrage algorithm resu lting in
non positive deﬁnite probability densities a giﬀen . The following subsection
describe shortly various aspects of quantum markets.
2.1 Quantum Zeno eﬀect
It has been experimentally veriﬁed that suﬃciently frequent measu rement
can slow down (accelerate) the dynamics of a quantum proces, wha t is called
thequantum (anti–)Zeno eﬀect [30]. Analogous phenomenon can be observed
in quantum games. If the market continuously measures the same s trategy
of the player, say the demand /an}bracketle{tq|ψ/an}bracketri}ht, and the process is repeated suﬃciently
often for the whole market, then the prices given by the algorithm Ado
8",2004-06-18T12:50:24Z,px qx is  recall iwinner t ref fd fs cour not note fd and fs may textbooks gi  t quantum zero it zero analogous  do
paper_qf_1.pdf,9,Quantum Game Theory in Finance,"  This is a short review of the background and recent development in quantum
game theory and its possible application in economics and finance. The
intersection of science and society is also discussed. The review is addressed
to non--specialists.
","not result from the supplying strategy /an}bracketle{tp|ψ/an}bracketri}htof the player. The necessary
condition for determining the proﬁt of the game is the transition of t he
player to the state /an}bracketle{tp|ψ/an}bracketri}ht[25]. If, simultaneously, many of the players change
their strategies then the quotation process may collapse due to th e lack of
opposite moves. In this way the quantum Zeno eﬀects explain stock exchange
crashes. Eﬀects of this crashes should be predictable because th e amplitudes
of the strategies /an}bracketle{tp|ψ/an}bracketri}htare Fourier transforms of /an}bracketle{tq|ψ/an}bracketri}ht. Another example
of the quantum market Zeno eﬀect is the stabilization of prices of an asset
provided by a monopolist.
2.2 Eigenstates of QandP
Let us suppose that the amplitudes for the strategies /an}bracketle{tq|ψ/an}bracketri}htkor/an}bracketle{tp|ψ/an}bracketri}htkhave
divergent integrals of their modulus squared. Such states live outs ide the
Hilbert space but have the natural interpretation as the despera te will of
thek-th player of buying (selling) of the amount dk(sk) of the asset G.
So the strategy /an}bracketle{tq|ψ/an}bracketri}htk=/an}bracketle{tq|a/an}bracketri}ht=δ(q,a) means, in the case of classifying
the player into the set {kd}, refusal of buying cheaper than at c=eaand
the will of buying at any price equal or higher than ea. In the case of a
”measurement” in the set {kd}the player declares the will of selling at any
price. The above interpretation is consistent with the Heisenberg u ncertainty
relation. The strategies /an}bracketle{tq|ψ/an}bracketri}ht2=/an}bracketle{tq|a/an}bracketri}ht(or/an}bracketle{tp|ψ/an}bracketri}ht2=/an}bracketle{tp|a/an}bracketri}ht) do not correspond
to the RW behaviour because the conditions d2,s2>0, if always satisﬁed,
allow for unlimited proﬁts (the readiness to buy or sell Gat any price). The
appropriate demand and supply functions give probabilities of coming oﬀ
transactions in a game when the player use the strategy /an}bracketle{tp|const/an}bracketri}htor/an}bracketle{tq|const/an}bracketri}ht
and RW, proposing the price, use the strategy ρ[6, 25]. The authors have
analyzed the eﬃciency of the strategy /an}bracketle{tq|ψ/an}bracketri}ht1=/an}bracketle{tq|−a/an}bracketri}htin a two–player game
when RW use the strategy with squared modulus of the amplitude equ al to
normal distribution [25]. The maximal intensity of the proﬁt [25] is equ al
to 0.27603 times the variance of the RW distribution function. Of cou rse,
the strategy /an}bracketle{tp|ψ/an}bracketri}ht1=/an}bracketle{tp|0,27603/an}bracketri}hthas the same properties. In such games
a=0.27603 is a global ﬁxed point of the proﬁt intensity function. This may
explain the universality of markets on which a single client facing the bid
makes up his/hers mind. Does it mean that such common phenomena h ave
quantal nature? The Gaussian strategy of RW [31] can be paramet erized
by a temperature–like parameter T=β−1. Any decrease in proﬁts is only
possible by reducing the variance of RW (i.e. cooling). Market compet ition is
9",2004-06-18T12:50:24Z,t  izero courier anotr zero ei gestatand  sugbert so it berg t gat t t t of i dot any market
paper_qf_1.pdf,10,Quantum Game Theory in Finance,"  This is a short review of the background and recent development in quantum
game theory and its possible application in economics and finance. The
intersection of science and society is also discussed. The review is addressed
to non--specialists.
","the mechanism responsible for the risk ﬂow that allows the market to attain
the ”thermodynamical” balance. A warmer market inﬂuences destr uctively
the cooler traders and they diminish the uncertainty of market pric es.
2.3 Correlated coherent strategies
We will deﬁne correlated coherent strategies as the eigenvectors of the anni-
hilation operator Ck[32]
Ck(r,η) :=1
2η/parenleftBig
1 +ir√
1−r2/parenrightBig
Qk+iηPk,
whereris the correlation coeﬃcient r∈[−1,1],η >0. In these strategies
buying and selling transactions are correlated and the product of d ispersions
fulﬁlls the Heisenberg–like uncertainty relation ∆ p∆q√
1−r2≥ℏE
2and is
minimal. The annihilation operators Ckand their eigenvectors may be pa-
rameterized by ∆ p=ℏE
2η, ∆q=η√
1−r2, andr. This leads to following form of
the correlated Wigner coherent strategy
W(p,q)dpdq =1
2π∆p∆q√
1−r2e−1
2(1−r2)/parenleftbig
(p−p0)2
∆2p+2r(p−p0)(q−q0)
∆p∆q+(q−q0)2
∆2q/parenrightbig
dpdq.
They are not giﬀens. It can be shown, following Hudson [33], that the y
form the set of all pure strategies with positive deﬁnite Wigner func tions.
Therefore pure strategies that are not giﬀens are represented in phase space
{(p,q)}by gaussian distributions.
2.4 Mixed states and thermal strategies
According to classics of game theory [34] the biggest choice of stra tegies is
provided by the mixed states ρ(p,q). Among them the most interesting are
the thermal ones. They are characterized by constant inclination to risk,
E(H(P,Q)) =const and maximal entropy. The Wigner measure for the
n-th exited state of harmonic oscillator has the form [35]
Wn(p,q)dpdq =(−1)n
πℏEe−2H(p,q)
ℏEωLn/parenleftbig4H(p,q)
ℏEω/parenrightbig
dpdq,
whereLnis then-th Laguerre polynomial. The mixed state ρβdetermined by
the Wigner measures Wndpdq weighted by the Gibbs distribution wn(β) :=
10",2004-06-18T12:50:24Z,correlated  ck ck b b pk iberg t ck and  winner ty it hudsowinner trefore mixed accordi amo ty t winner ee llis la guerre t winner gibbs
paper_qf_1.pdf,11,Quantum Game Theory in Finance,"  This is a short review of the background and recent development in quantum
game theory and its possible application in economics and finance. The
intersection of science and society is also discussed. The review is addressed
to non--specialists.
","e−βnℏEω/summationtext∞
k=0e−βkℏEωhas the form
ρβ(p,q)dpdq : =∞/summationdisplay
n=0wn(β)Wn(p,q)dpdq
=ω
2πxe−xH(p,q)/vextendsingle/vextendsingle/vextendsingle
x2
ℏEωtanh(βℏEω
2)dpdq.
So it is a two dimensional normal distribution. It easy to observe tha t by
recalling that1
1−text
t−1=/summationtext∞
n=0Ln(x)tnis the generating function for the
Laguerre polynomials. It seems to us that the above distributions s hould de-
termine the shape of the supply and demand curves for equilibrium ma rkets.
There are no giﬀens on such markets. It would be interesting to inve stigate
the temperatures of equilibrium markets. In contrast to the trad ers temper-
atures [31] which are Legendre coeﬃcients and measure ”trader’s qualities”
market temperatures are related to risk and are positive. The Fey nman path
integrals may be applied to the Hamiltonian to obtain equilibrium quantum
Bachelier model of diﬀusion of the logarithm of prices of shares that can be
completed by the Black–Scholes formula for pricing European option s [36].
2.5 Quantum auctions and bargaining
After tasting the exotic ﬂavour of quantum market games one may wish to
distinguish the class of quantum transactions (q-transactions) t hat is q-games
without institutionalized clearinghouses. This class includes quantum bar-
gaining (q-bargaining) and quantum auctions (q-auction). The par ticipants
of a q-bargaining game will be called Alice ( A) and Bob ( B). We will suppose
that they settle on beforehand who is the buyer (Alice) and who is th e seller
(Bob). A two–way q-bargaining that is a q-bargaining when the last c ondi-
tion is not fulﬁlled can be treated analogously. Alice enter into negotia tions
with Bob to settle the price for the transaction. Therefore the pr oper mea-
suring apparatus consists of the pair of traders in question. In q- auction the
measuring apparatus consists of a one side only, the initiator of the auction.
We showed [6] that the players strategies can be described in terms of polar-
izations, that is the states in a two–dimensional Hilbert space. If th e player
formulates the conditions of the transaction we say she has the po larization
(and is in the state |− →r/an}bracketri}htA=|/an}bracketri}ht). In q-bargaining this means that she puts
forward the price. In the opposite case, when she decides if the tr ansaction
is made or not, we say she has the polarization |/an}bracketri}ht. (She accepts or not the
11",2004-06-18T12:50:24Z,so it lla guerre it tre it ilegend re t  hamtoniacac li schools aquantum after  t alice bob  alice bob alice bob trefore i gbert  iis
paper_qf_1.pdf,12,Quantum Game Theory in Finance,"  This is a short review of the background and recent development in quantum
game theory and its possible application in economics and finance. The
intersection of science and society is also discussed. The review is addressed
to non--specialists.
","conditions of the proposed transaction.) There is an analogy of the isospin
symmetry in nuclear physics which says that nucleon has two polariza tion
states: proton and neutron. The vectors ( |/an}bracketri}ht,|/an}bracketri}ht) form an orthonormal basis
inHs, the linear hull of all possible Alice polarization states. The player 1
proposes a price and the player -1 accepts or reject the proposa l. Therefore
their polarizations are |/an}bracketri}htand|/an}bracketri}ht, respectively so the q-bargaining has the
polarization |/an}bracketri}ht-|/an}bracketri}ht[6, 19]. The transaction in question is accomplished if
the obvious rationality condition is fulﬁlled
[q+p≤0],
where the convenient Iverson notation [37] is used ([ expression ] denotes the
logical value (1 or 0) of the sentence expression ) and the parameters p= lnc-1
and−q= lnc1are random variables corresponding to prices at which the
respective players withdraw, the withdrawal prices . The variables pandq
describe (additive) proﬁts resulting from price variations. Their pr obability
densities are equal to squared absolute values of the appropriate wave func-
tions/an}bracketle{tp|ψ/an}bracketri}ht-1and/an}bracketle{tq|ψ/an}bracketri}ht1(that is their strategies). Note that the discussed
q-bargaining may result from a situation where several players hav e intention
of buying but they were outbid by the player 1 (his withdrawal price c1was
greater than the other players ones, c1>ck,k= 2,...,N ). This means that
all part in the auction behave like fermions (e.g. electrons) and they are sub-
jected to a sort of Pauli exclusion principle according to which two pla yers
cannot occupy the same state. This surprising statement consist in noticing
that the transaction in question is made only if the traders have opp osite
polarizations (and even that is not a guarantee of the accomplishme nt). The
fermionic character of q-bargaining parts was ﬁrst noted in [6] in a slightly
diﬀerent context. If at the outset of the auction there are seve ral bidding
players then the rationality condition takes the form
[qmin+p≤0]
whereqmin:= min
k=1,...,N{qk}is the logarithm of the highest bid multiplied by
−1. According to Ref. [6] the probability density of making the transa ction
with thek-th buyer at the price ck= e−qkis given by
dqk|/an}bracketle{tqk|ψk/an}bracketri}ht|2
/an}bracketle{tψk|ψk/an}bracketri}htN/productdisplay
m=1
m/negationslash=k/integraldisplay∞
−∞dqm|/an}bracketle{tqm|ψm/an}bracketri}ht|2
/an}bracketle{tψm|ψm/an}bracketri}ht/integraldisplay∞
−∞dp|/an}bracketle{tp|ψ-1/an}bracketri}ht|2
/an}bracketle{tψ-1|ψ-1/an}bracketri}ht[qk= min
n=1,...,N{qn}] [qk+p≤0].
(1)
12",2004-06-18T12:50:24Z,tre t hs alice t trefore t overs ot tir note  paul  t  accordi ref
paper_qf_1.pdf,13,Quantum Game Theory in Finance,"  This is a short review of the background and recent development in quantum
game theory and its possible application in economics and finance. The
intersection of science and society is also discussed. The review is addressed
to non--specialists.
","The seller is not interested in making the deal with any particular buye r
and the unconditional probability of accomplishing the transaction a t the
pricecis given by the sum over k=1,...,N of the above formula with qk=
−lnc. If we neglect the problem of determining the probability amplitudes
in (1) we easily note that the discussed q-bargaining is in fact an Englis h
auction (ﬁrst price auction), so popular on markets of rare goods . It is
interesting to note that the formula (1) contains wave functions o f payers
who were outbid before the end of the bargaining (cf the Pauli exclu sion
principle). The probability density of ”measuring” of a concrete valu eqof
the random variable qcharacterizing the player, according to the probabilistic
interpretation of quantum theory, is equal to the squared absolu te value of
the normalized wave function describing his strategy
|/an}bracketle{tq|ψk/an}bracketri}ht|2
/an}bracketle{tψk|ψk/an}bracketri}htdq.
Physicists normalize wave functions because conservation laws req uire that.
Therefore the trivial statement that if a market player may be per suaded into
striking a deal or not is a matter of price alone, corresponds to the physical
fact that a particle cannot vanish without any trace. The analysis o f an
English q-auction with reversed roles that is with selling bidders is analo gous.
The case when the polarization of the q-auction is changed to |/an}bracketri}ht-|/an}bracketri}htis more
interesting. In this case the player -1 reveals her withdrawal price and the
player 2 accepts it (as the rest of the players do) or not. Such an a uction is
known as the Vickrey’s auction (or the second price auction). The w inner
is obliged to pay the second in decreasing order price from all the bids (and
the withdrawal price of the player -1). In the quantum approach E nglish and
Vickrey’s auctions are only special cases of a phenomenon called q-a uction. In
the general case both squared amplitudes |/an}bracketle{t-|-/an}bracketri}ht|2and|/an}bracketle{t-|-/an}bracketri}ht|2
are non–vanishing so we have to consider them with weights corresp onding
to these probabilities. Such a general q-auction does not have cou nterparts
on the real markets. It should be very interesting to analyse the m otivation
properties of q-auctions eg ﬁnding out when the best strategy is t he one
corresponding to the player’s valuation of the good. If we consider only
positive deﬁnite probability measures then the bidder gets the highe st proﬁts
in Vickrey’s auction using strategies with public admission of his valuatio n
of the auctioned good. But it might not be so for giﬀen strategies be cause
positiveness of measures is supposed in proving the incentive chara cter of
Vickrey’s auctions [38]. The presence of giﬀens on real markets migh t not be
13",2004-06-18T12:50:24Z,t  el is it paul t psicists trefore t eh t isupick rey t ipick rey isuit  pick rey but pick rey t
paper_qf_1.pdf,14,Quantum Game Theory in Finance,"  This is a short review of the background and recent development in quantum
game theory and its possible application in economics and finance. The
intersection of science and society is also discussed. The review is addressed
to non--specialists.
","so abstract as it seems to be. Captain Robert Giﬀen who supposedly found
additive measures not being positive deﬁnite but present on market s in the
forties of the XIX century [39] probably got ahead of physicists in o bserving
quantum phenomena. Such departures from the demand law, if cor rectly
interpreted, does not cause any problem neither for adepts nor f or beginners.
Employers have probably always thought that work supply as funct ion of
payment is scarcely monotonous. The distinguished by their polariza tion
ﬁrst and second price auctions have analogues in the Knaster solut ion to the
pragmatic fair division problem (with compensatory payments for ind ivisible
parts of the property) [40]. Such a duality might also be found in elect ion
systems that as auctions often take the form of procedures of s olving fair
division problems [41]. It might happen that social frustrations caus ed by
election systems would encourage us to discuss such topics.
3 Conclusions
The commonly accepted universality of quantum theory should enco urage
physicist in looking for traces of quantum world in social phenomena. We
envisage markets cleared by quantum computer. We hope that the sketchy
analysis presented above would allow the reader to taste the exotic ﬂavours
of quantum markets. A quantum theory of markets provides new t ools that
can be used to explain of the very involved phenomena including interf erence
of (quantum) strategies [42] and diﬀusion of prices [43]. The resea rch into the
quantum nature of games may oﬀer solutions to very intriguing para doxes
present in philosophy and economics. For example, the Newcomb’s pa radox
analyzed in Ref. [7] suggests various ones. There are quantum gam es that
live across the border of our present knowledge. For example, con sider some
classical or quantum problem X. Let us deﬁne the game kXcl : you win if
and only if you solve the problem (perform the task) Xgiven access to only k
bits of information. The quantum counterpart reads: solve the pr oblemXon
a quantum computer or other quantum device given access to only kbits of
information. Let us call the game kXcl orkXq interesting if the correspond-
ing limited information–tasks are feasible. Let OckhamXcl (OckhamXq )
denotes the minimal kinteresting game in the class kXcl (kXq). Authors of
the paper [44] described the game played by a market trader who ga ins the
proﬁtPfor each bit (qubit) of information about her strategy. If we deno te
this game by MP thenOckhamM1
2cl= 2M1
2cland forP >1
2the game
14",2004-06-18T12:50:24Z,capt articial intellence robert gi suemployers t kaster suit conusns t   t for newcomer ref tre for   givet o   ockham  ockham  authors for  ockham
paper_qf_1.pdf,15,Quantum Game Theory in Finance,"  This is a short review of the background and recent development in quantum
game theory and its possible application in economics and finance. The
intersection of science and society is also discussed. The review is addressed
to non--specialists.
","OckhamMPcl does not exist. They also considered the more eﬀective game
1M2+√
2
4qfor whichOckhamM2+√
2
4q/ne}ationslash= 1M2+√
2
4qif the trader can operate
on more then one market. This happens because there are entang led strate-
gies that are more proﬁtable [45]. There are a lot of intriguing questio ns that
can be ask, for example for which Xthe meta–game Ockham (OckhamXq )cl
can be solved or when, if at all, the meta–problem Ockham (OckhamXq )q
is well deﬁned problem. Such problems arise in quantum memory analys is
[46]. We would like to stress that this ﬁeld of research undergoes an e ventful
development. Therefore now it is diﬃcult to predict which results wou ld turn
out to be fruitful and which would have only marginal eﬀect.
Recent research on the (quantum) physical aspects of informat ion pro-
cessing should result in a sort of total quantum paradigm and we dare to
say that quantum game approach became sooner or later a dominan t one.
Therefore we envisage markets cleared by quantum algorithms (co mputers)4.
Acknowledgements : This paper has been supported by the Polish Min-
istry of Scientiﬁc Research and Information Technology under the
(solicited) grant No. PBZ-MIN-008/P03/2003 .
References
[1] Zellinger, A., The quantum centennial, Nature408(2000) 639.
[2] Wiesner, S., Conjugate coding, SIGACT News 15/1 (1983) 78.
[3] Baaquie, B. E., Quantum ﬁeld theory of treasury bonds, Physical Review
E64(2001) 016121.
[4] Schaden, M., Quantum ﬁnance, Physica A 316(2002) 511.
[5] Waite, S., Quantum investing , Texere Publishing, London (2002).
4Let us quote the Editor’s Note to Complexity Digest 2001.27(4)
(http://www.comdig.org): ”It might be that while observing the due c eremonial of
everyday market transaction we are in fact observing capital ﬂow s resulting from quan-
tum games eluding classical description. If human decisions can be tr aced to microscopic
quantum events one would expect that nature would have taken ad vantage of quantum
computation in evolving complex brains. In that sense one could indee d say that quantum
computers are playing their market games according to quantum ru les”.
15",2004-06-18T12:50:24Z,ockham pl ty ockham  tre t ockham ockham ockham ockham su trefore recent trefore ackledgement  poh micie nti researinformattechnology no referencel lier t nature wie ner conjugate news baa qui quantum psical review sarequantum psics articial intellence te quantum tex ere pubhi londo editor note lexity dest it  in
paper_qf_1.pdf,16,Quantum Game Theory in Finance,"  This is a short review of the background and recent development in quantum
game theory and its possible application in economics and finance. The
intersection of science and society is also discussed. The review is addressed
to non--specialists.
","[6] Piotrowski, E. W., S/suppress ladkowski, J., Quantum Market Games, Physica A
312(2002) 208.
[7] Piotrowski, E. W., S/suppress ladkowski, J., Quantum solution to the Newcom b’s
paradox, International Journal of Quantum Information 1(2003) 395.
[8] Piotrowski, E. W., S/suppress ladkowski, J., The next stage: quantum game
theory, in Benton, C. V., (ed.), Mathematical Physics Research at
the Cutting Edge , Nova Science Publishers, Inc., New York (2004);
quant-ph/0308027.
[9] Penrose, R., Shadows of the Mind , Cambridge University Press, Cam-
bridge (1994).
[10] Wallace, D., Quantum theory of probabilty and decision theory, R evis-
ited; quant-ph/0211104.
[11] Nielsen, M. A., Chuang, I. L., Quantum Computation and Quantum
Information , Cambridge University Press, Cambridge (2000).
[12] Holevo, A., Statistical Structure of Quantum Theory , Springer Verlag,
Berlin (2001).
[13] Milnor, J., Games against nature, in Thrall, R. M., Coombs, C.
H.,,Davis, R. L., (eds.), Decision Processes , John Wiley & Sons, New
York (1954).
[14] Piotrowski, E. W., S/suppress ladkowski, J., An invitation to quantum game t he-
ory,International Journal of Theoretical Physics 42(2003) 1089.
[15] Meyer, D. A., Quantum strategies, Physical Review Letters 82(1999)
1052.
[16] Eisert, J., Wilkens, M., Lewenstein, M., Quantum Games and Quant um
Strategies, Physical Review Letters 83(1999) 3077.
[17] Iqbal A., Toor A.H., Backwards–induction outcome in a quantum g ame,
Physical Review A 65(2002) 052328.
[18] Iqbal A., Toor A. H., Quantum mechanics gives stability to a Nash
equilibrium, Physical Review A 65(2002) 022306.
16",2004-06-18T12:50:24Z,ptrow quantum market gampsics ptrow quantum new com internatnal journal quantum informatptrow t  matmatical psics researcui edge nova science pubrs inc new york prose shadows mind cambridge   cam wallace quantum nielse quantum tquantum informatcambridge   cambridge hole vo statistical struure quantum tory  verlag berlim nor gamthrl combs dis cisprocess rey sons new york ptrow ainternatnal journal toical psics meyer quantum psical review ters ei sert wl ens le nsteiquantum gamquant strategipsical review ters equal to or backwards psical review equal to or quantum nash psical review
paper_qf_1.pdf,17,Quantum Game Theory in Finance,"  This is a short review of the background and recent development in quantum
game theory and its possible application in economics and finance. The
intersection of science and society is also discussed. The review is addressed
to non--specialists.
","[19] Piotrowski, E. W., S/suppress ladkowski, J., Quantum English auctions, Physica
A318(2003) 505.
[20] Piotrowski, E. W., S/suppress ladkowski, J., Quantum bargaining games, Physica
A308(2002) 391.
[21] S/suppress ladkowski, J., Giﬀen paradoxes in quantum market games, Physica A
324(2003) 234.
[22] Velupillai, K., Computable Economics, the Arne Ryde Memorial Lec-
tures , Oxford University Press, Oxford (2000).
[23] Piotrowski, E. W., S/suppress ladkowski J., Arbitrage risk induced by tran saction
costs,Physica A 331(2004) 233.
[24] Stanley, H. E., Amaral, L. A. N., Gabaix, X., Gopikrishnan, P., Plero u,
V., Quantifying economic ﬂuctuations, Physica A 302 (2001) 126.
[25] Piotrowski, E. W., S/suppress ladkowski, J., The merchandising mathematic ian
model,Physica A 318(2003) 496.
[26] Piotrowski, E. W., S/suppress ladkowski, J., Quantum–like approach to ﬁna ncial
risk: quantum anthropic principle, Acta Phys. Pol. B32 (2001) 3873.
[27] Hatzinikitas, A., Smyrnakis, I., The noncommutative harmonic os cilla-
tor in more than one dimensions, J. Math. Phys. 43(2002) 113.
[28] Wigner, E., On the quantum correction for thermodynamic equilib rium,
Physical Review 40(1932) 749.
[29] Samuelson, P. A., Nordhaus, W. D., Microeconomics , McGraw–Hill,
New York (1989).
[30] Fischer, M. C., Gutierrez-Medina, B., Raizen, M. G., Observation of
the quantum Zeno and anti-Zeno eﬀects in an unstable system, Physical
Review Letters 87(2001) 040402.
[31] Piotrowski, E. W., S/suppress ladkowski, J., The thermodynamics of portf olios,
Acta Physica Polonica B32 (2001) 597.
[32] Dodonov, V. V., Kurmyshev, E. V., Man’ko, V. I., Physics Letters 79A
(1980) 150.
17",2004-06-18T12:50:24Z,ptrow quantum eh psics ptrow quantum psics gi psics velupl articial intellence arable economics are ri memorial lec oxford   oxford ptrow arb it rage psics stanley moral ga articial intellence go pi krishna pl ero qualyi psics ptrow t psics ptrow quantum a ph ys phat zi ia syria is t math ph ys winner opsical review ord has maoeconomic mc raw hl new york scr gutierrez medina articial intellence zeobservatzero zero psical review ters ptrow t a psics polo nica todo nov kur my s mapsics ters
paper_qf_1.pdf,18,Quantum Game Theory in Finance,"  This is a short review of the background and recent development in quantum
game theory and its possible application in economics and finance. The
intersection of science and society is also discussed. The review is addressed
to non--specialists.
","[33] Hudson, R. L., When is the Wigner quasi–probability density non–
negative?, Reports on Mathematical Physics 6(1974) 249.
[34] von Neumann, J. and Morgenstern, O., Theory of Games and Economic
Behavior , Princeton University Press, Princeton (1953).
[35] Tatarskii, V. I., The Wigner representation of quantum mechan ics,Sov.
Phys. Usb. 26(1983) 311.
[36] Hull, J., Options, Futures, and other Derivative Securities , Prentice
Hall, Englewoods Cliﬀs (1993).
[37] Graham, R. L., Knuth, D. E., Patashnik, O., Concrete Mathemat ics,
Addison–Wesley, Reading (1994).
[38] Klemperer, P., Auction theory: A guide to the literature, J. Economic
Surveys13(1999) 227.
[39] Stigler, G. J., Notes on the history of the Giﬀen paradox, J. Polit. Econ-
omy55(1947) 152.
[40] Luce, R. D., Raiﬀa, H., Games and Decisions , J. Wiley and Sons, New
York (1958).
[41] Brams, S. J., Taylor, A. D., Fair Division: From Cake–Cutting to Dis-
pute Resolution , Cambridge University Press, Cambridge (1996).
[42] Piotrowski, E. W., S/suppress ladkowski, J., Syska, J., Interference of q uantum
strategies, Physica A 318(2003) 516.
[43] Piotrowski, E. W., S/suppress ladkowski, J., Quantum diﬀusion of
prices and proﬁts, Physica A , in print; cond-mat/0207132.
http://ideas.repec.org/p/sla/eakjkl/12.html
[44] Piotrowski, E. W., S/suppress ladkowski, J., Quantum computer:
an appliance for playing market games; quant-ph/0305017.
http://ideas.repec.org/p/sla/eakjkl/16.html
[45] Piotrowski, E. W., Fixed point theorem for simple quantum strate gies
in quantum market games, Physica A 324(2003) 196.
[46] Koenig, R., Maurer, U., Renner, R., On the power of quantum mem ory;
quant-ph/0305154.
18",2004-06-18T12:50:24Z,hudsowwinner rets matmatical psics newmamorgenstertory gameconomic behr princeto  princetostars ki t winner sov ph ys usb hull optns futurrivative securitipraice hall elewood i graham south pat ash  conete ma tm at additsley readi klemer aueconomic surveys ter notgi polite ec oluck articial intellence gamcisns rey sons new york  taylor articial intellence divisfrom cake cui is resolutcambridge   cambridge ptrow sy ska interference psics ptrow quantum psics ptrow quantum ptrow xed psics  maer re nner on
paper_qf_2.pdf,1,"The Inverted Parabola World of Classical Quantitative Finance:
  Non-Equilibrium and Non-Perturbative Finance Perspective","  Classical quantitative finance models such as the Geometric Brownian Motion
or its later extensions such as local or stochastic volatility models do not
make sense when seen from a physics-based perspective, as they are all
equivalent to a negative mass oscillator with a noise. This paper presents an
alternative formulation based on insights from physics.
","The Inverted Parabola World of Classical Quantitative Finance:
Non-Equilibrium and Non-Perturbative Finance Perspective
Igor Halperin
NYU Tandon School of Engineering
e-mail:ighalp @gmail:com
August 11, 2020
Abstract:
Classical quantitative nance models such as the Geometric Brownian Motion or its later ex-
tensions such as local or stochastic volatility models do not make sense when seen from a
physics-based perspective, as they are all equivalent to a negative mass oscillator with a noise.
This paper presents an alternative formulation based on insights from physics.
I would like to thank Peter Carr for critical remarks. All possible errors are my own.
1arXiv:2008.03623v1  [q-fin.GN]  9 Aug 2020",2020-08-09T01:04:42Z,t inverted noia world assical quantitative nance noequibrium noature at ive nance speive or hal  iand oschoeineeri august abstra assical geometric brownish mot peter carr all  aug
paper_qf_2.pdf,2,"The Inverted Parabola World of Classical Quantitative Finance:
  Non-Equilibrium and Non-Perturbative Finance Perspective","  Classical quantitative finance models such as the Geometric Brownian Motion
or its later extensions such as local or stochastic volatility models do not
make sense when seen from a physics-based perspective, as they are all
equivalent to a negative mass oscillator with a noise. This paper presents an
alternative formulation based on insights from physics.
","1 Introduction
The novel ""The Inverted World"" by Christopher Priest paints a fascinating image of a world
where a city called the City of Earth slowly travels on railway tracks across an alien planet.
The city's engineers keep laying a fresh track for the city, and pick up the old track as it moves.
The city must move on to stay within 10 miles of the Optimum - which is a location where the
gravitational eld is not distorted, and matches the gravitational eld of the planet Earth.
But the world of the novel is not Earth. In this world, the ground is in a constant move
from the north to the south, as a result of some sort of a global gravitational catastrophe that
happened at some point in the past. Even as the Optimum stays in the same position, the
City will drift away from the Optimum if it does not move all the time. If the City of Earth
nds itself too far from the Optimum, gravitational distortions become too strong as a result of
moving grounds and their drift to the south. This is what makes the City crawls to the North
all the time. If it ever stops, it will eventually be pulled to the South, and destroyed at the
end by gravitational distortion forces, along with all its citizens. So it has to move forward
through a devastated land full of hostile tribes. The only alternative to a constant move in such
an inverted world, where the grounds are moving and the Sun looks like a rotating parabola, is
death.
The resolution of the many puzzles and gaps of history of the Inverted World comes only
towards the end of the book. The City of Earth was crawling the planet of Earth, never leaving
it. Moving grounds, a parabolic Sun, and other related puzzles of the Inverted World was caused
by side neurological eects of a UV radiation that was produced by the city's power generator.
The generator was based on an alternative energy method that was developed by a founder of
the city.
This novel, which I rst read many years ago, came repeatedly to my mind when I worked
on a model of asset price dynamic in an open and non-equilibrium market called the Quantum
Equilibrium-Disequilibrium (QED) model [5]. The QED model generalizes the Geometric Brow-
nian Motion (GBM) model by introducing two additional parameters, along with a non-linear
extension of a diusion equation driving the dynamics of the model, see also [4]. Surprisingly,
the QED model suggests that the notions of a market growth and market stability in this model
and in the GBM model are essentially opposite .
What was 'equilibrium' dynamics in the GBM model becomes non-equilibrium dynamics
from the perspective of the QED model. What was the commonly excepted average exponential
growth of asset prices becomes a fallfrom a point of instability towards a point of local stability.
This sounds much like an Inverted World vs a Normal World, the only question is which one is
the Inverted World?
This paper oers a non-technical introduction to the QED model, along with a reasoning
why it corresponds to a 'Normal World', while the GBM model, along with its multiple direct
descendants such as local or stochastic volatility models, describes an 'Inverted World'. As
I will try to argue, assumptions of closed-system dynamics, (quasi-)stationarity and linearity
made in classical nancial models do not adequately capture realities of real world nancial
markets, and in a sense can be viewed as `wrong limits' of a (yet unknown) `right' theory. Then
I show how these deciencies are addressed in the QED model that treats markets as open
and non-linear systems, and does notrely on a linearization of dynamics within a perturbation
theory to treat non-linearities. Instead, the QED model presents a non-perturbative approach to
handle non-linearities. This position paper discusses how insights from modern non-equilibrium
and non-perturbative physics can be fruitfully used for nancial modeling with non-linear and
non-equilibrium models such as the QED to better capture the true market dynamics.
2",2020-08-09T01:04:42Z,introdut t inverted world  priest city earth t t optimum earth but earth ieveoptimum city optimum  city earth optimum  city north  south so t sut inverted world t city earth earth movi suinverted world t  quantum equibrium is equibrium t geometric brow motsurprisy what what  inverted world normal world inverted world  normal world inverted world as tinstead 
paper_qf_2.pdf,3,"The Inverted Parabola World of Classical Quantitative Finance:
  Non-Equilibrium and Non-Perturbative Finance Perspective","  Classical quantitative finance models such as the Geometric Brownian Motion
or its later extensions such as local or stochastic volatility models do not
make sense when seen from a physics-based perspective, as they are all
equivalent to a negative mass oscillator with a noise. This paper presents an
alternative formulation based on insights from physics.
","2 GBM, Langevin equation, and Inverted Parabola
2.1 GBM and the Langevin equation
Since the groundbreaking work of Samuelson in 1965 [11], Geometric Brownian Motion (GBM)
model, also known as the the log-normal asset return model,
dXt=Xtdt+XtdWt (1)
remains the main work-horse of nancial engineering. In Eq.(1), Xtis an asset price at time t,
is the stock drift, is the stock volatility, and Wtis a standard Brownian motion. For what
follows, we can view the GBM model as a special linear case of a more general model called It o's
diusion
dXt=(Xt)dt+(Xt)dWt (2)
with a linear drift function (Xt) =Xtand a multiplicative (i.e. proportional to Xt) diusion
function(Xt) =Xt. Samuelson proposed the GBM model (1) as an improvement over an
Arithmetic Brownian Motion (ABM) model suggested by Bachelier in 1900 [1]. His objective
was to modify the ABM model to ensure non-negativity of stock prices. Note that the ABM
model itself can be viewed as a model with a constant drift and volatility terms.
A few years after Bachelier published his 1900 thesis that gave birth to the ABM model,
Paul Langevin proposed in 1908 an equation that later became known as the Langevin equation.
Langevin's work focused on a simplied analysis of overdamped Brownian particles within the
Einstein-Smoluchovski theory of classical diusion in the presence of an external potential eld
U(X). Such a eld can represent an impact of heavy molecules, a external gravitational or
electromagnetic eld, etc. [8]. The Langevin equation can be written in similar terms to It o's
diusion (2), except that in the Langevin dynamics, a drift term is given by the negative gradient
of the potential U(X). The (overdamped) Langevin equation with a multiplicative noise reads
dXt=",2020-08-09T01:04:42Z,aelic inverted noia aelic since samuelsogeometric brownish motxt td td  ieq xt is  is brownish for it xt xt xt  xt xt and xt xt xt samuelsoarithmetic brownish motcac lihis note cac lipaul aelic aelic aelic brownish  muovsk sut aelic it aelic t aelic xt
paper_qf_2.pdf,4,"The Inverted Parabola World of Classical Quantitative Finance:
  Non-Equilibrium and Non-Perturbative Finance Perspective","  Classical quantitative finance models such as the Geometric Brownian Motion
or its later extensions such as local or stochastic volatility models do not
make sense when seen from a physics-based perspective, as they are all
equivalent to a negative mass oscillator with a noise. This paper presents an
alternative formulation based on insights from physics.
","Figure 1: The classical potential U(x) corresponding to the GBM model with >0. Red dots
correspond to a \\\\particle"" representing the rm, coordinate Xtbeing the rm's stock price.
This is the potential of a harmonic oscillator with a negative mass. Such a system is globally
unstable, and the default state Xt= 0 is unreachable as the force of the negative gradient of
the potential pushes the particle away from the default boundary Xt= 0 for any value Xt>0,
producing an ever-accelerating and unbounded fall in this potential.
2.2 The Inverted Parabola World of the GBM model and its descendants
The inverted parabola potential (4) thus describes the most classical example of an unstable
system in physics - an inverted (negative mass) harmonic oscillator. Of course, the fact that
the GBM model is non-stationary for 6= 0 is evident and well known in the literature. On
the other hand, 'classical' nancial inter-temporal models (e.g. the Black-Scholes model [2,
10] or inter-temporal versions of the CAMP model [12]) often work under assumptions of a
general equilibrium or competitive market equilibrium. In these approaches, one assumes a
dynamic market equilibrium between rational nancial agents having instantaneous access to
either symmetric or asymmetric information. Here the concept of a market equilibrium refers
not to a price process, but rather to an equilibrium of supply and demand given a price level
(recall the Equilibrium in ""Inverted World""). Thus, the stock price is considered a 'reference
frame' to describe the supply-demand balance equations. As the stock price dynamics is non-
stationary, the same holds for the full system given by both nancial agents (traders) andprice
dynamics. This means that the concept of a market equilibrium under a non-stationary price
process can be at best applied only approximately for short times, as a local approximation .
To illustrate this point, imagine you step into an elevator on a top oor of a skyscraper. All
of a sudden, the elevator cable breaks, and now the elevator is in a free fall, with you trapped
inside.2According to elementary physics, as long as the elevator continues to freely fall, you
will be levitating inside of the elevator, being in a state of a local `equilibrium'. This is because
being inside of a freely falling elevator is equivalent to residing in a non-inertial reference frame,
where a ctitious 'anti-gravitational' force exactly cancels the gravitational force. This simple
example illustrates the point that from the point of view of an external observer who observes
both the elevator and you inside of the elevator, the dynamics of the full system most denitely
cannot be described as equilibrium dynamics proceeding indenitely in time.
Far from being a purely theoretical observation, the inverted parabola potential (4) implies a
2Of course, this is largely a hypothetical scenario, see e.g. https://science.howstuworks.com/science-vs-
myth/everyday-myths/question730.htm regarding practical safety measures to prevent it from happening.
4",2020-08-09T01:04:42Z, t red xt bei  suxt xt xt t inverted noia world t of o schools ire equibrium inverted world  as  to all accordi   far of
paper_qf_2.pdf,5,"The Inverted Parabola World of Classical Quantitative Finance:
  Non-Equilibrium and Non-Perturbative Finance Perspective","  Classical quantitative finance models such as the Geometric Brownian Motion
or its later extensions such as local or stochastic volatility models do not
make sense when seen from a physics-based perspective, as they are all
equivalent to a negative mass oscillator with a noise. This paper presents an
alternative formulation based on insights from physics.
","completely absurd behavior of stock prices in the GBM model, as well as its all direct descendants
such as local or stochastic volatility models, that becomes especially transparent in a small noise
limit.3While the fact the the zero-price level X= 0 is not attainable in the GBM model is well
known, the nature of this mechanism is rarely discussed. A critical observation is that not only
the GBM model is incompatible with corporate defaults due to its inability to reach the zero
levelX= 0, but rather that non-negative prices are obtained in the GBM model at the cost of
introducing a completely ctitious and absurd force, due to the negative gradient of the GBM
potential, that somehow saves a rm from default once it gets close to the zero level. More
than that, this force becomes unboundedly stronger as the price increases! The presence of such
an absurd ever-growing force for all positive prices appears to be too steep a price to pay for
non-negativity of stock prices, which was the original motivation for the GBM model.
Interestingly, the last observation that the repelling force actually increasses rather than
decreases as the price moves away from the default boundary Xt= 0 also implies the model
behavior should also become progressively less trustworthy for large values of Xt, that could be
expected in a long run for the GBM dynamics (1). The GBM model predicts that on average,
the price of a given stock should grow exponentially in time, but empirically, a very few stocks
have observable prices for a long period of, say, 100 years.4Most of stocks live much shorter
than this, and often end their life via mergers, acquisitions, or corporate bankruptcies. Recall
that none of such events should be possible according to the GBM model, again suggesting that
it contradicts the reality.5Also note that it would not be fair to use a long history of market
index portfolios such as e.g. Dow Jones or S&P500 as an evidence of an average exponential
long-term growth for individual stocks. Due to the fact that the composition of such market
index portfolios continuously changes, it embeds a survivorship bias that allows it to ignore the
fact that stocks can default, and proceed away with the implicit assumption that stocks are
immortal.
An analogy with the Inverted World mentioned in the introduction should become more
transparent to the reader at this point. An unlimited fallin an unbounded potential (4) de-
scribing a small noise dynamics of the GBM model and all its descendants can only be perceived
as an exponential growth only if the observer is somehow `inverted' as well. When viewed from
the perspective of the Langevin dynamics, an unbounded average exponential growth of assets
according to the GBM model turns out to be an unbounded fallin the inverted parabolic po-
tential. This is obviously a catastrophic scenario for most models in physics except dedicated
models designed to describe short-lived unstable systems (e.g. in some cosmological models).
3Analysis of a small noise limit is useful in order to not get 'fooled by randomness'. Note that this is not
the same as setting volatility to zero exactly . In such a strict limit, a stock becomes a riskless asset that
should earn a risk-free rate raccording to a no-arbitrage argument, and thus should be the same as cash in
a bank account. As the exponential growth ertfollows as a solution of a compound interest equation, nancial
mathematicians typically have no issue with taking the exponential law erton its face value, and formally applying
it for arbitrary times t! 1 arguing that 'we believe this will continue in the next 100 years or so, given
the past experience'. I believe that an appeal to the money bank account law ertas a `justication' for an
unbounded average exponential growth for stocks would be erroneous both nancially and mathematically. It
is wrong nancially because stocks are notcash, they can default, while a bank account is protected by state
regulations. It is also wrong mathematically because the limit = 0 is a singular limit: corporate defaults become
mathematically impossible in this limit, while their probability may remain small but non-zero for arbitrarily small
but non-zero values >0.
4I would like to thank Peter Carr for pointing out one such stock: Sotheby's (BID).
5While this only becomes evident in a long run, it does notmean that the GBM model is only `asymptotically
wrong' rather than being `qualitatively wrong'. As I argued above, a ctitious ever-growing force equal to the
negative gradient of the GBM potential is absurd on the whole positive semi-axis X0, that is, at each time
moment. On the other hand, the simplicity of the GBM potential (4) does not justify invoking of an asymptotic
analysis to identify regions of the state space X0 where the model becomes `too' wrong - it is wrong everywhere.
5",2020-08-09T01:04:42Z,w  t interesti xt xt t most recall also dow jondue ainverted world awaelic  analysis note ias it it peter carr sotby w as on
paper_qf_2.pdf,6,"The Inverted Parabola World of Classical Quantitative Finance:
  Non-Equilibrium and Non-Perturbative Finance Perspective","  Classical quantitative finance models such as the Geometric Brownian Motion
or its later extensions such as local or stochastic volatility models do not
make sense when seen from a physics-based perspective, as they are all
equivalent to a negative mass oscillator with a noise. This paper presents an
alternative formulation based on insights from physics.
","As an unbounded exponential expansion never occurs in most of other natural systems known to
physics, real `physical' markets should have mechanisms that eventually stop such an unbounded
expansion. As I argue next, such stabilization can arise from taking into account interactions
and non-linearities in the market dynamics.
2.3 Stabilization of dynamics by non-linearities
Both the classical GBM model and a majority of models used in quantitative trading6are
linear models, in the sense that they have a linear (or constant) drift. A linear or a constant
specication of a drift term for stock dynamics might appear a simplest reasonable choice, given
that a drift is harder to measure at small time steps  tthan a diusion term7.
As is known in physics, linear dynamic systems can typically be only considered approxi-
mations to real-world dynamics of natural systems, which are often non-linear . Non-linearities
capture interactions in physical systems, that could be produced either by interactions between
dierent elements of a system, or interactions with some external potential. In particular, within
the Langevin approach, the most common approach to incorporate complex interactions in a
physical system is to consider more complex potentials than a harmonic oscillator potential. One
popular choice are potentials expressed as polynomials in a state variable X. While the use of
a general polynomial potential can be justied as a Taylor expansion of an arbitrary potential,
for most systems encountered in statistical and quantum physics it usually suces to consider
polynomial potentials up to the fourth degree [7] (see also references in [5]).
One of the most popular non-linear potentials describing many systems in physics is the
so-called quartic potential
U(X) =",2020-08-09T01:04:42Z,as as stabizatboth as noiaelic one w taylor one
paper_qf_2.pdf,7,"The Inverted Parabola World of Classical Quantitative Finance:
  Non-Equilibrium and Non-Perturbative Finance Perspective","  Classical quantitative finance models such as the Geometric Brownian Motion
or its later extensions such as local or stochastic volatility models do not
make sense when seen from a physics-based perspective, as they are all
equivalent to a negative mass oscillator with a noise. This paper presents an
alternative formulation based on insights from physics.
","(5). Depending on model parameters, the behavior of the potential can match the quadratic
approximation well in a parametrically wide range of the price Xt. Clearly, if we set parameters
andgto zero exactly , then the two potentials are identical on the whole semi-axis X0.
On the other hand, for non-vanishing values of parameters andgthat control, respectively,
the cubic and quartic non-linear terms in the potential U(x), the latter can produce a wide variety
of shapes, depending on the values of parameters, as illustrated in Fig. 2.
Figure 2: Under dierent parameter choices in the quartic potential U(x) of Eq.(5), it can take
dierent forms. A stable state of the system corresponds to a minimum of the potential. The
potential on the left describes a metastable system with a local minimum at zero and a global
minimum at x= 3:3. If a particle is initially released near the global minimum, most of the time
it will experience a small diusive relaxation towards the global minimum, which, with a small
probability, can be replaced at each instance by a large sudden jump across the potential barrier
separating the two minima. The minimum at zero corresponds to the default state. For the
potential in the center, the state x= 3:3 becomes unstable, and the state x= 0 is metastable.
The potential on the right has two symmetric minima, and the particle can choose any of them
to minimize its energy. Such a scenario is called \\\\spontaneous symmetry breaking"" in physics.
As was argued in [5], it is the potential in the left graph in Fig. 2 that leads to the most
interesting dynamics of a stock market price. Instead of unstable dynamics of the GBM model,
with such a potential, dynamics can rather be metastable . Such metastable dynamics are dif-
ferent from globally stable dynamics such as e.g. the harmonic oscillator dynamics in that they
eventually change, though the time for this change to occur may be long, or very long, depending
on the parameters. In between of such infrequent transitions, dynamics are approximately equi-
librium (stationary) or quasi-equilibrium. Changes of the dynamics correspond to rare events
of transitions between local minima of the potential.
While an explanation of how this happens will be given momentarily, it is very important to
emphasize a critical role of a non-vanishing noise >0 for a realization of a scenario described
below. This is because any transitions between dierent local minima of a potential are only
possible when thermal uctuations are turned on by allowing for a non-zero >0. Ifis large,
uctuations become stronger and transitions happen more often, but in the strict opposite limit
= 0, any uctuations die o, and transitions between local minima of the potential are no
longer possible. Dynamics obtained in the strict limit = 0 are qualitatively dierent from
dynamics obtained for non-zero values >0, even though the actual numerical value of may
be very small numerically. This is the reason why appealing to an exponential bank account
law as a justication for a similar average behavior for stocks would be mathematically wrong
7",2020-08-09T01:04:42Z,pendi xt  o  unr eq t  t for t suas  instead suiw   dynamics 
paper_qf_2.pdf,8,"The Inverted Parabola World of Classical Quantitative Finance:
  Non-Equilibrium and Non-Perturbative Finance Perspective","  Classical quantitative finance models such as the Geometric Brownian Motion
or its later extensions such as local or stochastic volatility models do not
make sense when seen from a physics-based perspective, as they are all
equivalent to a negative mass oscillator with a noise. This paper presents an
alternative formulation based on insights from physics.
","- as was mentioned in Sect. 2.2, the limit !0 is singular (non-analytic).8
The potential shown on the left of Fig. 2 has a potential barrier between a metastable point
at the bottom of the local well, and the part of the potential for small values of x, where the
motion against the gradient of the potential means a fall to the zero price level x= 0. Due
to noise-induced uctuations, a particle representing a stock with value xtat timetplaced
initially to the right of the barrier, can hop over to the left of the barrier. In physics, solutions
of dynamics equations that describe such \\\\barrier-hopping"" transitions are called instantons .
The reason for this nomenclature is that the transitions between the meta-stable state and the
regime of instability (a \\\\fall"" to the zero level x= 0) happens almost instantaneously in time.
What might take a long time though is the time for this hopping to occur: depending on model
parameters, the waiting time can in principle even exceed the age of the observed universe. See
Fig. 3 for examples of an instanton, anti-instanton (an instanton going backward in time), and
a bounce (an instanton-anti-instanton pair, i.e. an instanton followed by an anti-instanton)
Figure 3: Instanton, anti-instanton, and bounce solutions. The instanton hops from the right
of a global maximum to the left of it, the anti-instanton proceeds in an opposite order, and the
bounce is made of the instanton followed by the anti-instanton.
In nancial terms, an event of hopping over the barrier en route to the zero level at x= 0
corresponds to a corporate bankruptcy (default). As the GBM model corresponds to the inverted
harmonic potential where the point x= 0 is unattainable, corporate defaults cannot be captured
by the GBM model. In contrast, with the quartic potential shown on the left of Fig. 2, corporate
defaults are perfectly possible, and correspond to the instanton-type hopping transitions between
dierent local minima of the meta-stable potential. Note that both the drift (the negative
gradient of the potential (5)) and volatility vanish at X= 0. This means the the zero level
X= 0 is an absorbing state: once the particle reaches this point, it stays there forever. This
is a highly desirable model behavior as it captures corporate default in a simple diusion-based
stock price model, in a sharp contrast with a failure of the GBM model to produce a defaultable
equity model, in addition to unrealistic dynamics for X > 0.
8A popular example of a non-analytical dependence on a model parameter is given by the function f(g) =
e",2020-08-09T01:04:42Z,se t  due it what    istatt ias i note  
paper_qf_2.pdf,9,"The Inverted Parabola World of Classical Quantitative Finance:
  Non-Equilibrium and Non-Perturbative Finance Perspective","  Classical quantitative finance models such as the Geometric Brownian Motion
or its later extensions such as local or stochastic volatility models do not
make sense when seen from a physics-based perspective, as they are all
equivalent to a negative mass oscillator with a noise. This paper presents an
alternative formulation based on insights from physics.
","3 The \\\\Quantum Equilibrium-Disequilibrium"" (QED) model
Unlike the GBM model, the QED model [5] incorporates capital inows and outows in the
market, along with capturing their price impact in the model construction. As I will show
below, capturing these phenomena using simple function approximations eectively produces
the Langevin dynamics with the quartic potential (5), thus oering a plausible mechanism for
stabilization of market dynamics by non-linearities as described in Sect. 2.3. Before providing a
mathematical formulation of the model, it is helpful to discuss empirical data.
3.1 Markets are open systems: importance of money ows and their impact
Traditional classical nance models such as the GBM model of Samuelson [11], the Black-Scholes
model [2], the CAPM model [12] etc. typically all assume that a market is a closed system that
does not exchange cash with outside investors (an \\\\outside world""). A common assumption for
stock dividends often made for modeling stock prices is that any dividends paid by a company
are immediately re-invested back into the stock by the shareholders. However, in addition to
current investors in a given stock at any point in time, the normal regime of the market is
that on average, there is an approximately continuous rate of cash inows into the market from
new investors, mainly due to various retirement plans programs. In other words, money is not
conserved in the market due to continuous inows (and outows) of new market participants.
Fig. 4 demonstrates the dynamics of combined inows into equity, bond, and hybrid funds
[3]. It shows that on average, there was a steady inow of around $325bn annually into the US
funds between 2004 and 2016, with a local drop around 2009 as a result of the economic crisis.
Assuming as a rough estimate that about two thirds of these inows are invested in stocks, this
gives rise to about $200bn injected every year into the stock market. The main origin of such
cash injection are retirement plans of the US workers.
Figure 4: Combined inows into equity, bond, and hybrid funds. The annual rate is approxi-
mately constant at the level of $325bn [3].
Should an annual injection of $200bn in the capital market be considered a large or a neg-
ligible eect? The total market capitalization of all stocks in the S&P500 index is about $25.5
trillion, or $25,500bn, so the inows are of the order of 1% of the total index value, which may
not be a numerically insignicant eect. In addition, the answer depends on how exactly these
9",2020-08-09T01:04:42Z,t quantum equibrium is equibrium unlike as aelic se before markets traditnal samuelso schools i it assumi t  combined t should t in
paper_qf_2.pdf,10,"The Inverted Parabola World of Classical Quantitative Finance:
  Non-Equilibrium and Non-Perturbative Finance Perspective","  Classical quantitative finance models such as the Geometric Brownian Motion
or its later extensions such as local or stochastic volatility models do not
make sense when seen from a physics-based perspective, as they are all
equivalent to a negative mass oscillator with a noise. This paper presents an
alternative formulation based on insights from physics.
","inows are distributed across dierent stocks. If retails or institutional investors are massively
driven to invest in a particular \\\\hot"" stock, after a relatively short period of increased returns
driven \\\\mechanically"" by the momentum, a long term impact of such investor \\\\crowding"" in the
stock normally amounts to diminishing long-term returns. The latter phenomenon is known as
the \\\\dumb money"" eect [9].
Therefore, to model the impact of investors ows and their impact on stock returns, we should
simultaneously incorporate two things into the modeling framework, which are both missing in
most conventional classical models such as the GBM: capital inows, and saturation/market
friction eects. As we will see next, the QED model incorporates both these eects, and moreover
it provides an explanation why these eect are critically important to ensure a long-term stability
(or, more accurately, meta-stability, as will be more clear below) of the resulting dynamics, no
matter how small these eects may be numerically.
3.2 The QED model
LetXtbe a total capitalization of a rm at time t, rescaled to a dimensionless quantity of the
order of one Xt1, e.g. by dividing by a mean capitalization over the observation period. We
consider discrete-time dynamics described, in general form, by the following equations:
Xt+t= (1 +rt+tt)(Xt",2020-08-09T01:04:42Z, t trefore as t  xt be xt  xt xt
paper_qf_2.pdf,11,"The Inverted Parabola World of Classical Quantitative Finance:
  Non-Equilibrium and Non-Perturbative Finance Perspective","  Classical quantitative finance models such as the Geometric Brownian Motion
or its later extensions such as local or stochastic volatility models do not
make sense when seen from a physics-based perspective, as they are all
equivalent to a negative mass oscillator with a noise. This paper presents an
alternative formulation based on insights from physics.
","In general, the rate of capital injection utinjected by investors in the market at time tshould
depend on the current market capitalization Xt(or current returns), plus possibly other factors
(e.g. alpha signals). In [5], we considered a simple quadratic choice for ut
ut= u+Xt+X2
t (8)
with three parameters  u,and.11Note that Eq.(8) implies that the total money ow utXt!0
in Eq.(6) when Xt!0. This ensures that no investor would invest in a stock with a strictly
zero price. Also note that the Eq.(8) can always be viewed as a leading-order Taylor expansion
of a more general nonlinear \\\\capital supply"" function u(Xt;zt) that can depend on both Xtand
signals zt. (alternatively, the capital supply ucan be made a function of returns rather than
prices [6]). Respectively, parameters  u,andcould be slowly varying functions of signals zt.
Here we consider a limiting case when they are treated as xed parameters, which may be a
reasonable assumption for time periods when an economic regime does not change too much.
Substituting Eq.(8) into Eqs.(6), neglecting terms O(t)2and taking the continuous time
limit t!dtwe obtain the \\\\Quantum Equilibrium-Disequilibrium"" (QED) model:
dXt=Xt+wTzt
",2020-08-09T01:04:42Z,ixt ixt note eq xt eq xt  also eq taylor xt xt and respeively re substituti eq quantum equibrium is equibrium xt xt tz
paper_qf_2.pdf,12,"The Inverted Parabola World of Classical Quantitative Finance:
  Non-Equilibrium and Non-Perturbative Finance Perspective","  Classical quantitative finance models such as the Geometric Brownian Motion
or its later extensions such as local or stochastic volatility models do not
make sense when seen from a physics-based perspective, as they are all
equivalent to a negative mass oscillator with a noise. This paper presents an
alternative formulation based on insights from physics.
","3.3 QED model and instantons: non-perturbative nance
For some physical systems, non-linearities can be handled approximately, by treating them as
small perturbations around a linear regime, using e.g. a perturbation theory in a small parameter
that quanties the stength of non-linearity. However, in many other cases arising in the natural
sciences, non-linearities should be treated as key ingredients of the dynamics.
For example, non-linearity is critical for self-organizing systems which cannot be described
using a perturbation theory around a linear regime. Another well-known example is provided by
instantons - barrier transition phenomena in statistical and quantum physics discussed above.
Probabilities of such barrier transitions cannot be obtained at any nite order of a perturba-
tion theory in a small parameter controlling the non-linearity. They are examples of so-called
non-perturbative phenomena. While instantons and other non-perturbative phenomena are very
important in many models of statistical physics and quantum eld theory12, they are not trace-
able using tools of perturbation theory, see e.g. references cited in [5].
Similarly, instantons in the QED model (see Fig. 3) are non-perturbative phenomena in
parameters ;g, and thus could not be seen at any nite order of a perturbation theory con-
structed around a strict limit = 0; g= 0 of the QED dynamics. As in this strict limit the
QED model would be identical to the GBM model, this means that while the latter could for-
mally be considered as a 'baseline', unperturbed model for construction of such a perturbative
expansion, instantons (and hence corporate defaults) would be entirely lost in such a scheme.
Non-perturbative methods to compute instanton-induced transition probabilities associated with
probabilities of corporate defaults are presented in [5]. As was illustrated in [5], this enables a
simultaneous calibration of the QED model to equity and credit markets, by a joint t to equity
returns and credit default swaps (CDS) spreads. In its turn, it enables using data from credit
markets to produce information on a long-term equity returns. The QED model is therefore a
rst defaultable equity model that captures corporate defaults without introducing additional
degrees of freedom such as hazard rates.
4 Summary
To summarize, starting with Samuelson's GBM model, many models used by practitioners for
modeling stock prices and derivatives prices, such as local or stochastic volatility models, relied
on the assumption of a linear (and typically positive) drift of a price process, or equivalently a
constant drift of a log-price process. In this paper I showed that, when interpreted in physics
terms, these models describe an oscillator with a negative mass (or equivalently a particle in an
inverted parabolic potential ) subject to noise, where dierences between specic models amount
to dierent ways of modeling noise. This makes them all models of stochastic dynamics in
anunstable potential, and conicts with conventional ways of analysis of natural systems in
physics where models typically describe uctuations around some stable or metastable state.
Aqualitatively wrong behavior describing an unlimited fall in such an unbounded potential is
obtained as a result. Samuelson's solution of the problem of negative prices in the ABM model
of Bachelier is unsatisfactory as it leads to a conict with basic physics.
I argued that such a pathological behavior can be avoided if the market is modeled as an
open system with a possible exchange of money with an outside world, along with a price
impact of the new money on stock prices. For a single-stock market, this produces a simple non-
linear two-parametric extension of the GBM model, with new parameters ;g, called ""Quantum
12Including e.g. quantum chromodynamics (QCD), the modern theory of strong interactions. To explain the
very existence of protons and neutrons, QCD needs to go beyond perturbation theory.
12",2020-08-09T01:04:42Z,for for anotr probabitity w simarly  as noas it suary to samuelsoi qualitative samuelsocac lifor quantum inudi to
paper_qf_2.pdf,13,"The Inverted Parabola World of Classical Quantitative Finance:
  Non-Equilibrium and Non-Perturbative Finance Perspective","  Classical quantitative finance models such as the Geometric Brownian Motion
or its later extensions such as local or stochastic volatility models do not
make sense when seen from a physics-based perspective, as they are all
equivalent to a negative mass oscillator with a noise. This paper presents an
alternative formulation based on insights from physics.
","Equilibrium-Disequilibrium"" (QED) model [5]. The QED model formally transforms into the
GBM model in the limit ;g!0. With non-zero parameters, it produces a qualitatively dif-
ferent behavior: while the GBM model describes unstable dynamics, the QED model describes
metastable dynamics where a diusive relaxation to a metastable state is followed by a rare large
negative move describing a transition to a distressed state or corporate bankruptcy. Such rare
large moves are due to noise-induced solutions of the model called instantons. Similarly to in-
stantons in physics, instantons in the QED model are non-perturbative phenomena: they cannot
be seen in a perturbative expansion of the model that could be attempted when parameters ;g
are small but non-zero. In particular, instanton disappear in the strict 'GBM limit' =g= 0.
The QED model oers a few important theoretical insights. While classical nancial models
have traditionally focused on modeling volatility while keeping simple linear assumptions of
the drift, the QED model suggests that the drift should instead be non-linear , and should
be identied prior to analyzing volatility patterns. In particular, it would be interesting to
reconsider various stochastic volatility and `rough volatility' models after xing the drift function
alone the lines suggested in this paper.
The QED model can be extended along multiple dimensions. In particular, it can be extended
to a market with multiple assets, producing the IQED (\\\\Interacting-assets QED"") model [6].
Other possible extensions can make the quartic potential random - for example, by allowing a
dependence of parameter on signals ztas in Eq.(11). This may make the dynamics of the
model more realistic and avoid a possible negative long-term drift that might be obtained in the
model if the potential is kept static. Clearly, for applying the model for a long term modeling,
one should better make some or all model parameters dependent on signals zt, assuming that
the latter carry information on a contemporaneous market environment. Proceeding in such
way would eectively promote the potential to a random quartic potential.
Obviously, an important practical question is that assuming that the QED model is a `right'
model, how important are non-perturbative eects implied by the model? Can we still rely
on traditional nancial engineering models that are all based on the assumption of a linear
or constant drift? If yes, when can we still rely on them? Such questions can (and should)
be answered for any particular stock market and any traditional model by comparing results
obtained with that model versus the QED model. In general, non-linear and non-perturbative
eects are notexpected to be critically important for small local price uctuations. However,
it is a comparison with a more general non-linear model such as the QED model that should
answer the question about a range of prices and times where traditional linear models can still
be used. The QED model could also be used for option pricing, and its predictions could be
analyzed and compared with traditional models both numerically and analytically using various
approximations. Results of such analysis will be presented elsewhere.
References
[1] L. Bachelier, \\\\Thorie de la speculation"", Annales Scientiques de L?cole Normale Su-
prieure ,17, 21-86 (1900). (English translation by A. J. Boness in P.H. Cootner (Editor):
The Random Character of Stock Market Prices , p. 17?75. Cambridge, MA: MIT Press
(1964).
[2] F. Black and M. Scholes, \\\\The Pricing of Options and Corporate Liabilities"", Journal of
Political Economy, Vol. 81(3), 637-654, 1973.
[3] Deutsche Bank, \\\\Grassroots Crowding Measures"" (2016).
13",2020-08-09T01:04:42Z,equibrium is equibrium t with susimarly it w it iinti otr eq   proceedi obvusly ca suit results referenccac lihorse annals cie nti normale su eh boncost ner editor t random charaer stock market priccambridge   schools t prici optns corate liabitijournal political economy vutsc bank grassroots owdi measures
paper_qf_2.pdf,14,"The Inverted Parabola World of Classical Quantitative Finance:
  Non-Equilibrium and Non-Perturbative Finance Perspective","  Classical quantitative finance models such as the Geometric Brownian Motion
or its later extensions such as local or stochastic volatility models do not
make sense when seen from a physics-based perspective, as they are all
equivalent to a negative mass oscillator with a noise. This paper presents an
alternative formulation based on insights from physics.
","[4] M. Dixon, I. Halperin, and P. Bilokon, Machine Learning in Finance: from Theory to
Practice , Springer 2020.
[5] I. Halperin and M.F. Dixon, \\\\Quantum Equilibrium-Disequilibrium: Asset Price Dynam-
ics, Symmetry Breaking, and Defaults as Dissipative Instantons"", Physica A 537, 122187,
https://doi.org/10.1016/j.physa.2019.122187 (2020).
[6] I. Halperin, \\\\Dumb Money, The Origin of Factors, Skew and Rare Events: Interacting-assets
\\\\Quantum Equilibrium-Disequilibrium"""", forthcoming (2020).
[7] L.D. Landau and E.M. Lifschitz, Statistical Physics , Elsevier (1980).
[8] P. Langevin, \\\\Sur la Th eorie du Mouvement Brownien"", Comps Rendus Acad. Sci. (Paris)
146, 530-533 (1908).
[9] A. Frazzini and O.A. Lamont, \\\\Dumb Money: Mutual Fund Flows and the Cross-Section of
Stock Returns"", Journal of Financial Economics , Elsevier, vol. 88(2), pages 299-322 (2008).
[10] R. Merton, \\\\Theory of Rational Option Pricing"", Bell Journal of Economics and Manage-
ment Science, Vol.4(1), 141-183, 1974.
[11] P. Samuelson, \\\\Rational theory of warrant pricing"", Industrial Management Review ,6
(Spring), 13-32 (1965).
[12] W.F. Sharpe, \\\\Capital asset prices: A theory of market equilibrium under conditions of
risk"", Journal of Finance ,19(3), 425?442 (1964).
14",2020-08-09T01:04:42Z,dixohal  ibo machine learni nance tory praice  hal  idixoquantum equibrium is equibrium asset price dysyetry breaki faults dissipatistatpsics hal  idumb money t orifaors skew rare events inti quantum equibrium is equibrium land  sit statistical psics else vier aelic sur th mouvement browne  end us acad sci paris jazz ilament dumb money mutual fund flows oss sestock urns journal nancial economics else vier mortotory ratnal optprici bell journal economics manage science vsamuelsoratnal industrial management review spri share capital journal nance
paper_qf_3.pdf,1,Self-Financing Trading and the Ito-Doeblin Lemma,"  The objective of the note is to remind readers on how self-financing works in
Quantitative Finance. The authors have observed continuing uncertainty on this
issue which may be because it lies exactly at the intersection of stochastic
calculus and finance. The concept of a self-financing trading strategy was
originally, and carefully, introduced in (Harrison and Kreps 1979) and expanded
very generally in (Harrison and Pliska 1981).
","Self-Financing Trading and the It^ o-D oblin Lemma1
Chris Kenyon2and Andrew Green3
The objective of the note is to remind readers on how self-nancing works in
Quantitative Finance. The authors have observed continuing uncertainty on
this issue which may be because it lies exactly at the intersection of stochas-
tic calculus and nance. The concept of a self-nancing trading strategy was
originally, and carefully, introduced in Harrison and Kreps [1979] and expanded
very generally in Harrison and Pliska [1981].
The Issue The valueYtof a portfolio (using notation as Due [2001]) com-
posed of stock Stand bondtwith holding atandbtcan be written (Equation
14 on page 90):
Yt=atSt+btt
the change in portfolio value, or gain process is given as (Equation 15 on page
90):
dYt=atdSt+btdt
Clearly, ifatis a delta hedge, i.e. a function of St, then applying the It^ o-D oblin
Lemma to the equation for Ytwould give:
dYt=atdSt+Stdat+datdSt+btdt+tdbt+dbtdt
and thetdbt+dbtdtterms are also simply a mathematical consequence of
applying the Lemma. So has Prof Due made a mistake that is still there in the
3rd edition of his text? This is the crux of this issue at the intersection between
stochastic calculus (the It^ o-D oblin Lemma) and nance (Due's equation 15),
i.e. the concept of a self-nancing portfolio.
The Resolution is simply the denitions in Harrison and Kreps [1979], Har-
rison and Pliska [1981] and reproduced in Due [2001] that a self-nancing
portfolio follows (page 89):
atSt+btt=a0S0+b00+Zt
0audSu+Zt
0btdu (1)
or
d(atSt+btt) =atdSt+btdt (2)
What this says is that the only change in portfolio value comes from the value
of the stock and bond (or cash account), whatever the trading strategy. The
trading strategy can move value between the stock and cash accounts but not
1The views expressed are those of the authors only, no other representation
should be attributed. Not guaranteed t for any purpose. Use at your own risk.
2Contact: chris.kenyon@lloydsbanking.com
3Contact: andrew.green2@lloydsbanking.com
1arXiv:1501.02750v1  [q-fin.PR]  12 Jan 2015",2015-01-12T18:47:16Z,self nanci tradi it lea chris canyoandrew greet quantitative nance t t harrisoreps harrisopli ska t issue t of du stand equatst equatst  st it lea would st std at st lea so prof du  it lea du t resolutharrisoreps har pli ska du st su st st what t t not use conta conta  
paper_qf_3.pdf,2,Self-Financing Trading and the Ito-Doeblin Lemma,"  The objective of the note is to remind readers on how self-financing works in
Quantitative Finance. The authors have observed continuing uncertainty on this
issue which may be because it lies exactly at the intersection of stochastic
calculus and finance. The concept of a self-financing trading strategy was
originally, and carefully, introduced in (Harrison and Kreps 1979) and expanded
very generally in (Harrison and Pliska 1981).
","create or destroy value. If this were not true then the basic result that all self-
nancing portfolios have the same rate of return in the risk-neutral measure
would be false (Harrison and Pliska [1981]).
Basically by denition of self-nancing the only change in portfolio value
comes from the value of the underlyings (the gain process). An additional self-
nancing4equation is implied , hereStdat+datdSt+tdbt+dbtdt0, but
it adds nothing since it is simply a direct consequence of the denition of self-
nancing. However, it is irrelevant because it is the denition that drives the
theory.
Discussion In short, you cannot apply the It^ o-D oblin Lemma to a portfolio's
value expressed in terms of its underlyings and get its gain process. This is by
denition (Harrison and Kreps [1979], Harrison and Pliska [1981]).
The denition of a self-nancing trading strategy chosen by Harrison and
Kreps [1979], Harrison and Pliska [1981] means that continuous time works like
a limit of discrete time trading. Trading strategies are predictable (no use of the
future) and have additional technical limits (e.g. quadratic bounds), consistent
with the It^ o-D oblin Lemma, that rule out things like doubling strategies and
other unwanted arbitrage mechanisms.
Mathematically it is possible to chose other denitions of self-nancing from
those chosen by Harrison and Kreps [1979], Harrison and Pliska [1981]. Then
you have a dierent theory, and one that is not what is currently accepted, and
been found useful over the last thirty or so years in Quantitative Finance. The
key point delivered by the denition is that all self-nancing portfolio provide
the same rate of return in the risk neutral measure. If the trading strategy
could change the rate of return then the theory would be broken as arbitrage
opportunities would be immediate. Hence we see that the current denition
of self-nancing, that portfolio values changes only through its underlyings, is
appropriate for Quantitative Finance. Recent modications Kenyon and Green
[2014] build on this framework, they do not contradict it.
Acknowledgements The authors would like to thank Prof Darrell Due for use-
ful pointers. Any errors remain their own.
References
D. Due. Dynamic asset pricing theory . Princeton, 2001. 3rd Edition.
J.M. Harrison and D.M. Kreps. Martingales and arbitrage in multiperiod securities
markets. Journal of Economic Theory , 20:381{408, 1979.
J.M. Harrison and S.R. Pliska. Martingales and stochastic integrals in the theory of
continuous trading. Stochastic Processes and their Applications , 11:215{260, 1981.
4Note that \\\\self-nancing condition"" or equation is applied to dierent pieces of this setup
by dierent authors.
2",2015-01-12T18:47:16Z, harrisopli ska basically astd at st discussiit lea  harrisoreps harrisopli ska t harrisoreps harrisopli ska tradi it lea matmatically harrisoreps harrisopli ska tquantitative nance t  nce quantitative nance recent canyogreeackledgement t prof barrel du any referencdu dynamic princetoeditharrisoreps martaljournal economic tory harrisopli ska martalstochastic processapplicatns note
paper_qf_3.pdf,3,Self-Financing Trading and the Ito-Doeblin Lemma,"  The objective of the note is to remind readers on how self-financing works in
Quantitative Finance. The authors have observed continuing uncertainty on this
issue which may be because it lies exactly at the intersection of stochastic
calculus and finance. The concept of a self-financing trading strategy was
originally, and carefully, introduced in (Harrison and Kreps 1979) and expanded
very generally in (Harrison and Pliska 1981).
","C. Kenyon and A. Green. Regulatory costs break risk neutrality. Risk, 27, September
2014.
3",2015-01-12T18:47:16Z,canyogreeregulatory risk september
paper_qf_4.pdf,1,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","1 
  
 
 
 
 
 
 
 
 
 
The theory of 
quantitative 
trading  
 
                      Andrea Berdondini  
 
 
 
 
 
 
 
 
 ",2021-12-27T14:48:03Z,t  be rd oin
paper_qf_4.pdf,2,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","2 
  
 
 
 
 
 
 
 
 
 
 
 
 
“The difference between an amateur trader and a 
professional trader is that the first is obsessed by the result  
while the seco nd is obsessed by the knowledge ” 
 
 
 
 
 
 
 
 
 
 
 
 
 
 ",2021-12-27T14:48:03Z,t
paper_qf_4.pdf,3,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","3 
 Contents  
 
Introduction                                                                                                          4                      
 
Structure of the book                                                                                           5 
 
Statistics                                                                                                                6 
      The uncertainty of the statistical  data                                                                                       7 
The information paradox                                                                                                          12 
Use of the fundamental problem of statistics to define the validity limit of aaaaaaa laa                               
Occam's razor principle                                                                                                            16                                                       
Resolution of the St. Petersburg paradox using Von Mises’ axiom of randomness                21 
Statistics the science of awareness                                                                                           27 
 
Quantitative trading                                                                                           29 
Description of a methodology from Econophysics as a verifi cation  technique llllllllllllllllllll   
for a financial  strategy                                                                                                              30 
The professional trader’s paradox                                                                                           36 
Application of the Von Mises’ axiom of randomness on the forecasts concerning llllllllllllllll  
the dynamics of a non -stationary system described by a numerical sequence                        39 
                                                                                                                                                   
    Psychology                                                                                                           44 
 The psychology of the professional trader                                                                               45 
 How meditation can improve our problem -solving skills                                                       47 
 
Sticker for your monitor                                                                                   50 
 
 
 
   
 
    ",2021-12-27T14:48:03Z,contents introdustruure statistics t t use occur resolutst petersburg vomisstatistics quantitative sipteco no psics t applicatvomispsychology t how sticker
paper_qf_4.pdf,4,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","4 
 Introduction  
 
The first step that you must take when you want to learn how to develop a quantitative trading 
system is to answer the following question: what characteristics must a system have to represent 
the theoretically most difficult situation possible where to make predictions?  
The first characteristic that this system must possess is that of having a very low ratio between 
the deterministic component and the random component. Therefore, the random component must 
overcome the deterministic component.  
The second characteristic that we need to determine concerns the number of degrees of freedom. 
This parameter determines how much difficult it is to test hypotheses. This consideration derives 
from the fundamental problem of statistics defined as follows: ""A statistical data does not represent 
useful information, but becomes useful information only when it is shown that it was not obtained 
randomly"" . Therefore, given a result, the probability of o btaining it randomly decreases as the 
degrees of freedom of the system increase. Consequently, systems with a low number of degrees 
of freedom are particularly dangerous, because it is particularly easy to get good results randomly 
and therefore you risk o verestimating an investment strategy. For this reason, a system that wants 
to be as difficult as possible where making predictions must have a low number of degrees of 
freedom.  
The third characteristic we have to choose is if to consider the system as erg odic (stationary) or 
non-ergodic (non -stationary). This choice is easy because it is much more difficult to make 
forecasts on a non -ergodic system. Indeed, in this case, past results may not be significant with 
respect to future results.  
In conclusion, th e system that represents, from the theoretical point of view, the most difficult 
situation in which to make predictions is a system in which there is a predominant random 
component, with a low number of degrees of freedom and not ergodic. Is there a system  that has 
all these 3 characteristics? The answer is yes indeed, the financial markets represent a system that 
respects all these conditions.  
I started this book with this consideration because I believe that the most important thing to 
understand, for any  person who wants to develop a quantitative trading system, is to know that you 
are facing the most difficult situation theoretically possible where to make predictions. It is this 
awareness that must guide us in the study of the financial markets. Without  this awareness, we will 
inevitably be led to underestimate the difficulty of the problem and this will lead us to make 
mistakes.  
 
 
 
 
 
 
 
 
 
 ",2021-12-27T14:48:03Z,introdut t trefore t   trefore consequently for t  ined iis t it without
paper_qf_4.pdf,5,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","5 
 Structure of the book  
 
This book consists of a selection of articles divided into three main themes:  
 
Statistics  
Quantitative Trading  
Psychology  
 
These three arguments are indispensable for the development of a quantitative trading system. 
Although the articles deal with very different topics, they are closely linked to each other, in 
practice they represent the obse rvation of the same problem from three different points of view.  
At the beginning of each chapter there will be an introductory paragraph where the results 
reported in the articles are summarize. The order of the articles was chosen so as to constitute a 
single logical reasoning that develops progressively.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 ",2021-12-27T14:48:03Z,struure  statistics quantitative tradi psychology tse although at t
paper_qf_4.pdf,6,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","6 
 Statistics  
 
Financial markets are characterized by a dominant random component with respect to the 
deterministic component. For this reason, the articles present are intended to explain the statistical 
analysis under these conditions.  
The main topic that is treated c oncerns the definition of the uncertainty of the statistical data. The 
traditional approach considers uncertainty as the dispersion of data around the true value. 
Therefore, it is based on the hypothesis that any divergence from uniformity is the result of  a 
deterministic component. This condition is realistic only in systems where the random component 
is negligible. Instead, in cases, such as in finance, where the random component prevails, it turns 
out to be an unrealistic hypothesis that leads to incorre ct conclusions.  For this reason, we will give 
a new definition of uncertainty suitable for a system in which there is a predominant random 
component. The parameter that has been chosen for its definition is represented by the probability 
of obtaining an eq ual or better result in a random way. Knowing how to calculate this parameter 
correctly represents the basis of statistics in finance. As I will show in the articles, this calculation 
is very difficult, and extremely easy to underestimate the uncertainty. Indeed, the mistake made is 
to evaluate the individual hypotheses independently without considering the hypotheses previously 
tested. This way of operating often leads, in systems with a low number of degrees of freedom 
where it is easy to obtain good resu lts randomly, to underestimating uncertainty.  
This approach will then be applied in different situations, such as in the case of the resolution of 
the St. Petersburg paradox. In this way, we will have concrete examples of how this method 
represents a comp letely new point of view in the evaluation of hypotheses.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 ",2021-12-27T14:48:03Z,statistics nancial for t t trefore  instead for t ki as ined   st petersburg in
paper_qf_4.pdf,7,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","7 
 The uncertainty of the statistical data  
 
Andrea Berdondini  
 
 ABSTRACT: Any result can be generated randomly and any random result is useless. Traditional methods 
define uncertainty as a measure of the dispersion around the true value and are based on the hypothesis that 
any divergence from uniformity is the result o f a deterministic event. The problem with this approach is 
that even non -uniform distributions can be generated randomly and the probability of this event rises as the 
number of hypotheses tested increases. Consequently, there is a risk of considering a ra ndom and therefore 
non-repeatable hypothesis as deterministic. Indeed, it is believed that this way of acting is the cause of the 
high number of non -reproducible results. Therefore, we believe that the probability of obtaining an equal 
or better result ran domly is the true uncertainty of the statistical data. Because it represents the probability 
that the data is useful and therefore the validity of any other analysis depends on this parameter.  
 
Introduction  
 
Any result can be generated randomly and any random result is useless. Traditional methods [1] 
and [2] define uncertainty as a measure of the dispersion around the true value and are based on the 
hypothesis that any divergence from uniformity is the result  of a deterministic event. The problem 
with this approach is that even non -uniform distributions can be generated randomly and the 
probability of this event rises as the number of hypotheses tested increases. Consequently, there is 
a risk of considering a random and therefore non -repeatable hypothesis as deterministic. Indeed, it 
is believed that this way of acting is the cause of the high number of non -reproducible results [3] 
and [4]. Therefore, we believe that the probability of obtaining an equal or bet ter result randomly 
is the true uncertainty of the statistical data, because it represents the probability that the data is 
useful and therefore the validity of any other analysis depends on this parameter.  
In addition, we will also address the problem of  determining the correct method of calculating 
the probability of obtaining an equal or better result randomly. Regarding this topic, we will see 
that the fundamental point, in calculating this probability value, is to consider the statistical data 
depende nt on all the other data generated by all the tested hypotheses.  
Considering the statistical data as non -independent has fundamental implications in statistical 
analysis. Indeed, all our random actions are not only useless, but will increase the uncertaint y of 
the statistical data. For this reason, in the following article [5], we highlight the importance of acting 
consciously in statistics.  
Furthermore, the evaluation of the uncertainty of the statistical data will be possible only by 
knowing all the attem pts made. In practice, the calculation of uncertainty is very difficult because 
not only we must consider all our attempts, but we must also consider the attempts made by every 
other person who is performing the same task as us. In this way, the uncertaint y of our statistical 
data also depends on the actions performed by the people who are working our own analysis. 
Indeed, a group of people who belong to a research network all having the same reputation who all 
work on the same problem can be considered wit h one person who carries out all the attempts made. 
Consequently, the calculation of uncertainty becomes something relative that depends on the 
information we have.  ",2021-12-27T14:48:03Z,t  be rd oiany traditnal t consequently ined trefore because introduany traditnal t consequently ined trefore iregardi consiri ined for furtr iiined consequently
paper_qf_4.pdf,8,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","8 
 Definition of uncertainty  
   
The aim of the definition of uncertainty of the statistical data that we are going to give is to 
determine a parameter that is linked to the repeatability of the result and that is universal and therefore, 
independent of the system in which we perform the statistical analysis.  
We define the uncertainty of the stati stical data as the probability of obtaining an equal or better 
result randomly.  
This definition considers the statistical data as a forecast, so a forecast is repeatable only if the 
process that generated it is non -random. Consequently, the calculation of uncertainty involves 
determining the type of process that generated the result. We can distinguish cognitive processes from 
random processes by their statistical property of generating non -reproducible results in a random way. 
Indeed, by using the informat ion on the system, on which we are performing a measurement, we can 
increase our probability of forecasting and this leads to a consequent decrease in the probability of 
obtaining the same result randomly.  
It is interesting to note that the repeatability of the statistical data and non -randomness of the process 
that produced it are two equivalent concepts. Indeed, the information leads to the repeatability of the 
result and at the same time generates results that cannot be reproduced randomly.  
To understa nd the definition given, we report the following example: We have to analyze a 
statistical datum represented by 1000 predictions on an event that can have only two results. The 1000 
predictions are divided into 600 successes and 400 failures. To calculate the probability of obtaining 
an equal or better result in a random way, we use the binomial distribution and we obtain the following 
value 1.4∙10−8%. 
Now, instead, let us consider a statistical datum represented by 10 predictions divided into 8 
successes a nd 2 failures. In this case, the probability of getting an equal or better result randomly is 
5.5%.  
Comparing the two results, we note that in the first case, although the number of successes is only 
60%, the uncertainty is almost zero, while in the second  case, with a probability of success of 80%, 
the uncertainty is much higher. This difference is due to the fact that the definition given, as 
mentioned, concerns only the repeatability of the result and not its accuracy. Therefore, it is a value 
that decre ases as the repetition of the result increases. The approach presented is very different from 
the classic approach, where uncertainty is seen as a measure of the dispersion of the data with respect 
to the true value.  
The fundamental point to understand is that the probability that statistical data is completely random 
and the estimate of its random component (dispersion around the true value) are two parameters that 
are only partially dependent on each other. The first decreases as the number of repetitions  of the 
measurement increases, the second does not and this is one of the reasons, why the traditional 
definition of uncertainty, in many cases, is not significant with regard to the repeatability of the result.  
The problem, as we have seen in the examples , is that there is always a greater or lesser probability 
that a purely random process generates the result. In this case, any analysis turns out to be wrong, for 
this reason, this value is considered the true uncertainty of the statistical result.  
 
Calcul ation of the uncertainty of the statistical data  
   
Correctly calculating the probability of getting an equal or better result randomly involves changing 
our approach to statistics. The approach commonly used in statistics is to consider the data produced ",2021-12-27T14:48:03Z,nitt   consequently  ined it ined to  t to  iari  trefore t t t t icalc ul correly t
paper_qf_4.pdf,9,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","9 
 by one method independent of the data produced by different methods. This way of proceeding seems 
the only possible one but, as we will show in the following paradox, it leads to an illogical result, 
which is instead solved by considering the data as non -independent.  
We think to have a computer with enormous computational capacity that is used to develop 
hypotheses about a phenomenon that we want to study. The computer works as follows: it creates a 
random hypothesis and then performs a statistical test. At  this point, we ask ourselves the following 
question: can there be a useful statistical test to evaluate the results of the hypothesis generated?  
If we answer yes, we get an illogical result because our computer would always be able, by 
generating a large number of random hypotheses, to find a hypothesis that passes the statistical test. 
In this way, we arrive at the absurd conclusion that it is possible to create knowledge randomly, 
because it is enough to have a very powerful computer and a statistical te st to understand every 
phenomenon.  
If we answer no, we get another illogical result because we are saying that no hypothesis can be 
evaluated. In practice, the results of different hypotheses are all equivalent and indistinguishable.  
How can we solve this logical paradox? The only way to answer the question, without obtaining an 
illogical situation, is to consider the results obtained from different methods depending on each other. 
A function that meets this condition is the probability of getting an equal or better result at random. 
Indeed, the calculation of this probability implies the random simulation of all the actions performed. 
Hence, random attempts increase the number of actions performed and consequently increase the 
probability of obtaining an eq ual or better result randomly. For this reason, generating random 
hypotheses is useless, and therefore if you use this parameter, as a measure of uncertainty, it is 
possible to evaluate the data and at the same time it is impossible to create knowledge by generating 
random hypotheses.  
Considering the statistical data as non -independent is a fundamental condition for correctly 
calculating the uncertainty. The probability of getting an equal or better result at random meets this 
condition.  
The dependence of statistical data on each other has profound implications in statistics, which will 
be discussed in the next section.  
 
Consequences of the non -independence of the statistical data  
   
Considering the statistical data dependent on each other in the calculation of uncertainty leads to 
three fundamental consequences in statistics.  
First fundamental consequence of the non -independence of the statistical data: our every random 
action always involves an increase in the uncertainty of the statistical data.  
Example: We need to analyze a statistical datum represented by 10 predictions about an event that 
can only have two results. The 10 predictions are divided into 8 successes and 2 failures. To calculate 
the probability of obtaining an equal or better resul t randomly we use the binomial distribution and 
we get the following value 5.5%. If before making these 10 predictions, we tested a different 
hypothesis with which we made 10 other predictions divided into 5 successes and 5 failures, the 
uncertainty of our  result changes. Indeed, in this case, we must calculate the probability of obtaining 
a result with a number of successes greater than or equal to 8 by performing two random attempts 
consisting of 10 predictions each. In this case, the probability becomes 10.6%, so the fact of having 
first tested a random hypothesis almost doubled the uncertainty of our second hypothesis. 
Consequently, increasing the random hypotheses increases the number of predictions that we will 
have to make, with the true hypothesis, t o have an acceptable uncertainty.  ",2021-12-27T14:48:03Z,  t at  i ihow t ined nce for consiri t t consequencconsiri rst   t to  ined iconsequently
paper_qf_4.pdf,10,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","10 
 Second fundamental consequence of the non -independence of the statistical data: every random 
action of ours and of every other person equivalent to us, always involves an increase in the 
uncertainty of the statistical dat a. 
By the equivalent term, we mean a person with the same reputation as us, therefore the data 
produced by equivalent people are judged with the same weight.  
Example: 10 people participate in a project whose goal is the development of an algorithm capable 
of predicting the outcome of an event that can have only two results. An external person who does 
not participate in the project but is aware of every attempt made by the participants evaluates the 
statistical data obtained. All participants make 100 predi ctions, 9 get a 50% chance of success, one 
gets a 65% chance of success. The uncertainty of the static data of the participant who obtains a 
probability of success of 65% is obtained by calculating the probability of obtaining a result with a 
number of suc cesses greater than or equal to 65 by performing ten random attempts consisting of 100 
predictions each. The probability obtained, in this way, is 16% instead if he was the only participant 
in the project the probability would have been 0.18%, therefore ab out 100 times lower.  
Third fundamental consequence of the non -independence of the statistical data: the calculation of 
the uncertainty varies according to the information possessed.  
Example: 10 people participate in a project whose goal is the development  of an algorithm capable 
of predicting the outcome of an event that can have only two results. In this case, people do not know 
the other participants and think they are the only ones participating in the project. All participants 
make 100 predictions, 9 g et a 50% chance of success and one gets a 65% chance of success. The 
participant who obtains a probability of success of 65% independently calculates the uncertainty of 
the result obtained. Not knowing that other people are participating in the project, ca lculate the 
probability of obtaining a result with a number of successes greater than or equal to 65 by performing 
a single random attempt consisting of 100 predictions; the probability obtained is 0.18%. An external 
person who is aware of every attempt ma de by the participants calculates the uncertainty of the 
participant's statistical data, which obtains a probability of success of 65%. It then calculates the 
probability of obtaining a result with a number of successes greater than or equal to 65 by makin g ten 
random attempts consisting of 100 predictions each. The probability obtained, in this way, is 16%, a 
much higher value than the uncertainty calculated by the participant. The uncertainty value calculated 
by the external person using more information is most accurate than the uncertainty value calculated 
by the individual participant. Consequently, the uncertainty value obtained by exploiting the greatest 
number of information must always be considered, in the case of the example, the most accurate 
uncertainty is that of 16%.  
The first and second fundamental highlighting consequence of the non -independence of the 
statistical data can be redefined by highlighting the non -randomness of the action.  
First fundamental consequence of the non -independence of  the statistical data: our every non -
random action always involves a decrease in the uncertainty of the statistical data.  
Second fundamental consequence of the non -independence of the statistical data: every non -random 
action of ours and of every other per son equivalent to us, always involves a decrease in the uncertainty 
of the statistical data.  
 
Conclusion  
   
The traditional definition of uncertainty implies considering true, for non -homogeneous data 
dispersions, the hypothesis that the result is not comp letely random. We consider this assumption the 
main problem of the definition of uncertainty. Indeed, whatever the statistical data obtained, there is ",2021-12-27T14:48:03Z,second by  aall t t third  iall t not ait t t consequently t rst second conust  ined
paper_qf_4.pdf,11,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","11 
 always a possibility that they are completely random and therefore useless.  
This error stems from the fact that the definition of uncertainty was developed in an environment 
where each method had a strong deterministic component. Therefore, calculating the probability of 
obtaining an equal or better result at random might seem useless. However, when we app ly statistics 
in fields such as finance, where the random component is predominant the traditional approach to 
uncertainty turns out to be unsuccessful. It fails for the simple reason that the hypothesis on which it 
is based may not be true. For this reaso n, we have defined the uncertainty of the statistical data as the 
probability of obtaining an equal or better result randomly. Since this definition of uncertainty is not 
linked to any hypothesis, it turns out to be universal.  The correct calculation of th is probability value 
implies considering the statistical data dependent on each other. This assumption, as we have shown 
through a paradox, makes the definition of uncertainty given consistent with the logical principle that 
it is not possible to create kn owledge randomly.  
The non -independence of the statistical data implies that each action performed has an effect on the 
calculation of uncertainty. The interesting aspect is that a dependence is also created between actions 
performed by different people. C onsequently, the calculation of uncertainty depends on the 
information in our possession, so it becomes something relative that can be determined absolutely 
only with complete knowledge of the information.  
 
Bibliography:  
[1] Bich, W., Cox, M. G., and Harris, P. M. Evolution of the ""Guide to the Expression of Uncertainty in 
Measurement"". Metrologia, 43(4):S161 –S166, 2006.  
[2] Grabe, M .,”Measurement Uncertainties in Science and Technology”, Springer 2005.  
[3] Munafò, M., Nosek, B., Bishop, D. et al. “A manifesto for re producible science”. Nat Hum Behav 1, 0021 
(2017). https://doi.org/10.1038/s41562 -016-0021.  
[4] Ioannidis, J. P. A. “Why most published research findings are false”. PLoS Med. 2, e124 (2005).  
[5] Berdondini, Andrea, “Statistics the Science of Awareness” (August 30 , 2021). Available at SSRN: 
https://ssrn.com/abstract=3914134.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 ",2021-12-27T14:48:03Z, trefore it for since t  t t biblgrap icox harris evolutgui essunr articial intellence ny measurement metro logic gra be measurement unr articial intellence tiscience technology  munose bishhum be ha loaid is w lo med be rd oi statistics science awareness august  articial intellence 
paper_qf_4.pdf,12,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","12 
 The information paradox   
Andrea Berdondini  
ABSTRACT:  The following paradox is based on the consideration that the value of a statistical datum does 
not represent useful information but becomes useful information only when it is possible to prove that it 
was not obtained in a random way.  In practice, the probability of obtaining the same result randomly must 
be very low in order to consider the result useful.  It follows th at the value of a statistical datum is something 
absolute, but its evaluation in order to understand whether it is useful or not is something relative depending 
on the actions that have been performed.  Consequently, a situation such as the one described in  this paradox 
can occur, wherein in one case it is practically certain that the statistical datum is useful, instead of in the 
other case the statistical datum turns out to be completely devoid of value. This paradox wants to bring 
attention to the importa nce of the procedure used to extract statistical information. Indeed, the way in which 
we act affects the probability of obtaining the same result in a random way and consequently on the 
evaluation of the statistical parameter . 
The information paradox   
We have two identical universes, in both universes the same person is present, that we will 
call John, he must perform the exact same task which is to analyze a database in order to extract 
useful correlations. As we have said the universes are equal, so the databases are identical and 
the person who has to do the work is the same.  The database that needs to be analyzed consists 
of a million parameters related to an event to be studied.  
 
In the universe ""1”, John acts as follows: he takes the whole database a nd calculates the 
correlation of the parameters with the event to be studied.  From this analysis he finds 50 
parameters with a high correlation with the event, the correlation found has a chance to happen 
randomly of 0.005%. Of these 50 parameters, John id entifies 10 that according to his experience 
can be useful in order to study the event. However it is important to point out that the assumptions 
made by John, on the 10 parameters, are only hypotheses based on his experience, they are not 
scientific demon strations that explain precisely the correlation of the 10 parameters with the 
event.  
 
In the universe ""2"", John acts in the following way: before analyzing the entire database he 
uses his knowledge of the event in order to select 10 parameters, that he be lieves are most 
correlated with the event, from the million parameters available. However, also in this case, it is 
important to point out that the assumptions made by John, on the 10 parameters, are only 
hypotheses based on his experience, they are not sc ientific demonstrations that explain precisely 
the correlation of the 10 parameters with the event.  Analyzing only these 10 parameters, he finds 
5 of them with a high correlation with the event, the correlation found has a chance to happen 
randomly of 0.00 5% (as in the previous case).  
 
In practice, the fundamental difference in the analysis method that John does in the two 
universes is that: in the first universe John uses his own experience after performing statistical 
analysis on the whole database, inste ad in the second universe, John uses his experience before 
to perform the statistical analysis in order to reduce the size of the database.  
 ",2021-12-27T14:48:03Z,t  be rd oit iit consequently  ined t   as t i from of   i  analyzi i  
paper_qf_4.pdf,13,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","13 
 Now let us see how this different approach affects the evaluation of the data obtained. To do 
this, we must calcula te the probability of obtaining the same results randomly in the two cases.  
 
In the first case, universe ""1"", in order to calculate the probability of obtaining the same results 
in a random way we must use the binomial distribution formula with the followi ng parameters:  
 
probability of victory (p) = probability of getting the same correlation randomly  
 
number of successes (k) = number of parameters that present the correlation considered  
 
number of tests (L) = total number of parameters present in the datab ase 
 
By entering these data within the binomial distribution formula:  
𝐹(𝑘,𝐿,𝑝)=(𝐿
𝑘)𝑝𝑘(1−𝑝)𝐿−𝑘  
p = 0.005%  
k = 50  
L = 1 Million  
We get a probability of 5.6% as a result.  
  
Now let's consider the second case, the universe ""2"", even in this situation, in order to calculate 
the probability of obtaining the same results in a random way we must use the binomial 
distribution formula with the following parameters:  
p = 0.005%  
k = 5  
L = 10  
The probability obtained in this case is 7.9 ∙10−18 % . 
 
Analyzing these  results it is easy to understand that a percentage of 5.6% makes the 
correlations found not significant. In order to understand how high this percentage is, we can 
also calculate the probability of obtaining, in a random way, more than 50 of parameters wi th 
the correlation considered, this probability is 46%.  
 
Now we analyze the percentage of the second case ( 7.9 ∙10−18 %) this percentage is 
extremely low, consequently we are practically certain that the correlation found is not random 
and therefore this r esult represents a useful information for studying the event.  
 
At this point, John must to decide whether to implement the correlations found or not. 
Obviously, exploiting the correlations found implies costs, therefore a wrong evaluation involves 
a high risk. In the universe “1” John is in a difficult situation, in f act the work done is not only 
useless but also dangerous because it can lead him to sustain wrong investments. Instead, in the 
second universe John knows that the probability that the correlation is random is almost zero, so ",2021-12-27T14:48:03Z, to iby ml  t analyzi i at  obvusly i instead 
paper_qf_4.pdf,14,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","14 
 he can invest with an acceptabl e risk.  
 
In conclusion, a simple procedural error has led to enormous consequences. In the first case 
the experience of john is useless, instead in the second case it was a key resource in order to 
extract useful information from a big database.  
 
In fact, in the case of the universe “1”, John can no longer use his own knowledge and the 
only thing he can do is transform his hypotheses into real scientific demonstrations, but in many 
situations, as in the financial field, doing it can be very difficult. Conse quently, when hypotheses 
are made after having carried out an analysis, these hypotheses risk being conditioned by the 
results and therefore lose value.  Instead, the hypotheses made before the analysis are not 
conditioned and the analysis of the data is used in order to verify them in a statistical way, as 
happened in the universe “2”.  
 
One of the fields where it is fundamental to calculate the probability of obtaining the same 
data in a random way, as a method of evaluating the correlations detected, is the financial one 
[1], [2].  
Conclusion  
In this article we have used a paradox to explain how a statistical datum does not represent a 
useful information, it becomes a useful information, to study an event, only when it is possible 
to prove that the probabi lity that it was obtained in a random way is very low. This consideration 
makes the application of statistics, as a method of evaluating a hypothesis, a ""relativistic"" 
science. In fact, as described in the paradox, the calculation of the probability of obt aining the 
same result in a random way is something of relative that depend from the method used and from 
the actions performed.  
 
These considerations have a great impact from an experimental point of view, because they 
teach us the importance of correct p lanning, in which we must always implement all the 
knowledge about the event we want to study. It is also essential keep track of all the operations 
performed on the data, because this information is necessary in order to calculate correctly the 
probabilit y of obtaining the same results in a random way.  
 
This way of interpreting statistical data is also very useful for understanding the phenomenon 
of overfitting, a very important issue in data analysis [3], [4].  The overfitting seen from this point 
of view is simply the direct consequence of considering the statistical parameters, and therefore 
the results obtained, as a useful information without checking  that them was not obtained in a 
random way . Therefore, in order to estimate the presence of overfittin g we have to use the 
algorithm on a database equivalent to the real one but with randomly generated values, repeating 
this operation many times we can estimate the probability of obtaining equal or better results in 
a random way. If this probability is hig h, we are most likely in an overfitting situation. For 
example, the probability that a fourth -degree polynomial has a correlation of 1 with 5 random 
points on a plane is 100%, so this correlation is useless and we are in an overfitting situation.  
 
This app roach can also be applied to the St Petersburg paradox [5], in fact also in this case ",2021-12-27T14:48:03Z,iii cons instead one conusi itse it  t trefore  for  st petersburg
paper_qf_4.pdf,15,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","15 
 the expectation gain is a statistical datum that must be evaluated before being used at the 
decisional level. In fact, the difficulty in solving the paradox stems from th e fact of considering 
a statistical datum always as a useful information. Analyzing the expectation gain it is possible 
to proof that we can obtain better result, randomly, with a probability that tends asymptotically 
to 50%. Consequently, the expectation gain that tends to infinity turns out to be a statistic data 
without value that cannot be used for decision -making purposes.  
 
This way of thinking gives an explanation to the logical principle of Occam's razor, in which 
it is advisable to choose the simple st solution among the available solutions. In fact, for example, 
if we want to analyze some points on a plane with a polynomial, increasing the degree increases 
the probability that a given correlation can occur randomly. For example, given 24 points on a 
plane, a second degree polynomial has a 50% probability of randomly having a correlation 
greater than 0.27, instead a fourth degree polynomial has a probability of 84% of having a 
correlation greater than 0.27 randomly. Therefore, the value of the correlat ion is an absolute 
datum but its validity to study a set of data is something relative that depends on the method 
used.  Consequently the simpler methods, being less parameterized, have a lower probability of 
a randomly correlation, so they are preferred ov er the complex methods.  
References  
[1] Andrea Berdondini, “Application of the Von Mises’ Axiom of Randomness on the Forecasts Concerning 
the Dynamics of a Non -Stationary System Described by a Numerical Sequence” (January 21, 2019). 
Available at SSRN: https://s srn.com/abstract=3319864 or http://dx.doi.org/10.2139/ssrn.3319864.  
[2] Andrea Berdondini, “Description of a Methodology from Econophysics as a Verification Technique for a 
Financial Strategy”, (May 1, 2017). Available at SSRN: https://ssrn.com/abstract=318478 1. 
[3] Igor V. Tetko, David J. Livingstone, and Alexander I. Luik, “Neural network studies. 1. Comparison of 
overfitting and overtraining”, Journal of Chemical Information and Computer Sciences 1995 35 (5), 826 -
833 DOI: 10.1021/ci00027a006.  
[4] Quinlan, J.R. (1986). “The effect of noise on concept learning”. In R.S. Michalski, J.G. Carbonell, & T.M. 
Mitchell (Eds.),Machine learning: An artificial intelligence approach(Vol. 2). San Mateo, CA: Morgan 
Kaufmann.  
[5] Andrea Berdondini, “Resolution of the St. Petersburg  Paradox Using Von Mises’ Axiom of Randomness” 
(June 3, 2019). Available at SSRN: https://ssrn.com/abstract=3398208.  
 
 
 
 
 
 
 
 ",2021-12-27T14:48:03Z,ianalyzi consequently  occur ifor trefore consequently referenc be rd oiapplicatvomisaxm randomness forecasts concerni dynamics nostatnary tem sibed numerical sequence uary  articial intellence   be rd oisiptmethodology eco no psics vericattechnique nancial strategy may  articial intellence  or te t did livistoalexanr lui neural arisojournal cmical informatuter sciencquint imichael  carbonyl mitcll eds machine avsamater morgakaufma be rd oiresolutst petersburg dox usi vomisaxm randomness june  articial intellence 
paper_qf_4.pdf,16,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","16 
 Use of the fundamental problem of statistics to define the validity limit 
of Occam's razor principle  
Andrea Berdondini  
ABSTRACT: In statistics, to evaluate the significance of a result, one of the most used methods is the 
statistical hypothesis test. Using this theory, the fundamental problem of statistics can be expressed as 
follows: "" A statistical data does not represent useful in formation, but becomes useful information only 
when it is shown that it was not obtained randomly "". Consequently, according to this point of view, among 
the hypotheses that perform the same prediction, we must choose the result that has a lower probability  of 
being produced randomly. Therefore, the fundamental aspect of this approach is to calculate correctly this 
probability value. This problem is addressed by redefining what is meant by hypothesis. The traditional 
approach considers the hypothesis as the set of rules that actively participate in the forecast. Instead, we 
consider as hypotheses the sum of all the hypotheses made, also considering the hypotheses preceding the 
one used. Therefore, each time a prediction is made, our hypothesis increases in co mplexity and 
consequently increases its ability to adapt to a random data set. In this way, the complexity of a hypothesis 
can be precisely determined only if all previous attempts are known. Consequently, Occam's razor principle 
no longer has a general va lue, but its application depends on the information we have on the tested 
hypotheses.  
Introduction  
The logical principle of Occam's razor [1], [2], suggests choosing the simplest hypothesis 
among those available. In this article, we will analyze this prin ciple using the theory of statistical 
hypothesis test [3], [4]. By exploiting this theory, we will reformulate the fundamental problem 
of statistics in such a way as to bring attention to the link between the statistical data and the 
probability that it wa s produced randomly. Consequently, according to this point of view, among 
the hypotheses that perform the same prediction, we must choose the result that has a lower 
probability of being produced randomly. Therefore, it becomes essential to calculate this 
probability value correctly.  
 
This problem is addressed by redefining what is meant by hypothesis. The traditional 
approach considers the hypothesis as the set of rules that actively participate in the forecast. 
Instead, we consider as hypotheses the sum of all the hypotheses made, also considering the 
hypotheses preceding the one used. Therefore, each time a prediction is made, our hypothesis 
increases in complexity and consequently increases its ability to adapt to a random data set. In 
this way, the com plexity of a hypothesis can be precisely determined only if all previous attempts 
are known. Consequently, Occam's razor principle no longer has a general value, but its 
application depends on the information we have on the tested hypotheses.  
 
Finally, we  use this new definition of hypothesis to understand the reason for the high 
percentage of non -reproducible results, in which the hypothesis test was used.  
The fundamental problem of statistics  
In statistics, to evaluate the significance of a result, one of the most used methods is the 
statistical hypothesis test. Using this theory, the fundamental problem of statistics can be ",2021-12-27T14:48:03Z,use occur  be rd oiiusi consequently trefore  t instead trefore iconsequently occur introdut occur iby consequently trefore  t instead trefore iconsequently occur nally t iusi
paper_qf_4.pdf,17,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","17 
 expressed as follows: "" A statistical data does not represent useful in formation, but becomes 
useful information only when it is shown that it was not obtained randomly "". 
 
This definition is particularly significant, because it highlights the two fundamental aspects 
of statistics, which are its uncertainty and the reason for its uncertainty.  Indeed, the purpose of 
statistics is the study of phenomena in conditions of uncertainty or non -determinism by 
exploiting the sampling of events related to the phenomenon to be studied. Knowing that the 
observed events can be randomly repr oduced with a probability that will never be zero, we 
understand the reason for the indeterminism that characterizes the statistics. This probability 
value is called universal probability [5].  
 
Through this definition of the fundamental problem of statisti cs, it is also possible to 
formulate the following paradox [6], which highlights how the evaluation of statistical results is 
dependent on each action performed on the analyzed data.  
The validity limit of Occam's razor principle  
In this paragraph, we will  see how the information regarding the development of a hypothesis 
is fundamental to define the validity limit of Occam's razor principle.  
Let us start by giving some definitions useful to formalize our theory.  
Given an experiment that measures N values of  a discrete variable X with cardinality C, we 
call D the set of dimension 𝐶𝑁, which includes all possible sequences 𝑋𝑁 of length N that can 
be observed.  
Now, we redefine the concept of hypothesis in order to define a chronological succession 
among the tested hypothesis.  
We call H(t) the hypothesis developed at time t. 
We call PH(t)  the set of sequences 𝑋𝑁∈𝐷 that the hypothesis H(t) is able to predict.  
We call NPH(t)  the cardinality of the set PH(t) . 
We call TH(t)  the set that includes all the hypothe ses up to time t. 
 𝑇𝐻(𝑡)={𝐻(𝑖1),𝐻(𝑖2),……,𝐻(𝑖𝑡)}  
We call TPH(t)  the union of all the sets PH(t)  relating to all the hypotheses H(t) ∈ TH(t) . 
𝑇𝑃𝐻 (𝑡)=⋃𝑃𝐻(𝑖)𝑡
𝑖=0 
We call NTPH(t)  the cardinality of the set TPH(t) . Consequently, NTPH(T)  defines the 
number of sequences, belonging to D, that the hypothesis TH(t)  is able to predict. It may happen 
that different hypotheses forecast the same sequence of values of X, having made the union of 
the sets PH(t)  these sequences are calculated only o nce. 
If we have only made a hypothesis H(t)=TH(t)  and NPH(t)=NTPH(t) . ",2021-12-27T14:48:03Z, ined ki  through t occur ioccur  give       consequently it 
paper_qf_4.pdf,18,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","18 
 If, on the other hand, more than one hypothesis has been tested  H(t)≠TH(t)  and 
NPH(t)≤NTPH(t) . 
We define the ability of the hypothesis TH(t)  to predict a sequence of N casual observation s 
of the i.i.d. random variable X with discrete uniform distribution and cardinality C, the ratio:  
 𝑁𝑇𝑃𝐻 (𝑡)
𝐶𝑁                                                                                                                                   (1) 
This ratio also defines the probability that the hypothesis TH(t)  can predict the results of an 
experiment, in which the cardinality of D is equal to 𝐶𝑁, in a completely random way.  
Knowing that a hypothesis TH(t)  can predict the results of an experiment  only in the following 
two conditions:  
1) TH(t)  is true.  
2) TH(t) is false and the prediction occurs randomly; the probability of this event is given by 
equation (1).  
Under these conditions, the probability that the hypothesis TH(t)  is true turns out to be:  
1−𝑁𝑇𝑃𝐻 (𝑡)
𝐶𝑁                                                                                                                           (2) 
Consequently, this equation defines the parameter that must be used in the evaluation o f H(t). 
So, if we want to compare two hypotheses H1(t)  and H2(t),  we have 4 possible results:  
1) NPH1(t)>NPH2(t)  and NTPH1(t)>NTPH2(t)  
2) NPH1(t)>NPH2(t)  and NTPH1(t)<NTPH2(t)  
3) NPH1(t)<NPH2(t)  and NTPH1(t)<NTPH2(t)  
4) NPH1(t)<NPH2(t)  and NTPH1(t)>NTPH2(t)  
NPH(t)  and NTPH(t)  define the number of sequences that hypothesis H(t) and the hypothesis 
TH(t)  are able to predict. Consequently, they can be used as a measure of their complexity, in 
fact, the more complex a hypothesis is, the greater the number of results it can predict.  
Analyzing the four possible results, we note that even if a hypothesis H1(t)  is less complex 
than a hypothesis H2(t)  (NPH1(t)<NPH2(t) ), it is possible to have a  hypothesis TH1(t)  more 
complex than a hypothesis TH2(t)  (NTPH1(t)>NTPH2(t) ). Consequently, using equation (2) as 
an evaluation method, hypothesis H1(t)  should be discarded in favor of hypothesis H2(t) . This 
situation can happen, for example, if H1(t)  is the last hypothesis of a long series of other 
hypotheses t ested previously.  
In the event that there is no information on the hypotheses to be evaluated, it must be assumed 
that the hypotheses have been developed under the same conditions. Therefore, in this case, not 
being able to calculate TH(t) , it is recommend ed to choose the simpler hypothesis H(t). 
Finally, from equation (2), we can deduce the following result: given a hypothesis H(t) the 
probability that is true can be calculated only if all the previously tested hypotheses are known.  ",2021-12-27T14:48:03Z,   ki unr consequently so consequently analyzi consequently  itrefore nally
paper_qf_4.pdf,19,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","19 
 Consequently, the complexity of a hypothesis not only depends on the mathematical formula 
that makes the prediction, but also depends on all the attempts made previously. Therefore, 
Occam's razor principle does not have an absolute value but its applicatio n depends on the 
information about the hypotheses.  
How to perform correctly the statistical hypothesis test  
It is interesting to note how the definition of hypothesis, which was given in the previous 
paragraph, can be seen as something extremely obvious or as something extremely innovative. 
Indeed, it may seem absolutely banal to consider all the hypotheses that have been tested, for the 
obvious reason that by running a large number of random hypotheses sooner or later there will 
be some hypothesis that w ill fit the data quite well. On the other hand, also considering the 
previous hypotheses represents a revolution in the evaluation of a hypothesis. In fact, from this 
point of view, the mere knowledge of the hypothesis that makes the prediction does not al low us 
to define its real complexity.  
 
Therefore, if in the statistical hypothesis test the p -value [7], [8], used as a threshold to reject 
the null hypothesis, is calculated considering only the hypothesis that actively participates in the 
prediction, it  means, that we are underestimating the complexity of the hypothesis. 
Consequently, the p -value, thus calculated, is wrong and therefore determines a false evaluation 
of the hypothesis. It is therefore believed that this systematic error, in the execution of the 
hypothesis test, is responsible for the high number of non -reproducible results [9], [10].  
 
Taking advantage of these considerations it is understood that evaluating a statistical result 
can be very difficult because some information can be hidden. For example, we are obliged to 
report the mathematical formula that makes the prediction but, instead, we may not report all 
previous failed attempts. Unfortunately, this information is essential for evaluating the 
hypothesis, because they are an integral part of the hypothesis. Indeed, if we test 10 hypotheses, 
we simply interpolate the data with those ten hypotheses and choose the hypothesis that passes 
the chosen evaluation test.  
 
This problem also depends on the increasing use of statistical software ca pable of quickly 
executing a huge number of mathematical models. Consequently, there is the risk of ""playing"" 
with this software by performing a multitude of analyzes and this sooner or later leads to a 
random correlation.  
 
For these reasons, the evaluati on of statistical results represents one of the most important 
challenges for scientific research. Unfortunately, it is a difficult problem to solve because, as 
mentioned, some information can always be hidden when writing an article. The simplest 
solution  adopted is to use more selective evaluation parameters, which in practice means making 
it unlikely to pass the evaluation test by developing random hypotheses. However, this solution 
has different problems in fact, in this way, we can discard correct hypo theses and cannot be 
applied to all fields of research. For example, in finance where the possible inefficiencies of the 
markets [11], which can be observed, are minimal, adopting very restrictive valuation methods 
means having to discard almost any hypoth esis. ",2021-12-27T14:48:03Z,consequently trefore occur how it ined oitrefore consequently it taki for unfortunately ined  consequently for unfortunately t for
paper_qf_4.pdf,20,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","20 
     
Conclusion  
In this article, we have discussed the logical principle of Occam's razor using the hypothesis 
test theory. This allowed us to reformulate the fundamental problem of statistics, in such a way 
as to make us understand the importance of correctly calculating  the probability of obtaining the 
same results randomly. Solving this problem involved redefining the concept of hypothesis. 
According to this point of view, by hypothesis we mean the sum of all tested hypotheses.  
Consequently, the complexity of a hypothe sis not only depends on the mathematical formula that 
makes the prediction but also depends on the previous hypotheses tested.  
 
Therefore, according to this approach, the logical principle of Occam's razor no longer has a 
general value if one considers as  a hypothesis only the set of rules that actively participate in the 
prediction. If, on the other hand, the hypothesis is considered as the sum of all the tested 
hypotheses, in this case, Occam's razor principle returns to have a general value.  
 
Finally, i t is noted that not considering all the tested hypotheses causes a systematic error in 
the application of the statistical hypothesis test. Therefore, it is hypothesized that this error, 
which leads to underestimate the complexity of a hypothesis, is the ca use of the high percentage 
of non -reproducible scientific results.  
References  
[1] Roger Ariew, “Ockham's Razor: A Historical and Philosophical Analysis of Ockham's Principle of 
Parsimony”, 1976.  
[2] Sober, Elliott (2004). ""What is the Problem of Simplicity?"". In Zellner, Arnold; Keuzenkamp, Hugo A.; 
McAleer, Michael (eds.). Simplicity, Inference and Modeling: Keeping it Sophisticatedly Simple. 
Cambridge, U.K.: Cambridge University Press. pp. 13 –31. ISBN 978 -0-521-80361 -8. Retrieved 4 August 
2012ISBN 0 -511-00748 -5 (eBook [Adobe Reader]) paper as pdf.  
[3] Fisher, R (1955). ""Statistical Methods and Scientific Induction"" (PDF). Journal of the Royal Statistical 
Society, Series B. 17 (1): 69 –78. 
[4] Borror, Connie M. (2009). ""Statistical decision making"". The Certified Quality E ngineer Handbook (3rd 
ed.). Milwaukee, WI: ASQ Quality Press. pp. 418 –472. ISBN 978 -0-873-89745 -7. 
[5] Cristian S. Calude (2002). “Information and Randomness: An Algorithmic Perspective”, second edition. 
Springer. ISBN 3 -540-43466 -6. 
[6] Berdondini Andrea, “The Information Paradox”, (July 8, 2019). Available at SSRN: 
https://ssrn.com/abstract=3416559.  
[7] Wasserstein, Ronald L.; Lazar, Nicole A. (7 March 2016). ""The ASA's Statement on p -Values: Context, 
Process, and Purpose"". The American Sta tistician. 70 (2): 129 –133. doi:10.1080/00031305.2016.1154108.  
[8] Hung, H.M.J.; O'Neill, R.T.; Bauer, P.; Kohne, K. (1997). ""The behavior of the p -value when the alternative 
hypothesis is true"". Biometrics (Submitted manuscript). 53 (1): 11 –22.  
[9] Munafò, M., N osek, B., Bishop, D. et al. “A manifesto for reproducible science”. Nat Hum Behav 1, 0021 
(2017). https://doi.org/10.1038/s41562 -016-0021.  
[10] Ioannidis, J. P. A. “Why most published research findings are false”. PLoS Med. 2, e124 (2005).  
[11] Black, F. (1971) “Ran dom Walk and Portfolio Management,” Financial Analyst Journal, 27, 16 -22. ",2021-12-27T14:48:03Z,conusioccur  solvi accordi consequently trefore occur  occur nally trefore referencroger rie ockham razor historical phosophical analysis ockham principle parsimony sober ell what problem simplicity iel lner arnold ke uz ena mp hugo mc peer michael simplicity inference moli keepi sophisticated ly simple cambridge cambridge   rieved august book adobe rear sr statistical methods scientic indujournal royal statistical society seriorr or ronnie statistical t certied quality handbook mwaukee quality  christiaca nu informatrandomness aalgorithmic speive  be rd oi t informatdox july  articial intellence  passer steironald lamar nicole mart statement valucontext process purpose t st hu ne bauer ohne t bmetric submied mubishhum be ha loaid is w lo med  rawalk tfmanagement nancial analyst journal
paper_qf_4.pdf,21,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","21 
 Resolution of the St. Petersburg paradox using Von Mises’ axiom of 
randomness  
Andrea Berdondini  
ABSTRACT: In this article we will propose a completely new point of view for solving one of the most 
important paradoxes concerning game theory. The method used derives from the study of non -ergodic 
systems. This circumstance may create a dependency between results that are often extremely difficult to 
detect and quantify, such as in the f ield of finance. Consequently, the expected gain obtained from data that 
may be correlated has a statistical value that is difficult to determine, thus it cannot be used for decision -
making purposes. Therefore, in this scenario, an alternative parameter to  be use during the decision -making 
process must be found. The solution develop shifts the focus from the result to the strategy’s ability to 
operate in a cognitive way by exploiting useful information about the system. In order to determine from a 
mathemat ical point of view if a strategy is cognitive, we use Von Mises' axiom of randomness. Based on 
this axiom, the knowledge of useful information consequently generates results that cannot be reproduced 
randomly. Useful information in this case may be seen as  a significant datum for the recipient, for their 
present or future decision -making process. In conclusion, the infinite behaviour in this paradox may be seen 
as an element capable of rendering the expected gain unusable for decision -making purposes. As a result, 
we are forced to face the problem by employing a different point of view. In order to do this we shift the 
focus from the result to the strategy’s ability to operate in a cognitive way by exploiting useful information 
about the system. Finally, by resolving the paradox from this new point of view, we will demonstrate that 
an expected gain that tends toward infinity is not always a consequence of a cognitive and non -random 
strategy. Therefore, this result leads us to define a hierarchy of values in d ecision -making, where the 
cognitive aspect, whose statistical consequence is a divergence from random behaviour, turns out to be 
more important than the expected gain.  
Introduction  
The St. Petersburg paradox represents one of the most important paradoxes in game theory. 
The classic solution used to solve uses special utility functions that implement the concept of 
marginal utility [1], [2], [3]. This type of approach has been strongly criticized in virtue of the 
fact that utility functions attempt to forma lize sociological behaviour from a mathematical point 
of view, and this is why they always have a subjectivity component. Moreover, many studies of 
behavioural economics [4], [5] highlight how people’s behaviour is often irrational. 
Consequently, the resol ution of this paradox still represents an open challenge and, as we will 
see, the search for an alternative solution may help us improve the decision -making process 
during the evaluation of a strategy.  
 In order to understand the method proposed in this ar ticle for resolving the St. Petersburg 
paradox, we must first explain the origins of this method. This approach was developed to study 
the strategies operating on non -ergodic systems. In particular, the primary field of application is 
characterized by the study of quantitative trading algorithms operating on financial markets. The 
non-ergodicity condition can make the results dependent on each other. Therefore, in this 
scenario a dependency is created between data which, like in the field of finance, is dif ficult to 
detect and quantify.  
 To explain to you the possible consequences at a decision -making level of this condition, I 
propose the following example: think about making a hundred bets on a hundred flips of a coin 
and winning one hundred times. In this  case, you will have obtained a hundred victories 
independent of each other to which a very high -expected gain will be associated. You therefore 
reach the right conclusion that the strategy used to predict the flip of the coin is most likely ",2021-12-27T14:48:03Z,resolutst petersburg vomis be rd oiit  consequently trefore t ivomisbased useful ias inally trefore introdut st petersburg t  oconsequently ist petersburg  it trefore to iyou
paper_qf_4.pdf,22,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","22 
 correct. Now l et's change the starting scenario and let's say we make 100 equal bets on a single 
coin flip, getting a hundred wins. Obviously, since the bets are completely dependent on each 
other, in this case they cannot be used to calculate the expected gain. In fact , it’s basically as if 
we made a single bet. Now let’s imagine, as a third and last scenario, that you are not able to see 
the person flipping the coin: if we win all the hundred times, we don’t know if they are dependent 
or independent of each other. This  third scenario generates a very important decision -making 
problem, because if I consider the results as independent and they are not, I risk overestimating 
the strategy. This wrong assessment can lead me to make the irrational choice of using a useless 
strategy. In finance the non -independence of the results creates a statistical phenomenon called 
clustering. This statistical characteristic determines the formation of groups of high returns, 
alternating with groups of low returns. In other words, it means that returns are not distributed 
evenly but tend to cluster together. The clustering phenomenon has disastrous effects in finance, 
because when you are going through winning phases you are led to consider the operations 
carried out as independent of each o ther. This implies that the expected gain, calculated from 
data that we mistakenly think to be independent, is overestimated, therefore the evaluation of the 
strategy will also be incorrect. So, this behaviour can subject us to unexpected risk. With regard s 
to this topic we have developed a paradox [6], which we have called “the professional trader’s 
paradox”. The name derives from the fact that we are inclined to consider that our operations are 
always independent, and therefore, when we face a series of w inning bets, we tend to 
overestimate the strategy used.  
 
 In conclusion, the expected gain, obtained from data that may not be independent, cannot be 
used for decision -making purposes, as it has a statistical value that is difficult to determine. 
Therefore , from this example we understand that there are situations, like in non -ergodic 
systems, where the expected gain is no longer a reliable parameter. Consequently, we can think 
that other situations may exist, like in the case of the infinite behaviour of t his paradox, where 
the expected gain is a datum that cannot be used in decision -making.  
 We begin to understand that the problem in resolving the St. Petersburg paradox may derive 
from considering the expected gain, and its variants (utility functions), as  the only possible point 
of view in the evaluation of a strategy. So, the question we have to ask ourselves is: is there a 
parameter that is better than the expected gain?  
 The answer we give to this question is focused on being able to understand, from a statistical 
point of view, if a strategy operates in a cognitive way by exploiting useful information present 
on the system. The useful information in this case can be seen as a datum subject to analysis that 
rendered it significant to the recipient for th eir present or future decision -making process. To 
determine mathematically if a strategy is cognitive, in the sense just described, we exploit the 
Von Mises' axiom of randomness. The axiom defines the statistical characteristic that must have 
a sequence in  order to be considered random. The axiom is the following: ""the essential 
requirement for a sequence to be defined as random consists in the complete absence of any rules 
that may be successfully applied to improve predictions about the next number "".  
 The meaning of this axiom is the following: when we understand a set of rules to which a 
numerical sequence is subject we can obtain results, intended as forecasts on the next number of 
the sequence, whose probability of being reproduced randomly tends towar d zero on increasing 
the number of forecasts made.  
 Consequently, the results obtained with a game strategy that implements information useful 
to improve our probability of winning, generates results that cannot be reproduced randomly. 
Basically, the proba bility of obtaining better results with a random strategy compared to a ",2021-12-27T14:48:03Z, obvusly i   i it  so with t itrefore consequently  st petersburg so t t to vomist t t consequently basically
paper_qf_4.pdf,23,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","23 
 cognitive strategy, which implements useful information, tends toward zero as the number of 
predictions made increases.  
This axiom is indeed a statistical method for evaluating the res ults obtained, without taking 
into consideration the absolute value of the expected gain. In fact, this method is based solely on 
the fact of being able to discriminate whether the results were obtained with a random strategy 
or through a cognitive strateg y that implements a set of rules to which the system is subject.  
 In this way we obtain a fundamental result in analysing the strategies of a particular class of 
zero-sum games, where there is a balance between the participants. Balance means the situation  
where none of the players has an implicit advantage over the others. The famous mathematician 
Daniel Bernoulli defined this particular class of games as: “mathematically fair game of chance”.  
This type of game plays a particularly important role in game t heory, because it represents a 
very frequent situation in various fields of interest such as finance.  
 
 If we analyse the results obtained by repeating the game of chance described in the St. 
Petersburg paradox a large number of times, in the next paragrap h we will demonstrate that better 
profits can be obtained with a probability that tends toward 50% by using a random strategy. In 
practice, the results of a purely random game strategy tend to be distributed symmetrically with 
respect to the expected theor etical gain derived from the strategy described in the paradox. This 
result indicates that the doubling -down strategy after each lost bet does not exploit any kind of 
useful information, and therefore it is a completely non -cognitive game method. Consequen tly, 
by taking the cognitive aspect as a parameter to be used in decision -making, and having 
demonstrated the complete absence within the strategy, we are able to solve the paradox by 
proving the irrationality of the game method.  
 
 In this article, we wan t to introduce the cognitive aspect, understood in the sense of acting in 
a non -random way by exploiting useful information about the system, as a fundamental element 
for improving the decisions theory. In fact, this paradox is useful to make us understand  that the 
knowledge of useful information about the system, capable of increasing our probability of 
victory, always involves an increase of the expected gain. However, the opposite is not true: an 
expected gain that tends to infinity does not imply that t he strategy exploits knowledge about the 
system and therefore is cognitive and not random. Consequently, a hierarchy of values is created, 
where the cognitive aspect is more important than the expected gain for decision -making 
purposes.  
Resolution of the St. Petersburg paradox  
In this paragraph, we will solve the St. Petersburg paradox by demonstrating that the 
doubling -down strategy after each lost bet is a non -cognitive strategy, which implements no 
useful information that can be used to improve the prob ability of success.  
In order to do this we have to define the random strategy, which we will use to calculate the 
probability of obtaining better results than those obtained with the gambling method defined in 
the paradox. In fact, as mentioned in the previous paragraph, this probability should tend to zero 
if the strategy being evaluated is a cognitive strategy that implements useful information. Firstly, 
we define some parameters that are fundamental to characterize our random strategy of 
reference.  
The first parameter we need is the expected value EV of the game obtained by using the ",2021-12-27T14:48:03Z, iibalance t daniel bernoulli   st petersburg i cons que iiconsequently resolutst petersburg ist petersburg iirstly t
paper_qf_4.pdf,24,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","24 
 doubling -down strategy after each bet lost. Given a number of flips equal to L, with a 50% 
probability of winning and placing the value of the first bet equal to 1, we ha ve: 
𝐸𝑉=𝐿
2 
The second parameter is the average bet AB. By carrying out L bets of Bn value, we have:  
𝐴𝐵=(𝐵1+𝐵2…+𝐵𝐿)
𝐿 
Knowing that the first bet B1 is equal to 1, and double downing after every bet lost and 
returning to value 1 when we win the bet, we have:  
𝐴𝐵=∑(𝐿−𝑛+1)2𝑛−1
2𝑛𝐿𝐿
𝑛=2+1 
𝐴𝐵=∑(𝐿−𝑛+1)
2𝐿𝐿
𝑛=2+1 
𝐴𝐵=𝐿(𝐿−1)
21
2𝐿+1 
𝐴𝐵=𝐿−1
4+1 
At this point the random game strategy will be defined as follows: given a number of flips 
equal to L, L bets of AB constant value will be made, randomly choo sing whether to bet heads 
or tails on each bet. To calculate whether a strategy of this type can obtain better results compared 
to the expected value EV of the strategy of the paradox, just use the binomial distribution 
formula.  
𝐹(𝑘,𝐿,𝑝)=(𝐿
𝑘)𝑝𝑘(1−𝑝)𝐿−𝑘  
P = probability of winning  
K = number of wins  
L = number of tosses  
By using the binomial distribution formula, given a value of L, we can obtain the probability 
of achieving better results with the random strategy described above. The results for the  L values 
ranging from 10 to 200 are shown in Figure 1. Looking at the figure, we see how the probability 
tends asymptotically toward 50%. Therefore, we have a 50% chance of getting better or worse 
results. Basically, the strategy described in the paradox tends asymptotically toward a random 
strategy. Consequently, the doubling -down strategy turns out to be a strategy that does not 
implement useful information for improving our likelihood of victory. Thus, using the cognitive 
aspect as a method of evaluatio n has proven the irrationality of the strategy.  
 
 We use very similar approaches, where the strategy being evaluated is compared with an 
equivalent random strategy, in the financial field to analyse the results generated by a trading ",2021-12-27T14:48:03Z,givet by bki at to by t  looki trefore basically consequently  
paper_qf_4.pdf,25,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","25 
 strategy [7], [8].  
 
 
FIG. 1 : Probability of obtaining better results with a random strategy, given a number of L 
tosses ranging from 10 to 200.  
Conclusion  
In this article we use the St. Petersburg paradox to introduce a parameter related to the 
cognitive aspect of a strategy, as a fundamental element to help our decision -making in all those 
situations where the expected gain turns out to be an unreliable parameter. This approach was 
developed by studying non -ergodic systems. In this scenario the results can be non -independent, 
so the expected gain becomes a parameter with a statistical value that is difficult to determine. 
Therefore, it cannot be used in decision -making and a new parameter needs to be found. The 
parameter chosen is related to the strategy’s ability to operate in  a cognitive way (the cognitive 
term indicates the strategy’s ability to operate in a non -random way by exploiting useful 
information about the system, capable of making us increase the probability of victory).  
  
 To determine mathematically if a strategy is cognitive, we used the von Mises' axiom of 
randomness. Based on this axiom, strategies that implement useful information about the system 
generate results that cannot be reproduced randomly. Thus, we compared the paradox strategy 
with a completely rando m but equivalent strategy from the point of view of the total betting 
value. From this comparison, we have demonstrated that the random strategy gets better results 
with a probability that tends toward 50% as the number of tosses increases. Basically, the strategy 
",2021-12-27T14:48:03Z,probabity conusist petersburg  itrefore t to misbased  from basically
paper_qf_4.pdf,26,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","26 
 tends to converge to a random strategy instead of diverging as we would expect from a cognitive 
strategy.  In fact, if a strategy implements useful information on the system, the probability of 
randomly obtaining better results tends toward zero. T his result indicates that the doubling -down 
strategy after each lost bet is not a cognitive strategy that exploits useful information about the 
system, and therefore by taking the cognitive aspect as an evaluation parameter we have solved 
the paradox.  
 
 In conclusion, the St. Petersburg paradox teaches us that an expected gain that tends toward 
infinity does not always imply the presence of a cognitive and non -random strategy. Thus 
knowledge, meaning the exploitation of useful information capable of making us increase the 
probability of victory, always implies an increase in the expected gain, but the opposite is not 
true; an expected gain that tends toward infinity can also be obtained in the absence of knowledge 
about the system. Consequently, from the dec ision -making aspect we can create a hierarchy of 
values, where knowledge is more important than the expected gain. In fact, the expectation of 
victory can be difficult to estimate as in the case of non -ergodic systems or be a non -useful data 
if the develop ed strategy has a high degree of overfitting. In all these cases the calculation of the 
probability of obtaining the same results randomly becomes a much more reliable parameter, 
since this datum is influenced only by the real knowledge we have of the syst em and not by the 
noise. In fact, a statistical datum does not represent a useful information, but becomes a useful 
information only when it is possible to proof that it was not obtained in a random way. In practice, 
the probability of obtaining the same r esult randomly must be very low in order to consider the 
result useful.  
References  
[1] Paul Samuelson,  (March 1977). ""St. Petersburg Paradoxes: Defanged, Dissected, and Historically 
Described"". Journal of Economic Literature. American Economic Association. 15  (1): 24 –55. JSTOR 
2722712.  
[2] Robert J.Aumann,  (April 1977). ""The St. Petersburg paradox: A discussion of some recent comments"". 
Journal of Economic Theory. 14 (2): 443 –445. doi:10.1016/0022 -0531(77)90143 -0. 
[3] Robert Martin, (2004). ""The St. Petersburg Parado x"". In Edward N. Zalta. The Stanford Encyclopedia of 
Philosophy (Fall 2004 ed.). Stanford, California: Stanford University. ISSN 1095 -5054. Retrieved 2006 -
05-30. 
[4] Daniel Kahneman and Amos Tversky (1979) “Prospect Theory: An Analysis of Decision under Risk”,  
Econometrica, 47(2), 263 – 291. 
[5] Hersh Shefrin, (2002) “Beyond Greed and Fear: Understanding behavioral finance and the psychology of 
investing”. Oxford University Press.  
[6] Andrea Berdondini, “The Professional Trader’s Paradox”, (November 20, 2018). Availabl e at SSRN: 
https://ssrn.com/abstract=3287968.  
[7] Andrea Berdondini, “Application of the Von Mises’ Axiom of Randomness on the Forecasts Concerning 
the Dynamics of a Non -Stationary System Described by a Numerical Sequence” (January 21, 2019). 
Available at SSRN : https://ssrn.com/abstract=3319864 or http://dx.doi.org/10.2139/ssrn.3319864.  
[8] Andrea Berdondini, “Description of a Methodology from Econophysics as a Verification Technique for a 
Financial Strategy”, (May 1, 2017). Available at SSRN: https://ssrn.com/abstract=3184781.  
 
 ",2021-12-27T14:48:03Z,iist petersburg  consequently iiiireferencpaul samuelsomarst petersburg dox raed disseed historically sibed journal economic re economic associatrobert au manapr t st petersburg journal economic tory robert martit st petersburg  do iedward alta t stanford encyopedia phosop fall stanford calornia stanford  rieved daniel kane maamos o prospe tory aanalysis cisrisk econometric r sh s fr ibeyond greed fear unrstandi oxford    be rd oit professnal trar dox november  articial intellence abl  be rd oiapplicatvomisaxm randomness forecasts concerni dynamics nostatnary tem sibed numerical sequence uary  articial intellence   be rd oisiptmethodology eco no psics vericattechnique nancial strategy may  articial intellence 
paper_qf_4.pdf,27,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","27 
 Statistics the science of awareness  
Andrea Berdondini  
ABSTRACT:  The uncertainty of the statistical data is determined by the value of the probability of 
obtaining an equal or better result randomly. Since this probability depends on all the actions performed, 
two fundamental results can be deduced. Each of our random a nd therefore unnecessary actions always 
involves an increase in the uncertainty of the phenomenon to which the statistical data refers. Each of our 
non-random actions always involves a decrease in the uncertainty of the phenomenon to which the 
statistical data refers.   
Introduction  
This article proves the following sentence:  
""The only thing that cannot be created randomly is knowledge "" 
A true story of a true coincidence  
Ann is a researcher, is a clever and beautiful researcher, one day she decides to do the following 
experiment: she wants to understand if she has some special abilities that allow her to extract the 
number 1 from a bag containing one hundred different numbers mixed in a random way.  
Day 1, Ann takes the bag and randomly pulls out a number. The drawn number is not 1, so she 
failed, the drawn number is put back into the bag.  
Day 2, Ann takes the bag and randomly pulls out a number. The drawn number is not 1, so she 
failed, the drawn number is put back into the bag.  
……………  
……………  
The days pass an d Ann fails every attempt but continues his experiment.  
……………  
……………  
Day 100, Ann takes the bag and randomly pulls out a number. The number drawn is 1, so she is 
successful. The probability of finding number 1 by performing a random extraction is 1/100; th is 
value represents an acceptable error that makes the result significant to support the hypothesis that 
the extraction is non -random. But Ann knows that this probability does not represent the uncertainty 
of her success, because this value does not take i nto account previous attempts. Therefore, she 
calculates the probability of randomly extracting the number 1, at least once, in a hundred attempts. 
The probability value thus calculated is 63%, since this value is very high, she cannot consider the 
success  obtained as significant statistical data to support the hypothesis that the extraction is non -
random.  
Ann concludes the experiment and deduces, from the results obtained, that she has no special 
ability and the extractions are all random.  
 
John is a dat a scientist, one day he is entrusted with the following task: he has to develop an 
algorithm capable of predicting the result of an experiment whose result is determined by a value 
from 1 to 100.  ",2021-12-27T14:48:03Z,statistics  be rd oit since eaeaintrodu t anday ant day ant t anday ant t but antrefore t an
paper_qf_4.pdf,28,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","28 
  Day 1, an incredible coincidence begins, at the same time t hat Ann pulls a number John tests his 
own algorithm. The number generated by the algorithm does not coincide with the result of the 
experiment, so he failed.  
Day 2, the incredible coincidence continues, at the same time that Ann pulls a number John tests 
a new algorithm. The number generated by the algorithm does not coincide with the result of the 
experiment, so he failed.  
……………  
……………  
The days pass, the coincidence continues and John fails every attempt.  
……………  
……………  
Day 100, the incredible coincidence cont inues, at the same time that Ann pulls a number John 
tests his new algorithm. The number generated by the algorithm coincides with the result of the 
experiment, so he is successful. The probability of predicting the result of the experiment by running 
a random algorithm is 1/100; this value represents an acceptable error that makes the result 
significant to support the hypothesis that the algorithm used is non -random.  
For this reason, John writes an article in which presents the result obtained. The article  is 
accepted, John is thirty years old and this is his hundredth article.  
Awareness breeds awareness  
We call ""researcher"" a person who knows only his own attempts regarding the study of a certain 
phenomenon.  
We call ""reviewer"" a person who does not actively participate in the study of a particular 
phenomenon but knows every single attempt made by each researcher.  
Researcher 1: develops an algorithm that obtains the result R1 with respect to a phenomenon F. 
The probability of getting a result equal to  or better than R1 in a random way is 1%.  
Researcher 2: develops an algorithm that obtains the result R2 with respect to a phenomenon F. 
The probability of getting a result equal to or better than R2 in a random way is 1%.  
Reviewer: defines a new result RT= R1∩R2. The probability of getting a result equal to or better 
than RT in a random way is 0.01%. Consequently, the uncertainty of the result RT is 0.01%.  
The absence of awareness reduces awareness  
We call ""researcher"" a person who knows only his own a ttempts regarding the study of a certain 
phenomenon.  
We call ""reviewer"" a person who does not actively participate in the study of a particular 
phenomenon but knows every single attempt made by each researcher.  
Researcher 1: develops an algorithm that obta ins the result R1 with respect to a phenomenon F. 
The probability of getting a result equal to or better than R1 in a random way is 1%.  
Researcher 2: develops an algorithm that obtains the result R2 with respect to a phenomenon F. 
The probability of getti ng a result equal to or better than R2 in a random way is 100%.  
Reviewer: defines a new result RT= R1∩R2. The probability of getting a result equal to or better 
than RT in a random way is 2%. Consequently, the uncertainty of the result RT is 2% . 
 ",2021-12-27T14:48:03Z,day an t day an t t  day an t t for  t  awareness   researcr t researcr t revier t consequently t   researcr t researcr t revier t consequently
paper_qf_4.pdf,29,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","29 
 Quantita tive trading  
 
In this chapter, we will apply the statistical concepts discussed previously in the development of 
a quantitative trading system.  
  
The first article “Description of a method of econophysics as a technique for verifying a financial 
strategy” deals with all the fundamental topics in the development of a quantitative trading system. 
Indeed, in addition to evaluating a trading strategy, this article also talks about many other aspects, 
such as the development of control methods on a running strat egy. 
 
The successive articles concern the non -ergodicity of the financial markets. In this situation, the 
data can be non -independent, so their statistical significance is difficult to define. Under these 
conditions, the risk associated with a trading algo rithm is defined by its ability to predict the 
evolution of the system.  Whenever a correct forecast is made on an evolution of the system, this 
forecast generates data that is independent of the previous data. From a statistical point of view, the 
independ ence of the data determines a lower probability of obtaining an equal or better result 
randomly. Consequently, this probability value is indicative of the number of correctly predicted 
system evolutions. As we have seen in the statistics articles, this par ameter also represents the 
uncertainty of the statistical data.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 ",2021-12-27T14:48:03Z,quant it it siptined t iunr wnefrom consequently as
paper_qf_4.pdf,30,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","30 
 Description of a methodology from Econophysics as a verification 
technique for a financial strategy  
 
Andrea Berdondini  
 
 ABSTRACT.   In this article, I would like to draw attention to a method inspired by the analysis of stochastic models 
typical of quantum physics, and to utilise it to test a financial strategy. We start from the principal question asked by 
anyone involved in financi al investments: Are the results obtained due to a correct interpretation of the market, or are 
they merely fortuitous? This is a plain question and it can be given an equally clear answer. The results obtained are 
due to a correct interpretation of the mar ket if the probability of obtaining equal or better results randomly is very 
small (i.e. tends to zero as the number of times the strategy is used increases).  
 
Description of the methodology  
    
   The logic underlying this method is very simple in essence: it consists in calculating the 
probability of obtaining the same results randomly. As we will see in the examples below, this 
technique is applied not only to results from a trial phase: it is also applied as a control method 
when the strategy is used on a real trading account.  
   In what follows, I present a short logical proof of the soundness of this method. The term 
‘soundness’ was introduced by the famous mathematician David Hilbert and is used to indicate the 
absence of any contradiction within a mathematical logical proof. Indeed, contradiction is one of 
the main defects of methods of analysis based on equity line (performance) assessment.  
   The short demonstration I’m going to outline is  based on two fundamental axioms:  
1) Whenever we understand any kind of deterministic market process, the probability of our 
financial operation being successful increases by more than 50% (Von Mises’ axiom of disorder 
from the early 1920s).  
2) The probability  of randomly obtaining a result that has been obtained through cognitive 
awareness of a deterministic market process tends to zero as the number of times the strategy is 
used increases.  
   The first axiom is derived from the famous “axiom of randomness” (or the ‘principle of the 
impossibility of a gambling system’) formulated by the mathematician Von Mises, whose original 
definition I quote: “the essential requirement for a sequence to be  defined as random consists in the 
complete absence of any rules that may be successfully applied to improve predictions about the 
next number”.  
   As a consequence of the two axioms given above, any correct market analysis will always tend 
to increase the  probability of our prediction beyond the 50% mean, and this results in a consequent 
decrease in the probability of obtaining the same result randomly.  
   I will demonstrate this to you with a simple example. Suppose we are playing heads or tails with 
a rigged coin that gives us an above -50% probability of winning (let’s say it’s 60%). What is the 
probability of losing out after 10 coin tosses? Approximately 16.6% ...and after 50 tosses? 
Approximately 5.7% ...and after 100 tosses? Approximately 1.7%. As you  can see, the probability 
tends to zero, and here the rigged coin represents a financial strategy that is implementing a correct ",2021-12-27T14:48:03Z,sipteco no psics  be rd oii are  t siptt as it did gbert ined t wnevomist t vomisas suppose what approximately approximately approximately as
paper_qf_4.pdf,31,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","31 
 market analysis.  
   By basing our method of assessment specifically on the calculation of this probability, we develop 
a method  that is by definition free of contradictions. The absolute value of the probability turns out 
to be a sound estimate of the validity of our strategy.  
   The term ‘deterministic process’ which I used during the proof refers to the utilisation of a correct 
financial strategy, definable as the identification of a deterministic and non -random component that 
regulates the system we are studying (in our case, a financial market).  
   Methods based on studying the equity line may produce a positive outcome and at the same time 
have a 50% probability of obtaining the same amount of profit by chance. In this way, such methods 
lead to a contradiction, given that obtaining the same outcome randomly implies the absence of a 
cognitive process, which is just what is meant  by assuming a “correct interpretation of the market”.  
  These kind of methods are often based on the market stationary hypothesis (ergodic hypothesis). 
This hypothesis is considered by many experts not correct, on this topic have been written many 
article s the most famous is that written by the Nobel prize for physics Gell -Mann [1], two other 
interesting articles on this topic are [2], [3].  
   Figure 1 shows an equity line obtained with a purely random strategy. The algorithm is defined as 
follows: Each day you toss a coin to decide whether to open a buy position on the Nasdaq Index.  
The interval time chosen is from 1 January 2016 to 31 July 2017. If the position is opened, you toss 
another coin the next day to decide whether to close it or leave it open. As you can appreciate, this 
strategy functions in a completely inane and random way. Nevertheless, the equity line achieved is 
satisfactory:  indeed, if we calculate the probability of obtaining an equivalent or better result 
randomly, we get a probability of approximately 50%. We therefore know that the result is void of 
significance, in spite of the equity line.  
   To conclude, it follows tha t the parameter to be linked to the validity of a financial strategy is not 
its performance but its statistical property of generating non -reproducible results in a random way.  
 
 
Figure 1: shows the equity line of a purely random trading system in which operations are opened and closed by simulating the toss 
of a coin.  0100200300400500600700800900
0 50 100 150 200 250 300 350 400 450Profit
Number of trades",2021-12-27T14:48:03Z,by t t methods itse  nobel gel man t eanasdaq inx t uary july  as nevertless  to  prot number
paper_qf_4.pdf,32,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","32 
  
 
Description of the techniques used in operational practice of this verification method  
 
   How does one calculate the probability of randomly generating an equivalent or better 
performanc e? There are two ways: the first (and more precise) is to estimate this probability using 
the Monte Carlo method. The accuracy of this method is linked to the number of times we carry out 
the simulation. Its strength is the ability to obtain very precise v alues: its drawback is due mainly to 
the long calculation times required to obtain the estimate of probability.  
   The second method (which I have identified) uses exact formulae taken from statistics; each 
formula is applied to a particular class of random variables. Unfortunately, financial operations do 
not fall under any type within this class of variables. This  problem is solved by applying a transform 
to the financial operations, which renders them suitable for the chosen analytical formula.  
   This transform adds an error to the calculation of our probability, but it has the advantage of being 
calculable in on e single equation and therefore without requiring massive computational resources, 
as do the Monte Carlo methods.  
 
 
Use of the method as a control parameter of a strategy  
 
   This method is utilised not just during the test phase, but is also extremely use ful as a way of 
monitoring the trading system. Each time we carry out an operation, we update the probability value 
for obtaining that result randomly. We do not calculate this probability across the whole sample of 
operations conducted, but extrapolate it  from to the N most recent operations. The optimal value for 
N depends on the strategy, and in particular on the frequency with which operations are conducted 
over time. Once N has been set, we calculate our probability and compare it with a probability we  
have established that represents the level of risk we take to be acceptable (this definition of risk will 
be explained in a separate section). If the probability value exceeds the parameter set by us, the 
trading system locks itself and continues trading in virtual mode only. When the probability falls 
below the threshold parameter we have set, the trading system resumes actual trading. In this way, 
trades are effected only when the market is understood, and the trading system is blocked when we 
are operat ing in a regime considered to be random.  
   This method is much more efficient than the usual performance -based methodologies; such 
methods carry the risk of allowing themselves to incur unnecessary losses. It may happen with this 
method that the strategy is blocked even when trading at a profit, given that a random regime has a 
50% probability of success.  
   Having said this, obviously a trading system will have its internal performance controls, but their 
purpose is purely to monitor for possible system c rashes or any programming bugs.  
   What we have described thus far can be fine -tuned. A characteristic of all good quantitative trading 
systems is to be capable of operating even at high frequencies while leaving unchanged the logical 
schema on which the trading system is based. This ena bles us to run a trading system solely as a 
method of monitoring (hence in virtual mode), at a very high frequency of operations, and to obtain 
thereby a much more numerous statistical sample in less time, increasing the reactivity of our method 
of control . ",2021-12-27T14:48:03Z,sipthow tre   t its t unfortunately     use  ea t once  wi it hi what 
paper_qf_4.pdf,33,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","33 
  Use of the method to in the process of developing a financial strategy  
    
  This approach has another great merit, which is to help us direct our research in the right direction. 
Let us assume we develop two strategies:  
1) The first has profits on a hist oric series of 10% annuities, but with a very high probability of 
obtaining the same results randomly;  
2) a second, instead, has low profits of 1%, but with an extremely low probability of obtaining the 
same results randomly.  
   It is perfectly obvious that,  if we follow the theory we have expounded, we will discard the first 
strategy, as there is a very high probability that the 10% profit has been obtained by mere chance. 
The second strategy yields low profits but the low probability value obtained means we  are on the 
right track for understanding a deterministic and non -random market process (which, if studied more 
closely, could lead to more profitable financial strategies).  
   If we had not applied this method, we might have thought that the first strategy (with higher gains) 
was the right one. But this would have been at risk of losing money over the medium to long term. 
We would have ended by discarding the second strategy and missing an opportunity to study and 
understand something important we had sensed about the market.  
 
Example of the use of this methodology  
 
      I report the following practical example; the figure shows the trend of a hypothetical stock. The 
value of this stock on a thousand time intervals rises one unit 60% of the time and goes down by one 
unit 40% of the time. In order to simplify the calculatio ns the price movement is unitary. Now let us  
look at two strategies that execute 500 trades, each trade lasts an interval of time. The first strategy 
execute only buys  and in order to choose when to buy, flips a coin, if it win opens a trades if loses 
it waits for the next unit of time and repeats the operation. The second strategy, on the other hand, 
is a strategy that sells only, but does not do it in a random way, it uses information that allows it a 
10% advantage in determining the drops of the value s tock.  
   The first strategy gets a profit of 100 by winning 60% of trades. Now we calculate the probability 
of obtaining a better result in a random way, to do this we use the binomial cumulative distribution 
function with the following parameters:  
p = 60%  (probability of win)  
k > 300 (number of wins)  
L = 500 (total number of tests)  
   The probability of victory is 60% because in the graph shown the value of the stock rises by one 
unit 60% of the time and goes down by one unit 40% of the time.  
   Using thes e data, we get a probability to get better results randomly of the 48.3%.  
   Now let us consider the second strategy, this strategy has a total result of zero in practice it 
performs 250 winning trades and 250 losing trades (the 10% advantage allows it to increase the 
probability of victory from 40% to 50%). Also in this case, we calculate the probability of obtaining 
better results randomly, to do this we use the formula of the binomial cumulative distribution 
function with the following parameters:  ",2021-12-27T14:48:03Z,use   t it t  but   t i t t t  t usi  also
paper_qf_4.pdf,34,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","34 
 p = 40 % 
k > 250  
L = 500  
   Using these data, we get a probability to get better results randomly of the  2.5∙10−4 %.  
 
 
   Analyzing these results it is clear that the profit of 100 made by the first strategy is not significant 
and this evaluation is correct because we know that the strategy flip a coin in order to decide whether 
to open a buy position. The second strategy does not make any profit but such a low probability 
makes this result significant and even this evaluation is correct, in fact we know that this strategy 
implements useful information that allows it to increase the probability of victory by 10%.  
   What would have happened if we had used the competition evalua tion method? We will have 
discarded the second strategy and reward the first strategy, a completely random strategy. Where is 
the error? The error derives from considering result as an always useful element for future profits 
instead the fundamental elemen t to be winning is to be able to understand the rules to which a system 
is subject, in this case the financial market. This knowledge allows us to act in a non -random way 
and this feature can be detected only if we calculate the probability of obtaining be tter results at 
random.  
 
A new definition of risk  
 
   There are many definitions of risk in the financial field: risk is in any case seen as the probability 
(uncertainty) of incurring a given loss. If we recall the statistical example given above of tossin g the 
rigged coin, we can see how risk is intimately linked to our understanding of the market and as such 
how it tends to zero the more times we repeat the statistical experiment described above.  
   The value of this probability can never be zero: think,  for example, of the actions we perform in 
our daily lives, actions that all have a certain level of risk – understood as the probability of bringing 
about a negative event. We take these actions into consideration nevertheless, because we know that 
Figura 2: trend of a stock that rises 60% of the time and falls the remaining 40%.  ",2021-12-27T14:48:03Z,usi analyzi t what  wre t  tre  t   guru
paper_qf_4.pdf,35,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","35 
 the ri sk associated with them is so low as to be statistically acceptable – for example the risks 
associated with travelling by plane.  
   It therefore becomes extremely important to implement methods that evaluate the validity of our 
strategy in a sound way, so that we can estimate risk and plan the investment correctly.  
 
Conclusion  
   
  In this article, I wanted to draw your attention to a different way of viewing the performance of a 
trading system, a way that is not bound to its absolute value, but linked to one of its statistical 
properties. As I demonstrated in the first section, th is involves well defined behaviours when we 
operate with cognitive awareness on the market. The approach is a fundamental one because it 
recognises the high likelihood of being successful on financial markets, even over long periods, in a 
completely random  way. Let’s not forget that financial markets have only two possible directions. 
This implies that, even fortuitously, there is a 50% chance of making the right choice. Furthermore, 
such trends can continue for years. Therefore, it is crucial to look away from the profit line and to 
appreciate in a rigorous and scientific way whether our strategies are the product of chance or of a 
true understanding of the market. One thus trades only if one understands the market, thereby actually 
reducing the element of fortuitousness. Investing in this way has nothing to do with chance but 
becomes cognitively aware and more secure.  
   Gambling is defined as follows:  
“The gaming activity in which profit is sought after and in which winning or losing occurs 
predominately b y chance, skill being of negligible importance ” 
   From this definition, it follows that if the element of fortuitousness is not factored into the 
investment decision -making process, it is never possible to prove that money invested is free of 
exposure to chance, and therefore to uncontrolled risk. The calculation of probability illustrated 
above therefore becomes an essential and irreplaceable requirement for bringing investment out of 
the area of gambling, making it more cognitively aware, and therefore l ess risky.  
References  
 
[1] O. Peters, M. Gell -Mann.  “Evaluating gambles using dynamics ”, Chaos: An Interdisciplinary Journal of 
Nonlinear Science , 2016; 26 (2): 023103 DOI:  10.1063/1.4940236  
 
[2] O. Peters, “Optimal leverage from non -ergodicity,” Quant. Finance 11,1593 –1602 (2011).  
 
[3] O. Peters and A. Adamou, “Stochastic market efficiency,” preprint arXiv:1101.4548 (2011).  
 
 
 
 
 ",2021-12-27T14:48:03Z,it conusias t   furtr trefore one investi gambli t from t referencpeters gel manevaluati chaos ainterdisciplinary journal nonlinear science peters optimal quant nance peters adam stochastic 
paper_qf_4.pdf,36,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","36 
 The professional trader’s paradox  
 
Andrea Berdondini  
 
 ABSTRACT: In this article, I will present a paradox whose purpose is to draw your attention to an important topic in 
finance, concerning the non -independence of the financial returns (non -ergodic hypothesis). In this paradox, we have 
two people sitting at a table separ ated by a black sheet so that they cannot see each other and are playing the following 
game: the person we call A flip a coin and the person we'll call B tries to guess the outcome of the coin flip. At the end 
of the game, both people are asked to estimate  the compound probability of the result obtained. The two people give 
two different answers, one estimates the events as independent and the other one considers the events as dependent, 
therefore they calculate the conditional probability differently. This  paradox show how the erroneous estimation of 
conditional probability implies a strong distortion of the forecasting skill, that can lead us to bear excessive risks.  
 
The professional trader’s paradox  
 
   In order to explain how much danger is considering the financial returns as independent, I want 
to present to you this paradox. We have two people  sitting at a table separated by a black sheet so 
that they cannot see each other and are playing the follo wing game: the person we call A flip a coin 
and the person we'll call B tries to guess the outcome of the coin toss. This game lasts an arbitrary 
time interval and the person A has the freedom to choose how many tosses to make during the 
chosen time interv al, the person B does not see the coin toss but can at any time, within the time 
interval, make a bet. When he makes a bet if he guesses the state the coin is in now, he wins. The 
person A decides to make a single coin flip (just at the beginning of the ga me) we say that the result 
is head, the person B decides within the same time interval to make two equal bets, betting both 
times on the exit of the head. The result is that B made two winning bets.  
 Now we ask ourselves this question: what is the correct compound probability associated with the 
result of this game? Let us ask this question to the person B who answers: every time I had bet I 
could choose between head and cross so I had a 50%  chance of winning the bet; I won two bets so 
the compound probabil ity is  0.5∙0.5=25% . Now let us say the same question to A the person 
who flip the coin, he replies: the probability is 50%  I have flip the coin only one time within the 
defined time interval, so its prediction probability cannot be higher at 50% . The fact  that the other 
player has made two bets has in practice only divided a bet in two is a bit 'as if to the racecourse 
we are made two distinct bets on the same horse on the same race, this way of acting does not 
increase the forecasting skill. Both answers seem more than reasonable, but as every mathematical 
paradox, the two answers contradict each other. At this point, will you ask yourself which of the 
two answers is correct?  
 We can resolve this paradox using the mathematical formula of the compound prob ability:  
P(E1 ∩ E2) = P(E1 | E2) P(E2) = P(E2 | E1) P(E1).  
 The probability that both events (E1, E2) occur is equal to the probability that E1 occurs P(E1) 
multiplied by the conditional probability of E2 given E1, P(E2 | E1).  
 Seeing the formula, we immed iately understand that the difference in response given by A and B 
is due to the different estimation of conditional probability P(E2 | E1). Person B estimates the ",2021-12-27T14:48:03Z,t  be rd oiiiat t  t i   t    t both at  t i son
paper_qf_4.pdf,37,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","37 
 conditional probability in this way P(E2 | E1) = P(E2) treating the events as completely 
independent, while person A estimates the conditional probability in this other way P(E2 | E1) = 1 
treating the events as completely dependent.  
 Which of the two answers is correct? The right answer is given by the person who has the 
knowledge to correctly es timate the conditional probability P(E2 | E1) and between the two players 
only the person that flip the coin can correctly estimate the conditional probability. Player B, on 
the other hand, not being able to see A that flip the coin, therefore he does not have the necessary 
information to estimate this probability correctly. Another way to understand this result can be 
found analysing the following question: what is the probability in this game of winning twice in a 
row by betting both times on the head?  
 The answer to this question is not always the same but it depends if after the first bet the person 
that flip the coin performs a new launch or not. If you make a new launch, the probability is 0.5 ∙
0.5=25%  if instead as in the case of this paradox no furth er coin flip is performed the probability 
is 50% . So, in order to answer correctly, you need to know the number of launch made and this 
information is knows only from the person (A) that perform the coin flip and he's the only one can 
be correctly calculat e the conditional probability.  
 If we bring this paradox on the financial markets, we understand that player A represent the 
financial instruments and player B represent the traders who try to beat the market. This gives us 
an extremely important result: a ll the traders make the same mistake, doing the same thing that 
player B did in this paradox. They consider their trades as completely independent of each other 
and this involves as we have seen, a strong distortion of the forecasting skill that can lead t he traders 
to acquiring a false security that may lead them to bear excessive risks.  
 Player B, like the traders, think that the statistical information about his forecasting skill depends 
on his choice (I choose head instead of cross, I buy instead of sel ling) this is a big mistake because 
this statement is true only when these kinds of bets are independent of each other. In practice, this 
statement is true only when I place a bet by event in this case, the results are independent of each 
other and therefo re these bets have a statistical meaning.  
 The problem is that in everyday life this equivalence is always respected. Therefore, our brain 
considers this equivalence always true so when we make trading we mistakenly consider our 
operations as independent despite the statistical evidence of non -independence (non -normal 
distribution of the results).  
 
Conclusion  
   
  In this short article, I wanted to introduce one of the most important topics in finance, which 
concerns the non -independence of the results. Considering the financial returns as independent is 
equivalent to considering the financial markets stationary ( ergodic hypothesis).  
 This hypothesis is considered by many experts not correct, on this topic have been written many 
articles [1], [2], [3]. What is the reason why such significant statistical evidence has been ignored, the 
main reason is the total lack o f methods able to estimate the conditional probability P (A | B).  
 In my previous article [4] I have explained an innovative method used in order to evaluate a financial 
strategy under the condition of the market non -stationary hypothesis (non -ergodic hypo thesis). This 
approach is based on the axiom of disorder (von Mises), this mathematical axiom applied on financial 
markets can be enunciated in this way:  ",2021-12-27T14:48:03Z,whit player anotr t  so   ty player it trefore conusiconsiri  what i mises
paper_qf_4.pdf,38,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","38 
 ""Whenever we understand any kind of deterministic market process, the probability of our financial 
operation being successful increases by more than 50% "" (Von Mises’ axiom of disorder from the 
early 1920s).  
 As a consequence of this axiom given above, any correct market analysis will always tend to increase 
the probability of our prediction beyond the 50% mean, and this results in a consequent decrease in 
the probability of obtaining the same result randomly.  To conclude, it follows that the parameter to 
be linked to the validity of a financial strategy, is not its performance but its statistical property of 
generating non -reproducible results in a random way.  
   I will demonstrate this to you with a simple ex ample. Suppose we are playing heads or tails with a 
rigged coin that gives us an above -50% probability of winning (let’s say it’s 60%). What is the 
probability of losing out after 10 coin tosses? Approximately 16.6% ...and after 50 tosses? 
Approximately 5. 7% ...and after 100 tosses? Approximately 1.7%. As you can see, the probability 
tends to zero, and here the rigged coin represents a financial strategy that is implementing a correct 
market analysis.  
 Now we return to the paradox that I exposed and we note  how the presence of a dependence between 
the first and the second bet has modified the conditional probability of the second one from 0.5 to 1. 
This increase of the conditional probability has the consequence that the result of the second bet can 
be obtai ned randomly.  
 In fact, if we move the second bet randomly within the time interval from the first bet to the second 
one, the result is always the same because the player who flip the coin (player A) does not execute 
other coin tosses in this time interval . Consequently, the second bet cannot be considered to evaluate 
the forecast skill.  Therefore, considering a system not stationary involves a reduction of the number 
of events to be considered for a statistical evaluation so if a data set proves to be stat istically 
significant under the condition of stationarity of the system, the same data set may no longer be 
statistically significant if the system is considered non -stationary.  
References  
 
[1] J. Barkley Rosser Jr. ‘Reconsidering ergodicity and fundamental un certainty’. In: Journal of Post Keynesian 
Economics 38 (3 2015), 331 –354. doi: 10.1080/01603477.2015.1070271 (1).  
[2] M. Sewell. “History of the efficient market hypothesis”. Technical report, University College London, 
Department of Computer Science, January 2011.  
[3] O. Peters, M. Gell -Mann. “Evaluating gambles using dynamics”, Chaos: An Interdisciplinary Journal of 
Nonlinear Science, 2016; 26 (2): 023103 DOI: 10.1063/1.4940236.  
[4] Berdondini, Andrea, “Description of a Methodology from Econophysics as a Verification  Technique for a 
Financial Strategy “,(May 1, 2017). Available at SSRN: https://ssrn.com/abstract=3184781.  
 
 
 
 
 
 ",2021-12-27T14:48:03Z,wnevomisas to suppose what approximately approximately approximately as   iconsequently trefore referencbarley roster jr consiri ijournal post keynesiaeconomics sll history technical  college londopartment uter science uary peters gel manevaluati chaos ainterdisciplinary journal nonlinear science be rd oi siptmethodology eco no psics vericattechnique nancial strategy may  articial intellence 
paper_qf_4.pdf,39,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","39 
 Application of the Von Mises’ axiom of randomness on the forecasts 
concerning the dynamics of a non -stationary system described by a 
numerical sequence  
 
Andrea Berdondini  
 
 ABSTRACT: In this article, we will describe the dynamics of a non -stationary system using a numerical sequence, in 
which the value of terms varies within the number of degrees of freedom of the system. This numerical sequence allows 
us to use the Von Mises’ axiom of randomness  as an analysis method concerning the results obtained from the forecasts 
on the evolutions of a non -stationary system. The meaning of this axiom is as follows: when we understand a pattern 
about a numerical sequence, we obtain results, intended as forecast on the next sequence number, which cannot be 
reproduced randomly. In practice, this axiom defines a statistical method capable of understanding, if the results have 
been obtained by a random algor ithm or by a cognitive algorithm that implements a pattern present in the system. This 
approach is particularly useful for analysing non -stationary systems, whose characteristic is to generate non -independent 
results and therefore not statistically signifi cant. The most important example of a non -stationary system are financial 
markets, and for this reason, the primary application of this method is the analysis of trading strategies.  
 
Introduction  
 
 The first problem that must be faced, in order to apply the axiom of randomness of Von Mises  as an 
analysis method, is to be able to describe the evolution of a system using a numerical sequence. For 
this purpose, we use a sequence in which the value of t erms can change within the number of degrees 
of freedom of the system. Then we also introduce a temporal progression represented by a series of 
increasing integers. With these two sequences, we can characterize a dynamic that describes the 
evolution of the  system that we are studying. In this way, we can apply the Von Mises’ axiom of 
randomness, which defines the statistical characteristic of a random sequence. The axiom is as 
follows: “ the essential requirement for a sequence to be defined as random consis ts in the complete 
absence of any rules that may be successfully applied to improve predictions about the next number ”. 
 This axiom tells us that when we understand a pattern about a numerical sequence, we can obtain 
results, intended as forecast on the ne xt sequence number, which cannot be reproduced randomly. 
Knowing that the values of our numerical sequence represent the dynamics of a system, we have 
found a method to evaluate the results. Therefore, if the probability of obtaining equal or better result s 
randomly is very small (i.e. tends to zero as the number of times the strategy is used increases), it 
means that the forecasting method uses rules present in the system. Consequently, we can use the 
developed method in order to predict the evolution of t he system; this approach is fundamental in the 
study of non -stationary systems. In fact, if we are in a condition of non -stationarity, the results can 
be non -independent, the consequence of which is a reduction in their statistical value. In order to 
expla in such a complex topic in a simple way, I have created a paradox [1], in which two players 
challenge each other in a game, where is possible to move from a stationary to a non -stationary 
condition by changing the rules of the game. The consequence is that  the data at the beginning are 
independent, and then once the changes are made, they become dependent on each other and therefore 
useless for statistical purposes.  
 For that reason, a large number of data obtained under these conditions, can be not statist ically useful 
in order to understand if the method that produced the data is not random. In this case, the performance ",2021-12-27T14:48:03Z,applicatvomis be rd oii vomist i t introdut vomisfor twith ivomist  ki trefore consequently iit for in
paper_qf_4.pdf,40,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","40 
 of the forecasting algorithm is no longer a reliable data; therefore we need to find a new statistical 
indicator in order to evaluate the  results. In fact it happens very often in finance, that a strategy that 
despite having obtained excellent results by performing a numerous number of operations, suddenly 
stop working generating large losses, proving in this way to be a completely random s trategy. There 
are many researches on the non -stationarity of the financial markets; some articles about this topic 
are [2], [3], [4], [5].  
 
Description of the dynamics of a non -stationary system by means of a numerical 
sequence  
 
 We start by considering a non -stationary system characterized by two degrees of freedom. We made 
this choice for two reasons: first in order to simplify the treatment, second among the systems with 
two degree of freedom there are the financial markets, th at represent the primary application of the 
method that we will go to expose.  
 Then we associate to this system a succession of values that can vary between 1 and 2. In practice 
when on this system there is a change in the direction in which a determinis tic process is acting, we 
have the result that the value of the succession change respect to the previous term. When we speak 
of ""deterministic process"", we indicate a deterministic and non -random force that acts on the system. 
We also introduce the simpli fication that the deterministic process acts as a constant force, without 
variations in intensity in the direction of the two degrees of freedom; this allows us to characterize 
the dynamics of the system with a single numeric succession.  
 Finally, we add a  temporal metric to the system; we can do this by using a series of increasing integer 
numbers, in this way every value of my succession unambiguously corresponds to a value in my 
temporal progression.  
 The result obtained is the following: we have two seq uences, one that describes the direction of the 
force acting on the system F and the other one that describe the progression of the time T:  
 
F 1   1   1   1   1   1   1   2   2   2    2    2    1   1   1   1    1   1   1   2  
T 1   2   3   4   5   6   7   8    9  10  11  12  13 14 15 16 17  18 19 20  
 
 Analysing these two sequences we can note that the force (deterministic process) that acts on the 
system changes direction three time, the first at time T = 8, the second at time T = 13 and the third at 
time T =  20. Figure 1 shows the dynamics described by the two sequences; the temporal progression 
has also been included in the graph.  ",2021-12-27T14:48:03Z,itre sipt  tiw nally t analysi 
paper_qf_4.pdf,41,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","41 
  
Figure 1: The dynamics of the system described by the two sequences.  
 
Application of the Von Mises’ axiom of randomness as an a nalysis method concerning 
the results obtained by a forecasting algorithm  
 
 At this point, we are going to apply the Von Mises’ axiom of randomness in order to evaluate the 
results obtained by a forecasting algorithm. According to the axiom, reported and analysed in the 
introduction of this article, if we have developed a non -random method that implements a pattern 
present in the system, we will be able to predict when the term of the succession varies respect the 
previous term.  
 This statement is crucial, because we are saying that only the forecasts concerning system changes 
are useful in order to understand whether the method used is random or not. In fact, if we make two 
forecasts about two terms of the succession at time 8 and 12, betting that the value of the succession 
is two, we obtain two successes. Are these two results i ndependent of each other? This question is 
fundamental, because only if they are independents are useful for statistical purposes in order to 
evaluate the forecasting method.  
 In order to answer to this question we must use the formula of the compound prob ability that we 
report:  
P (E1∩E2) = P (E1 | E2) P (E2) = P (E2 | E1) P (E1)                        (1)  
 The probability that both events (E1, E2) occurs, is equal to the probability that E1 occurs P(E1), 
multiplied by the conditional probability of E2 give n E1 occurs P(E2 | E1).  
 Seeing formula (1) we understand that the correct calculation of conditional probability P (E2 | E1) 
depends on whether the events are independent or not. In fact, in the case of independent events P(E2 
| E1) = P(E2), instead in th e case of events completely dependent on each other P (E2 | E1) = 1 or 0.  
",2021-12-27T14:48:03Z, t applicatvomisat vomisaccordi  iare  it i in
paper_qf_4.pdf,42,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","42 
  In our example the probability P(E1) = 1/8. In fact, the probability of winning in a random way 
betting that the value of the sequence within the first 8 time positions is equal to 2 is 1/8, since in the 
first seven positions the sequence has value 1 and only at the eighth position has value 2. In practice, 
we can see this value like the probability of a random draw of a marble with the value 2 from a bag 
that contains eight marbles,  seven of them with the value 1 and one with the value 2. Therefore, P 
(E1) represents the probability of obtaining the same result using a random strategy. In this way, we 
are able to derive a probability from a succession of data concerning the dynamics of the system.  
 Now we try to calculate the value of P (E2); studying the time interval from the ninth position to the 
twelfth position, we see that the system remains constant. Therefore, the probability of betting 
randomly on the value 2 and win, within of this time interval, it is equal to 1.  This is because all 
positions have value two, so a random strategy obtains the same result with a probability of 100%.  In 
this case, we can treat the second bet as completely dependent from the first bet.  This mean s that we 
can consider the two bets, from the statistical point of view, like a single bet.  
 Now let's try to shift the second bet from the twelfth position to the twentieth position, in this case 
we will have P(E2) = 5/7. In fact, in the time interval fro m the ninth position to the twentieth position, 
there are seven values equal to 1 and five values equal to 2. With this variation in the second forecast, 
the compound probability will be equal to:  
P (E1∩E2) = P (E1) P (E2 | E1) = P (E1) P (E2) = 1/8 ∙ 5/7 = 0.089                                                 
 The two events in this case are independent of each other, so they are useful for statistical purposes 
in order to understand if the method used can predict the evolution of the system.  
 From these two examples, we can deduce the following conclusion : when we want to study a 
deterministic process, every time that it determines a change in the system, we have the possibility to 
execute a predic tion on the evolution of the system, whose probability of success with a random 
strategy turns out to be minor 1. This involves a decrease in the compound probability whose 
meaning, according to the Von Mises' axiom of randomness, is to indicate the presen ce of a 
deterministic process that acts on the system.  
 In practice, whenever that a deterministic process changes the system status, we can detect it by 
making a prediction. Consequently, in order to detect a deterministic process with a low error, it is 
necessary that it has produced a statistically significant number of variations in the system. On which 
our forecasting method has carried out a large number of independent predictions. Therefore, every 
forecast must concern a single variation of the syste m. 
 This approach is crucial for non -stationary systems where the forecasts may be not independent. For 
that reason, it is fundamental to calculate the compound probability, which is the probability of 
obtaining the same results in a random way. We also re member that within the compound probability 
(1), there is the calculation of the conditional probability, in which it is taken into account if the events 
are dependent or independent from each other. In fact, the goal of this method is to discard all non -
independent forecasts,  whose contribution is to make me overestimate the forecast skill of the 
algorithm. The consequence of this erroneous evaluation can lead us, for example in the financial 
field, to bear excessive risks.  
 
 
 ",2021-12-27T14:48:03Z,iiitrefore i trefore  i  iwith t from  vomisiconsequently otrefore  for  it
paper_qf_4.pdf,43,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","43 
 Conclusion  
 
 The analysis of results, concerning the evolution of a non -stationary system, represents one of the 
most important problems of applied mathematics still unresolved. In this article, I propose the use of 
the Von Mises’ axiom of randomness as a method of evaluating the data  obtained under these 
conditions. This axiom, as explained previously, defines a statistical characteristic that assumes a 
well-defined behaviour when we operate consciously. In practice, the results generated by a non -
random forecasting algorithm, that im plements knowledge about the considered system, cannot be 
reproduced randomly.  Consequently, the probability of obtaining equal or better results randomly 
tends to zero  as the number of times the strategy is used increases.  In this way, we shift our attent ion 
from the value of the result, which under the non -stationary condition may have been produced with 
non-independent forecasts, to its statistical characteristic correlated to its non -random behaviour.  
 In order to apply this method, we have defined a si mple mathematical model whose task is to describe 
the dynamics of a system by means of a numerical sequence.  Using this mathematical model, we have 
shown how to compute compound probability from a series of results obtained with a forecasting 
algorithm.  Then, analysing some examples, we have deduced some important considerations.  In 
particular, we have seen that when there is a change in the system, we have the possibility to make a 
prediction whose probability of success is less than 1. The consequence of this is a decrease of the 
compound probability whose meaning, according to the Von Mises’ axiom of randomness, is to 
indicate the presence of a pattern on the system . Therefore, if we want to detect it with a low error 
we must have made a statistically sig nificant number of independent predictions, concerning each a 
single variation of the system. In this way, the forecast algorithm proves to be able to predict future 
system evolutions. This characteristic, therefore, becomes fundamental in estimating the r isk of a 
strategy that operates on a non -stationary system, such as the financial markets. In fact, a correct risk 
assessment must always take in considerations the forecast skill of the algorithm about the future 
evolutions of the system.  
 In the next art icle, we will introduce a purely random component within the dynamics of a non -
stationary system. In this way the deterministic process will be replaced by a stochastic process 
described by a probability density function. This step is essential in order to  apply this method in the 
financial field . 
References  
 
[1] Berdondini Andrea, “The Professional Trader’s Paradox”, (November 20, 2018). Available at 
SSRN:  https://ssrn.com/abstract=3287968 . 
[2] J. Barkley Rosser Jr. ‘Reconsidering ergodicity and fundamental uncertainty’. In: Journal of Post Keynesian 
Economics 38 (3 2015), 331 –354. doi: 10.1080/01603477.2015.1070271 (1).  
[3] M. Sewell. “History of the efficient market hypothesis”. Technical report, Univ ersity College London, 
Department of Computer Science, January 2011.  
[4] O. Peters, M. Gell -Mann. “Evaluating gambles using dynamics”, Chaos: An Interdisciplinary Journal of 
Nonlinear Science, 2016; 26 (2): 023103 DOI: 10.1063/1.4940236.  
[5] Berdondini, Andrea, “Description of a Methodology from Econophysics as a Verification Technique for a 
Financial Strategy “,(May 1, 2017). Available at SSRN: https://ssrn.com/abstract=3184781.  ",2021-12-27T14:48:03Z,conust ivomis iconsequently iiusi tit vomistrefore i iii referencbe rd oi t professnal trar dox november  articial intellence  barley roster jr consiri ijournal post keynesiaeconomics sll history technical uiv college londopartment uter science uary peters gel manevaluati chaos ainterdisciplinary journal nonlinear science be rd oi siptmethodology eco no psics vericattechnique nancial strategy may  articial intellence 
paper_qf_4.pdf,44,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","44 
 Psychology  
 
The first article deals with the mental predisposition called ""dissociat ion from the result"" that 
characterizes professional traders. This attitude is fundamental, because in the financial markets, 
being characterized by a low number of degrees of freedom, it is particularly easy to obtain good 
results randomly. Therefore, in these conditions, the result can be a little significant. Consequently, 
developing a dissociation from the result is essential in order not to overestimate your trading 
strategy. Indeed, future results depend on our knowledge of the system on which we make  
predictions and not on past results.  
In the second article, we show how meditation is an efficacious mental training method to 
improve our approach to problem solving. The importance of this type of practice derives from its 
ability to reduce our irration al and therefore random actions. As we have seen in previous articles, 
acting randomly is not only useless but also increases the uncertainty of future results. Therefore, 
this last article, despite talking about a topic that seems to be distant from the t opic covered in this 
book, completes all the concepts discussed previously.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 ",2021-12-27T14:48:03Z,psychology t  trefore consequently ined it as trefore
paper_qf_4.pdf,45,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","45 
 The psychology of the professional trader  
 
Andrea Berdondini  
 
 ABSTRACT: In this article, we will analyze a mental attitude that distinguishes professional traders. This 
characteristic can be summarized with the following concept: ""dissociation from the result"" . This type of 
mental predisposition is very important, because the way we relate to the result affects our ability to behave 
rationally.  
 
The basic feature of the human mind  
 
To understand how the mind behaves in relation to the result of the actions, we must understand 
the environment in which it evolved. Man has evolve d in a context in which the link between action 
and result has always been very strong.  
The non -randomness of the result has an important consequence: if the action leads to a benefit, 
the action that generated it turns out to be correct.  Therefore, a very strong connection is created 
between the action and the result. Consequently, to a useful result for our survival we are led to 
consider the action performed as rational and correct. The brain also strengthens this bond in a 
physiologica l way by producing a physical sensation of happiness. In practice, the brain rewards 
us for the action done and wants it to be repeated over time because considered useful for our 
survival.  
 
How the mind is deceived  
 
Now we analyze how a mind, evolved und er the circumstances described in the previous 
paragraph, behaves when it has to face situations in which the outcome of the result is subject to a 
significant random component. In these situations, an extremely important thing happens: the result 
of our a ction will never be constant but will have a certain degree of randomness.  
In this situation, the link between action and result is broken, and it is no longer true that a useful 
result corresponds to a rational action.  Situations in which there is no link  between the rationality 
of the action and the result are situations almost exclusively created by man as in the case of 
gambling. For example, if we play heads or tails, we have 50% of probability to win and 50% to 
lose, and obviously, there is no rationa l link between action and result. However, for the brain it 
makes no difference, if you win a bet it will reward you with a feeling of happiness and this is 
because for our mind the link between result and action still exists. For this reason, the 
psycholo gical pathology called ludopathy, that afflicts gamblers, is the direct consequence of an 
evolved mind in an environment where there is a very strong link between action and result.  
In this situation, the mind is deceived and considers actions that are irr ational and destructive as 
rational and useful. This behavior is also called cognitive distortion, and the people who are affected 
are really convinced that their irrational actions can lead them to victory.  ",2021-12-27T14:48:03Z,t  be rd oii  t to mat trefore consequently t ihow  iisituatns for for i
paper_qf_4.pdf,46,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","46 
 At this point, you will surely have understood t hat the main problem is due to the direct link that 
the mind associates between action and result. Therefore, it is precisely on this key element that 
arises the difference in approach between an ordinary person and a professional trader.  
People who unders tand this problem understand the importance of training their mind about the 
dissociation from the result. In this way, they develop a new awareness about the result, which 
allows them to maintain a rational and winning response even in situations where th e result is 
subject to a random component.  
 
The mind of a professional trader  
 
In the previous chapter, the cognitive characteristic that distinguishes an ordinary person from a 
professional trader was identified. This characteristic can be summarized wit h the following 
concept: ""dissociation from the result"". In other words, it is about breaking the link that our brain 
creates between action and result.  
This mental approach is present among professionals from very different sectors; for example, it 
is nor mal for a professional poker player to get angry when he makes a stupid play and to be happy 
when he makes an intelligent play, regardless of the outcome of the individual plays. In fact, these 
players have perfectly understood that there is no direct link  between the result of a single operation 
and the action performed. What makes them successful is always being able to perform a series of 
rational and correct actions without getting involved in emotions. About this argument, I also report 
an interesting statement by the professional trader Linda Raschke whose interview was featured in 
the 06 -2017 issue of Trader Magazine. When asked, "" Is Trading a game for you?”  Linda Raschke 
replies “Yes. I barely check the account balance. Because this unnecessarily aff ect me both 
positively and negatively and does not change the fact that I have to make the right decision again. 
And if you always make the right decisions, performance becomes simple in the long run” . As you 
can see, there is such a strong dissociation fr om the single result that she does not even care about 
its balance (an amateur trader checks his balance every 5 seconds).  
Viewed from this point of view, trading is no longer random, but is a purely cognitive process, 
where you only operate if you understand the financial markets by always performing rational and 
never emotional actions. At this point, becomes easy to understand the difference between an 
amateur trader and a professional trader: the first is obsessed by the result, while the second is 
obsessed by the knowledge.  
 
 
 
 
 
 
 
 ",2021-12-27T14:48:03Z,at trefore people it i i iwhat at linda ra ske trar magazine wis tradi linda ra ske ybecause and as vied at
paper_qf_4.pdf,47,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","47 
 How meditation can improve our problem -solving skills   
 
Andrea Berdondini  
 
 ABSTRACT : The goal of this article is to try to explain easily, how the practice of meditation affects our 
problem -solving skills. To do this,  I use a mathematical logical method to characterize problems based on 
the number of their possible solutions. In this way, it easy to explain the two primary approaches that the 
mind uses in solving problems. The first approach is iterative, optimal in so lving simple problems 
(problems with a low number of solutions). The second approach is the logical one, optimal in solving 
complex problems (problems with a high number of solutions). The interesting aspect of these two methods 
is that their mode of actio n is the opposite. The iterative method is based on action, while the logical method 
is a reflective approach, in which any unnecessary action takes us away from solving the problem. 
Consequently, we will see how through the practice of meditation, we can shift our mental predisposition 
towards the logical approach by inhibiting our propensity for the iterative approach. This is a very important 
result because it can allow us to improve our ability to solve complex problems. This type of attitude is 
fundame ntal in a society where technological progress is making all simple and repetitive jobs less 
indispensable.  
 
The two fundamental methods used by the mind in solving problems  
 
From a mathematical logical point of view, the problems can be divided according to the number 
of possible solutions they can have. Therefore, using this approach, we can define two classes of 
problems:  
1) “Simple” problems: the problems in which the space of possible solutions is constituted by a 
small number of elements. Where the ti me required to try all possible solutions iteratively, is limited 
and acceptable. Example: a padlock that has 100 possible combinations, if I can try a different 
combination every 5 seconds, I will try all the combinations in an acceptable time.  
2) “Comple x” problems: the problems in which the space of possible solutions is constituted by 
a large number of elements. Where the time to try all possible solutions iteratively, tends to infinity 
or to an unacceptable time. Example: proving a mathematical theorem , doing it iteratively trying 
random solutions, takes a time that tends to infinity.  
Thanks to this classification, we can study, in a simple way, the two fundamental approaches that 
the mind uses in solving problems.  
The first approach is iterative, the mind does not try to solve the problem but tries all possible 
combinations. This approach is the optimal one for solving “simple” problems and was 
fundamental in the initial part of human evolution.  
The second approach is the logical one; the mind creates a model of the problem and tries to solve 
it. This approach is the optimal one for solving “complex” problems, and its importance has 
increased in the course of human evolution. Consequently, this category of problems is also the one 
that most characterize s our problem -solving skills.  
Comparing these two methods, the interesting thing we notice is that they act oppositely. The 
iterative method is based on action, faster I act, faster I solve the problem. Instead, the logic -based ",2021-12-27T14:48:03Z,how  be rd oit to it t t t consequently   t from trefore simple wre   le wre  thanks t  t  consequently ari t instead
paper_qf_4.pdf,48,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","48 
 method is thoughtful; any in correct action takes us away from the solution.  
To explain the importance of not acting irrationally, when trying to solve a complex problem, I 
like to give the following example: imagine that you are a hiker who got lost in the jungle, what 
are you going to do? If we try to ask this question to a survival expert, he will answer that the best 
thing to do is to do nothing, and wait for help, because any of your actions will only tire you and 
put you in danger. The same thing happens in solving complex proble ms, in which every irrational 
action is not only useless but it makes us lose energy and time. This example makes us understand 
how different the two mental approaches are, and how fundamental our mental predisposition is to 
be successful in situations whe re a type of problem predominates.  
Another useful point of view, to understand the importance of these two mental approaches, is to 
comprehend why there is so much interest in algorithms based on artificial intelligence. The reason 
for such interest stems from the fact that through artificial intelligence the algorithms are moving 
from an iterative approach to a logical approach. In fact, for example, the software developed to 
play chess, until recently used iterative approaches. In practice, the software s imulates all possible 
combinations and chose the best move. This method had two important limitations: it needed a very 
powerful computer and could not be applied to games like the “go” in which the possible move 
combinations are very high. With the advent  of artificial intelligence, these virtual players have 
gone from an iterative approach to a logical approach with incredible results. Google’s Deepmind 
research team has developed the first software capable of beating the human champion of “go”, on 
this t opic I recommend reading the article published on nature “Mastering the Game of Go without 
Human Knowledge”.  
Now you can understand why the knowledge of these two different mental approaches is 
fundamental for studying the dynamics that involve our problem -solving skills.  
 
Meditation as mental training to improve the problem -solving skills  
 
In this section, we will try to explain the implications of meditation on problem -solving skills. 
The term meditation refers to a large number of techniques, even very d ifferent from each other, 
whose task is to bring complete awareness of the present moment. One of the oldest and best -known 
techniques, and consequently among the most practiced, is called vipassana. The practice of this 
meditation is performed by sitting cross -legged while remaining completely still in a mental state 
in which we observe everything that happens. Mainly the observation is directed towards thoughts 
that tend to manifest themselves and towards one’s breathing.  
If we now analyze the two mental approaches, described in the previous paragraph, it is easy to 
understand how the practice of this type of meditation tends to be in contrast with the iterative 
method used in solving problems. As described in the previous paragraph, this approach is based  
on action, in practice, I act as quickly as possible without ever stopping. Hence, sitting still for no 
purpose represents the opposite of this method.  
Consequently, the constant and repeated practice of this type of meditation leads over time to 
inhibit our propensity to act impulsively. There are many scientific studies on this topic that show 
how meditation reduces our propensity to multitasking (hyperactivity) and all those irrational and 
emotional behaviors. This is an important fact because the itera tive approach is based on random 
(irrational) and continuous actions with a strong emotional component.  
Meditation in this way modifies our problem -solving skills making us more reflective, ",2021-12-27T14:48:03Z,to  t  anotr t ii with  ep mind masteri game go humakledge  meditatit one t articial intellence only  as nce consequently tre  meditatn
paper_qf_4.pdf,49,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","49 
 consequently increasing our propensity towards the use of the logi cal approach in solving problems. 
This result is significant because when we talk about problem -solving we are talking, in most cases, 
about the ability to solve complex problems. In fact, in a society where technological progress has 
an exponential trend,  our ability to solve problems of this type becomes an increasingly important 
and requested skill.  
Another fundamental aspect to keep in mind, regarding the importance of training the mind to a 
more reflective approach, is to understand the impact that new  technologies have on our minds. To 
answer this question, we need to understand how most of the applications that are used on 
smartphones, tablets, etc. are developed.  
The main purpose of these applications is to create an addiction, and to do this they take advantage 
of the iterative approach that the mind uses to solve simple problems. This is done because, in this 
situation, the person is forced to perform a continuou s series of actions, which will correspond to a 
series of results, the consequence of which is a stimulation of the reward system present in our 
brain. With this technique, the user of the application will compulsively experience a succession of 
emotions, the result of which is to create a real addiction.  
In conclusion, this type of technology is changing the approach to solving the problems of the 
new generations, favouring the iterative approach over the logical one. For this reason, it is essential 
to counteract the conditioning caused by these applications with techniques such as meditation, 
which inhibit our propensity to solve problems iteratively.  
 
Conclusion  
 
In this article, I have used a simple mathematical logical analysis to relate our problem -solving 
skills and the practice of meditation. In this way, we find a similarity between two very different 
realities. On the one hand, we have a scientific formalism, in which through the analysis of a 
mathematical data the optimal approach is found to sol ve a class of problems. On the other hand, 
we have meditation, which represents a topic mainly studied in the philosophical field. So we have 
two extremely distant points of view which, however, as we have seen, tend to have incredibly 
similar elements of convergence. Indeed, the practice of meditation represents a way of acting 
contrary to the iterative approach. Consequently, meditation acts by inhibiting our propensity to act 
iteratively, making us prefer the logical approach, fundamental in solving comp lex problems. We 
have also seen how modern technologies are influencing new generations to hyperactive and 
compulsive approaches. So, it becomes essential to contrast this type of mental conditioning, with 
something that goes to act in the opposite directi on leading us to act more thoughtful. Meditation, 
from this point of view, can be seen as a practice that acts on some primary aspects used by the 
mind in many of its processes, such as problem -solving. In this way, we can partly understand why 
something s o simple has such profound implications in many areas of the brain. For these reasons, 
I believe that meditation will become an increasingly important formative practice.  
 
 
 
 
 ",2021-12-27T14:48:03Z, ianotr to t  with ifor conusiiooso ined consequently  so meditatifor
paper_qf_4.pdf,50,The theory of quantitative trading,"  Abstract: This book consists of a selection of articles divided into three
main themes: Statistics, Quantitative Trading, Psychology. These three
arguments are indispensable for the development of a quantitative trading
system. The order of the articles was chosen so as to constitute a single
logical reasoning that develops progressively.
","50 
 Sticker for your monitor  
 
Cut out the sentence below and  apply it on your work monitor.  
 
 
Trying to create knowledge randomly is one of the most favourite hobbies for traders. This 
sentence is intended to remind you of the uselessness of this way of acting.  
 
",2021-12-27T14:48:03Z,sticker cut tryi 
paper_qf_5.pdf,1,Quantum Computing for Financial Mathematics,"  Quantum computing has recently appeared in the headlines of many scientific
and popular publications. In the context of quantitative finance, we provide
here an overview of its potential.
","arXiv:2311.06621v1  [q-fin.MF]  11 Nov 2023QUANTUM COMPUTING FOR FINANCIAL MATHEMATICS
ANTOINE JACQUIER, OLEKSIY KONDRATYEV, GORDON LEE, AND MUGA D OUMGARI
Abstract. Quantum computing has recently appeared in the headlines of many scientiﬁc and pop-
ular publications. In the context of quantitative ﬁnance, w e provide here an overview of its potential.
Financial mathematics as a standalone discipline enjoyed many period s of rapid development fol-
lowed by periods of relative calm. As a synthetic discipline that exists a t the cross-section of applied
mathematics, ﬁnancial theory, computer science, prudential re gulation, ..., it beneﬁts from discoveries
and breakthroughs in all these ﬁelds.
The birth ofﬁnancial mathematics is often attributed to the docto ralthesis ofLouis Bachelier, “The
Theory of Speculation”, defended in 1900 (and written under the s upervision of Henri Poincar´ e) and
which applied a stochastic process (later called a Brownian motion) to the modelling of ﬁnancial assets
for the ﬁrst time. Since then, ﬁnancial mathematics has been inﬂue nced by stochastic calculus (Itˆ o’s
lemma, Girsanov’s theorem), control theory (Kalman ﬁlter), stat istics (Kolmogorov-Smirnov test),
but also spurred by progresses in microprocessors, ﬁnancial der egulation, ultrafast communication,
object-oriented programming, to name just a few.
In this regard, quantum computing opens a new chapter for ﬁnanc ial mathematics as it promises
enormous computing power at very low cost. To understand why th is is the case, one must appreciate
the importance of computation in general. At the end of the day, all ﬁnancial problems (pricing, risk
management, credit scoring, discovery of trading signals, data en cryption, portfolio optimisation) are
of computational nature, and the task of ﬁnancial mathematics is to provide the most eﬃcient and
convenient tools to perform this computation.
What is computation? A computation can be deﬁned as a transforma tion of one memory state
into another, or a function that transforms information. In the c ase of classical digital computing,
the fundamental memory unit is a binary digit (bit) of information. Functions that operate on bits
of information are called logic gates , namely Boolean functions that can be combined into circuits
capable of performing additions, multiplications, and more complex op erations. But is the Boolean
logic the only or even the most general way to realise digital computa tion? The answer is clearly
no. Classical computing is just a special case of a more general com putational framework now called
quantum computing . A classical bit is a two-state system that can exist in either of two d iscrete
deterministic states, traditionally denoted ‘0’ and ‘1’. All classical bit s are independent – ﬂipping the
state of a given bit does not aﬀect the states of other bits. These two features of classical computing
can be generalised by allowing the bit to exist in a superposition of the t wo states ‘0’ and ‘1’, and by
allowing the states of diﬀerent bits to be entangled (a certain form o f correlation). It is also clear how
quantum computing obtained its name – superposition and entanglem ent are the key characteristics
of the states of quantum systems, and it is tempting to perform co mputation through the controlled
evolution of a quantum system , i.e., by running a quantum computer .
Superposition and entanglement are two features responsible for the extraordinary power of quan-
tum computing. They allow for more general computation: more gen eral deﬁnition of the memory
Date: November 14, 2023.
2010Mathematics Subject Classiﬁcation. 68Q12, 68T07, 65D15.
Key words and phrases. quantum computing, quantitative ﬁnance.
AJ is supported by the EPSRC grants EP/W032643/1 and EP/T032 146/1.
1",2023-11-11T17:39:16Z, nov abstra quantum inancial as t louis cac lit tory speculatnri pointer brownish since it gir nov kal malmogorov mir nov ito at what ifunns booleabut booleat assical all tse it supositty date november matmatics subje ass key
paper_qf_5.pdf,2,Quantum Computing for Financial Mathematics,"  Quantum computing has recently appeared in the headlines of many scientific
and popular publications. In the context of quantitative finance, we provide
here an overview of its potential.
","2 ANTOINE JACQUIER, OLEKSIY KONDRATYEV, GORDON LEE, AND MUG AD OUMGARI
state in comparison with classical digital computing and a wider range of possible transformations
of such memory states. The fundamental memory unit in quantum c omputing is a quantum binary
digit(qubit). Mathematically, the state of a qubit is a unit vector in the tw o-dimensional complex
vector space. Quantum logic gates are represented by the norm-preserving unitary operators (un itary
matrices) acting on qubit states. Once the computation is complete (the initial system state has been
transformed by the quantum circuit – a sequence of quantum logic gates), the qubit states can be
measured (projected onto the basis states). Qubits in their basis states correspond to classical bits (all
superpositions have been collapsed). The rest of the computation al protocol can be done classically
after the read-out of the bitstring, which is the output of the qua ntum computer.
A natural question arises: what is the reason for this superior mod e of computation not being
used until very recently? The answer is that although quantum mec hanics was formulated almost a
century ago (Dirac’s seminal work ”The Principles of Quantum Mecha nics” was published in 1930),
the realisation of the rules of quantum mechanics in the computation al protocol performed on classical
digital computers requires an enormous amount of memory. Expon ential gains in computing power
are oﬀset by exponential memory requirements.
In order to perform quantum computations eﬃciently, we need to u se actual quantum mechanical
systems, with their ability to encode information in their states. To illu strate this point, the state
of a quantum system consisting of nentangled qubits can be described by specifying 2nprobability
amplitudes, a huge amount of information even for very small syste ms (n∼100) and it would be
impossibletostorethisinformationinclassicalmemory. Ittookdeca desoftechnologicalprogressbefore
Quantum Processing Units (QPUs)––devices that control quantu m mechanical systems performing
computations––became feasible.
The current state-of-the-art QPUs have several hundred qub its and the qubit ﬁdelity is still in-
suﬃcient for the fault-tolerant computation. However, these sy stems are already large enough and
qubit ﬁdelity high enough for such quantum computers to be useful. Among many possible quantum
computing technologies, two qubit types stand out as most develop ed (and promising):
•Qubits made of superconducting circuits (coherence time: ∼103µs) [20]
one-qubit gate two-qubit gate
Gate time: ∼10−2µsGate time: ∼10−2-10−1µs
Fidelity: 99.99% Fidelity: 99.9%
•Qubits made of trapped ions (coherence time: >108µs) [3]
one-qubit gate two-qubit gate
Gate time: ∼1-10µsGate time: ∼10µs
Fidelity: 99.9999% Fidelity: 99.9%
Thesequbitcharacteristicsareindicativeofthefactthatweappr oachingthethresholdbeyondwhich
various error correction algorithms become feasible and we may ﬁna lly enter the era of fault-tolerant
quantum computing.
Whilstquantum supremacy (a term coined by Preskill in 2012) was demonstrated by Google in 201 9
on a specially designed problem, recent years have witnessed clear s igns ofquantum advantage —ﬁrst
productive applications of quantum computers to real-world proble ms hard for classical computers.
That said, given the scarcity and cost of full-ﬂedged quantum comp uters, emulators play a pivotal
role in the current quantum ecosystem. These emulators simulate q uantum operations on classical
hardware, allowing researchers and developers to design, test, a nd reﬁne quantum algorithms without
requiring direct access to a quantum machine, moving forward the s tate of the art in its research but
by-passing their current limited availability. Graphics Processing Unit s (GPUs), given their parallel",2023-11-11T17:39:16Z,t matmatically quantum once qu  t t dfrac t principlquantum mecha e oito it took c quantum processi units us t us amo qu  gate gate lity lity qu  gate gate lity lity tse qu bit charaeristics are indicative of t fa that  apps whst quantum pre skl  that tse graphics processi unit us
paper_qf_5.pdf,3,Quantum Computing for Financial Mathematics,"  Quantum computing has recently appeared in the headlines of many scientific
and popular publications. In the context of quantitative finance, we provide
here an overview of its potential.
","QUANTUM COMPUTING FOR FINANCIAL MATHEMATICS 3
processing capabilities, have emerged as the go-to hardware for e mulating quantum systems. Their
architectureiswell-suitedforhandlingthematrixoperationsfunda mentaltoquantummechanics. Tech
giants in recent years have been creating frameworks for quantu m computing in public: IBM’s Qiskit
development kit allows any pythonuser to implement and test quantum algorithms. Google provides
theCirqframework, enabling developers to create, edit, and invoke quant um circuits on real and
simulated quantum devices. Microsoft’s Quantum Development Kit inc ludes the Q ♯language, which
can be used to write quantum algorithms that are run on classical sim ulators. Xanadu’s pennylane
software framework is also speciﬁcally designed to implement quantu m machine learning tools.
Several areas deserve a particular attention as the most relevan t ﬁnancial mathematics problems.
Optimisation
Digital quantum computing oﬀers the possibility of solving NP-hard co mbinatorial optimisation prob-
lems using variationalmethods such as the VariationalQuantum Eige nsolver[14, 17] and the Quantum
ApproximateOptimisationAlgorithm[6]. Bothalgorithmscanbe usedto solveawiderangeofﬁnance-
related optimisation problems [11]. Moreover, variational algorithms are noise-resistant and, therefore,
suitable [4, 21] for running on the current generation of Noisy Inte rmediate-Scale Quantum (NISQ)
computers [2, 18]. Additionally, classically hard optimisation problems n aturally lend themselves to be
addressed by analog quantum computers that realise the principles of Adiabatic Quantum Computing.
The ﬂagship ﬁnancial use case is discrete portfolio optimisation wher e we can see the ﬁrst experimental
evidences of a quantum speedup [22].
Quantum Machine Learning
It is a combination of quantum computing and AI that will likely generat e the most exciting opportu-
nities, including a whole range of possible applications in ﬁnance. We hav e already witnessed the ﬁrst
promising results achieved with parameterised quantum circuits tra ined as either generative models
(such as Quantum Circuit Born Machine [12], which can be used as a syn thetic data generator) or
discriminative models (such as Quantum Neural Networks [7] that c an be trained as classiﬁers). The
possible use cases include market generators, data anonymisers, credit scoring, and the generation of
trading signals. The Quantum Generative Adversarial Networks is a nother type of generative QML
model with signiﬁcant potential [1, 5, 10, 16, 19]. Similar to classical G ANs, a quantum GAN is com-
posed of a generator and of a discriminator with the ability to distingu ish quantum states. Since each
quantum state encodes a probability distribution, the quantum GAN discriminator can be used to
verify whether the datasets in question were drawn from the same probability distribution with direct
application to the time series analysis, detection of structural bre aks, and monitoring of alpha decay.
PDE Solvers
Harrow, Hassidim and Lloyd [9] devised a quantum algorithm to solve line ar systems, beating classical
computation times. Linear systems are ubiquitous in applications, an d many aspects of mathematical
ﬁnance rely on being able to solve such (low- or high-dimensional) syst ems. A particularly important
application is solving Partial Diﬀerential Equations (PDEs). A quantu m algorithm for linear PDEs
can be used to eﬃciently price European and Asian options in the Black -Scholes framework [8].
Quantum Monte Carlo
Montanaro [15] described a quantum algorithm which can accelerate Monte Carlo methods in a very
general setting. The algorithm estimates the expected output va lue of an arbitrary randomised or
quantum subroutine with bounded variance, achieving a near-quad ratic speedup over the best possible
classical algorithm. For the recent advances in quantitative ﬁnanc e applications we refer interested
readers to [13].
Quantum Semideﬁnite Programming
The key idea behind Quantum Semideﬁnite Programming (QSDP) is base d on the observation that a
normalised positive semideﬁnite matrix can be naturally represented as a quantum state. Operations",2023-11-11T17:39:16Z,tir teqi t  ir framework msoft quantum velopment kit canada sevl optiatdital variatal quantum e quantum approximate optiatalgorithm both algorithms cabe onoisy ite scale quantum additnally diabetic quantum uti t quantum machine learni it  quantum circuit bormachine quantum neural networks t t quantum gentive adversarial networks simar ns since solnarrow has si lloyd linear partial di equatns a  schools quantum   montana ro   t for quantum semi  prograi t quantum semi  prograi oatns
paper_qf_5.pdf,4,Quantum Computing for Financial Mathematics,"  Quantum computing has recently appeared in the headlines of many scientific
and popular publications. In the context of quantitative finance, we provide
here an overview of its potential.
","4 ANTOINE JACQUIER, OLEKSIY KONDRATYEV, GORDON LEE, AND MUG AD OUMGARI
on quantum states can sometimes be computationally cheaper to pe rform on a quantum computer
than classical matrix operations. This idea prompted the developme nt of quantum algorithms for
semideﬁnite programming. In ﬁnance, QSDP can be used for the max imum risk analysis and robust
portfolio construction [11].
As a conclusion, after decades of theoretical results and promise s, quantum computing is now
progressively becoming a reality. While full-scale quantum computers are not here yet to replace their
classical counterparts, they are nevertheless already useful b oth to speed up speciﬁc procedures in
classical algorithms (giving birth to the hybrid classical-quantum era ) and to provide new angles and
new ways of thinking about old problems (so-called quantum-inspired algorithms ). Rather than giving
way to quantum scepticism, we would instead quote David Deustch: ‘Quantum computation is [ ···]
nothing less than a distinctly new way of harnessing nature’ [The Fabric of Reality (1997), Chapter 9],
and embrace it as a new tool, that will help us apprehend more accura tely the numerous problems
faced in Quantitative Finance.
References
[1]A. Assouel, A. Jacquier, and A. Kondratyev ,A quantum generative adversarial network for distribution s,
Quantum Machine Intelligence, 4 (2022), p. 28.
[2]K. Bharti, A. Cervera-Lierta, T. H. Kyaw, T. Haug, S. Alperin-Lea , A. Anand, M. Degroote, H. Heimonen,
J. S. Kottmann, T. Menke, W.-K. Mok, S. Sim, L.-C. Kwek, and A. As puru-Guzik ,Noisy intermediate-scale
quantum (NISQ) algorithms , arXiv:2101.08448, (2021).
[3]C. Bruzewicz, J. Chiaverini, R. McConnell, and J. Sage ,Trapped-ion quantum computing: progress and
challenges , Applied Physics Reviews, 6 (2019).
[4]M. Cerezo, A. Arrasmith, R. Babbush, S. C. Benjamin, S. Endo, K. Fujii, J. McClean, K. Mitarai, X. Yuan,
L. Cincio, and P. Coles ,Variational quantum algorithms , Nature Reviews Physics, 3 (2021), pp. 625–644.
[5]S. Chakrabarti, H. Yiming, T. Li, S. Feizi, and X. Wu ,Quantum Wasserstein generative adversarial networks ,
Advances in Neural Information Processing Systems, 32 (201 9).
[6]E. Farhi, J. Goldstone, and S. Gutmann ,A quantum approximate optimization algorithm , arXiv:1411.4028,
(2014).
[7]E. Farhi and H. Neven ,Classiﬁcation with quantum neural networks on near term pro cessors, arXiv:1802.06002,
(2018).
[8]F. Fontanela, A. Jacquier, and M. Oumgari ,A quantum algorithm for linear PDEs arising in ﬁnance , SIAM
Journal on Financial Mathematics, 12 (2021), pp. 98–114.
[9]A. W. Harrow, A. Hassidim, and S. Lloyd ,Quantum algorithm for linear systems of equations , Physical review
letters, 103 (2009), p. 150502.
[10]Y. Huang, H. Lei, X. Li, and G. Yang ,Quantum maximum mean discrepancy GAN , Neurocomputing, 454 (2021),
pp. 88–100.
[11]A. Jacquier and O. Kondratyev ,Quantum Machine Learning and Optimisation in Finance: On th e Road to
Quantum Advantage , Packt Publishing Ltd, 2022.
[12]A. Kondratyev ,Non-diﬀerentiable learning of quantum circuit Born machin e with genetic algorithm , Wilmott,
2021 (2021), pp. 50–61.
[13]Y. Li and A. Neufeld ,Quantum Monte Carlo algorithm for solving Black-Scholes PD Es for high-dimensional
option pricing in ﬁnance and its proof of overcoming the curs e of dimensionality , arXiv:2301.09241, (2023).
[14]J. R. McClean, J. Romero, R. Babbush, and A. Aspuru-Guzik ,The theory of variational hybrid quantum-
classical algorithms , New Journal of Physics, 18 (2016), p. 023023.
[15]A. Montanaro ,Quantum speedup of Monte Carlo methods , Proceedings of the Royal Society A, 471 (2015).
[16]M. Y. Niu, A. Zlokapa, M. Broughton, S. Boixo, M. Mohseni, V. Smely anskyi, and H. Neven ,Entangling
quantum generative adversarial networks , Physical Review Letters, 128 (2022), p. 220505.
[17]A. Peruzzo, J. McClean, P. Shadbolt, M.-H. Yung, X.-Q. Zhou, P. J. Love, A. Aspuru-Guzik, and J. L.
O’brien ,A variational eigenvalue solver on a photonic quantum proce ssor, Nature communications, 5 (2014).
[18]J. Preskill ,Quantum computing in the NISQ era and beyond , Quantum, 2 (2018).
[19]H. Situ, Z. He, Y. Wang, L. Li, and S. Zheng ,Quantum GAN for generating discrete distribution , Information
Sciences, 538 (2020), pp. 193–208.
[20]A. Somoroff, Q. Ficheux, R. Mencia, H. Xiong, R. Kuzmin, and V. Man ucharyan ,Millisecond coherence in
a superconducting qubit , Physical Review Letters, 130 (2023).
[21]D. Stilck Franc ¸a and R. Garcia-Patron ,Limitations of optimization algorithms on noisy quantum de vices,
Nature Physics, 17 (2021), pp. 1221–1227.",2023-11-11T17:39:16Z, ias w ratr did s tquantum t fabric reality chapter quantitative nance referencas so uel jac er d rat ye quantum machine intellence hart cer v lie rta kya aug al  ilea aand  groove him net manmemok sim kw ek as ui noisy  bru ze wi cz chi oimc cor sage trapped applied psics reviews cerarea smith bab bush benjamiendo fui mc eamit ar articial intellence yuaic colvariatal nature reviews psics chara bart yi mi li  zi wu quantum passer steiadvancneural informatprocessi tems far hi goldstone gunma far hi eveass  montane la jac er ou gary journal nancial matmatics narrow has si lloyd quantum psical hu lei li ya quantum neuro uti jac er d rat ye quantum machine learni optiatnance oroad quantum advantage pack pubhi ltd d rat ye noborpot li nefeld quantum    schools  mc earomero bab bush spur ui t new journal psics montana ro quantum   proceedis royal society ni lok pa brought boi xo chosesm ely eveentali psical review ters u zz mc eahad bolt yu hou love spur ui nature pre skl quantum quantum situ  wa li c quantum informatsciencsom or off c ux mecia so ku zm imameconds psical review ters stl ck franc garcia patrolimitatns nature psics
paper_qf_5.pdf,5,Quantum Computing for Financial Mathematics,"  Quantum computing has recently appeared in the headlines of many scientific
and popular publications. In the context of quantitative finance, we provide
here an overview of its potential.
","QUANTUM COMPUTING FOR FINANCIAL MATHEMATICS 5
[22]D. Venturelli and A. Kondratyev ,Reverse quantum annealing approach to portfolio optimizat ion problems ,
Quantum Machine Intelligence, 1 (2019), pp. 17–30.
Department of Mathematics, Imperial College London, and th e Alan Turing Institute
Email address :a.jacquier@imperial.ac.uk
Abu Dhabi Investment Authority (ADIA), and Department of Mathe matics, Imperial College London
Email address :oleksiy.kondratyev@adia.ae
Email address :gtylee@gmail.com
University College London and Lloyds Banking
Email address :Mugad.Oumgari@lloydsbanking.com",2023-11-11T17:39:16Z,venture lli d rat ye reverse quantum machine intellence partment matmatics imial college londoala institute em articial intellence abu dhabi investment authority partment ma t imial college londoem articial intellence em articial intellence  college londolloyd banki em articial intellence mug ad ou gary
paper_qf_6.pdf,1,"Four Points Beginner Risk Managers Should Learn from Jeff Holman's
  Mistakes in the Discussion of Antifragile","  Using Jeff Holman's comments in Quantitative Finance to illustrate 4 critical
errors students should learn to avoid: 1) Mistaking tails (4th moment) for
volatility (2nd moment), 2) Missing Jensen's Inequality, 3) Analyzing the
hedging wihout the underlying, 4) The necessity of a numeraire in finance.
","Four Points Beginner Risk Managers Should Learn from Jeff
Holman’s Mistakes in the Discussion of Antifragile
Nassim Nicholas Taleb
January 2014
Abstract
Using Jeff Holman’s comments in Quantitative Finance to illustrate 4 critical errors students should learn
to avoid: 1) Mistaking tails (4th moment) for volatility (2nd moment), 2) Missing Jensen’s Inequality,
3) Analyzing the hedging wihout the underlying, 4) The necessity of a numéraire in ﬁnance.
The review of Antifragile by Mr Holman (dec 4, 2013) is replete with factual, logical, and analytical errors.
We will only list here the critical ones, and ones with generality to the risk management and quantitative
ﬁnance communities; these should be taught to students in quantitative ﬁnance as central mistakes to
avoid, so beginner quants and risk managers can learn from these fallacies.
1 Conﬂation of Second and Fourth Moments
It is critical for beginners not to fall for the following elementary mistake. Mr Holman gets the relation of
the VIX (volatility contract) to betting on ""tail events"" backwards. Let us restate the notion of ""tail events""
in the Incerto (that is the four books on uncertainty of which Antifragile is the last installment): it means a
disproportionate role of the tails in deﬁning the properties of distribution, which, mathematically, means
a smaller one for the ""body"".1
Mr Holman seems to get the latter part of the attributes of fattailedness in reverse. It is an error to
mistake the VIX for tail events. The VIX is mostly affected by at-the-money options which corresponds
to the center of the distribution, closer to the second moment not the fourth (at-the-money options are
actually linear in their payoff and correspond to the conditional ﬁrst moment). As explained about sev-
enteen years ago in Dynamic Hedging (Taleb, 1997) (see appendix), in the discussion on such tail bets, or
""fourth moment bets"", betting on the disproportionate role of tail events of fattailedness is done by selling
the around-the-money-options (the VIX) and purchasing options in the tails, in order to extract the sec-
ond moment and achieve neutrality to it (sort of becoming ""market neutral""). Such a neutrality requires
some type of ""short volatility"" in the body because higher kurtosis means lower action in the center of the
distribution.
1The point is staring at every user of spreadsheets: Kurtosis, or scaled fourth moment, the standard measure of fattailedness,
entails normalizing the fourth moment by the square of the variance.arXiv:1401.2524v1  [q-fin.GN]  11 Jan 2014",2014-01-11T11:44:39Z,four points beginner risk mana should learjeff humamistakdiscussanti frage nas sim nicholas tale uary abstra usi jeff humaquantitative nance mistaki missi jenseinequality analyzi t t anti frage mr huma cosecond fourth moments it mr huma icer to anti frage mr humait t as dynamic edgi tale sut kurt os is  
paper_qf_6.pdf,2,"Four Points Beginner Risk Managers Should Learn from Jeff Holman's
  Mistakes in the Discussion of Antifragile","  Using Jeff Holman's comments in Quantitative Finance to illustrate 4 critical
errors students should learn to avoid: 1) Mistaking tails (4th moment) for
volatility (2nd moment), 2) Missing Jensen's Inequality, 3) Analyzing the
hedging wihout the underlying, 4) The necessity of a numeraire in finance.
","TALEB Errors Quants Should Avoid
A more mathematical formulation is in the technical version of the Incerto : fat tails means ""higher
peaks"" for the distribution as, the fatter the tails, the more markets spend time between ",2014-01-11T11:44:39Z,errors quant should oid icer to
paper_qf_6.pdf,3,"Four Points Beginner Risk Managers Should Learn from Jeff Holman's
  Mistakes in the Discussion of Antifragile","  Using Jeff Holman's comments in Quantitative Finance to illustrate 4 critical
errors students should learn to avoid: 1) Mistaking tails (4th moment) for
volatility (2nd moment), 2) Missing Jensen's Inequality, 3) Analyzing the
hedging wihout the underlying, 4) The necessity of a numeraire in finance.
","TALEB Errors Quants Should Avoid
times5. There is a second critical mistake in the discussion: Mr Holman’s calculations here exclude the
payoff from actual in-the-moneyness.
One should remember that the VIX is not a price, but an inverse function, an index derived from a
price: one does not buy ""volatility"" like one can buy a tomato; operators buy options correponding to such
inverse function and there are severe, very severe nonlinearities in the effect. Although more linear than
tail options, the VIX is still convex to actual market volatility, somewhere between variance and standard
deviation, since a strip of options spanning all strikes should deliver the variance (Gatheral,2006). The
reader can go through a simple exercise. Let’s say that the VIX is ""bought"" at 10% -that is, the component
options are purchased at a combination of volatilities that corresponds to a VIX at that level. Assume
returns are in squares. Because of nonlinearity, the package could beneﬁt from an episode of 4% volatility
followed by an episode of 15%, for an average of 9.5%; Mr Holman believes or wants the reader to
believe that this 0.5 percentage point should be treated as a loss when in fact second order un-evenness
in volatility changes are more relevant than the ﬁrst order effect.
3 The Inseparability of Insurance and Insured
One should never calculate the cost of insurance without offsetting it with returns generated from pack-
ages than one would not have purchased otherwise.
Even had he gotten the sign right on the volatility, Mr Holman in the example above analyzes the
performance of a strategy buying options to protect a tail event without adding the performance of the
portfolio itself, like counting the cost side of the insurance without the performance of what one is insuring
that would not have been bought otherwise. Over the same period he discusses the market rose more than
100%: a healthy approach would be to compare dollar-for-dollar what an investor would have done (and,
of course, getting rid of this ""VIX"" business and focusing on very small dollars invested in tail options that
would allow such an aggressive stance). Many investors (such as this author) would have stayed out of
the market, or would not have added funds to the market, without such an insurance.
4 The Necessity of a Numéraire in Finance
There is a deeper analytical error.
A barbell is deﬁned as a bimodal investment strategy, presented as investing a portion of your portfolio
in what is explicitly deﬁned as a ""numéraire repository of value"" ( Antifragile ), and the rest in risky securities
(Antifragile indicates that such numéraire would be, among other things, inﬂation protected). Mr Holman
goes on and on in a nihilistic discourse on the absence of such riskless numéraire (of the sort that can
lead to such sophistry as ""he is saying one is safer on terra ﬁrma than at sea, but what if there is an
earthquake?"").
5An event this author witnessed, in the liquidation of Victor Niederhoffer, options sold for $.05 were purchased back at up to
$38, which bankrupted Refco, and, which is remarkable, without the options getting close to the money: it was just a panic rise in
implied volatility.
c2014 N.N. Taleb 3",2014-01-11T11:44:39Z,errors quant should oid tre mr humaone although gatr al t   because mr humat insebity insurance insured one evemr humaomany t necessity num nance tre anti frage anti frage mr humaavior wier offer ref co tale
paper_qf_6.pdf,4,"Four Points Beginner Risk Managers Should Learn from Jeff Holman's
  Mistakes in the Discussion of Antifragile","  Using Jeff Holman's comments in Quantitative Finance to illustrate 4 critical
errors students should learn to avoid: 1) Mistaking tails (4th moment) for
volatility (2nd moment), 2) Missing Jensen's Inequality, 3) Analyzing the
hedging wihout the underlying, 4) The necessity of a numeraire in finance.
","TALEB Errors Quants Should Avoid
Figure 1: First Method to Extract the Fourth Moment, from Dynamic Hedging, 1997.
The familiar Black and Scholes derivation uses a riskless asset as a baseline; but the literature since
around 1977 has substituted the notion of ""cash"" with that of a numéraire , along with the notion that one
can have different currencies, which technically allows for changes of probability measure. A numéraire
is deﬁned as the unit to which all other units relate . ( Practically, the numéraire is a basket the variations of
which do not affect the welfare of the investor.) Alas, without numéraire, there is no probability measure,
and no quantitative in quantitative ﬁnance, as one needs a unit to which everything else is brought back
to. In this (emotional) discourse, Mr Holton is not just rejecting the barbell per se, but any use of the
expectation operator with any economic variable, meaning he should go attack the tens of thousand
research papers and the existence of the journal Quantitative Finance itself.
Clearly, there is a high density of other mistakes or incoherent statements in the outpour of rage in Mr
Holman’s review; but I have no doubt these have been detected by the Quantitative Finance reader and,
as we said, the object of this discussion is the prevention of analytical mistakes in quantitative ﬁnance.
To conclude, this author welcomes criticism from the ﬁnance community that are not straw man
arguments, or, as in the case of Mr Holmam, violate the foundations of the ﬁeld itself.
c2014 N.N. Taleb 4",2014-01-11T11:44:39Z,errors quant should oid  rst method extra fourth moment dynamic edgi t  schools praically alas imr boltoquantitative nance  mr humaquantitative nance to mr holm am tale
paper_qf_6.pdf,5,"Four Points Beginner Risk Managers Should Learn from Jeff Holman's
  Mistakes in the Discussion of Antifragile","  Using Jeff Holman's comments in Quantitative Finance to illustrate 4 critical
errors students should learn to avoid: 1) Mistaking tails (4th moment) for
volatility (2nd moment), 2) Missing Jensen's Inequality, 3) Analyzing the
hedging wihout the underlying, 4) The necessity of a numeraire in finance.
","TALEB Errors Quants Should Avoid
Figure 2: Second Method to Extract the Fourth Moment , from Dynamic Hedging, 1997.
c2014 N.N. Taleb 5",2014-01-11T11:44:39Z,errors quant should oid  second method extra fourth moment dynamic edgi tale
paper_qf_6.pdf,6,"Four Points Beginner Risk Managers Should Learn from Jeff Holman's
  Mistakes in the Discussion of Antifragile","  Using Jeff Holman's comments in Quantitative Finance to illustrate 4 critical
errors students should learn to avoid: 1) Mistaking tails (4th moment) for
volatility (2nd moment), 2) Missing Jensen's Inequality, 3) Analyzing the
hedging wihout the underlying, 4) The necessity of a numeraire in finance.
","TALEB Errors Quants Should Avoid
Appendix (Discussion of Betting on Tails of Distribution in Dynamic Hedging, 1997 )
From Dynamic Hedging , pages 264-265:
A fourth moment bet is long or short the volatility of volatility. It could be achieved either with
out-of-the-money options or with calendars. Example: A ratio ""backspread"" or reverse spread is
a method that includes the buying of out-of-the-money options in large amounts and the selling
of smaller amounts of at-the-money but making sure the trade satisﬁes the ""credit"" rule (i.e., the
trade initially generates a positive cash ﬂow). The credit rule is more difﬁcult to interpret when
one uses in-the-money options. In that case, one should deduct the present value of the intrinsic
part of every option using the put-call parity rule to equate them with out-of-the-money.
The trade shown in Figure 1 was accomplished with the purchase of both out-of-the-money puts
and out-of-the-money calls and the selling of smaller amounts of at-the-money straddles of the
same maturity.
Figure 2 shows the second method, which entails the buying of 60- day options in some amount
and selling 20-day options on 80% of the amount. Both trades show the position beneﬁting from
the fat tails and the high peaks. Both trades, however, will have different vega sensitivities, but
close to ﬂat modiﬁed vega.
Appendix: The Body, The Shoulders, and The Tails
From the new book (Taleb 2014): We assume tails start at the level of convexity of the segment of the
probability distribution to the scale of the distribution.
Where Do the Tails Start? The Crossovers and Tunnel Effect.
Notice in Figure 3 a series of crossover zones, invariant to a. Distributions called ""bell shape"" have a
convex-concave-convex shape (or quasi-concave shape).
Let X be a random variable, the distribution of which p(x)is from a general class of all unimodal
one-parameter continous pdfs pwith supportDRand scale parameter . Let p(.)be quasi-concave
on the domain, but neither convex nor concave. The density function p(x)satisﬁes: p(x)p(x+)for
all>0, and x>xandp(x)p(x",2014-01-11T11:44:39Z,errors quant should oid  discussbei articial intellence ls distributdynamic edgi from dynamic edgi it  t it   both both  t body t shoulrs t articial intellence ls from tale  wre do articial intellence ls start t ossovers tunnel effe notice  distributns  rand  t
paper_qf_6.pdf,7,"Four Points Beginner Risk Managers Should Learn from Jeff Holman's
  Mistakes in the Discussion of Antifragile","  Using Jeff Holman's comments in Quantitative Finance to illustrate 4 critical
errors students should learn to avoid: 1) Mistaking tails (4th moment) for
volatility (2nd moment), 2) Missing Jensen's Inequality, 3) Analyzing the
hedging wihout the underlying, 4) The necessity of a numeraire in finance.
","TALEB Errors Quants Should Avoid
a4a
a3 a2 a1“Shoulders”
/LParen1a1,a2/RParen1,
/LParen1a3,a4/RParen1“Peak”
(a2,a3/RParen1
Right tail
Left tail0.10.20.30.40.50.6a4a
a3 a2 a1“Shoulders”
/LParen1a1,a2/RParen1,
/LParen1a3,a4/RParen1“Peak”
(a2,a3/RParen1
Right tail
Left tail
/Minus4 /Minus2 2 40.10.20.30.40.50.6
Figure 3: Fatter and Fatter Tails through perturbation of . The mixed distribution with stochastic volatil-
ity coefﬁcientfvig4
i=1=f0,1
4,1
2,3
4g. We can see crossovers a1through a4. The ""tails"" proper start at a4on
the right and a1on the left.
3. There exist intermediate tunnels, the ""shoulders"", where p(x)p(x)ifx2",2014-01-11T11:44:39Z,errors quant should oid shoulrs arearearearepeak arerht left shoulrs arearearearepeak arerht left minus minus  fatr fatr articial intellence ls t  t tre
paper_qf_6.pdf,8,"Four Points Beginner Risk Managers Should Learn from Jeff Holman's
  Mistakes in the Discussion of Antifragile","  Using Jeff Holman's comments in Quantitative Finance to illustrate 4 critical
errors students should learn to avoid: 1) Mistaking tails (4th moment) for
volatility (2nd moment), 2) Missing Jensen's Inequality, 3) Analyzing the
hedging wihout the underlying, 4) The necessity of a numeraire in finance.
","TALEB Errors Quants Should Avoid
In ﬁgure 3, the crossovers for the intervals are numerically f",2014-01-11T11:44:39Z,errors quant should oid in
paper_qf_7.pdf,1,Volatility made observable at last,"  The Cartier-Perrin theorem, which was published in 1995 and is expressed in
the language of nonstandard analysis, permits, for the first time perhaps, a
clear-cut mathematical definition of the volatility of a financial asset. It
yields as a byproduct a new understanding of the means of returns, of the beta
coefficient, and of the Sharpe and Treynor ratios. New estimation techniques
from automatic control and signal processing, which were already successfully
applied in quantitative finance, lead to several computer experiments with some
quite convincing forecasts.
","arXiv:1102.0683v1  [q-fin.CP]  3 Feb 2011Volatility made observable at last
MichelFliess1, C´ edricJoin2,3, Fr´ ed´ eric Hatt4
1LIX (CNRS, UMR 7161), ´Ecole polytechnique, 91228 Palaiseau, France
Michel.Fliess@polytechnique.edu
2CRAN (CNRS, UMR 7039), Nancy-Universit´ e, BP 239, 54506 Vandœ uvre-l` es-Nancy, France
Cedric.Join@cran.uhp-nancy.fr
3´Equipe Non-A, INRIA Lille – Nord-Europe, France
4Lucid Capital Management, 2 avenue Charles de Gaulle, BP 351, 2013 Luxembourg, Luxembourg
hatt@lucid-cap.com
Abstract — The Cartier-Perrin theorem, which was published in 1995
and is expressed in the language of nonstandard analysis , permits,
for the ﬁrst time perhaps, a clear-cut mathematical deﬁniti on of the
volatility of a ﬁnancial asset. It yields as a byproduct a new under-
standing of the means of returns, of the beta coeﬃcient, and o f the
Sharpe and Treynor ratios. New estimation techniques from a uto-
matic control and signal processing, which were already suc cessfully
applied in quantitative ﬁnance, lead to several computer ex periments
with some quite convincing forecasts.
Keywords —Time series, quantitative ﬁnance, trends, returns, volat il-
ity, beta coeﬃcient, Sharpe ratio, Treynor ratio, forecast s, estimation
techniques, numerical diﬀerentiation, nonstandard analy sis.
I. Introduction
Although volatility, which reﬂects the price ﬂuctuations,
isubiquitousin quantitativeﬁnance(see, e.g., [3], [18], [22],
[28], [32], [37], and the references therein), Paul Wilmott
writes rightly ([37], chap. 49, p. 813):
Quite frankly, we do not know what volatility currently is,
never mind what it may be in the future .
Our title is explained by sentences like the following one in
Tsay’s book ([35], p. 98):
...volatility is not directly observable ...
The lack moreover of any precise mathematical deﬁnition
leadstomultiplewaysforcomputingvolatilitywhichareby
no meansequivalent andmight evenbe sometimes mislead-
ing (see, e.g., [20]). Our theoretical formalism and the cor-
responding computer simulations will conﬁrm what most
practitioners already know. It is well expressed by Gunn
([21], p. 49):
Volatility is not only referring to something that ﬂuctuate s
sharply up and down but is also referring to something that
moves sharply in a sustained direction .
The existence of trends [11] for time series, which should
be viewed as the means, oraverages, of those series, yields
•anaturalandstraightforwardmodel-freedeﬁnitionofthe
variance (resp. covariance) of one (resp. two) time series,
•simple forecasting techniques which are based on similar
techniques to those in [11], [12], [13], [14].
Exploiting the above approach to volatility for the return
of some ﬁnancial asset necessitates some care due to the
highly ﬂuctuating character of returns. This is accom-
plished by considering the means of the time series asso-
derived as byproducts:1. We complete [13] with a new deﬁnition of the classic
beta coeﬃcient for returns. It should bypass most of the
existing criticisms.
2. The Sharpe ([30], [31]) and Treynor ratios, which are
famous performance measures for trading strategies (see,
e.g., [3], [28], [34], [37], and the references therein), are
connected to a quite arbitrary ﬁnancial time series. They
might lead to new and useful trading indicators .
Remark 1 : Thegraphicalrepresentationofalltheabove
quantities boils down to the drawing of means which has
been already successfully achieved in [11], [12], [13], [14].
Our paper is organized as follows. After recalling the
Cartier-Perrin theorem [6], Section II deﬁnes (co)variances
and volatility. In order to apply this setting to ﬁnancial
returns, Section III deﬁnes the means of returns and sug-
gests deﬁnitions of the beta coeﬃcient, and of the Sharpe
and Treynor ratios. Numerous quite convincing computer
experiments are shown in Section IV, which displays also
excellent forecastsforthe volatility. Some shortdiscussions
on the concept of volatility may be found in Section V.
II. Mean, variance and covariance revisited
A. Time series via nonstandard analysis
A.1 Inﬁnitesimal sampling
Take the time interval [0 ,1]⊂Rand introduce as often
innonstandard analysis the inﬁnitesimal sampling
T={0 =t0< t1<···< tν= 1}
whereti+1−ti, 0≤i < ν, isinﬁnitesimal ,i.e., “very
small”.1Atime series X(t) is a function X:T→R.
A.2S-integrability
TheLebesgue measure onTis the function mdeﬁned on
T\\\\{1}bym(ti) =ti+1−ti. The measure of any interval
[c,d[⊂I,c≤d, is its length d−c. Theintegralover [c,d[
of the time series X(t) is the sum
/integraldisplay
[c,d[Xdm=/summationdisplay
t∈[c,d[X(t)m(t)
1",2011-02-03T13:46:49Z,  volatity micl flijoifr hat ecole pal articial intellence eau france micl flinancy  and nancy france metric joiequip nolle nord  france lucid capital management charlgauge luxemrg luxemrg abstra t carrier soit share ty nor new keywords time share ty nor introdualthough paul pot quite our say t our it gunvolatity t eloiti   it t share ty nor ty remark t graphical representatof all t above our after carrier soseiseshare ty nor numerous sesome semeatime itake rand time t le be gue tis t t integral odm
paper_qf_7.pdf,2,Volatility made observable at last,"  The Cartier-Perrin theorem, which was published in 1995 and is expressed in
the language of nonstandard analysis, permits, for the first time perhaps, a
clear-cut mathematical definition of the volatility of a financial asset. It
yields as a byproduct a new understanding of the means of returns, of the beta
coefficient, and of the Sharpe and Treynor ratios. New estimation techniques
from automatic control and signal processing, which were already successfully
applied in quantitative finance, lead to several computer experiments with some
quite convincing forecasts.
","Xis said to be S-integrable if, and only if, for any interval
[c,d[ the integral/integraltext
[c,d[|X|dmislimited2and, ifd−cis
inﬁnitesimal, also inﬁnitesimal.
A.3 Continuity and Lebesgue integrability
XisS-continuous attι∈Tif, and only if, f(tι)≃f(τ)
whentι≃τ.3Xissaidtobe almost continuous if, andonly
if, it isS-continuous on T\\\\R, whereRis araresubset.4
XisLebesgue integrable if, and only if, it is S-integrable
and almost continuous.
A.4 Quick ﬂuctuations
A time series X:T→Ris said to be quickly ﬂuctuating ,
oroscillating , if, and only if, it is S-integrable and/integraltext
AXdm
is inﬁnitesimal for any quadrable subset.5
A.5 The Cartier-Perrin theorem
LetX:T→Rbe aS-integrable time series. Then,
according to the Cartier-Perrin theorem [6],6the additive
decomposition
X(t) =E(X)(t)+Xﬂuctuation (t) (1)
holds where
•themean, oraverage,E(X)(t) is Lebesgue integrable,7
•Xﬂuctuation (t) is quickly ﬂuctuating.
The decomposition (1) is unique up to an inﬁnitesimal.
Remark 2 : E(X)(t), which is “smoother” than X(t),
provides a mathematical justiﬁcation [11] of the trendsin
technical analysis (see,e.g., [2], [25]).
Remark 3 : Calculations of the means and of its deriva-
tives, if they exist, are deduced, via new estimation tech-
niques, from the denoising results in [17], [27] (see also
[19]), which extend the familiar moving averages, which
are classic in technical analysis (see, e.g., [2], [25]).
B. Variances and covariances
B.1 Squares and products
Take two S-integrable time series X(t),Y(t), such that
their squares and the squares of E(X)(t) andE(Y)(t) are
alsoS-integrable. The Cauchy-Schwarz inequality shows
that the products
•X(t)Y(t),E(X)(t)E(Y)(t),
•E(X)(t)Yﬂuctuation (t),Xﬂuctuation (t)E(Y)(t),
•Xﬂuctuation (t)Yﬂuctuation (t)
are allS-integrable.
B.2 Diﬀerentiability
Assume moreover that E(X)(t) andE(Y)(t) arediﬀer-
entiable in the following sense: there exist two Lebesgue
integrable time series f,g:T→R, such that, ∀t∈T,
2A real number is limitedif, and only if, it is not inﬁnitely large.
3a≃bmeans that a−bis inﬁnitesimal.
4The set Ris said to be rare[6] if, for any standard real number
α >0, there exists an internal set B⊃Asuch that m(B)≤α.
5A set is quadrable [6] if its boundary is rare.
6Remember that this result led to a new foundation [9] of the an al-
ysis of noises in automatic control and in signal processing . A more
down to earth exposition may be found in [26].
7E(X)(t) was called trendwith the possible exception of a limited number of val-
ues oft,E(X)(t) =E(X)(0) +/integraltextt
0f(τ)dτ,E(Y)(t) =
E(Y)(0)+/integraltextt
0g(τ)dτ. Integrating by parts shows that the
products E(X)(t)Yﬂuctuation (t) andXﬂuctuation (t)E(Y)(t)
are quickly ﬂuctuating [9].
Remark 4 : Let us emphasize that the product
Xﬂuctuation (t)Yﬂuctuation (t)
is not necessarily quickly ﬂuctuating.
B.3 Deﬁnitions
1. Thecovariance of two time series X(t) andY(t) is
cov(XY)(t) =E((X−E(X))(Y−E(Y)))(t)
≃E(XY)(t)−E(X)(t)×E(Y)(t)
2. Thevariance of the time series X(t) is
var(X)(t) =E/parenleftbig
(X−E(X))2/parenrightbig
(t)
≃E(X2)(t)−(E(X)(t))2
3. Thevolatility ofX(t) is the corresponding standard de-
viation
vol(X)(t) =/radicalbig
var(X)(t) (2)
The volatility of a quite arbitrary time series seems to be
precisely deﬁned here for the ﬁrst time.
Remark 5 : Another possible deﬁnition of the volatility
(see [20]), which is not equivalent to Equation (2), is the
following one
E(|X−E(X)|)(t)
It will not be exploited here.
III. Returns
A. Deﬁnition
Assume from now on that, for any t∈T,
0< m < X (t)< M
wherem,Mareappreciable .8This is a realistic assump-
tion ifX(t) is the price of some ﬁnancial asset A. Thelog-
arithmic return , orlog-return ,9ofXwith respect to some
limited time interval∆ T >0 isthe time series R∆Tdeﬁned
by
R∆T(X)(t) = ln/parenleftbiggX(t)
X(t−∆T)/parenrightbigg
= lnX(t)−lnX(t−∆T)
FromX(t)
X(t−∆T)= 1+X(t)−X(t−∆T)
X(t−∆T), we know that
R∆T(X)(t)≃X(t)−X(t−∆T)
X(t−∆T)(3)
ifX(t)−X(t−∆T) is inﬁnitesimal. The right handside of
Equation (3) is the arithmetic return.
Thenormalized logarithmic return is
r∆T(X)(t) =R∆T(t)
∆T(4)
8A real number is appreciable if, and only if, it is neither inﬁnitely
small nor inﬁnitely large.
9The terminology continuously compounded return",2011-02-03T13:46:49Z,is continuity le be gue is t is articial intellence to be is is le be gue quick is dm t carrier so be tcarrier sole be gue t remark remark calculatns variance squartake t catc schwartz di  le be gue t is suremember integrati remark   t co variance t variance t volatity t remark anotr equatit urns   mare appreciable  t log with  from t equatt normalized t
paper_qf_7.pdf,3,Volatility made observable at last,"  The Cartier-Perrin theorem, which was published in 1995 and is expressed in
the language of nonstandard analysis, permits, for the first time perhaps, a
clear-cut mathematical definition of the volatility of a financial asset. It
yields as a byproduct a new understanding of the means of returns, of the beta
coefficient, and of the Sharpe and Treynor ratios. New estimation techniques
from automatic control and signal processing, which were already successfully
applied in quantitative finance, lead to several computer experiments with some
quite convincing forecasts.
","B. Mean
B.1 Deﬁnition
ReplaceX:T→Rby
lnX:T→R, t/mapsto→ln(X(t))
where the logarithms of the prices are taken into account.
Apply the Cartier-Perrin theorem to ln X. Themean, or
average, ofr∆T(t) given by Equation (4) is
¯r∆T(X)(t) =E(lnX)(t)−E(lnX)(t−∆T)
∆T(5)
As a matter of fact r∆T(X) and ¯r∆T(X) are related by
r∆T(X)(t) = ¯r∆T(X)(t)+quick ﬂuctuations
Assume that E(X) andE(lnX) are diﬀerentiable accord-
ing to Section II-B.2. Call the derivative of E(lnX)
thenormalized mean logarithmic instantaneous return and
write
¯r(X)(t) =d
dtE(lnX)(t) (6)
Note that E(lnX)(t)≃ln(E(X)(t)) if in Equation (1)
Xﬂuctuation (t)≃0. Then ¯ r(X)(t)≃d
dtE(X)(t)
E(X)(t).
B.2 Application to beta
Take two assets AandBsuch that their normalized
logarithmic returns r∆T(A)(t) andr∆T(B)(t), deﬁned by
Equation (4), exist.10Following Equation (5), consider the
space curve t,¯r∆T(A)(t),¯r∆T(B)(t) in the Euclidean space
with coordinates t,x,y. Its projection on the x,yplane is
the plane curve Cdeﬁned by
xC(t) = ¯r∆T(A)(t),yC(t) = ¯r∆T(B)(t)
The tangent of Cat a regular point, which is deﬁned by
dxC(t)
dt,dyC(t)
dt, yields, ifdxC(t)
dt/\\\\e}atio\\\\slash= 0,
∆yC≈β(t)∆xC (7)
where
•∆xC=xC(t+h)−xC(t), ∆yC=yC(t+h)−yC(t);
•h∈Ris “small”;
•
β(t) =dyC(t)
dt
dxC(t)
dt(8)
WhenyC(t) may be viewed locally as a smooth function of
xC(t), Equation (8) becomes
β(t) =dyC
dxC
B.3 The Treynor ratio of an asset
LetβM(A)(t) be the beta coeﬃcient deﬁned in Section
III-B.2 for Awith respect to the market portfolio M. De-
ﬁne theTreynor ratio and theinstantaneous Treynor ratio
ofAwith respect to Mrespectively by
TRM,∆T(A)(t) =¯r∆T(A)(t)
βM(A)(t),TRM(A)(t) =¯r(A)(t)
βM(A)(t)
10C. Volatility
Formulae (2), (4), (5), (6) yield the following mathemat-
ical deﬁnition of the volatility of the asset A:
vol∆T(A)(t) =/radicalbig
E(r∆T−¯r∆T)2(t) (9)
which yields
vol∆T(A)(t)≃/radicalBig
E(r2
∆T)(t)−(¯r∆T(t))2
The value at time tofvol∆T(A) may be viewed as the
actualvolatility (see, e.g., [37], chap. 49, pp. 813-814).
Remark 6 : A crucial diﬀerence between Formula (9)
and the usual historical , orrealized, volatilities (see, e.g.,
[37], chap. 49, pp. 813-814) lies in the presence of a non-
constant mean. It is often assumed to be 0 in the existing
literature.
Remark 7 : There is no connection with
•theimpliedvolatility, which is connected to the Black-
Scholes modeling (see, e.g., [37], chap. 49, pp. 813-814),
•the recent model-free implied volatility (see [4], and [23],
[29]), although the origin of our viewpoint may be partly
traced back to our model-free control strategy ([10], [24]).
D. The Sharpe ratio of an asset
Deﬁne the Sharpe ratio of the asset Aby
SR∆T(A)(t) =¯r∆T(A)(t)
vol∆T(A)(t)(10)
According to [1], p. 52, it is quite close to some utilization
of the Sharpe ratio in high-frequency trading.
IV. Computer experiments
We have utilized the following three listed shares:
1. IBM from 1962-01-02 to 2009-07-21 (11776 days) (Fig-
ures 1 and 2),
2. JPMORGAN CHASE (JPM) from 1983-12-30 until
2009-07-21 (6267 days) (Figures 3),
3. COCA COLA (CCE) from 1986-11-24 until 2009-07-21
(5519 days) (Figures 4).
Figures 1 and 3 show a “better” behavior for the normal-
ized mean logarithmic return (6), i.e., ¯r(t) is less aﬀected
by an abrupt short price variation. Such variations are
nevertheless causing important variations on our volatility,
with only a “slow mean return”. We suggest an adap-
tive threshold for attenuating this annoying feature, which
does not reﬂect well the price behavior. Note the excellent
volatility forecasts which are obtained via elementary nu-
merical recipes as in [11], [12], [13], [14]. Our forecasting
results, which are easily computable, seem to be more re-
liable than those obtained via the celebrated ARCH type
techniques, which go back to Engle (see [36] and the refer-
ences therein).11
The beta coeﬃcients is computed with respect to the
S&P 500 (see Figures 5). The results displayed in Figures
6 are obtained via the numerical techniques of [13].
11",2011-02-03T13:46:49Z,mea replace by apply carrier sot meaequatas  secall note equattapplicattake and suequatfollowi equateuiaits c t cat is quatt ty nor  sewith  ty nor ty nor with respeively volatity e b t remark  it remark tre  schools t share  share by accordi share uter   s s s su note our eagle t s t s
paper_qf_7.pdf,4,Volatility made observable at last,"  The Cartier-Perrin theorem, which was published in 1995 and is expressed in
the language of nonstandard analysis, permits, for the first time perhaps, a
clear-cut mathematical definition of the volatility of a financial asset. It
yields as a byproduct a new understanding of the means of returns, of the beta
coefficient, and of the Sharpe and Treynor ratios. New estimation techniques
from automatic control and signal processing, which were already successfully
applied in quantitative finance, lead to several computer experiments with some
quite convincing forecasts.
","0 2000 4000 6000 8000 10000 120000100200300400500600700
Time
(a)Daily price
0 2000 4000 6000 8000 10000 12000−1.4−1.2−1−0.8−0.6−0.4−0.200.2
Time
(b)Normalized logarithmic return r(t)0 2000 4000 6000 8000 10000 12000−5−4−3−2−10123x 10−3
Time
(c)Normalized mean logarithmic return ¯ r(t)
0 2000 4000 6000 8000 10000 1200000.010.020.030.040.050.060.07
Time
(d)vol(IBM)(t) (–) and 5 days forecasting (- -)0 2000 4000 6000 8000 10000 1200000.010.020.030.040.050.060.07
Time
(e)vol(IBM)(t) (–) and 20 days forecasting (- -)
Fig. 1. IBM",2011-02-03T13:46:49Z,time articial intellence ly time normalized time normalized time time 
paper_qf_7.pdf,5,Volatility made observable at last,"  The Cartier-Perrin theorem, which was published in 1995 and is expressed in
the language of nonstandard analysis, permits, for the first time perhaps, a
clear-cut mathematical definition of the volatility of a financial asset. It
yields as a byproduct a new understanding of the means of returns, of the beta
coefficient, and of the Sharpe and Treynor ratios. New estimation techniques
from automatic control and signal processing, which were already successfully
applied in quantitative finance, lead to several computer experiments with some
quite convincing forecasts.
","0 2000 4000 6000 8000 10000 12000−0.2−0.15−0.1−0.0500.050.10.15
Time
(a)Modiﬁed normalized logarithmic return r(t)0 2000 4000 6000 8000 10000 1200000.0050.010.0150.020.0250.03
Time
(b)vol(IBM)(t) (–) and 20 days forecasting (- -)
Fig. 2. IBM
01000 2000 3000 4000 5000 6000 7000050100150
Time
(a)Daily price01000 2000 3000 4000 5000 6000 7000−0.4−0.3−0.2−0.100.10.20.3
Time
(b)Modiﬁed normalized logarithmic return r(t)
01000 2000 3000 4000 5000 6000 7000−2.5−2−1.5−1−0.500.511.522.5x 10−3
Time
(c)Normalized mean logarithmic return ¯ r(t)01000 2000 3000 4000 5000 6000 700000.010.020.030.040.050.06
Time
(d)vol(JPM)(t) (–) and 20 days forecasting (- -)
Fig. 3. JPMORGAN CHASE (JPM)",2011-02-03T13:46:49Z,time modi time  time articial intellence ly time modi time normalized time 
paper_qf_7.pdf,6,Volatility made observable at last,"  The Cartier-Perrin theorem, which was published in 1995 and is expressed in
the language of nonstandard analysis, permits, for the first time perhaps, a
clear-cut mathematical definition of the volatility of a financial asset. It
yields as a byproduct a new understanding of the means of returns, of the beta
coefficient, and of the Sharpe and Treynor ratios. New estimation techniques
from automatic control and signal processing, which were already successfully
applied in quantitative finance, lead to several computer experiments with some
quite convincing forecasts.
","0 1000 2000 3000 4000 5000 6000010203040506070
Time
(a)Daily price0 1000 2000 3000 4000 5000 6000−0.25−0.2−0.15−0.1−0.0500.050.10.150.2
Time
(b)Modiﬁed normalized logarithmic return r(t)
0 1000 2000 3000 4000 5000 6000−2−1.5−1−0.500.511.522.5x 10−3
Time
(c)Normalized mean logarithmic return ¯ r(t)0 1000 2000 3000 4000 5000 600000.0050.010.0150.020.0250.030.035
Time
(d)vol(CCE)(t) (–) and 20 days forecasting (- -)
Fig. 4. COCA COLA (CCE)
Figure 7 displays the Sharpe ratio of S&P 500. With
∆t= 10 a trend is diﬃcult to guess in Figure 7-(a). Figure
7-(b) on the other hand, where ∆ t= 100, exhibits a well-
deﬁned trend which yields a quite accurate forecasting of
10 days.
V. Conclusion
Although we have proposed a precise and elegant math-
ematical deﬁnition of volatility, which
•yields eﬃcient and easily implementable computations,
•will soon be exploited for a dynamic portfolio manage-
ment [15],
the harsh criticisms against its importance in ﬁnancial en-
gineering should certainly not be dismissed (see, e.g., [33]).
Note forinstance that wehavenot triedhere toforecastex-
treme events, i.e., abrupt changes (see [16]) with this tool.
This aim has been already quite successfully achieved in
[11], [12], [13], [14], not via volatility but by taking ad-
vantage of indicators that are related to prices and not to
returns.
References
[1] Alridge I., High-Frequency Trading . Wiley, 2010.
[2] B´ echu T., Bertrand E., Nebenzahl J., L’analyse technique (6e
´ ed.). Economica, 2008.
[3] Bodie Z., Kane A.,MarcusA.J., Investments (7thHill, 2008.[4] Britten-Jones M., Neuberger A., Option prices, implied price
processes, and stochastic volatility. J. Finance , vol. 55, pp. 839-
866, 2000.
[5] Campbell J.Y., Lo A.W., MacKinlay A.C., The Econometrics
of Financial Markets . Princeton University Press, 1997.
[6] Cartier P., Perrin Y., Integration over ﬁnite sets. In Nonstandard
Analysis in Practice , F. & M. Diener (Eds), Springer, 1995, pp.
195-204.
[7] Diener F., Diener M., Tutorial. In F. & M. Diener (Eds): Non-
standard Analysis in Practice . Springer, pp. 1-21, 1995.
[8] Diener F., Reeb G., Analyse non standard . Hermann, 1989.
[9] Fliess M., Analyse non standard du bruit. C.R. Acad. Sci. Paris
Ser. I, vol. 342, pp. 797-802, 2006.
[10] Fliess M., Join C., Commande sans mod` ele et commande ` a
mod` ele restreint. e-STA, vol. 5 (n◦4), pp. 1-23, 2008 (available
athttp://hal.archives-ouvertes.fr/inria-00288107/en/ ).
[11] Fliess M., Join C., A mathematical proof of the existenc e of
trends in ﬁnancial time series. In Systems Theory :Modeling,
Analysis and Control , A. El Jai, L. Aﬁﬁ, E. Zerrik (Eds),
Presses Universitaires de Perpignan, 2009, pp. 43–62 (avai lable
athttp://hal.archives-ouvertes.fr/inria-00352834/en/ ).
[12] Fliess M., Join C., Towards new technical indicators fo r trading
systems and riskmanagement. 15thIFACSymp. System Identif.,
Saint-Malo, 2009 (available at
http://hal.archives-ouvertes.fr/inria-00370168/en/ ).
[13] Fliess M., Join C., Systematic risk analysis: ﬁrst step s towards
a new deﬁnition of beta. COGIS, Paris, 2009 (available at
http://hal.archives-ouvertes.fr/inria-00425077/en/ ).
[14] Fliess M., Join C., Delta hedging in ﬁnancial engineeri ng: to-
wards a model-free setting. 18thMedit. Conf. Control Automat.,
Marrakech, 2010 (available at
http://hal.archives-ouvertes.fr/inria-00479824/en/ ).
",2011-02-03T13:46:49Z,time articial intellence ly time modi time normalized time   share with   conusalthough note  referencal ridge hh frequency tradi rey bertrand ne  ahl economic bodikane marcus investments hl wriejonneu berger optnance campbell lo mac kilay t econometrics nancial markets princeto  carrier sointegratinonstandard analysis praice tiene eds  tiene tiene tutorial itiene eds noanalysis praice  tiene ree analyse rmaflianalyse acad sci paris ser flijoicoand flijoiitems tory moli analysis contrel articial intellence er rik eds  articial intellence re pnaflijoitowards sym tem int  articial intellence nt male flijoitematic paris flijoilta med it conf contrauto mat marrakech
paper_qf_7.pdf,7,Volatility made observable at last,"  The Cartier-Perrin theorem, which was published in 1995 and is expressed in
the language of nonstandard analysis, permits, for the first time perhaps, a
clear-cut mathematical definition of the volatility of a financial asset. It
yields as a byproduct a new understanding of the means of returns, of the beta
coefficient, and of the Sharpe and Treynor ratios. New estimation techniques
from automatic control and signal processing, which were already successfully
applied in quantitative finance, lead to several computer experiments with some
quite convincing forecasts.
","0 5000 10000 1500002004006008001000120014001600
Time
(a)Daily price0 5000 10000 15000−0.1−0.0500.050.10.15
Time
(b)Modiﬁed normalized logarithmic return r(t)
0 5000 10000 15000−2−1.5−1−0.500.511.5x 10−3
Time
(c)Normalized mean logarithmic return ¯ r(t)0 5000 10000 1500000.0050.010.0150.020.025
Time
(d)vol(S&P500)(t) (–) and 20 days forecasting (- -)
Fig. 5. S&P 500
sˆ ure syst` emes complexes, Agadir, 2011 (soon available at
http://hal.archives-ouvertes.fr/ ).
[16] Fliess M., Join C., Mboup M., Algebraic change-point de tection.
Applicable Algebra Engin. Communic. Comput. , vol.21, pp.131-
143, 2010.
[17] Fliess M., Join C., Sira-Ram´ ırez H., Non-linear estim ation is
easy.Int. J. Model. Identif. Control , vol. 4, pp. 12-27, 2008
(available at
http://hal.archives-ouvertes.fr/inria-00158855/en/ ).
[18] Franke J., H¨ ardle W.K., Hafner C.M., Statistics of Financial
Markets (2nded.). Springer, 2008.
[19] Garc´ ıa Collado F.A., d’Andr´ ea-Novel B., Fliess M., M ounier H.,
Analyse fr´ equentielle des d´ erivateurs alg´ ebriques. XX IIeColl.
GRETSI, Dijon, 2009 (available at
http://hal.archives-ouvertes.fr/inria-00394972/en/ ).
[20] Goldstein D.G., Taleb N.N., We don’t quite know what we a re
talking about when we talk about volatility. J. Portfolio Man-
agement , vol. 33, pp. 84-86, 2007.
[21] Gunn M., Trading Regime Analysis . Wiley, 2009.
[22] Hull J.C., Options, Futures, and Other Derivatives (7thed.).
Prentice Hall, 2007.
[23] Jiang G.J., Tian Y.S., The model-free implied volatili ty and its
information content. Rev. Financial Studies , vol. 18, pp. 1305-
1342, 2005.
[24] Join C., Robert G., Fliess M., Vers une commande sans mod ` ele
pour am´ enagements hydro´ electriques en cascade. 6eConf. Inter-
nat. Francoph. Automat., Nancy, 2010 (available at
http://hal.archives-ouvertes.fr/inria-00460912/en/ ).
[25] KirkpatrickC.D.,DahlquistJ.R., Technical Analysis: The Com-
plete Resource for Financial Market Technicians (2nded.). FT
Press, 2010.
[26] Lobry C., Sari T., Nonstandard analysis and representa tion of
reality.Int. J. Control[27] Mboup M., Join C., Fliess M., Numerical diﬀerentiation withannihilators in noisy environment. Numer. Algor. , vol. 50, pp.
439-467, 2009.
[28] Roncalli T., La gestion d’actifs quantitative . Economica, 2010.
[29] Rouah F.D., Vainberg G., Option Pricing Models and Volatility .
Wiley, 2007.
[30] Sharpe W.F., Mutual fund performance. J. Business , vol. 39,
pp. 119-138, 1966.
[31] Sharpe W.F., The Sharpe ratio. J. Portfolio Management , vol.
21, pp. 49-58, 1994.
[32] Sinclair E., Volatility Trading . Wiley, 2008.
[33] Taleb N.N., Errors, robustness, and the fourth quadran t.Int. J.
Forecasting , vol. 25, pp. 744-759, 2009.
[34] Treynor J.L., Treynor on Institutional Investing . Wiley, 2008.
[35] Tsay R.S., Analysis of Financial Time Series (2nded.). Wiley,
2005.
[36] Watson M., Bollerslev T., Russel J. (Eds), Volatility and Time
Series Econometrics – Essays in Honor of Robert Engle . Oxford
University Press, 2010.
[37] Wilmott P., Paul Wilmott on Quantitative Finance , 3 vol. (2nd
ed.). Wiley, 2006.",2011-02-03T13:46:49Z,time articial intellence ly time modi time normalized time  aga dir flijoibo up algebra articial intellence applicable algebra e icom muni com put flijoisir ram noimint  contrfrank ha ner statistics nancial markets  arc coll do and novel flianalyse ie coll dixogoldsteitale  tfmaguntradi regime analysis rey hull optns futurotr rivativpraice hall lia that rev nancial studijoirobert flivers conf inter franco ph auto mat nancy kirkpatrick dal qui st technical analysis t com resource nancial market technicians  obey sari nonstandard icontrbo up joiflinumerical umer lg or roncalli la economic rou ah articial intellence berg optprici mols volatity rey share mutual business share t share tfmanagement since articial intellence volatity tradi rey tale errors iforecasti ty nor ty nor institutnal investi rey say analysis nancial time serirey  roller lev russell eds volatity time serieconometrics essays honor robert eagle oxford   pot paul pot quantitative nance rey
paper_qf_7.pdf,8,Volatility made observable at last,"  The Cartier-Perrin theorem, which was published in 1995 and is expressed in
the language of nonstandard analysis, permits, for the first time perhaps, a
clear-cut mathematical definition of the volatility of a financial asset. It
yields as a byproduct a new understanding of the means of returns, of the beta
coefficient, and of the Sharpe and Treynor ratios. New estimation techniques
from automatic control and signal processing, which were already successfully
applied in quantitative finance, lead to several computer experiments with some
quite convincing forecasts.
","−2 −1.5 −1 −0.5 0 0.5 1 1.5
x 10−3−5−4−3−2−10123x 10−3
(a)C: ¯rIBM(¯rS&P500)0 2000 4000 6000 8000 10000 12000−40−30−20−1001020304050
Time
(b)IBM’s β(t)
−2 −1.5 −1 −0.5 0 0.5 1 1.5
x 10−3−2.5−2−1.5−1−0.500.511.522.5x 10−3
(c)C: ¯rJPM(¯rS&P500)01000 2000 3000 4000 5000 6000 7000−15−10−50510152025
Time
(d)JPM’s β(t)
Fig. 6.
0 5000 10000 15000−2−1.5−1−0.500.511.522.5
Time
(a)SR10(S&P500)0 5000 10000 15000−60−40−200204060
Time
(b)SR100(S&P500) (–) and 10 days forecasting
(- -)680070007200740076007800800082008400−20−15−10−5051015202530
Time
(c)Zoom of 7-(b)
Fig. 7.",2011-02-03T13:46:49Z,time time  time time time zoom 
paper_qf_8.pdf,1,Free Lunch,"  The concept of absence of opportunities for free lunches is one of the
pillars in the economic theory of financial markets. This natural assumption
has proved very fruitful and has lead to great mathematical, as well as
economical, insights in Quantitative Finance. Formulating rigorously the exact
definition of absence of opportunities for riskless profit turned out to be a
highly non-trivial fact that troubled mathematicians and economists for at
least two decades. The purpose of this note is to give a quick (and,
necessarily, incomplete) account of the recent work aimed at providing a simple
and intuitive no-free-lunch assumption that would suffice in formulating a
version of the celebrated Fundamental Theorem of Asset Pricing.
","arXiv:1002.2741v1  [q-fin.GN]  14 Feb 2010FREE LUNCH
CONSTANTINOS KARDARAS
Abstract. The concept of absence of opportunities for free lunches is one of the pillars in the
economic theory of ﬁnancial markets. This natural assumpti on has proved very fruitful and has
lead to great mathematical, as well as economical, insights in Quantitative Finance. Formulating
rigorously the exact deﬁnition of absence of opportunities for riskless proﬁt turned out to be a highly
non-trivial fact that troubled mathematicians and economi sts for at least two decades. The purpose
of this note is to give a quick (and, necessarily, incomplete ) account of the recent work aimed at
providinga simple and intuitive no-free-lunch assumption that would suﬃce in formulating a version
of the celebrated Fundamental Theorem of Asset Pricing.
In the process of building realistic mathematical models of ﬁnancial markets, absence of oppor-
tunities for riskless proﬁt is considered to be a minimal normative assumption in order for the
market to be in equilibrium state. The reason is quite obviou s. If opportunities for riskless proﬁt
were present in the market, every economic agent would try to reap them. Prices would then
instantaneously move in response to an imbalance between su pply and demand. This sudden price-
movement would continue as long as opportunities for riskle ss proﬁt are still present in the market.
Therefore, in market equilibrium, no such opportunities sh ould be possible.
The aforementioned simple and very natural idea has proved v ery fruitful and has lead to great
mathematical, as well as economical, insight in the theory o f Quantitative Finance. Formulating
rigorously the exact deﬁnition of “absence of opportunitie s for riskless proﬁt” turned out to be a
highly non-trivial fact that troubled mathematicians and e conomists for at least two decades1. As
the road unfolded, the valuable input of the theory of stocha stic analysis in ﬁnancial theory was
obvious; in the other direction, the development of the theo ry of stochastic processes beneﬁted
immensely from problems that emerged purely from these ﬁnan cial considerations.
Sincethelate seventies, it has beenafolklore fact that the reis adeep connection between absence
of opportunities for riskless proﬁt and the existence of a ri sk-neutral measure2, that is, a probability
that is equivalent to the original one under which the discou nted asset price processes has some
kind of martingale property. Existence of such measures are of major practical importance, since
they open the road to pricing illiquid assets or contingent c laims in the market. The above folklore
result has been called the Fundamental Theorem of Asset Pric ing.
The easiest and most classical way to formulate the notion of riskless proﬁt is via the so-called
arbitrage strategy An arbitrage is a combination of positions in the traded asse ts that requires
Key words and phrases. Free lunch, arbitrage, Fundamental Theorem of Asset Pricin g, separating hyperplane
theorem, Equivalent Martingale Measures.
1The exact market viability deﬁnition is still sometimes the source of debate.
2Also called an Equivalent Martingale Measure.
1",2010-02-14T01:32:57Z,  abstra t  quantitative nance ti t fundamental torem asset prici it  pric trefore t quantitative nance ti as since t late existence t fundamental torem asset pri t akey free fundamental torem asset pri iequivalent martale measurt also equivalent martale measure
paper_qf_8.pdf,2,Free Lunch,"  The concept of absence of opportunities for free lunches is one of the
pillars in the economic theory of financial markets. This natural assumption
has proved very fruitful and has lead to great mathematical, as well as
economical, insights in Quantitative Finance. Formulating rigorously the exact
definition of absence of opportunities for riskless profit turned out to be a
highly non-trivial fact that troubled mathematicians and economists for at
least two decades. The purpose of this note is to give a quick (and,
necessarily, incomplete) account of the recent work aimed at providing a simple
and intuitive no-free-lunch assumption that would suffice in formulating a
version of the celebrated Fundamental Theorem of Asset Pricing.
","2 CONSTANTINOS KARDARAS
zero initial capital and results in nonnegative outcome wit h a strictly positive probability of the
wealth beingstrictly positive at a ﬁxed time-point in the fu ture(after liquidation has taken place, of
course). Naturally, the previous formulation of an arbitra ge presupposes that a probabilistic model
for the random movement of liquid asset prices has been set up . In [5], a discrete-state-space,
multi-period discrete time ﬁnancial market was considered . For this model, the authors showed
the equivalence between the economical “No Arbitrage” (NA) condition and the mathematical
stipulation of existence of an equivalent probability that makes the discounted asset-price processes
martingales.
Crucialintheproofoftheresultin[5]wastheseparatinghy perplanetheoreminﬁnite-dimensional
Euclidean spaces. One of the convex sets to be separated is th e class of all terminal outcomes re-
sulting from trading and possible consumption starting fro m zero capital; the other is the positive
orthant. The NA condition is basically the statement that th e intersection of these two convex sets
consists of only the zero vector.
After the publication of [5], a saga of papers followed that w ere aimed, one way or another, at
strengthening the conclusion by considering more complica ted market models. It quickly became
obvious that thepreviousNA condition is nolonger suﬃcient toimply theexistence of arisk-neutral
measure; it is too weak. In inﬁnite-dimensional spaces, sep aration of hyperplanes, made possible by
means of the geometric version of the Hahn-Banach theorem, r equires the closedness of the set Cof
all terminal outcomes resulting from trading and possible c onsumption starting from zero capital.
The simple NA condition does not imply this in general. This h as lead Kreps in [7] to deﬁne a free
lunchas a generalized, asymptotic form of an arbitrage.
Essentially, a free lunch is a possibly inﬁnite-valued rand om variable fwithP[f≥0] = 1 and
P[f >0]>0 that belongs to the closureofC. Once an appropriate topology is deﬁned on L0, the
space of all random variables, in order for the last closure ( call it/tildewideC) to make sense, the “No Free
Lunch” (NFL) condition states that3/tildewideC∩L0
+={0}. Kreps, in [7], used this idea with a very weak
topology on locally convex spaces and showed the existence o f aseparating measure4. However,
apart from trivial cases, this topology does not stem from a m etric, which means that closedness
cannot be described in terms of convergence of sequences. Th is makes the deﬁnition of a free lunch
quite nonintuitive.
After [7], there were lots of attempts to introduce a conditi on closely related to NFL which would
be more economically plausible, albeit still equivalent to NFL, and would prove equivalent to the
existence of a risk-neutral measure. In general ﬁnite-hori zon, discrete-time markets, it was shown
in5[1] that the plain NA condition is equivalent to NFL. This see med to suggest the possibility of
a nice counterpart of the NFL condition for more complicated models. Delbaen, in [2], treated the
3L0
+is the subset of L0consisting of nonnegative random variables.
4A separating measure is a probability Qequivalent to the original one such that all elements of Chave nonpositive
expectation with respect to Q. In the case of a continuous-time market model with locally b ounded asset-prices, a
separating measure automatically makes the discounted ass et prices localmartingales — this was proved in [3].
5For a compact and rather elementary proof of this result, see [6].",2010-02-14T01:32:57Z,naturally ifor no arb it rage ucial it proof of t result ieuiaone t after it ihasband of t  reps essentially once no free lunreps th after i l beeequivalent he ifor
paper_qf_8.pdf,3,Free Lunch,"  The concept of absence of opportunities for free lunches is one of the
pillars in the economic theory of financial markets. This natural assumption
has proved very fruitful and has lead to great mathematical, as well as
economical, insights in Quantitative Finance. Formulating rigorously the exact
definition of absence of opportunities for riskless profit turned out to be a
highly non-trivial fact that troubled mathematicians and economists for at
least two decades. The purpose of this note is to give a quick (and,
necessarily, incomplete) account of the recent work aimed at providing a simple
and intuitive no-free-lunch assumption that would suffice in formulating a
version of the celebrated Fundamental Theorem of Asset Pricing.
","FREE LUNCH 3
case of continuous-time, bounded and continuous asset pric es and used a neat condition, equivalent
to NFL, called6“No Free Lunch with Bounded Risk” (NFLBR) that can be stated i n terms of
sequence convergence. Essentially, the NFLBR condition pr ecludes asymptotic arbitrage at some
ﬁxed point in time, when the overall downside risk of all the w ealth processes involved is bounded.
Later, [8] treated the case of inﬁnite-horizon discrete-ti me models, where the NFLBR condition
was once again used. At this point, with the continuous-path and inﬁnite-horizon discrete-time
cases resolved, there seemed to be one more “gluing” step in o rder to reach a general version of
the FTAP for semimartingale models. Not only did F. Delbaen a nd W. Schachemayer make this
step for semimartingale models, they actually further weak ened the NFLBR condition to the “No
Free Lunch with Vanishing Risk” (NFLVR) condition, where th e previous asymptotic arbitrage at
some ﬁxed point in time is precluded and the overall downside risk of all the wealth processes tends
to zeroin the limit. In more precise mathematical terms, the NLFVR c ondition can be stated as
C∩L0={0}, whereCis the closure in the very strong L∞-topology of (almost sure) uniform
convergence.
The NFLVR condition was ﬁnally the one that proved itself to b e the most fruitful in obtaining a
general version of theFundamental Theoremof Asset Pricing ; see[3] and[4]. Itis botheconomically
plausible and mathematically convenient. Needless to say, and like many great results in science,
the ﬁnal simplicity and clarity of the result’s statement ca me with the price that the corresponding
proof was extremely technical.
References
[1]R. C. Dalang, A. Morton, and W. Willinger ,Equivalent martingale measures and no-arbitrage in stocha stic
securities market models , Stochastics Stochastics Rep., 29 (1990), pp. 185–201.
[2]F. Delbaen ,Representing martingale measures when asset prices are con tinuous and bounded , Math. Fin., 2
(1992), pp. 107–130.
[3]F. Delbaen and W. Schachermayer ,A general version of the fundamental theorem of asset pricin g, Math.
Ann., 300 (1994), pp. 463–520.
[4]F. Delbaen and W. Schachermayer ,The fundamental theorem of asset pricing for unbounded stoc hastic
processes , Math. Ann., 312 (1998), pp. 215–250.
[5]J. M. Harrison and D. M. Kreps ,Martingales and arbitrage in multiperiod securities marke ts, J. Econom.
Theory, 20 (1979), pp. 381–408.
[6]Y. Kabanov and C. Stricker ,A teachers’ note on no-arbitrage criteria , in S´ eminaire de Probabilit´ es, XXXV,
vol. 1755 of Lecture Notes in Math., Springer, Berlin, 2001, pp. 149–152.
[7]D. M. Kreps ,Arbitrage and equilibrium in economies with inﬁnitely many commodities , J. Math. Econom., 8
(1981), pp. 15–35.
[8]W. Schachermayer ,Martingale measures for discrete-time processes with inﬁn ite horizon , Math. Finance, 4
(1994), pp. 25–55.
Constantinos Kardaras, Mathematics and Statistics Depart ment, Boston University, 111 Cumming-
ton Street, Boston, MA 02215, USA.
E-mail address :kardaras@bu.edu
6The appellation to this condition was actually coined by W. S chachemayer in [8].",2010-02-14T01:32:57Z,no free lunnd risk essentially later at not l beesae mayer no free lunvanishi risk iis t fundamental torem of asset prici it is needless referencla mortowli er equivalent stochastic stochastic rep l beerepresenti math l beescr mayer math anl beescr mayer t math anharrisoreps martaleco nom tory kabanov st kicker pro babel it leure notmath  berlireps arb it rage math eco nom scr mayer martale math nance constant os karla ras matmatics statistics part bosto sui street bostot
paper_qf_9.pdf,1,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","The Quantitative Finance Aspects of Automated
Market Markers in DeFi
Version v0.1
Stefan Loesch
stefan@topaze.blue
topaze.blue
January 24, 2022
Abstract
Automated Market Makers (AMMs) are a class of smart contracts on
Ethereum and other blockchains that ""make markets"" autonomously. In
other words, AMMs stand ready to trade with other market participants
that interact with them, at the conditions determined by the AMM. In this
this paper, which relies on the existing and growing corpus of literature avail-
able, we review and present the key mathematical and quantitative nance
aspects that underpin their operations, including the interesting relationship
between AMMs and derivatives pricing and hedging.
This paper is a chapter of The AMM Book (theammbook.org) which
embeds it into a wider and less technical context, including economics, reg-
ulations and a description of the related eco system.
1arXiv:2212.10974v1  [q-fin.MF]  21 Dec 2022",2022-12-21T12:22:50Z,t quantitative nance aspes automated market markers   versstefalow suary abstra automated market makers ms tre um ims ims  t book  c
paper_qf_9.pdf,2,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","Contents
1 Preface 4
2 Introduction 5
3 General AMM Mathematics 6
3.1 Key concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.2 The micro economics of the price response function . . . . . . . . . 8
3.2.1 Demand and supply curves in microeconomics . . . . . . . . 8
3.2.2 Supply and demand curves in nancial markets . . . . . . . 10
3.2.3 Supply and demand curves in market making . . . . . . . . 11
3.2.4 Aggregating PRFs . . . . . . . . . . . . . . . . . . . . . . . 12
3.2.5 Optimal routing . . . . . . . . . . . . . . . . . . . . . . . . . 13
3.3 AMM characteristic functions . . . . . . . . . . . . . . . . . . . . . 14
3.3.1 Scaling symmetry . . . . . . . . . . . . . . . . . . . . . . . . 14
3.3.2 Transformations . . . . . . . . . . . . . . . . . . . . . . . . . 15
3.3.3 Conditions characteristic functions must satisfy . . . . . . . 16
3.4 The mathematics of multi-asset pools . . . . . . . . . . . . . . . . . 16
3.4.1 Denitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3.4.2 Consistency and geometry . . . . . . . . . . . . . . . . . . . 17
4 AMMs and nancial derivatives 19
4.1 Black Scholes and derivatives pricing . . . . . . . . . . . . . . . . . 19
4.2 Calls, puts and European prole matching . . . . . . . . . . . . . . 21
4.2.1 The strike density function and the Cash Gamma . . . . . . 22
4.3 Power law proles under Black Scholes . . . . . . . . . . . . . . . . 23
4.4 Analysing the constant product AMM as a nancial derivative . . . 24
5 Mathematics of specic AMMs 28
5.1 Constant product (k=x*y) . . . . . . . . . . . . . . . . . . . . . . . 28
5.2 Constant sum (k=x+y) . . . . . . . . . . . . . . . . . . . . . . . . . 32
5.3 Concentrated, range-bound and levered liquidity . . . . . . . . . . . 33
5.3.1 Range-bound liquidity . . . . . . . . . . . . . . . . . . . . . 34
5.3.2 Concentrated and levered liquidity . . . . . . . . . . . . . . 36
5.3.3 Divergence loss with concentrated liquidity . . . . . . . . . . 39
5.4 Modied weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
5.5 Modied curve aka (2 token) stableswap . . . . . . . . . . . . . . . 45
5.5.1 Dynamic Chi . . . . . . . . . . . . . . . . . . . . . . . . . . 46
5.6 Multi asset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
5.6.1 Equal weights . . . . . . . . . . . . . . . . . . . . . . . . . . 47
2",2022-12-21T12:22:50Z,contents preface introdugenl matmatics key t mand supply supply aregatfs optimal scali transformatns conditns t  consistency ms  schools calls at cash gaa   schools analysi matmatics ms constant constant concentrated rae concentrated divergence modi modi dynamic chi multi equal
paper_qf_9.pdf,3,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","5.6.2 Variable weights . . . . . . . . . . . . . . . . . . . . . . . . . 49
6 Conclusion 50
7 References 51
8 Appendix 55
8.1 Website . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
8.2 Glossary and technical glossary . . . . . . . . . . . . . . . . . . . . 55
8.3 Formulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
8.4 Projects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
8.5 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
9 License 56
9.1 Attribution 4.0 International (CC BY 4.0) . . . . . . . . . . . . . . 56
3",2022-12-21T12:22:50Z,variable conusreferenc bsite glossary s projes referenclicense aributinternatnal
paper_qf_9.pdf,4,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","1 Preface
Automated Market Makers, short AMMs, are smart contracts that autonomously
make markets in tokens on a blockchain, in particular the Ethereum blockchain.
We will in the years to come see a convergence between traditional nance (\\\\TradFi"")
and the emerging decentralized nance (\\\\DeFi""), and whilst it is too early to un-
derstand where it will end up, it is highly likely that AMMs will play a central
role in this convergence.
AMMs, like trading venues in traditional nance, are places where assets change
owners. That means they are in the center of the nancial system { without them
the nancial system could not exist. They are also highly complex entities, both
in their own right, and in their interaction with other parts of the system. This is
the reason why we are currently working on The AMM Book (theammbook.org)
- it is important for everyone, especially in the TradFi and regulatory community
who have not yet been exposed to the topic, to understand what those AMMs are
and how they work.
The book covers AMM from various dierent angles { their technical implemen-
tation, their economics, their regulation, and, last but certainly not least, their
internal mechanics. AMMs are following a passive trading strategy: they oer
to trade with everyone who approaches them, on the terms determined by their
internal algorithms. It is well known since Black, Scholes and Merton wrote their
seminal papers [Black Scholes 73, Merton 73] that trading strategies and nancial
derivatives are closely related. This suggests { and it turns out this is true { that
the quantitative nance apparatus that underpins modern option pricing theory
is very well suited to study AMMs.
We reserved a chapter in our book describing and reviewing the quantitative -
nance aspects of Automated Market Makers. This is a highly specialst topic that
is covered both by industry practitioners and by academics in the world's lead-
ing universities, with those two groups exhibiting a signicant overlap. The pri-
mary vehical for advancing knowledge in that world are peer-reviewed papers. We
therefore decided to publish this chapter independently from the book so that the
community can review it { and to publish it as early as possible so that at the
time the book is ready for publication the paper has undergone a thorough review
and revision process.
Without further ado, please let us thank you for reading this paper and please, do
contact us with any comments, suggestions and in particular errors.
London, January 2022
4",2022-12-21T12:22:50Z,preface automated market makers ms tre um  tr ad    ms ms that ty  t book tr ad  ms t ms it  schools morto schools morto ms  automated market makers  t  without londouary
paper_qf_9.pdf,5,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","Stefan Loesch ( stefan@topaze.blue )
2 Introduction
An AMM is an automated agent, specically a smart contract on a blockchain,
that is holding two or more assets, and that is willing to trade with anyone who
matches the AMM's price. In this paper, we only consider independent orun-
tethered AMMs, ie AMMs who do not rely on any external information in their
decision making process.
If we place ourselves at a specic point in time, with a specic state of the AMM
(notably, its current asset holdings) then the response of the AMM is determined
only by its response algorithm . There are various ways to specify this algorithm,
but essentially it must allow the AMM, and its users, to determine at which ef-
fective price the AMM is willing to trade for every potential transaction that is
presented to it.
To give an example { and to already introduce some notations we will use through-
out this paper { we assume the AMM contains two tokens, CSH and RSK. CSH is
the numeraire asset, and RSK is the risk asset. Those designations are arbitrary
and could be swapped, but it is one of the inconveniences of mathematical nance
that for most calculation it is necessary to choose a numeraire, and this choice is
often arbitrary.
There are numerous ways how the response algorithm could be structured, de-
pending on the practical application. Example include the following
•Fixed Price Token Distribution Contract. A xed price token distri-
bution contract is selling RSK at a xed price against CSH, until it runs out
of RSK tokens. At this point it halts, as it is not buying RSK at any price.
•Increasing Price Token Distribution Contract. An increasing price
contract increases the price with the number of tokens sold. If this price
goes to innity when the amount of tokens held in the contract goes to zero
then it is possible to ensure that its token supply is never fully depleted.
This contract will also only sell RSK but never buy it.
•Fixed Price Trading Contract. A xed price trading contract is similar
to the corresponding token distribution contract, except that it is not only
selling RSK, but it is also buying it, at a xed price against CSH. This
contract stops trading in one direction if runs out of either RSK or CSH. It
does not halt however, it will always trade in at least one direction.
5",2022-12-21T12:22:50Z,stefalow sintroduaims ms  tre to those tre  xed price tokedistributcontra at ineasi price tokedistributcontra a  xed price tradi contra  it
paper_qf_9.pdf,6,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","•Increasing Price Trading Contract. This contract is also similar to its
corresponding distribution contract, except that it is trading both ways. If
the trading price of RSK goes to innity when the contract runs out of
RSK, and to zero when it runs out of CSH, the contract will never run out
of tokens, and therefore will always be willing to trade in either direction,
albeit possibly at prices that are unattractive when compared to the market.
3 General AMM Mathematics
3.1 Key concepts
We have seen that an AMM requires a response algorithm to handle the trades
proposed to it. There are a number of dierent yet ultimately equivalent ways to
formalize this algorithm. The rst one is the price response function (PRF )
that we denote (x). The PRF associates a price to every quantity of RSK that
someone wants to sell ( x >0; AMM buys) or buy ( x <0; AMM sells). The
price can formally be zero or innity respectively if the AMM is not ready to trade.
A market maker is expecting to make money buying and selling, and is generally
doing so by quoting a bid/ask spread or charging a fee. Economically the impact of
both is that the price at which an AMM is buying is higher than the one at which
it is buying. This can be modelled with the bid/ask spread which corresponds to
a discontinuity of the price function at  x= 0 with
lim
x!0+(x)",2022-12-21T12:22:50Z,ineasi price tradi contra   genl matmatics key  tre t t t economically 
paper_qf_9.pdf,8,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","•theprice response function (PRF ) which lends itself best to economic anal-
ysis,
•theindierence curve which lends itself best to actual implementation of
AMMs, and
•thecharacteristic function which is in many ways the most elegant of those
objects and lends itself best to a mathematical analysis.
3.2 The micro economics of the price response function
3.2.1 Demand and supply curves in microeconomics
The PRF is closely related to the concept of demand and supply curves at the base
of microeconomic analysis. Before we move on we give a very brief review of the
topic for those who may not be familiar with it. For a more thorough discussion,
see any microeconomics textbook, eg [Pyndick Rubinfeld].
The concept of the supply curve is rooted in the cost curves which originated
in commodity markets. The context is that there are many dierent producers
of a fully interchangeable goods (\\\\commodity goods""), and they produce those
goods at a certain individual cost. The cost curve is the curve that rst sorts the
producers by their production cost, and that then plots the produced quantities
on the x axis and the corresponding cost on the y axis. By construction the cost
curve is an upward-sloping step function. In practice it is often approximated with
a continuous function. An example for a cost curve is shown in the graphics below
For simplicity we will ignore xed costs here and assume all costs are variable.
8",2022-12-21T12:22:50Z,ms t mand t before for py dick rub nefeld t t t by iafor
paper_qf_9.pdf,9,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","Also cost can include a cost of equity , in other words a minimum prot margin.
With this conditions it is reasonable to assume that producers are willing to sell
whenever the price is above their individual cost level, and to be content not selling
whenever the price is below. In other words, the cost curve turns into the market
supply curve . It is a two-way association, but generally it serves to associate a
supply level with a given price, as shown in the chart below
Complementing the supply curve is the demand curve . It is based on the same
mental model and it constructed similarly. The assumption is that there are nu-
merous buyers in the market, that they all are willing to buy below a certain
price, and that they are content not buying above that price. Again, those buyers
are sorted by price, this time in descending price order. The resulting demand
curve is a downward-sloping step function that again is often approximated with
a continuous function.
Combining demand and supply curve in the same diagram allows determininig
theequilibrium price , which is at the price level where the supply and demand
curves meet. All buyers to the left of this point are, by construction, willing to
buy at the equilibrium price or below, and all sellers to the left are willing to sell
at the equilibrium price or above. By construction, the quantity sold matches the
quantity bought, so everyone to the left of the intersection point will transact, and
everyone to the right of it will not, and all transactions will happen at the equilib-
rium price. This price is also sometimes referred to as market clearing price as
this is the price at which the market clears, ie where no possible transactions are
open. A combined supply and demand curve and determination of the associated
clearing price is shown below.
9",2022-12-21T12:22:50Z,also with iit implementi it t ag articial intellence t combini all by 
paper_qf_9.pdf,10,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","3.2.2 Supply and demand curves in nancial markets
The concept of supply and demand curves is also useful in nancial markets,
provided one understands the mental model the underpins it.
Let's start with an excessively simplistic but nevertheless enlightening view of the
world: we assume that every market participant has a view on every asset in the
market, in the sense that they have a view on its fair price. Moreover we assume
that market participants will act on their views, meaning they will be buyers if the
asset is available below what they consider its fair value, and they will be sellers
if someone is willing to pay more.
In a multi-asset world this can get exceedingly complex, so we take refuge in
our two asset world of RSK can CSH. We can then again assemble the market
participants and sort them by their price assessment of RSK vs CSH. Increasing
or decreasing does not matter in this case, and we choose decreasing. In this
case the curve ressembles a demand curve. We know what the supply curve is:
the supply of RSK is xed, so the supply curve is a vertical line. The market
clearing price is where the vertical line intersects with the demand-like curve we
have constructed. This relationship is shown in the chart below
10",2022-12-21T12:22:50Z,supply t  oi ineasi i t 
paper_qf_9.pdf,11,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","As stated before, this model is overly simplistic. Most market participants do not
trade that way, and would not even trade that way if there were zero transaction
costs. Most people will hold on to their investments for a while. At best we can
assume that they sell above certain sell threshold to take prots, and buy below a
certain buy threshold if they think that there is sucient potential. This situation
can be described nicely with a supply and demand curve: the demand curve is
people adding RSK to their portfolio on the downside, and the supply curve is
people selling RSK on the upside.
However, reality is even more complex. For example, if markets fall, market par-
ticipants may want to cut their losses, so they sell on the downside. Similarly,
people may want to buy on the upside for fear of missing out. Those dynamics
donott into a simple static supply and demand curve model. At the very least
one needs to assume that the market price dynamics itself feeds back into market
supply and demand curves. This destroys a lot of the simplicity and elegance of
this approach.
3.2.3 Supply and demand curves in market making
We have seen under the previous heading that supply and demand curves can be
applied in markets in general, but that they suer from some shortcomings. One
area where they work well however is in the analysis of market makers, ie market
participants that stand ready to engage in trades in case other market participants
are willing to meet their price.
The best manifestation of supply and demand curves is in a market's order book,
11",2022-12-21T12:22:50Z,as most most at  for simarly those at  supply  one t
paper_qf_9.pdf,12,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","more specically its limit order book (\\\\buy at this price or below; sell at this
price or above""). Those ordder are supply and demand that will hit the market
when prices move. However, the supply and demand from the order book does
not necessarily correspond to the real short term supply and demand. There are
many participants who monitor the market continously which allows them to add
or cancel orders in reaction to price movements. Therefore the eective market
demand and supply is usually dierent curve shown in the public order book.
With those caveats out of the way, we remind ourselves of the price response
functions (PRFs ) that we discussed above: it turns out that is simply describes a
static order book. When prices increase (decrease) the PRF will trigger a known
amount of sell (buy) orders. In other words:
an AMM eectively is a static order book.
3.2.4 Aggregating PRFs
A market is the superpositon of its participants. In fact, it is a linear superposition,
which allows us to easily aggregate the PRFs of multiple AMMs. Moreover it
allows us to treat every liquidity position as its own individual micro AMM, greatly
simplifying the AMM mathematics.
Supply and demand curves and PRFs are aggregated along the x-axis, not the y-
axis. Practically speaking this corresponds to putting all individual positions into
a single big pool and sorting them anew by price. This will interlace the positions
coming from the underlying curves, placing those with similar prices close to each
other. An example for this is shown in the chart below.
12",2022-12-21T12:22:50Z,those tre trefore with fs wiaregatfs ifs ms osupply fs praically  an
paper_qf_9.pdf,13,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","The continous case is exactly the same: if the have two PRFs 1(x);2(x) then
those will get aggregated along the x-axis. If we denote ",2022-12-21T12:22:50Z,t fs 
paper_qf_9.pdf,14,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","covered we know how to route the trade. This algorithm does work in presence
of percentage fees as those can simply be converted into a price adjustment. It
does however not work for per-trade fees or gas costs. In case those are relevant {
usually they are not { it is possible to include them in a numerical optimization
algorithm.
3.3 AMM characteristic functions
The second topic we need to discuss in more detail is that of characteristic func-
tions . As a reminder, a characteristic function f(x;y) determines a series of indif-
ference curves yk(x) with the help of the condition f(x;yk(x)) =k.
3.3.1 Scaling symmetry
The rst point to make is that the characteristic functions preserve the symmetry
of the underlying situation, ie there is no numeraire-related symmetry break which
in nance often muddies the waters. For example, if we look at the indierence
functiony(x), then we have implicitly designated yas the numeraire asset and x
as the risk asset, and what happens at the upside when the price of the risk asset
goes to innity looks dierent to the downside when it goes to zero, even though
because of the underlying symmetry both situations are exactly the same.
To make things more concrete, let's peek ahead and introduce the most well
known characteristic function we'll discuss below { the constant product func-
tionf(x;y) =xyand its indierence function yk(x) =k=x. As before, x
represents the number of RSK tokens, and ythe number of CSH tokens, both in
their own native units.
As bothxandyuse their own native tokens we need to analyze what happens if
they re-denominate. Generally in nance, redenomination applied to everything
should not change anything in the real world. This is not a very deep result {
it simply means that it should not matter if we record prices in dollars, cents, or
millions of dollars.
In particular, if we transform all our denominations the same way { say instead of
using USD and EUR we use USD cents and EUR cents, and we simularly add to
decimals to all other denominations { then nothing should change at all. Again,
this is not deep { it simply means that the price of a EUR in USD terms is the
same as the price of a EUR cent in terms of USD cents.
Mathematically this means we have a representation of the multiplative group of
positive numbers 2R+on our state space that acts according to : (x;y)!
(x;y ), and we want to understand the implied action on on our characteristic
14",2022-12-21T12:22:50Z, it it as scali t for to as as genlly  iag articial intellence matmatically
paper_qf_9.pdf,15,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","function. Generally, interesting objects in nance transform under this symmetry
in one of two ways
1. They are invariant which means that f(x;y ) =f(x;y)
2. They transform linearly (orhomogenously of order 1 but this is a mouthful)
which means that f(x;y ) =f(x;y)
Exchange ratios { ie the price of one asset expressed in terms of another asset { are
an example of invariant objects . Quantities on the other hand are linear objects .
Again, this is not deep: the rst statement says that I can look at EUR and USD
or EUR cent and USD cent. The second one says that when I look at cent instead
of EUR and USD all EUR and USD related quantities get multiplied by 100.
When we look at f(x;y) =xywe see that it is neither linear nor invariant, so
maybe it is not ideal. We will park this point for now and come back to it in a
moment.
3.3.2 Transformations
The second point to make is that, if we are only interested in the indierence
curvesyk(x) determined by f(x;y) =k, then our problem is overdetermined, in
the sense that there will be other characteristic functions fthat yield the same
set of indierence curves.
Let's consider a bijective and suitably regular function h:R!R. It is easy to see
that theyk(x) implied by f(x;y) =kare exactly the same as the yh(k)(x) implied
byh(f(x;y)) =h(k). In other words: if fis a characteristic function, then the
composite function fh=hfis also a characteristic function. Provided the kare
transformed accordingly it is fully equivalent to f.
Coming back to f(x;y) =xywe see that if we use h() =pthen f(x;y) =
fh(x;y) =pxy. It is easy to verify that in this case our characteristic function
transforms linearly, ie
f(x;y ) =f(x;y)
This in turn suggests that the quantity kwith k=f(x;y) may be nancially
meaningful. It turns out that it is: kis a measure of the pool size that, contrary
to its total monetary value, is invariant under changes in relative prices. In other
words: because kis the pool invariant, if we ignore fees it does not change when
someone is trading against the pool. An increase in ktherefore indicates either an
addition of liquidity or a pool prot (eg because of fees earned), and an decrease
either a removal of liquidity of losses due to leakage or exploits.
15",2022-12-21T12:22:50Z,genlly ty ty exe quantitiag articial intellence t w transformatns t  it iprovid comi it  it ian
paper_qf_9.pdf,16,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","3.3.3 Conditions characteristic functions must satisfy
We are now analysing what conditions a function fmust satisfy to be a valid
characteristic function. We recall from above that the price response function is
(x) =",2022-12-21T12:22:50Z,conditns  
paper_qf_9.pdf,17,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","but we know from the symmetry discussion above that the two functions above
are equivalent, and fis easier to deal with than f. We, in this section only, also
adopt the convention that xwithout an index represents the entire vector
x(x0;x1;:::;xN)
which allows us to abbreviate our characteristic function as f(x).
We can choose a numeraire asset if we want { in which case we choose x0{ or
we can treat the whole problem as a symmetrical problem where all assets are
considered risky.
We need exhange ratios ij(x) for each of the pairs xi;xj, and we adopt the con-
vention that the second index ( jin this case) is the numeraire. The ij(x) are
determined by partial derivatives.
ij(x) =@jf(x)
@if(x)
where we use the shortcut notation @i@=@xi. Note that the numeraire index is
in the numerator.
For convenience we also dene the single-index functions ii0, so if the second
index is missing the numeraire is implied, and we have
i(x) =@0f(x)
@if(x)
3.4.2 Consistency and geometry
When we have a system of prices, those price systems must be arbitrage free.
Firstly, the price for the reverse exchange must be the inverse of the price, ie
ij=1
ji
Secondly, the exchange ratio of a direct exchange between xi;xkmust be the same
as the exchange going via xj, therefore
ik=ijjk
17",2022-12-21T12:22:50Z,   t note for consistency wrstly secondly
paper_qf_9.pdf,18,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","Both of those conditions can be easily veried going back to the denitions of the
ij. We note that this means that the iare sucient to span the entire price
space via the relationship
ij=i00j=i
j
This of course is nothing but the well known fact that, in an arbitrage free system,
it is possibly to choose a numeraire, and once all prices in the numeraire are xed
all cross exchange ratios are xed as well.
Geometrically we can think of what we called the invariance curve in two dimen-
sions as an invariance hypersurface (technically, a codimension-1 manifold embed-
ded intoRN+1). This hypersurface (which from now on we will simply refer to as
indierence \\\\surface"") is dened by
f(x) =k
or, equivalently, by the dierential condition
df(x) =X
@if(x)dxi= 0
Like in the two-dimensional case, we need prices to be positive, ie @if(x)>0.
In other words, like in the two-dimensional case, the gradient vector rf(x) =
(@0;@1;:::;@N)f(x) must point into the rst (\\\\top-right"") quadrant of RN+1, ie
all vector components must be positive (we ignore the all-negative case here; we
can use",2022-12-21T12:22:50Z,both   geometrical  like in
paper_qf_9.pdf,19,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","4 AMMs and nancial derivatives
4.1 Black Scholes and derivatives pricing
Before we look at the relationship between AMMs and derivatives, here a brief
reminder of the key elements of derivatives pricing and hedging that will be im-
portant in what follows. For more details see [Hull] or any other option pricing
text book.
We start with the Black Scholes PDE (Black Scholes partial dierential equa-
tion) which reads
@
@t=",2022-12-21T12:22:50Z,ms  schools before ms for hull   schools  schools
paper_qf_9.pdf,20,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","Finally we are looking at Gamma which is dened as
",2022-12-21T12:22:50Z,nally gaa
paper_qf_9.pdf,23,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","usually makes sense to look at the associated dierential form  (K)dK. The unit
ofis CSH. The unit of  (K)dKis RSK which we can either obtain by formal
calculation, or by noting that \\\\1 call"" brings exactly one unit of Delta, and Delta
is measured in RSK. Like we have a Cash Delta to complement the Delta we also
have a cash strike density function (K) =K(K), ie
(K) =K00(K)
Again we look at the dierential form (K)dKwhich is now denoted in CSH.
Both(K) and (K) are closely related to the Cash Gamma
",2022-12-21T12:22:50Z,t t is lta lta like cash lta lta ag articial intellence whiboth cash gaa
paper_qf_9.pdf,24,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","(;t) =et=()(;t= 0) =e()t)(;t= 0)
ie an exponential growth of the function over time that is preserving its shape. As
shown above, instead of the characteristic period we can also use the exponential
growth rath = 1=.
If we chose = 0:5, ie=px, and we as discussed above assume a vanishing
r;dthen
0:5=8
2;  0:5=2
8
4.4 Analysing the constant product AMM as a nancial
derivative
In this section we analyze the constant product AMM within a Black Scholes and
nancial derivatives framework. We want this section to be self-contained, so we
may repeat some arguments that we have made elsewhere in this paper.
The rst thing to understand is that an AMM is eectively an \\\\investment ve-
hicle"" following a particular trading strategy. This strategy is not self-nancing
as it \\\\hands over"" some of its proceeds to the arbitrageurs. When looking at the
consolidated position of an AMM and its arbitrageurs the trading strategy is self
nancing however, and therefore out general quantitative nance frameworks ap-
ply. We simply have to ensure to split the AMM component from the arbitrageur
component at the end.
Assuming ecient markets, the constant product AMM at every point in time will
have 50% of its value locked in the risk asset, and the other 50% in the numeraire
asset. This can be easily shown using its indierence function y=k=xand the
price function (x) =k=x2: if we multiply those we nd that (x)x=y, the left
hand side being the value of the risk asset, and the right hand side being that of
the numeraire asset.
We now recall from the previous section that the square root prole retains its
form and grows exponantially with a rate =2=8 when going forwards in time.
We can also easily calculate the cash Delta of that prole as
cash=d
d=1
2p
=1
2()
24",2022-12-21T12:22:50Z,as  analysi i schools  t  w assumi    lta
paper_qf_9.pdf,25,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","This equation shows that the replication strategy of the square root prole (its
\\\\delta hedge"") keeps 50% of the portfolio value in the risk asset. Therefore 50%
are in the numeraire, therefore they both are of equal value at the same time, and
therefore the constant product AMM strategy corresponds to a square root value
function.
To summarize, we have shown that the particular trading strategy an AMM follows
should lead to the following result when moving forward in time
(;t) = exp(2
8t)p

In reality however the time evolution is as follows
(;t) =p

the dierence being the funds that are lost to arbitrageurs: When we analyse the
trading strategy of an AMM then we see that if the price moves from 0to1
then the AMM allows arbitrageurs to trade at the geometric average of the prices,p01. This price is the same on the way up as it is on the way down, which proves
that, ignoring fees, and AMM hands over allGamma gains to the arbitrageurs.
That's what we see above: the factor exp(2
8t), corresponding to a growth rate of
2
8, is entirely lost for the AMM and handed over to arbitrageurs instead.
We have drawn a few charts that illustrate this evolution. The rst one is the
square root prole over time at dierent vols. The grey line is the initial prole at
t= 0, and the blue and orange lines are the proles after 1 year, at 75% and 150%
vol respectively. The dierence between the grey and the colored lines is what is
being handed over to arbitrageurs (ignoring fees), and we see that for vols beyond
100% this can become substantial.
25",2022-12-21T12:22:50Z, trefore to iw gaa that  t t t
paper_qf_9.pdf,26,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","Now instead of looking at a cross section at dierent points in time, we transport
a single point through time The x axis is the time in years, and the y axis is the
growth factor. We see nothing much happening at 50% vol, but at from 100% vol
onwards the growth becomes substantial, and very big beyond 150% vol.
The nal chart here gives as a feel for how the vol impacts growth. On the x axis
we have the volatility . The blue line (left scale) is (), the characteristic time
scale in years. The orange line (right scale) is the one year growth rate et=",2022-12-21T12:22:50Z, t  t ot t
paper_qf_9.pdf,27,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","Finally we are looking at a modied weight AMM. Below we are plotting the co-
ecientfrom the Black Scholes equation above which is
(;) =1
22(1",2022-12-21T12:22:50Z,nally below  schools
paper_qf_9.pdf,28,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","5 Mathematics of specic AMMs
5.1 Constant product (k=x*y)
We now apply the above concepts to the most fundamental of AMMs, the con-
stant product k=xyAMM. As the name implies, this AMM historically has
thecharacteristic function we already encountered above, namely
k=f(x;y) =xy
wherex;yare the token amounts in their native units respectively. As before, we
considerythenumeraire (asset) CSH , andxtherisk asset RSK . We have seen
that there are certain degrees of freedom in choosing a characteristic function, and
we choose an equivalent one which is
k=f(x;y) =pxy
This function satises the linearity / homogenity condition , ief(x;y ) =f(x;y).
As a consequence, kserves as a linear measure of the pool size that is invariant
under trading { it only changes when liquidity is added (including via fees) or
removed.
We obtain the indierence curve yk(x) by isolating y
y=yk(x) =k2
x
and the price response function (PRF ) by deriving the indierence curve with
respect tox
(x) =",2022-12-21T12:22:50Z,matmatics ms constant  ms as as   as 
paper_qf_9.pdf,29,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","We denote=p=p0the current price ratio of the pool, ie the ratio of the current
pricepdivided byp0, the price at the time the pool was seeded (we assume a single
seeding event for simplicity; we cando that because as we discussed previously,
we can consider every position its own little micro AMM, and the actual AMM
being the combination of those). The portfolio value ratio () is
() =p

The normalized portfolio value here is dened as the ratio of the current portfolio
value and the initial of the portfolio value, ie (t= 0) = 1.
We have previously proven the above formula, but we'll show it again here for ease
of reference. It is easiest to work our way backwards. It is well known that there is
a correspondance between option proles and hedge portfolios. More specically,
in order to hedge a payo prole, the replicating strategy holds Delta units of
the risk asset. The remainder of the portfolio value then is held in the numeraire
asset. The Delta is the derivative of the prole with respect to the price. As this
is in units of the risk asset it must be multiplied with to be converted into the
numeraire. We nd that the Cash Delta
cash=d
dp
=1
2p

In other words, half of the value is invested in the risk asset, and therefore the
other half must be invested in the numeraire asset. Going backwards this means
that if we hold at all times the same amount in the numeraire and the risk asset,
our payo prole will the the square root prole. This concludes our proof.
From the above we can calculate the normalized cash strike density function
which is00(), ie
cash() =",2022-12-21T12:22:50Z, t t  it it  lta t t lta as  cash lta oi  from
paper_qf_9.pdf,30,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","portfolio and the value of the AMM porfolio) we note that the initial portfolio
was 50:50 in CSH:RSK. The HODL value of that initial portfolio (ie its value had
the portfolio composition not changed) behaves like1+
2, with our normalization
(t= 0) = 1. The divergence loss  is therefore
() =1 +
2",2022-12-21T12:22:50Z,t t
paper_qf_9.pdf,31,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","Note that the asymmetry in the chart is entirely due to the choice of numeraire.
As shown by the characteristic function, the underlying model is fully symmetric
in RSK and CSH, but any choice of numeraire breaks this symmetry.
In the next chart we show the percentage DL, ieHODL",2022-12-21T12:22:50Z,note as in
paper_qf_9.pdf,32,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","is beyond 20%
5.2 Constant sum (k=x+y)
Before moving on to variations of the constant product AMM we want to make a
quick detour via the constant sum AMM with the characteristic function
fp(x;y) =px+y
It is easy to see that this AMM has a linear indierence curve that crosses the
axis at (0;k) and (k=p;0). The price response function p(x) is the constant
function
p(x) =p
meaning that, ignoring fees, the AMM always buys and sells at the same price p.
Of course the AMM can run out of either CSH or RSK in which case it will be stuck
at the boundary until someone is willing to trade with it in the right direction.
Within the range this AMM has neither slippage nor Gamma and therefore it does
not oer arbitrageurs an incentive to bring it back into equilibrium. An unmodied
constant sum AMM will generally be stuck at one of its boundaries most of the
time.
Theportfolio value function of the constant sum AMM is
32",2022-12-21T12:22:50Z,constant before it t of withaa at tfol
paper_qf_9.pdf,33,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","() =min(;1)
This is because it will always be 100% invested in the underperforming asset, ie
in CSH on the upside, and in RSK on the downside. Note that the above prole
is a short option prole with a strike at = 1.
Thestrike density function is
() =(",2022-12-21T12:22:50Z, note t strike
paper_qf_9.pdf,34,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","•Range-bound liquidity. Range-bound liquidity is liquidity that is only
available to make markets in a certain price range; typically the range is
limited at both sides but that does not have to be the case.
•Levered liquidity. Levered liquidity is liquidity that is amplied, for ex-
ample in a concentrated liquidity setting where excess liquidity is removed
from a range bound AMM.
•Concentrated liquidity. Concentrated liquidity is a range-bound liquidity
where allexcess collateral (ie collateral that can never level the pool) is
removed; this means that outside the range the AMM only holds one of the
two assets
5.3.1 Range-bound liquidity
As dened just above, range-bound liquidity is liquidity that is only available to
make markets in a certain range. We have already seen and extreme example
of this type above, when we looked at the constant sum AMM. Another easy
application of this concept is a constant product AMM where some collateral is
removed without adusting the indierence curve. Instead, when the AMM runs
out of collateral it stops trading. More precisely, when it runs out of one token it
will no longer sell it { it simply cannot as it can't deliver. It however stands ready
to buy that token, provided the price is right.
As an example we have a standard RSK / CSH constant product AMM, and we
remove some CSH. The AMM sells CSH and buys RSK when the RSK price goes
down. This means that when a certain price 0is reached, the AMM runs out of
CSH and can therefore no longer buy RSK. If the drop of RSK continues and the
price <  0is outside the range, the AMM simply pauses. However, as soon as
the price >  0moves back into the range, the AMM starts buying CSH again
and is back in the game.
In this example the range was implied by the removed liquidity. In practical ap-
plications users arguably prefer to specify a price range where to provide liquidity.
We have seen above that the price is directly linked to the collateral outow, but
there is a catch: this only works on a specic indierence curve. If the AMM
moves curves, eg because liquidity is added or removed or it earns fees, the range
changes (wider with more liquidity and vice versa). There are two solutions to
this conundrum:
1. Do not pay fees into the pool but keep them separate; this reduces collateral
eciency, but it ensures that the AMM remains on the same indierence
curve
34",2022-12-21T12:22:50Z,rae rae severed severed concentrated concentrated rae as  anotr instead  it as t   ii  tre do
paper_qf_9.pdf,35,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","2. Adjust the pool constant ksuch that the apparent liquidity is bigger, and
that therefore the AMM will run out of tokens at exactly the same price
point.
The former solution may be more practical within the high gas cost environment {
and this is for example the way Uniswap v3 does it { but we want to briey discuss
the alternative here, at the example of a constant product AMM. We recall that
y=k=xand the price =k=x2. We know want to keep the price constant and
solve fork. An easy calculation yields
x(k) =r
k
; y(k) =p
k
wherex(k);y(k) is a paramterised boundary curve. This curve is a straight line
going through the origin. We have drawn an example in the chart below. Here
the blue, orange and grey curves are indierence curves at various values of k, and
the yellow line is the cuto point at a unity price. So if the AMM wants to remain
above unity price it can only use the parts of the curves that are above the yellow
line, and if it wants to trade below unity price it must remain on the parts of the
curves below the yellow line.
In practice it may be easier for range-restricted AMMs to operate based on an
unrestricted characteristic function and then explicitly impose the boundary
constraints at the level of the indierence curve . However, there may be situations
where operating at the characteristic function level to start with is more suitable.
Below we'll sketch a process for creating a restriced characteristic function from
an unrestricted one and a boundary condition.
35",2022-12-21T12:22:50Z,adjust t uni swap   a  re so ims below
paper_qf_9.pdf,36,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","We are looking specically at a lower bound for the amount of RSK held, and we
call this boundary x0. The intuition behind this contstruction is that, once we get
close (or slightly beyond) x0, we are transported on the fast track to x= 0. We do
this by dening a function ^ x""(x) which is described in the chart below: for x>x 0,
ie in the desired range, we have ^ x""(x) =x. Forx<x 0the turbo kicks in however,
and the function is
x<x 0)x""(x) =x0",2022-12-21T12:22:50Z, t  for
paper_qf_9.pdf,37,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","tokens, or never even contributing them in the rst place. Whilst inside the range,
the AMM will behave as if the liquidity had not been removed. So everything else
being equal, the fees earned per unit of liquidity contributed will be higher, but
so will be the divergence loss. Ultimately Miller-Modigliani applies (guratively,
not actually) in the sense that leverage does not economically matter in ecient
markets , and whilst this leverage increase the returns it also increases the risk, so
the risk-adjusted returns remain the same.
We are now looking at details for the levered constant product AMM, ie the
Uniswap v3 model. As before, is our normalized price ratio, and 0;1is the
liquidity range, using the same normalization. To understand this intuitively, 
will start at 100%, and 0;1= 80%;120% means a range that is 20% up and down
from the starting price. We have also added a portfolio notional factor n0here
that we can use to normalize our portfolio value however we want. The AMM
portfolio value below / in / above the range respectively is given by the three
equations below (see eg [Lambert21])
() =n08 < 0
() =n0p
01p",2022-12-21T12:22:50Z,whst so ultimately mler modi lia ni  uni swap as to  t lambert
paper_qf_9.pdf,38,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","and we see that the normalization of those formulas is such that for n0= 1, we hold
exactly 1 unit of RSK below the range. This is why we cannot simply set 0= 0
in the formula above but we need to adjust n0to have the correct normalization.
Above the range we hold no RSK but only CSH.
For the numeraire asset CSH we nd
Nn() = 08 < 0
Nn() =n0p
01p",2022-12-21T12:22:50Z, above for nnn
paper_qf_9.pdf,39,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","5.3.3 Divergence loss with concentrated liquidity
Divergence loss within a concentrated, leveraged liquidity is somewhat complex.
This is not so much for technical reasons, but for reasons of interpretation. In the
frameworks we have discussed so far, and those that we will discuss below, the
liquidity pool composition was xed at either 1:1 for the constant product pool,
or at: 1",2022-12-21T12:22:50Z,divergence divergence  in
paper_qf_9.pdf,40,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","3. The price drops to just below 95; now the portfolio is 100% in RSK, ex-
changed at a price of 100; in other words it holds 1.5 RSK which, at a price
of 95, are worth 142.5 CSH. This is a DL of 7.5 CSH.
4. The price drops to 50. The portfolio does not change (and does not earn
fees) so it still holds 1.5 RSK, worth 75 CSH. The DL is now 75 CSH.
5. Scenario A: the position is unwound; Scenario B: the position remains.
6. RSK recovers to its original value of 150 CSH. In Scenario A, DL disappears
and is exactly zero. Scenario B is complex:
1. If the DL was crystallized CSH at 75 it remains at 75 CSH
2. If the DL was crystallized in CSH at 7.5 (because beyond that the
positions was de facto inactive) it remains at 7.5 CSH
3. If the losses are translated into RSK at the then prevailing exchange
ratio, which is 1.5 RSK or 0.15 RSK respectively, and then crystallized,
the DL is 225 or 22.5 CSH respectively
4. If the position is carried forward as 1.5 RSK the DL becomes a gain of
0.5 RSK, or 75 CSH
The distinction between in-range and out-of-range DL depens on the purpose of
the calculation. Once the range has been crossed the position no longer earns fees,
which distorts measures like a fee / DL ratio { for those optimal behaviour of the
LPs may be a good working assumption. To compute actual LP returns however,
DL outside the range should probably be taken into account.
The other choices however all correspond to genuine alternative trading strategies,
and with the DL being an opportunity loss, all of them are in theory acceptable.
There is one caveat however which is that the Divergence Loss should not really
become a gain, so if the last of those denitions is used the measure should probably
be renamed.
In our view, the most useful DL measures are the one that crystallyze either into
CSH, or into a joint numeraire when running a multi-pool analysis which is the one
we used in [Loesch21]. Whether to use in-range for full IL numbers is a question
of judgement; in our view the in-range IL number is the one better suited for
theoretical fee/IL ratios, and the full number is better for estimating at actual
opportunity losses incurred.
40",2022-12-21T12:22:50Z,t  t t t scenar scenar iscenar scenar     t once ps to t tre divergence loss ilow sr
paper_qf_9.pdf,41,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","5.4 Modied weights
The modied weights AMM is very similar to the constant product AMM, with the
crucial dierence that now the two assets may have dierent weights, which as we
will see leads to a dierent portfolio composition. The characteristic function
of the modied weights AMM is
k=f(x;y) =xy1",2022-12-21T12:22:50Z,modi t t
paper_qf_9.pdf,43,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","need to remind ourselves however that, even though the situation is now some-
what asymmetric to start with, our choice of numeraire does introduce additional
asymmetries.
The rst chart shows the DL with the risk asset appreciating up to 5x against
the numeraire (from this point onwards the curves continue mostly linearly to the
right). We see that the = 0:5 (constant product) curve shows a signicantly
higher DL than the two other curves. This makes sense as on the one hand the
= 0:9 (risk asset dominant) portfolio predominently holds the risk asset and
therefore does not lag that much if it rallies. The = 0:1 (numeraire dominant)
curve on the other hand measure the DL against a porfolio that has very little of
the risk asset to start with, so again the relative losses are less.
One the risk asset downside, the terminal value is entirely driven by the HODL
portfolio: the AMM portfolio goes to zero in all three cases. Therefore the more
the HODL portfolio loses the lower the DL.
In the next chart we show the percentage DL , ie that DL relative to HODL, dened
asHODL",2022-12-21T12:22:50Z,t   t one trefore in
paper_qf_9.pdf,44,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","The nal chart here is the same chart as the previous one, but we are zooming
into2(0;1) to look at the risk asset downside. We see that again the constant
product AMM performs worst in percentage DL , even though ultimately the risk-
asset dominant AMM ( = 0:9) catches up or even slightly exceeds the losses in
relative terms. The numeraire-dominant AMM ( = 0:1) does consistently better
here.
44",2022-12-21T12:22:50Z,t  t
paper_qf_9.pdf,45,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","5.5 Modied curve aka (2 token) stableswap
When discussing the constant sum ( k=x+y) AMM which provides liquidity
at one specic price only we already alluded to two dierent options what could
happen at the boundary
•Option 1 : no special treatment at the boundaries x= 0;y= 0, the AMM
simply stops trading; that is also the model that Uniswap v3 is running at
the boundaries of the range
•Option 2 : the curve is modied towards the boundary x!0;y!0 such
that the characteristic functions always become tangent to the axis' and
therefore the AMM never runs out of assets
The stableswap model, introduced by [Egorov19], chooses Option 2, albeit in a
multi-token environment. We here go with the reduced two-token formula de-
scribed in [Niemerg20]. In this case, the characteristic function is
f;k=k(x+y) +xy
whereis a mixing parameter: for = 0 the AMM is a constant product AMM,
and in the limit !1 it becomes a constant sum AMM. Note that the kis
now part of the characteristic function { as we'll see in a moment it is there for
dimensional reasons (note that the above characteristic function does nothave our
usual scaling properties; it scales quadratically).
Theindierence curves are now dened by the equation
f;k(x;y) =k(x+y) +xy=k(;k) =k2+k2
4
where for the time being we consider as previously as a model parameter. As
before, for= 0 we recover the constant product formula, for !1 the constant
sum, and the x;y;k all scale linearly under the scale symmetry we previously
discussed, as per the design goals in [Egorov19].
The key dierence here when compared to the other AMMs we have discussed so
far is that kis now part of the characteristic function and therefore also part if
the indierence curve, ie it appears on the right hand side of the equation. For a
single indierence curve ( kxed) this does not matter. However, if kchanges, eg
when assets are contributed to or withdrawn from the pool, then the characteristic
function changes as well. Therefore a pair x;yno longer necessarily determines
the state of the AMM { we may need to specify kin addition to x;yas there
45",2022-12-21T12:22:50Z,modi woptuni swap optt eg or ov opt nie erg inote t idi as eg or ov t ms for trefore
paper_qf_9.pdf,46,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","may be multiple, or even an innite number of kthat lead to the same portfolio
composition.
This chart from [Egorov19] shows the shape of the above curve compared to con-
stant product and constant sum:
5.5.1 Dynamic Chi
As described briey in [Egorov19] and in more detail in [Feito] the constant (also
calledin those papers, but our kis theirD) is not a constant but it is dynamic.
The idea is that the ideal state of the pool is to have the same number of both
tokens (and therefore the same value, as their natural price ratio is unity). The
curve as shown above has very little convexity in the middle, and therefore very
little slippage. There is very little incentive for arbitrageurs to rebalance the pool,
and it may remain o kilter for a long time.
The stableswap mechanism therefore makes dynamic: the further the token ratio
is away from unity, the smaller the , therefore the closer the curve is to constant
product, therefore the higher the convexity, therefore the higher the slippage, and
nally therefore the higher the incentive for arbitrageurs to step in and balance
the pool.
Asnow depends on xin an implicit manner we can no longer analytically
calculate the price response function and the other objects. However, intuitively
we know how they look:
•theprice response function places most of the volume around unity
price; however, when it gets close to the x axis it suddenly falls to zero, and
when it gets to the right boundary it goes to innity
46",2022-12-21T12:22:50Z, eg or ov dynamic chi as eg or ov  to t t tre t as ver
paper_qf_9.pdf,47,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","•theportfolio value functionis similar to that of the constant sum AMM,
ie it ressembles an short put option prole that has been shifted upwards to
go through the origin, with a bit of convexity added on either side
•thestrikes and the Gamma are placed away from the unity price point
(not very far in absolute numbers, but very far away in terms of realistic
movements)
5.6 Multi asset
5.6.1 Equal weights
As previously discussed, the characteristic function of a multi-asset AMM is the
product of the token amounts in native currencies. Therefore the constant product
AMM is a specic case of the multi-asset AMM, albeit a rather special one as things
get more complex in in higher dimensions. The most commonly used function is
the straight product
k=f(x0;x1;:::;xN) =x0x1xN=NY
i=0xi
As before it makes often sense to use a function that has scales linearly, ie f(x0;x 1;:::) =
f(x0;x1;:::) because in this case the constant kis a measure of the pool size that
is not impacted by divergence loss. So instead of using the straight product we
are using the geometric average
k=f(x0;x1;:::;xN) =N+1px0x1xN= NY
i=0xi! 1
N+1
Theindierence curve is not an curve but a whole indierence surface . We
choosex0as the numeraire that we refer to as CSH { a choice that is as arbitrary as
choosingyin thexycase { and the x1:::xNare the risk assets RSK1. . . RSKN.
Isolatingx0we nd
x0;k(x1;x2;:::;xN) =kN+1
x1x2xN
In order to alleviate the notations we introduce the vector x= (x1;:::xN), ie the
vector of quantities of the risk assets, but excluding the numeraire asset. Because
we have multiple assets we also now have multiple price response functions
47",2022-12-21T12:22:50Z,gaa multi equal as trefore t as so t idi  are isolati ibecause
paper_qf_9.pdf,49,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","5.6.2 Variable weights
We have previously looked at the case where all assets in the pool have the same
weight. Like in the two-dimensional case we can achieve variable weights by intro-
ducing a coecient vector =0;:::;N. We will assume thatPi= 1 which
ensures the homogenity of the function, ie we get kinstead of k. The character-
istic function is then
k=f(x) =NY
i=0xi
i
Note that our N+1pterm has been absorbed in the which in the equally weighted
case are equal to i=1
N+1.
Similarly to the two-asset case we dene
i=i
0)X
i=1",2022-12-21T12:22:50Z,variable  like  t note simarly
paper_qf_9.pdf,50,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","In the variable-weight multi-asset AMM in equilibrium with the mar-
ket, the relative monetary value of RSKi and RSKj holdings is i=j
(highermeans bigger weight)
Thenormalized portfolio value function in this case is
(1:::N) =1
1N
N=NY
i=1i
i
It is easy to see that the Cash Delta is i@i=iso indeed the portfolio compo-
sition is in line with the coecients iand they sum up to .
The HODL portfolio in this case is 0units for the numeraire asset andPii
for the risk assets, so the divergence loss is
(1:::N) =0+NX
i=1ii",2022-12-21T12:22:50Z,iki kj t normalized it cash lta t
paper_qf_9.pdf,51,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","•Concentrated Liquidity design of Uniswap v3 where liquidity provider
are free to place their liquidity anywhere on the price curve, and where it is
maximally levered, allowing to create arbitrary price response functions.
One class of AMMs we have deliberately excluded are any whose design includes
external data providers and oracles as those are in our view very dierent designs
that pose very dierent challenges. We plan to cover those in a subsequent paper.
The world of AMMs is fast moving and we will keep this paper updated with the
important developments in this space. Please check on theammbook.org/paper for
the most recent version or - possibly a bit behind - on Arxiv once we have nished
the initial review cycle.
7 References
•[BlackScholes73] F Black, M Scholes: \\\\The Pricing of Options and Cor-
porate Liabilities"" (The Journal of Political Economy, 1973) url pdf
•[Merton73] R Merton: \\\\Theory of Rational Option Pricing"" (The Bell
Journal of Economics and Management Science, 1973) url pdf
•[Glosten94] L Glosten: \\\\Is the Electronic Open Limit Order Book In-
evitable?"" (The Journal of Finance, 1994) url pdf
•[Derman99] E Derman, K Demeter, M Kamal and J Zou: \\\\A Guide to
Volatility and Variance Swaps"" (The Journal of Derivatives, 1999) url pdf
•[Abernethy10] J Abernethy, Y Chen, JW Vaughan: \\\\An Optimization-
Based Framework for Automated Market-Making"" (arXiv, 2010) url pdf
•[Othman10] A Othman, T Sandholm: \\\\Automated market-making in the
large: the gates hillman prediction market"" (Proceedings of the 11th ACM
conference on Electronic commerce, 2010) url pdf
•[Othman12] A Othman: \\\\Automated Market Making: Theory and Practice
(PhD Thesis)"" (Carnegy Mellon, 2012) url pdf
•[Othman13] A Othman, D Pennock, D Reeves, T Sandholm: \\\\A Prac-
tical Liquidity-Sensitive Automated Market Maker"" (ACM Transactions on
Economics and Computation, 2013) url pdf
•[Buterin17] V Buterin: \\\\On Path Independence"" (Private, 2017) url pdf
•[Buterin17b] V Buterin: \\\\Let's run on-chain decentralized exchanges the
way we run prediction markets"" (Reddit, 2017) url pdf
51",2022-12-21T12:22:50Z,concentrated liquidity uni swap one ms  t ms please ar  referenc schools  schools t prici optns cor liabitit journal political economy mortomortotory ratnal optprici t bell journal economics management science gl oftegl ofteis eleronic opelimit orr book it journal nance r mar maed kamal zo gui volatity variance swaps t journal rivativabernet abernet cafghaaoptiatbased framework automated market maki  th math maand holm automated proceedis eleronic th math maautomated market maki tory praice ph tsis care gy meloth math maperock reevand holm pra liquidity sensitive automated market maker transans economics tbut eribut eriopath inpennce private but eribut eri reddit
paper_qf_9.pdf,52,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","•[Hertzog17] E Hertzog, G Benartzi, G Benartzi: \\\\Continuous Liquidity and
Asynchronous Price Discovery for Tokens through their Smart Contracts;
aka""Smart Tokens"""" (Bancor, 2017) url pdf
•[Lu17] A Lu: \\\\Building a Decentralized Exchange in Ethereum"" (Medium,
2017) url pdf
•[Buterin18] V Buterin: ""Improving front running resistance of x*y=k mar-
ket makers"" (Private, 2018) url pdf
•[Hertzog18] E Hertzog, G Benartzi, G Benartzi, O Ross: \\\\Bancor Pro-
tocol - Continuous Liquidity for Cryptographic Tokens through their Smart
Contracts"" (Bancor, 2018) url pdf
•[Zargham18] M Zargham, Z Zhang, V Preciado: \\\\A State-Space Modeling
Framework for Engineering Blockchain-Enabled Economic Systems"" (Arxiv,
2018) url pdf
•[Adams19] H Adams: \\\\Uniswap Birthday Blog | V0"" (Uniswap, 2019) url
pdf
•[Daian19] P Daian, S Goldfeder, T Kell, Y Li, X Zhao, I Bentov, L Brei-
denbach, A Juels: \\\\Flash Boys 2.0: Frontrunning, Transaction Reordering,
and Consensus Instability in Decentralized Exchanges"" (Arxiv, 2019) url pdf
•[DiMaggio19] M Di Maggio: \\\\Survey of Automated Market Making Algo-
rithms"" (Medium, 2019) url pdf
•[Egorov19] M Egorov: \\\\StableSwap - a ecient mechanism for Stablecoin
liquidity"" (Curve, 2019) url pdf
•[Kereiakes19] E Kereiakes, D Kwon, M Di Maggio, N Platias: \\\\Terra
Money: Stability and Adoption"" (Terra, 2019) url pdf
•[Martinelli19] F Martinelli, N Mushegian: \\\\Balancer Whitepaper"" (Bal-
ancer, 2019) url pdf
•[Pintail19] Pintail: \\\\Uniswap: A Good Deal for Liquidity Providers?"" (Medium,
2019) url pdf
•[Angeris20] G Angeris, A Evans, T Chitra: \\\\When does the tail wag the
dog? Curvature and market making"" (Arxiv, 2020) url pdf
•[Angeris20a] G Angeris, T Chitra: \\\\Improved Price Oracles"" (Proceedings
of the 2nd ACM Conference on Advances in Financial Technologies, 2020)
url pdf
52",2022-12-21T12:22:50Z,r og r og beart zi beart zi continuous liquidity asynchronous price divery tokens smart contras smart tokens banjo lu lu budi centralized exe tre um medium but eribut eriimprovi private r og r og beart zi beart zi ross banjo pro continuous liquidity yptographic tokens smart contras banjo  ham  ham  pre cia do state space moli framework eineeri block articial intellence enabled economic tems ar  adams adams uni swap birthday blog uni swap articial intellence aarticial intellence agold fe r ell li hao bent bre ju els flash boys front runni transarenri consensus instabity centralized exar  di mag  di mag  survey automated market maki al go medium eg or ov eg or ov stable swap stable coicurve ker eia kker eia kkwodi mag  plat as terra money stabity adoptterra martii martii mus  gaibalance white pa bal pint articial intellence pint articial intellence uni swap good al liquidity provirs medium aer is aer is evans chira wcurvature ar  aer is aer is chira improved price orae proceedis conference advancnancial technologies
paper_qf_9.pdf,53,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","•[Bukov20] A Bukov, M Melnik: \\\\Mooniswap by 1inch exchange)"" (1inch,
2020) url pdf
•[Clark20] J Clark: \\\\The Replicating Portfolio of a Constant Product Mar-
ket"" (SSRN, 2020) url pdf
•[Dodo20] DODO Team: \\\\Why There is No Impermanent Loss on DODO""
(DODOex, 2020) url pdf
•[Dodo20a] DODO Team: \\\\Introducing DODO: 10x Better Liquidity than
Uniswap"" (DODOex, 2020) url pdf
•[Feito20] A Feito: \\\\A stablecoin exchange"" (Curve, 2020) url pdf
•[KPMG20] KPMG: \\\\Automated Market Makers - Innovations, Challenges,
Prospects"" (KPMG, 2020) url pdf
•[Loesch20] S Loesch, N Hindman: \\\\Bancor Protocol v2.1 - Economic and
Quantitative Finance Analysis"" (Private, 2020) url pdf
•[Lyons20] R Lyons, G Viswanath-Natraj: \\\\What keeps stable coins stable?""
(NBER, 2020) url pdf
•[Michael20] Michael: \\\\Amplied Liquidity: Designing Capital Ecient Au-
tomated Market Makers in Bancor V2"" (Bancor, 2020) url pdf
•[Niemerg20] A Niemerg, D Robinson, L Livnev: \\\\YieldSpace: An Auto-
mated Liquidity Provider for Fixed Yield Tokens"" (Yield, 2020) url pdf
•[Noyes20] C Noyes: \\\\Liquidity Provider Wealth"" (Paradigm, 2020) url pdf
•[THORChain20] THORChain Team: \\\\THORChain - A decentralized liq-
uidity network"" (THOrChain, 2020) url pdf
•[Tassy20] M Tassy, D White: \\\\Growth rate of a liquidity provider's wealth
in xy=c automated market makers"" (Private, 2020) url pdf
•[Zhang20] D Zhang: \\\\Curve. 101 | How it works and its meteoric rise""
(Private, 2020) url pdf
•[Adams21] Hayden Adams and Noah Zinsmeister and Moody Salem and
River Keefer and Dan Robinson: \\\\Uniswap v3 Core"" (Uniswap, 2021) url
pdf
•[Angeris21] G Angeris, A Agrawal, A Evans, T Chitra, S Boyd: \\\\Con-
stant Function Market Makers: Multi-Asset Trades via Convex Optimiza-
tion"" (Arxiv, 2021) url pdf
53",2022-12-21T12:22:50Z,bu v bu v mel  moois wap ark ark t replicati tfconstant produ mar todo team w tre no manent loss ex todo team introduci beer liquidity uni swap ex  to  to curve automated market makers innovatns challeprospes low slow shind mabanjo protoceconomic quantitative nance analysis private lns lns vi swath raj what michael michael amp li liquidity sni capital au market makers banjo banjo nie erg nie erg robinsoliv new yield space aauto liquidity provir xed yield tokens yield notnotliquidity provir alth dm articial intellence articial intellence team articial intellence or articial intellence as sy as sy white groh private   curve how private adams hand adams noah imister moody salem rikeep darobinsouni swap core uni swap aer is aer is aga wal evans chira boyd cofunmarket makers multi asset tras convex opt  ar 
paper_qf_9.pdf,54,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","•[Angeris21a] G Angeris, A Evans, T Chitra: \\\\Replicating Market Makers""
(Arxiv, 2021) url pdf
•[Angeris21b] G Angeris, A Evans, T Chitra: \\\\A Note on Privacy in Con-
stant Function Market Makers"" (Arxiv, 2021) url pdf
•[Angeris21c] G Angeris, H Kao, R Chiang, C Noyes, T Chitra: \\\\An analysis
of Uniswap markets"" (Arxiv, 2021) url pdf
•[Angeris21d] G Angeris, A Evans, T Chitra: \\\\Replicating Monotonic Pay-
os Without Oracles"" (Arxiv, 2021) url pdf
•[Clark21] J Clark: \\\\A deconstructed constant product market"" (SSRN,
2021) url pdf
•[Dodo21] DODO Team: \\\\DODO - A Next-Generation On-Chain Liquidity
Provider Powered by Pro-active Market Maker Algorithm"" (DODOex, 2021)
url pdf
•[Egorov21] M Egorov: \\\\Automatic market-making with dynamic peg"" (Curve,
2021) url pdf
•[Evans21] A Evans, G Angeris, T Chitra: \\\\Optimal Fees for Geometric
Mean Market Makers"" (Arxiv, 2021) url pdf
•[Falakshahi21] H Falakshahi, M Mariapragassam, R Ajaja: \\\\Automated
Market Making with Synchronized Liquidity Pools"" (SSRN, 2021) url pdf
•[Grove21] C Grove: \\\\Price Formulation of CFMMs"" (Private, 2021) url
pdf
•[Lambert21] G Lambert: \\\\Understanding the Value of Uniswap v3 Liquid-
ity Positions"" (Medium, 2021) url pdf
•[Loesch21] S Loesch, N Hindman, MB Richardson, N Welch: \\\\Impermanent
Loss in Uniswap v3"" (Arxiv, 2021) url pdf
•[Mota21] M Mota: \\\\Understanding StableSwap"" (Private, 2021) url pdf
•[Nguyen21] A Nguyen, L Luu, M Ng: \\\\Dynamic Automated Market Mak-
ing"" (Kyber, 2021) url pdf
•[Onomy21] Onomy Protocol: \\\\An Analysis of Automated Market Makers
vs. Order Books"" (Onomy, 2021) url pdf
•[Platypus21] Platypus Team: \\\\Platypus AMM Technical Specication"" (Platy-
pus, 2021) url pdf
54",2022-12-21T12:22:50Z,aer is aer is evans chira replicati market makers ar  aer is aer is evans chira note privacy cofunmarket makers ar  aer is aer is kao  notchira auni swap ar  aer is aer is evans chira replicati monotypic pay without orae ar  ark ark todo team next gentoarticial intellence liquidity provir ed pro market maker algorithm ex eg or ov eg or ov automatic curve evans evans aer is chira optimal fegeometric meamarket makers ar  fa lak shah fa lak shah maria drag assam aja automated market maki synchronized liquidity pools grove grove price tms private lambert lambert unrstandi value uni swap liquid positns medium low slow shind marichardsolsh manent loss uni swap ar  mot mot unrstandi stable swap private uyeuyelu  dynamic automated market mak ky ber ono my ono my protocaanalysis automated market makers orr books ono my plateaus plateaus team plateaus technical spec plat
paper_qf_9.pdf,55,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","•[Tassy21] M Tassy: \\\\A dierent view at AMMs"" (Private, 2021) url pdf
•[Tassy21a] M Tassy: \\\\From Uniswap to option AMMs (Part I): A dierent
view at AMMs"" (Private, 2021) url pdf
•[Lambert22] G Lambert: \\\\How to deploy delta-neutral liquidity in Uniswap""
(Medium, 2022) url pdf
•[Element22] Element: \\\\The Element Protocol Construction Paper"" (Ele-
ment, Undated) url pdf
•[Hull] J Hull: \\\\Options, Futures, and Other Derivatives"" (Pearson, 2018)
url pdf
•[Loesch] S Loesch: \\\\A Guide to Financial Regulation for Fintech Entrepreneurs""
(Wiley, 2018) url pdf
•[Pyndick-Rubinfeld] R Pyndick, D Rubinfeld: \\\\Microeconomics"" (Pear-
son, 2018) url pdf
8 Appendix
8.1 Website
The website of the book is at theammbook.org. The paper itself can be found on
the website at theammbook.org/paper
8.2 Glossary and technical glossary
A glossary of general terms is at theammbook.org/glossary. A technical (formula)
glossary is at theammbook.org/techglossary
8.3 Formulas
Key AMM-related formulas are are theammbook.org/formulas
8.4 Projects
A list of AMM and AMM-related projects is at theammbook.org/projects
8.5 References
A list of AMM-related references (academic papers, blogs, books) is at theamm-
book.org/references
55",2022-12-21T12:22:50Z,as sy as sy ms private as sy as sy from uni swap ms part ms private lambert lambert how uni swap medium element element t element protocconstrupa ele undated hull hull optns futurotr rivativpearsolow slow sgui nancial regulatteentrepreneurs rey py dick rub nefeld py dick rub nefeld maoeconomic pear  bsite t t glossary s key projes references
paper_qf_9.pdf,56,The Quantitative Finance Aspects of Automated Market Markers in DeFi,"  Automated Market Makers (AMMs) are a class of smart contracts on Ethereum and
other blockchains that ""make markets"" autonomously. In other words, AMMs stand
ready to trade with other market participants that interact with them, at the
conditions determined by the AMM. In this this paper, which relies on the
existing and growing corpus of literature available, we review and present the
key mathematical and quantitative finance aspects that underpin their
operations, including the interesting relationship between AMMs and derivatives
pricing and hedging.
","9 License
This work is licensed under a Creative Commons Attribution 4.0 International
License.
9.1 Attribution 4.0 International (CC BY 4.0)
This is a human-readable summary of (and not a substitute for) the license.
This deed highlights only some of the key features and terms of the
actual license. It is not a license and has no legal value. You should
carefully review all of the terms and conditions of the actual license
before using the licensed material.
You are free to:
•Share | copy and redistribute the material in any medium or format
•Adapt | remix, transform, and build upon the material
for any purpose, even commercially. The licensor cannot revoke these freedoms as
long as you follow the license terms.
Under the following terms:
•Attribution | You must give appropriate credit, provide a link to the
license, and indicate if changes were made. You may do so in any reasonable
manner, but not in any way that suggests the licensor endorses you or your
use.
•No additional restrictions | You may not apply legal terms or techno-
logical measures that legally restrict others from doing anything the license
permits.
Notices:
You do not have to comply with the license for elements of the material in the public
domain or where your use is permitted by an applicable exception or limitation. No
warranties are given. The license may not give you all of the permissions necessary
for your intended use. For example, other rights such as publicity, privacy, or moral
rights may limit how you use the material.
56",2022-12-21T12:22:50Z,license  eative coons aributinternatnal license aributinternatnal   it you you share adapt t unr aributyou you no you noticyou no t for
paper_qf_10.pdf,1,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","Text mining arXiv: a look through
quantitative finance papers
Michele Leonardo Bianchia,1
aFinancial Stability Directorate, Bank of Italy, Rome, Italy
This version: January 4, 2024
Abstract. This paper explores articles hosted on the arXiv preprint server with
the aim to uncover valuable insights hidden in this vast collection of research. Em-
ploying text mining techniques and through the application of natural language
processing methods, we examine the contents of quantitative finance papers posted
in arXiv from 1997 to 2022. We extract and analyze crucial information from the
entire documents, including the references, to understand the topics trends over
time and to find out the most cited researchers and journals on this domain. Ad-
ditionally, we compare numerous algorithms to perform topic modeling, including
state-of-the-art approaches.
Key words: quantitative finance, text mining, natural language processing, un-
supervised clustering, topic modeling, research trends, named entity recognition.
1 Introduction
Quantitative finance is a field of finance that studies mathematical and statistical
models and applies them to financial markets and investments, for pricing, risk
management, and portfolio allocation. These models are needed to analyze finan-
cial data, to find the price of financial instruments and to measure their risk (see
Vogl (2022) and Bianchi et al. (2023)). Readers are referred to Derman (2011) for
an insightful exploration of the role of models in finance and to Ippoliti (2021) for
some philosophical remarks on mathematics and finance.
The world of finance is always moving forward even in times of crisis. Inno-
vations in finance come from the development of new financial services, products
or technologies. Research trends in quantitative finance are driven not only by
innovations, but also by structural changes in financial markets or by changes in
regulation (Cesa (2017) and Carmona (2022)). When a structural change occurs
some models are not anymore able to explain the phenomena observed in the mar-
ket, consequently quants and researchers start working on new models. Examples
of such changes are when the implied volatility smile appeared in 1987 (see Derman
and Miller (2016)) or the Euribor-OIS spread materialized in 2007 (see Bianchetti
and Carlicchi (2012)). Research activities driven by new products are, for instance,
1The author thanks arXiv, ChatGPT, and Google Scholar for use of their open access inter-
operability and Sabina Marchetti for her comments and suggestions. This publication should not
be reported as representing the views of the Bank of Italy. The views expressed are those of the
author and do not necessarily reflect those of the Bank of Italy.
1arXiv:2401.01751v1  [cs.DL]  3 Jan 2024",2024-01-03T14:06:06Z,text  micle leonardo brannancial stabity direorate bank  rome   uary abstra   em   ad key introduquantitative tse vo gl branrears r maip polite t ino researccar mona ws r mamler eu rib or bi ti carl chi researt    scholar sayi marci  bank  t bank   
paper_qf_10.pdf,2,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","the development of pricing models for interest rate and equity derivatives started
in ’90s, the structuring of credit products in the early 2000s, or the recent research
trend on cryptocurrencies. New technologies applied to finance are namely the
increasing role of big data and the advent of machine learning techniques. Regula-
tion have an impact on the development of new quantitative tools for measuring,
managing and monitoring financial risks (e.g. the Basel Accords).
In this paper we explore the arXiv preprint server, the dominant open-access
preprint repository for scholarly papers in the fields of physics, mathematics,
computer science, quantitative biology, quantitative finance, statistics, electrical
engineering and systems science, and economics. The papers in arXiv are not
peer-reviewed but there are advantages in submitting to this repository, mainly to
disseminate a paper without waiting for the peer review and publishing process,
which can be slow (see Huisman and Smits (2017)). The arXiv collection provides
a unique source of data to conduct various studies, including bibliometric, trend,
and citation network analyses (see Clement et al. (2019)). It is a valuable resource
for advancing scientific knowledge and conducting research on research, often re-
ferred to as meta-research . For example, Eger et al. (2019) and Viet and Kravets
(2019) perform trend detection on computer science papers stored in arXiv, Lin
et al. (2020) conduct a case study of computer science preprints submitted to arXiv
from 2008 to 2017 to quantify how many preprints have eventually been printed in
peer-reviewed venues, Tan et al. (2021) explore the images of around 1.5 million of
papers held in the repository, Okamura (2022) investigates the citations of more
than 1.5 million preprints on arXiv to study the evolution of collective attention
on scientific knowledge, Bohara et al. (2023) train a state-of-art classification ap-
proach, and Fatima et al. (2023) design an algorithm to help researchers to perform
systematic literature reviews.
We study all papers on quantitative finance, a small portion of the entire arXiv
containing more than two millions of works at the time of writing. The choice is
also motivated by our experience in this domain and by scientific curiosity.
The code is run on a standard desktop environment, without recurring to a
big cluster. Scaling to a large number of papers may be not trivial. Dealing
with a large amount of data requires significant computing resources, including
processing power and memory, to manipulate and analyze the data efficiently. It is
not simple, and maybe even impossible, to explore more than two million of papers
with a standard desktop environment like ours.
The studies of papers on finance topics is not new in the literature. Burton
et al. (2020) review the history of a well-known journal in this field and highlight
its growth in terms of productivity and impact. The authors present a bibliomet-
ric analysis and identify key contributors, themes, and co-authorship patterns and
suggest future research directions. Ali and Bashir (2022) conduct a systematic lit-
erature review and a bibliometric analysis on around 3,000 articles on asset pricing
sourced from the top 50 finance and economics journals, spanning a 47-year period
from 1973 to 2020. As observed by the authors, the exclusion of certain publica-
tions may potentially offer an alternative perspective on the landscape of existing
asset pricing research. By using bibliometric and network analysis techniques, in-
cluding the Bibliometrix Tool of Aria and Cuccurullo (2017), Sharma et al. (2023)
investigate more than 4,000 papers on option pricing appeared from 1973 to 2019.
2",2024-01-03T14:06:06Z,new re gul basel accords i t  hu is masmith t  ement it for eviet kr vets  li taoka   oara tima   t t scali ali it t burtot ali baseip as by bib l me trip toaria ucc rul lo sharma
paper_qf_10.pdf,3,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","They follow the procedure suggested by Donthu et al. (2021). Their study aims
to pinpoint high-quality research publications, to discern trends in research, to
evaluate the contributions of prominent researchers, to assess contributions from
different geographic regions and institutions, and, ultimately, to examine the in-
terconnectedness among these aspects. The works of Burton et al. (2020), Ali and
Bashir (2022) and Sharma et al. (2023) are focused on asset pricing or on a specific
journal, their corpus is obtained by searching in the Scopus database some specific
keywords, and the bibliometric analysis relies on VOS viewer (see Van Eck and
Waltman (2010)) and Gephi (see Bastian et al. (2009)).
Our study explores all papers on quantitative finance collected in arXiv till the
end of 2022 (around 16,000) and it considers text mining techniques implemented
in Python to extract information directly from the portable document format (pdf)
files containing the full text of the papers, excluding images, without relying on
ad-hoc software or proprietary databases. Westergaard et al. (2018) found that
examining the full text of documents significantly improved text mining compared
to studies that only explored information collected from abstracts.2Their finding
highlights the importance of using complete textual content for more comprehen-
sive and accurate text mining and analysis.
The main objectives of our work are twofold. First, we explore the topics of the
quantitative finance papers collected in arXiv in order to describe the evolution
of topics over time. After having evaluated the performance of various clustering
algorithms, we investigate on which themes researchers have focused their attention
in the period from 1997 to 2022. Second, we try to understand who are the most
prominent authors and journals in this field. Both analyses are performed with
data mining techniques and without actually reading the papers.
The remainder of the paper is organized as follows. First, we provide a brief
description of the data analyzed in this work (Section 2). Then, in Section 3
the preprocessing phase is discussed by offering further insights on the papers
analyzed in our work. In Section 4 we compare various clustering algorithms and,
after having selected the best performer, we explore, by splitting our corpus in
30 clusters, the evolution of topics over time. Finally, in Section 5 we describe
an entity extraction process to investigate authors and journals with the largest
number of occurrences in the corpus considered in this work. Section 6 concludes.
2 Data description
In this section we provide a description of the papers analyzed in this work. As
observed above, there are various domains in arXiv (i.e. physics, mathematics,
computer science, quantitative biology, quantitative finance, statistics, electrical
engineering and systems science, and economics) and each domain has is own
categories. The categories within the quantitative finance domain are the following:
•computational finance (q-fin.CP) includes Monte Carlo, PDE, lattice and
other numerical methods with applications to financial modeling;
2As a crosscheck, we conduct the analysis on both abstracts and full texts. The analysis using
the full text data shows better results.
3",2024-01-03T14:06:06Z,ty dothu tir t burtoali baseip sharma sc opus vaeck artmage phi bastieour  pythosterga ard tir t rst  after second both t rst setseisenally sesedata ias  t   as t
paper_qf_10.pdf,4,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","•economics (q-fin.EC) is an alias for econ.GN and it analyses micro and macro
economics, international economics, theory of the firm, labor economics, and
other economic topics outside finance;
•general finance (q-fin.GN) is focused on the development of general quanti-
tative methodologies with applications in finance;
•mathematical finance (q-fin.MF) examines mathematical and analytical meth-
ods of finance, including stochastic, probabilistic and functional analysis, al-
gebraic, geometric and other methods;
•portfolio management (q-fin.PM) deals with security selection and optimiza-
tion, capital allocation, investment strategies and performance measurement;
•pricing of securities (q-fin.PR) discusses valuation and hedging of financial
securities, their derivatives, and structured products;
•risk management (q-fin.RM) is about risk measurement and management of
financial risks in trading, banking, insurance, corporate and other applica-
tions;
•statistical finance (q-fin.ST) includes statistical, econometric and econophysics
analyses with applications to financial markets and economic data;
•trading and market microstruture (q-fin.TR) studies market microstructure,
liquidity, exchange and auction design, automated trading, agent-based mod-
eling and market-making.
These categories are assigned by the authors when they submit their papers.
Even if it is possible to select multiple couples of domain-category belonging to
more than one domain, we select as reference category only the first category
within the quantitative finance domain. Figure 1 shows the numbers of papers on
quantitative finace submitted to arXiv between 1997 and 2022. The increase in
the last three years is mainly due to the q-fin.EC category.
As observed in Section 1, the code is implemented in Python and it is run under
Ubuntu 22.04 on a desktop with an AMD Ryzen 5 5600g processor with 32GB of
RAM. As we will describe in the following, numerous packages are considered.
As far as the collection process is concerned, we retrieve data from arXiv by
selecting all categories within quantitative finance (i.e. q-fin). We collect articles
metadata and pdf files for all articles from 1997 to 2022 for a total of around 16,000
articles (18GB of data).
While the metadata are obtained through urllib.request and feedparser , the
pdf files are downloaded by means of the arxiv package. The metadata can be
collected by following the suggestions provided in the arXiv web-pages. They are
a fundamental input of the analysis and include the link to the paper main web-
page, from where it is possible to extract the paper identification code ( id, e.g.
2005.06390). The metadata contain information like authors names, paper title,
primary category, submission and last update dates, abstract, and publication
data when available (e.g. digital object identifier, DOI). Subsequent updates of
4",2024-01-03T14:06:06Z,tse eve  t as sepythoubuntu ry zeas as   w t  ty t subsequent
paper_qf_10.pdf,5,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","Figure 1: Categories by year.
the papers can be stored in the repository and for this reason there is the version
number at the end of the paper id.
Since a paper could be assigned to multiple categories, a web-scraping tool
written in Python allows us to retrieve from the paper main web-page the list of
all categories of each single paper. We select from this list the subset of categories
within the quantitative finance domain. Thus we assign as reference category of
a paper the first category appearing in this subset. Starting from this list, we are
able to filter and analyze all papers in the nine categories within q-fin.
Thepdftotext package allows us to extract the text from pdf files. Each paper
becomes a single (long) string. As discussed in Section 3, the length of these strings
vary accross papers (see Figure 5), also because some documents are not papers
(e.g. there are both theses and books). As a first assessment of the corpus, for each
document we estimate the readability of the papers through textstat . As shown in
Figure 2 the Flesch reading ease score (see DuBay (2004)) is on average equal to
65.7 (plain English), the lower and upper quartile are 59.91 (fairly difficult to read,
but not far from the plain English) and 71.95 (fairly easy to read), respectively,
and 99 per cent of the papers are in the range from 40.28 (difficult to read) and
88.20 (easy to read). There is only one paper with a negative value, but this is
caused by the text contained in the figures. All other papers are above 17.17, that
is above the extremely difficult to read level.
3 Text preprocessing
This section describes the text processing steps. The preprocessing phase is per-
formed with nltk:(1) we split the text in tokens; (2) we extract the numbers rep-
resenting years in the text;3(3) we identify all strings containing alphabet letters,
and we refer to them as words even in the case they do not belong to the English
3We assume these numbers have 4 digits. We do not explore this data in the empirical analysis.
5",2024-01-03T14:06:06Z, categorisince pytho  starti t pdf to text eaas se as as  flesh du bay eh eh tre all text  t eh  
paper_qf_10.pdf,6,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","Figure 2: The Flesch reading ease score is reported. The vertical line represents the median,
the dashed line is the quantile of level 0.25 (0.75), and the dotted line is the quantile of level
0.005 (0.995).
vocabulary; this step allows also to remove some symbols which are not recognized
as letters in text analysis. Then, (4) we remove all stopwords and all words with
lenght less than 3 characters; we also check whether there are words with more
than 25 characters (quite uncommon in English); (5) we conduct a lemmatization
by means of a part-of-speech tagger considering nouns, verbs, adjectives and ad-
verbs; (6) we check if the paper is written in English by means of the langdetect
package and we discard all non English papers. Both the extraction phase and the
preliminary text analysis is parallelized by means of the multiprocessing package.
We refer to the output of this first preprocessing phase as lemmatized data .
Thus, we analyze the frequency of the words across the whole corpus and we
remove all words appearing less than 25 times. We discard also some words fre-
quently used in writing papers on quantitative finance and which do not help in
understanding the topic of the paper. The list of these words includes, for ex-
ample, “proof” and “theorem”, verbs commonly used in mathematical sentences
(e.g.,“assume”,“satisfy”, and “define”), mathematical functions (e.g. “min”, and
“log”) and adverbs. The complete list is available upon request. In Figure 3 the
list of the top 100 most frequent words obtained after this cleaning phase and
their percentage of appearance is shown. The word “model” is extremely frequent
(one every 100 words). The word “http” is also quite common, indicating that the
papers full texts contain numerous internet links.
After a first preprocessing phase, we conduct an n-gram analysis by considering
thePhrases model of the gensim package. We ignore all words and bigrams with
total collected count over the entire corpus lower than 250 and set the score thresh-
old equal to 10. We find the bigrams and then, to find trigrams and fourgrams,
we apply again the same model to the trasformed corpus including bigrams. This
approach allows us to have a better ex-post understanding of the corpus which is
full of n-grams (e.g. Monte Carlo simulation, Eisenberg and Noe, or bank balance
sheet). The wordclouds of the main bigrams and trigrams (fourgrams) are depicted
in Figure 4.
It should be noted that some topic modeling algorithms analized in Section 4 do
not need text preprocessing. In those cases the input is just a single list containing
6",2024-01-03T14:06:06Z, t flesh t teh eh eh both    t t i t t after phras     eisenberg no t  it sein
paper_qf_10.pdf,7,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","Figure 3: Frequent words of the corpus and their percentage of appearance.
7",2024-01-03T14:06:06Z, frequent
paper_qf_10.pdf,8,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","Figure 4: Bigrams and tri(four)grams word clouds based on frequency with parameters min
count equal to 250 and threshold equal to 10.
8",2024-01-03T14:06:06Z, b rams
paper_qf_10.pdf,9,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","Figure 5: Papers length, in terms of number of words, for raw, lemmatized and cleaned data.
The x-axis values are in thousands. The scale of x-axis varies across the three datasets. The
vertical line represents the median, the dashed line is the quantile of level 0.25 (0.75), and the
dotted line is the quantile of level 0.01 (0.99).
the whole paper text. While we refer to this latter input as raw data , we define
the output of the preprocessing as cleaned data .
In Figure 5 we show how the length of the papers varies over the three data
preprocessing phases. Starting from the raw data, containing all words and sym-
bols, after a preliminary cleaning step we obtain the lemmatized data and then,
after the last cleaning steps, the cleaned data. The number of words varies from a
median value of 8,824 words for the raw data to around 2,518 words for the cleaned
ones.
4 Topics trend
Now we are in the position to perform a topics trend analysis. We employ a topic
modeling approach to identify the subjects discussed in the documents examined
in this study, and then we observe how these topics change over time. Topic mod-
eling refers to a class of statistical methods used to determine which subjects are
prevalent in a given corpus. In Section 4.1 we select the best performing model
among some selected approaches presented in the related literature. We evaluate
these approaches by assessing their ability to accurately match the nine q-fin cat-
egories that researchers assign to their work when submitting it to arXiv. Then in
Section 4.2, after having split the papers into 30 clusters, each one representing a
specific topic, we discuss the evolution of research trends over time.
9",2024-01-03T14:06:06Z, pas t t t w i starti t topics   topic ise  tsen
paper_qf_10.pdf,10,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","4.1 Algorithms performance on full texts
Topic modeling algorithms are widely used in natural language processing and text
mining to uncover latent thematic structures in a collection of documents. Differ-
ent algorithms have been developed, each with its own strengths and limitations
(see Sethia et al. (2022)). The choice of a topic modeling algorithm depends on
specific factors, such as the desired level of topic granularity and computational
constraints. Some algorithms may require substantial computational resources and
large amounts of training data. Here, we compare different algorithms by looking
at some standard performance measures.
Since is not simple to assess the performance of different topic modeling algo-
rithms (see R¨ udiger et al. (2022)), we start by comparing the clusters assigned by
each algorithm on the entire corpus to the nine clusters defined by the q-fin cate-
gories described in Section 2, that is the categories that researchers assign to their
work during the submission process to arXiv. By exploiting a Bayesian optimiza-
tion strategy, Terragni et al. (2021) present a framework for training, analyzing,
and comparing topic models where the competitor models are trained by searching
for their optimal hyperparameter configuration, for a given metric and dataset.
Here we consider a simpler approach in which we compare the models by looking
at some metrics. Evaluating topic modeling algorithms typically involves the use
of performance measures, and it is important to note that different algorithms can
yield varying results across these metrics.
We consider the following models.
•K-means . We perform a clustering analysis by considering the K-means al-
gorithm implemented in scikit-learn . This algorithm groups data points into
K clusters by minimizing the distance between data points and their cluster
center. The document word matrix is created through the CountVectorizer
function, that converts the corpus to a matrix of token counts. We ignore
terms that have a document frequency strictly higher than 75%. T
•LDA . By considering the same document word matrix analyzed with the
K-means algorithm, we perform topic modeling with the latent Dirichlet
allocation (LDA). LDA is a well-known unsupervised learning algorithm. As
observed in the seminal work of Blei et al. (2003), the basic idea is that
documents are represented as random mixtures over latent topics, where each
topic is characterized by a distribution over words. We study two different
implementations of LDA (i.e. scikit-learn andgensim ).
•Word2Vec . We train a word embedding model (i.e. Word2Vec) and then we
perform a clustering analysis by considering again the K-means approach.
An embedding is a low-dimensional space into which high-dimensional vec-
tors are projected. Machine learning on large inputs like sparse vectors repre-
senting words is easiser if embeddings are considered. Ideally, an embedding
captures some of the semantics of the input by placing semantically similar
inputs close together in the embedding space. The Word2Vec neural net-
work introduces distributed word representations that capture syntactic and
semantic word relationships (see Mikolov et al. (2013)). More in details, we
10",2024-01-03T14:06:06Z,algorithms topic dfer seth t some re since se by bayesiaterra gni re evaluati    t count veor ize  by di ri as ble  word vec  word vec amachine ially t word vec mike lov 
paper_qf_10.pdf,11,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","generate document vectors using the trained Word2Vec model, that is, we
get numerical vectors for each word in a document, and then the document
vector is the weighted average of the vectors. Thus, the K-means algorithm
is applied to the matrix representing the corpus. We consider the Word2Vec
model implemented in gensim .
•Doc2Vec . We create a vectorised representation of each document through
the Doc2Vec model and then we perform a clustering analysis by consid-
ering the K-means approach. The Doc2Vec extends Word2Vec and it can
learn distributed representations of varying lengths of text, from sentences
to documents (see Le and Mikolov (2014)). We consider the Doc2Vec model
implemented in gensim .
•Top2Vec . We study the Top2Vec model, an unsupervised learning algorithm
that finds topic vectors in a space of jointly embedded document and word
vectors (see Angelov (2020)). This algorithm directly detects topics by per-
forming the following steps. First, embedding vectors for documents and
words are generated. Second, a dimensionality reduction on the vectors is
implemented. Third, the vectors are clustered and topics are assigned. This
algorithm is implemented in an ad-hoc library named Top2Vec and it auto-
matically provides information on the number of topics, topic size, and words
representing the topics.
•BERTopic. We study a BERTopic model, which is similar to Top2Vec in
terms of algorithmic structure and uses BERT as an embedder. As described
in the seminal work of Grootendorst (2022), from the clusters of documents,
topic representations are extracted using a custom class-based variation of
term frequency-inverse document frequency (TF-IDF). This is the main dif-
ference with respect to Top2Vec. The algorithm is implemented in an ad-hoc
library named BERTopic . The main downside with working with large doc-
uments, as in our case, is that information will be ignored if the documents
are too long. A limited number of tokens are treated and anything longer is
cut off. Since we are dealing with large documents, to work around this issue,
we first split each documents in chunks of 300 tokens, thus we fit the model
on these chunks. BERTopic does not allow one to directly select the number
of topics, for this reason on the first step we obtain a number of topics much
larger than the desired one. Since we obtain for each chunk the corresponding
topic, we have for each document a list of possibly different topics and the
length of these lists varies across documents (i.e. the length of a single list
depends on the length of the corresponding document). To cluster this list of
lists of topics, we consider each integer representing a topic as a word. Thus
we use the Word2Vec algorithm described above to find similarities between
these list of topics. Each topic label, that is the number representing the
topic, is treated as a string, and Word2Vec transforms it into a numerical
vector. We then apply K-means clustering to group these lists based on their
similarity in the vector space. The resulting clusters reveal relationships and
patterns among these lists and allow us to select the number of clusters we
need for our purposes.
11",2024-01-03T14:06:06Z,word vec   word vec doc vec  doc vec t doc vec word vec le mike lov  doc vec tvec  tvec aelo  rst second third  tvec topic  topic tvec as root end or st  tvec t topic t since topic since to  word vec eaword vec  t
paper_qf_10.pdf,12,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","In theory both BERTopic and Top2Vec should use raw data since these algo-
rithms rely on an embedding approach, and keeping the original structure of the
text is of paramount importance (see Egger and Yu (2022)). However, raw data
extracted from quantitative finance papers have a considerable amounts of formu-
las, symbols and numbers that may affect the algorithms performance. For this
reason, we consider both raw and cleaned data. Additionally, for these state-of-the-
art algorithms the number of extracted topics tends be large, but the algorithms
offer the possibility to reduce the number of topics and of outliers, that can be
larger than expected. The parameters of a BERTopic model have to be carefully
chosen to avoid memory issues. Alternativelly, it is possible to perform an online
topic modeling, that is the model is trained incrementally from a mini-batch of
instances. This result in a less resource demanding approach in terms of memory
and CPU usage, but it generates less rich and less comprehensive outputs and for
these reasons we do not consider this incremental approach here.
In Table 1 we report the following similarity measures between true and pre-
dicted cluster labels: (1) the rand score (RS) is defined as the ratio between the
number of agreeing pairs and the number of pairs, and it ranges between 0 and
1, where 1 stands for perfect match; (2) the adjusted rand score (ARS), that is
the rand score adjusted for chances, has a value close to 0 for random labeling,
independently of the number of clusters and samples, and exactly 1 when the clus-
terings are identical (up to a permutation), however is bounded below by -0.5 for
especially discordant clusterings; (3) the mutual info score (MI) is independent of
the absolute values of the labels (i.e. a permutation of the cluster labels does not
change the value of the score); (4) the normalized mutual information (NMI) is
a normalization of the MI to scale the results between 0 (no mutual information)
and 1 (perfect correlation); (5) cluster accuracy (CA) is based on the Hungarian
algorithm to find the optimal matching between true and predicted cluster labels;
(6) to compute the purity score (PS), each cluster is assigned to the class which is
most frequent in the cluster, and the similarity measure is obtained by counting the
number of correctly assigned papers and dividing by the number of observations.
This latter score increases as the number of clusters increases and for this reason,
it cannot be used as a trade off between the number of clusters and clustering
quality, that is to find the optimal number of clusters.
The measures presented in Table 1 demonstrates that the Doc2Vec approach,
when coupled with K-means clustering on cleaned data, outperforms other models.
This is evident from the higher MI and PS measures it achieves compared to its
competitors. Moreover, Doc2Vec exhibits practical advantages, as it is straightfor-
ward to implement and significantly reduces computing time when compared to
more advanced techniques like BERTopic. Interpreting the results of the Doc2Vec
approach is simple, as it allows for the easy identification of the most representa-
tive documents by retrieving the centroid vectors of each cluster. The Word2Vec
approach, when coupled with K-means clustering on cleaned data, shows also a
good performance.
It is worth noting that there are no significant differences in performance mea-
sures when applying either the Top2Vec or BERTopic methods to raw or cleaned
data. This could be attributed to the fact that raw data contain mathematical for-
mulas that do not contribute substantial additional information, even if, as shown
12",2024-01-03T14:06:06Z,itopic tvec  er yu for additnally t topic alternative ll  itable huaria t table doc vec  odoc vec topic interpi doc vec t word vec it tvec topic 
paper_qf_10.pdf,13,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","RS ARS MI NMI CA PS
K-means 0.570 0.029 0.232 0.136 0.271 0.297
LDA scikit-learn 0.823 0.194 0.608 0.284 0.376 0.460
LDA gensim 0.788 0.085 0.276 0.131 0.275 0.314
Word2Vec K-means 0.832 0.200 0.613 0.283 0.371 0.427
Doc2Vec K-means 0.831 0.220 0.699 0.325 0.388 0.490
Top2Vec raw 0.810 0.195 0.501 0.239 0.365 0.404
Top2Vec cleaned 0.811 0.206 0.530 0.387 0.253 0.416
BERTopic raw 0.826 0.238 0.608 0.289 0.436 0.458
BERTopic cleaned 0.821 0.239 0.574 0.276 0.398 0.429
Table 1: Algorithms performance. We report the rand score (RS), the adjusted rand score
(ARS), the mutual info score (MI), the normalized mutual info score (NIM), the cluster accuracy
(CA), and the purity score (PS).
in Figure 5, raw data have a larger number of words. This finding indicating an
equivalence between raw or cleaned data could be also attributed to the relatively
simple structure commonly found in quantitative finance papers. It is important to
note that these findings may not generalize to papers or books with more intricate
and complex text structures and without formulas.
LDA implemented in scikit-learn has a better performance than the LDA im-
plementation in gensim . The plain K-means does not show satisfactory results,
even if the algorithm can be implement without a great effort.
Finally, as an overall assessment, it is important to highlight that the per-
formance metrics reported in Table 1 do not demonstrate particularly impressive
results. This could partly stem from the wide-ranging nature of each q-fin cate-
gory, encompassing numerous subtopics and arguments. Conversely, some papers
can be classified under multiple categories, as it is not always obvious how to select
a single definitive category for a given work.
4.2 Empirical study
As shown in Section 4.1, the best performing model is Doc2Vec with K-means
clustering applied on cleaned data. This model is considered to have a better
understanding of the topics discussed in the quantitative finance papers analized
in this work. To obtain the desired number of topics we perform again a K-means
clustering analysis. To extract the most representative documents, we retrieve the
centroid vectors of each cluster. These centroids represent the average position
of all document embeddings assigned to a particular cluster. For each cluster
centroid, we find the nearest neighbors among the original Doc2Vec embeddings.
These nearest neighbors are the documents that are closest to the centroid in the
embedding space and can be considered as the main documents of that cluster.
Thus we select for each cluster the 20 most representative documents and we
find a label for the topic on the basis of the documents titles. Note that the
underlying meanings of the topics are subject to human interpretation. However,
also this phase is automated by asking the topic to ChatGPT (GPT-3.5) after
having provided the list of 20 titles (see Ebinezer (2023)).
Given the size of our sample, a reduction down to 30 topics can be considered
a good compromise for a topic analysis. The selected number of topics strikes a
13",2024-01-03T14:06:06Z,word vec doc vec tvec tvec topic topic table algorithms    it t nally table  conversely emical as sedoc vec  to to tse for doc vec tse  note  eb nez er givet
paper_qf_10.pdf,14,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","Figure 6: Topics trend by year across the sample of around 16,000 papers in the q-fin categories.
14",2024-01-03T14:06:06Z, topics
paper_qf_10.pdf,15,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","Figure 7: We show the topics clusters projected on the 2-dimensional space through the standard
t-distributed stochastic neighbor embedding (t-SNE) algorithm. The numbers represent the
topics reported in Table 2.
good balance between ensuring a sufficient quantity of documents for each topic
and maintaining the desired level of granularity. This approach allows us to extract
meaningful insights from the data while avoiding an excessive division of content
that could affect our ability to identify overarching trends and patterns. As shown
in Figure 6, most of the topics have an increasing trend (topic 28 seems to be the
only exception). This is also motivated by the growth in the number of papers
shown in Figure 1. For each topic, the list of 20 titles is the input for the question
we ask to the ChatGPT chatbot. The topics label (i.e. the ChatGPT reply to the
question) together with the title of the most representative paper are reported in
Table 2.
It is interesting to observe that topics related to decentralized finance and
blockchain technology (2) and stock price prediction with deep learning and news
sentiment analysis (20) show a remarkable increase. This is also true for health,
policy, and social impact studies, represented by the topic 16, and diverse per-
spectives in education, innovation, and economic development (0). Both topics
are oriented towards economics. These topics trends depend also by the introduc-
tion in 2014 of the q-fin.EC category within the arXiv quantitative finance papers.
Classical quantitative finance subjects like portfolio optimization techniques and
strategies (6), stochastic volatility modeling and option pricing (7), game theory
and strategic decision-making (19) high-order numerical methods for option pric-
ing in finance (23) as well as, new themes appeared in the literature in the last
years, like deep reinforcement learning in stock trading and portfolio management
(4) and environmental and economic impacts of mobility technologies (25) have
attracted the interest of researchers in the analized period. The representativeness
of topic 28 is limited, mainly because the number of papers in this cluster is low,
15",2024-01-03T14:06:06Z,  t table  as    for  t  table it  both tse  assical t
paper_qf_10.pdf,16,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","and one could merge it with another cluster (i.e. 8).
It is clear that some topics are more related to economics than to finance. This
also depends on the presence of the q-fin.EC category. For articles in this category
there is not always a flawless alignment with quantitative finance.
To visualize the clusters, in Figure 7 the documents vectors are projected in the
2-dimensional space through the standard t-distributed stochastic neighbor embed-
ding (t-SNE) algorithm. The larger the distance between topics, the more distinct
the papers in those topics are in the original high-dimensional space. Conversely,
it is also possible to have a better view on how close are topics each other (e.g. 8
and 28). It appears that the most specialized or narrowly-focused topics tend to
occupy peripheral positions, while themes that are more aligned with economics
are positioned closer to the center.
5 Extracting authors and journals
In this section we extract the author surnames by means of the spacy package. More
in details, starting from the raw data, we perform a named-entities recognition.
Since this approach extract both first names and surnames, we remove all first
names by checking if these names are included in a list of about 67,000 first names.
It should be noted that the number of occurence of the surname of the author
in a paper strongly depends on the citation style. Surnames are always reported
in the references, but they do not necessary appear in the main text of a paper.
Additionally, even if the author-date style is widely used (i.e. the citation in the
text consists of the authors name and year of publication), the surname of the
first author appears more frequently (e.g. Bianchi is more probable than Tassinari,
even if Bianchi and Tassinari are coauthors of the same papers, together with other
coauthors).
The algorithm is able to find the names and surnames occuring in the text.
These are included in the PERSON entity types. The first 100 authors by number
of occurences in the corpus are selected. In order to have additional information on
these authors, we obtain topics, number of citations, h-index and i10-index from
Google Scholar (see Table 3). It should be noted that not all authors are registered
in Google Scholar, even if they made a significant contribution to the field (e.g.
Markowitz) or there are authors with the same surname and belonging to the same
research field (see also Figure 3 in Sharma et al. (2023)). This is the case for some
researchers we find in our corpus (e.g. Zhou, Bayraktar and Chakrabarti). For
these last authors is not simple to find a perfect match in Google Scholar even if
their number of occurrences is generally high.4
The algorithm is also able to find the most cited journals, included in the
ORG entity types: Journal of Finance (4490 occurences)∗, Mathematical Finance
(3785), Journal of Financial Economics (3325)∗, Physica A (3137)∗, Quantitative
Finance (3044), Econometrica (2473)∗, Journal of Econometrics (1971), American
4For the reasons described above we exclude from Table 3 the following researcher: Zhou,
Markowitz, Peng, Jacod, Merton, Guo, Lo, Follmer, Yor, Almgren, Embrechts, Bayraktar,
Artzner, Weber, Jarrow, Feng, Samuelson, Tang, Chakrabarti, Glasserman, Tsallis, Leung, Sato,
Zariphopoulou, Kramkov, Karoui, Cizeau, Cao and Christensen.
16",2024-01-03T14:06:06Z,it  for to  t conversely it extrai i since it surnamadditnally branas idari branas idari t tse t i scholar table it  scholar marwitz  sharma  hou bay rak tar chara bart for  scholar t journal nance matmatical nance journal nancial economics psics quantitative nance econometric journal econometrics for table hou marwitz pe jack mortoguo lo oll mer or alm greem brht bay rak tar art ner ber narrow fe samuelsota chara bart glassmats all is lu sat  ip hope lou ram v taro ui size au  christened
paper_qf_10.pdf,17,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","Economic Review (1878), Insurance Mathematics and Economics (1667), Review
of Financial Studies (2636)∗, Journal of Banking and Finance (1542)∗, Physical
Review (1538), Journal of Economic Dynamics (1289), Energy (1267), Opera-
tions Research (1242), The Quarterly Journal of Economics (1238)∗, Journal of
the American Statistical Association (1204), Management Science (1160), Euro-
pean Journal of Operational Research (1066), Quantum (1043), IEEE Transactions
(996), Journal of Political Economy (990), Journal of Economic Theory (977), En-
ergy Economics (946), International Journal of Theoretical and Applied Finance
(946), SIAM Journal on Financial Mathematics (888), Science (865), Expert Sys-
tems with Applications (845), Applied Mathematical Finance (743), Finance and
Stochastics (736), Mathematics of Operations Research (652), PLoS (602), The
Annals of Applied Probability (593), Stochastic Processes and their Applications
(525), Energy Policy (520), Internationl Journal of Forecasting (520), The Euro-
pean Physical Journal (514), Journal of Empirical Finance (510), Journal of Risk
(509). We consider only journals with more than 500 occurrences and we esclude
publishing houses. The journals with the asterisk symbol are identified with more
than one name. It should be noted that some well-known journals are slightly
below 500 occurences. Furthermore, it is worth noting that papers with a strong
mathematical focus tend to receive significantly fewer citations compared to papers
that lean more towards economics or finance.
It is important to acknowledge that while the arXiv repository serves as a
valuable resource for scholarly papers, it may not encompass the entirety of the
quantitative finance research landscape. While the repository strives to be inclu-
sive and comprehensive, there may be variations in the representation of scholars
from different countries. Some scholars may have a relatively higher presence due
to their active participation in submitting their research to arXiv. The platform
content is reliant on authors voluntarily submitting their work, which introduces
the possibility of selection bias. As a result, some authors and their contributions
may not be represented. Therefore, our analysis and conclusions should be inter-
preted within the context of the available arXiv data, recognizing that there may
be additional research and authors in the field of quantitative finance who have
chosen alternative avenues for publishing their work. The same observation is true
for the findings described in Section 4.2.
It is possible that influential scholars may not be as consistently represented or
that, for various reasons, they have not regularly submitted their work to arXiv (see
also Metelko and Maver (2023)). In the study of Sharma et al. (2023), focused on
option pricing, some well-known authors are cited but they do not appear among
the first 100 authors in our analysis. This discrepancy could be influenced by
factors such as publication preferences, possible copyright issues, or institutional
practices that may vary across different academic communities.
As a final remark, in our view, researchers in quantitative finance should con-
sider submitting their work to arXiv due to the potential benefits it offers (see also
Mishkin et al. (2020)). The delay between arXiv posting and journal publication,
which can sometimes be more than a year, underscores the importance of submit-
ting preprints to the repository. By doing so, researchers can help the community
to understand research trends in their field more promptly, while also accelerating
the dissemination of their own findings. This approach aligns with the findings of
17",2024-01-03T14:06:06Z,economic review insurance matmatics economics review nancial studijournal banki nance psical review journal economic dynamics energy oa researt quarterly journal economics journal statistical associatmanagement science euro journal oatnal researquantum transans journal political economy journal economic tory eeconomics internatnal journal toical applied nance journal nancial matmatics science ext ys applicatns applied matmatical nance nance stochastic matmatics oatns researlo t annals applied probabity stochastic processapplicatns energy policy internatnal journal forecasti t euro psical journal journal emical nance journal risk  t it furtr it  w some  t as trefore  t seit  met  oisharma  as  mis hk it  by 
paper_qf_10.pdf,18,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","Wang et al. (2020), who show that rapid and open dissemination through preprints
helps scholarly and scientific communication, enhancing the reception of findings
in the field. In the meanwhile, as a possible alternative to gain a comprehensive
understanding of the entire landscape, future studies may consider incorporating
other reputable academic databases and journals to ensure a more holistic explo-
ration of quantitative finance research and its authors. Network approaches would
also help to identify cliques and highly connected groups of authors. Insights from
network clusters could further improve the understanding of the textual data in-
vestigated in this work.
6 Conclusions
In this study, we explore the field of quantitative finance through an analysis of
papers in the arXiv repository. Our objectives are twofold: first, we investigate the
evolution of topics over time, second, we identify prominent authors and journals in
this domain. By employing data mining techniques, we achieve these goals without
reading the papers individually.
The preprocessing phase, when needed, ensures the suitability of the data for
subsequent analyses. Topic modeling helps in gaining insights and understanding
the main themes and trends within our large dataset. By applying topic modeling
algorithms, we identify the best performer and examine the temporal evolution of
quantitative finance topics. This analysis reveals the changing research trends and
highlights the emergence and decline of various topics over time.
Furthermore, we conduct an entity extraction process to identify influential
authors and journals in the field. Through quantifying authors and journals oc-
currences, we shed light on the researchers who have made notable contributions
to quantitative finance.
Our study demonstrates the power of data mining techniques in uncovering
insights from a large-scale preprint repository. Our work not only showcases the
power of data mining but also highlights the continued growth and dynamism of
quantitative finance as a discipline. The techniques explored in this work can assist
researchers in exploring and identifying novel research topics, discovering connec-
tions between different research areas, and staying up-to-date with the latest de-
velopments in the field. Furthermore, our methodology may serve as a roadmap for
future studies on broader datasets or in other scientific domains utilizing text min-
ing techniques. Although scaling to a larger number of papers may pose challenges,
our approach provides valuable insights.
Finally, we believe that quantitative finance researchers should consider sharing
their work on arXiv to potentially accelerate the dissemination and impact of their
findings and to enhance the community understanding of research trends.
18",2024-01-03T14:06:06Z,wa inetwork inshts conusns i our by t topic by  furtr through our our t furtr although nally 
paper_qf_10.pdf,19,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","References
A. Ali and H.A. Bashir. Bibliometric study on asset pricing. Qualitative Research
in Financial Markets , 14(3):433–460, 2022.
D. Angelov. Top2vec: Distributed representations of topics. https://arxiv.org/
abs/2008.09470 , 2020.
M. Aria and C. Cuccurullo. bibliometrix: An R-tool for comprehensive science
mapping analysis. Journal of Informetrics , 11(4):959–975, 2017.
M. Bastian, S. Heymann, and M. Jacomy. Gephi: an open source software for
exploring and manipulating networks. In Proceedings of the international AAAI
conference on web and social media , volume 3, pages 361–362, 2009.
M. Bianchetti and M. Carlicchi. Interest rates after the credit crunch: multiple-
curve vanilla derivatives and SABR. https://arxiv.org/abs/1103.2567 , 2012.
M.L. Bianchi, G.L. Tassinari, and F.J. Fabozzi. Fat and heavy tails in asset man-
agement. The Journal of Portfolio Management , 2023.
D. M Blei, A.Y. Ng, and M.I. Jordan. Latent Dirichlet allocation. Journal of
Machine Learning Research , 3:993–1022, 2003.
K. Bohara, A. Shakya, and B. Debb Pande. Fine-tuning of RoBERTa for docu-
ment classification of arXiv dataset. In G. Shakya, S.and Papakostas and K.A.
Kamel, editors, Mobile Computing and Sustainable Informatics , pages 243–255,
Singapore, 2023. Springer Nature Singapore.
B. Burton, S. Kumar, and N. Pandey. Twenty-five years of The European Journal
of Finance (EJF): A retrospective analysis. The European Journal of Finance ,
26(18):1817–1841, 2020.
R. Carmona. The influence of economic research on financial mathematics: Evi-
dence from the last 25 years. Finance and Stochastics , 26(1):85–101, 2022.
M. Cesa. A brief history of quantitative finance. Probability, Uncertainty and
Quantitative Risk , 2(1):1–16, 2017.
C.B. Clement, M. Bierbaum, K.P. O’Keeffe, and A.A. Alemi. On the use of arXiv
as a dataset. https://arxiv.org/abs/1905.00075 , 2019.
E. Derman. Models behaving badly: Why confusing illusion with reality can lead to
disaster, on Wall Street and in life . Wiley, 2011.
E. Derman and M.B. Miller. The volatility smile . Wiley, 2016.
N. Donthu, S. Kumar, D. Mukherjee, N. Pandey, and W.M. Lim. How to conduct a
bibliometric analysis: An overview and guidelines. Journal of Business Research ,
133:285–296, 2021.
W.H. DuBay. The principles of readability. ERIC , 2004.
19",2024-01-03T14:06:06Z,referencali baseip bib l metric qualitative researnancial markets aelo tdistributed aria ucc rul lo ajournal ifor metrics bastieleymajack my ge phi iproceedis bi ti carl chi interest branas idari fab oz fat t journal tfmanagement ble  jordalatent di ri journal machine learni researoara shaky  bb pa ne ro ta  ishaky papa postal camel mobe uti just articial intellence table informatics siae  nature siae burtokumar hand tnty t ajournal nance t ajournal nance car mona t evi nance stochastic cprobabity unr articial intellence ny quantitative risk ement tier aum fefefe alex o r mamols w wall street rey r mamler t rey dothu kumar uk r ee hand lim how ajournal business researdu bay t
paper_qf_10.pdf,20,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","S. Ebinezer. Transform your topic modeling with ChatGPT: Cutting-edge NLP.
https://medium.com/ , 2023.
S. Eger, C. Li, F. Netzer, and I. Gurevych. Predicting research trends from arXiv.
https://arxiv.org/abs/1903.02831 , 2019.
R. Egger and J. Yu. A topic modeling comparison between LDA, NMF, Top2Vec,
and BERTopic to demystify Twitter posts. Frontiers in sociology , 7, 2022.
R. Fatima, A. Yasin, L. Liu, J. Wang, and W. Afzal. Retrieving arXiv, SocArXiv,
and SSRN metadata for initial review screening. Information and Software Tech-
nology , 161:107251, 2023. ISSN 0950-5849.
M. Grootendorst. BERTopic: Neural topic modeling with a class-based TF-IDF
procedure. https://arxiv.org/abs/2203.05794 , 2022.
J. Huisman and J. Smits. Duration and quality of the peer review process: the
author’s perspective. Scientometrics , 113(1):633–650, 2017.
E. Ippoliti. Mathematics and finance: Some philosophical remarks. Topoi , 40:
771–781, 2021.
Q. Le and T. Mikolov. Distributed representations of sentences and documents. In
International conference on machine learning , pages 1188–1196. PMLR, 2014.
J. Lin, Y. Yu, Y. Zhou, Z. Zhou, and X. Shi. How many preprints have actually
been printed and why: a case study of computer science preprints on arXiv.
Scientometrics , 124(1):555–574, 2020.
Z. Metelko and J. Maver. Exploring arXiv usage habits among Slovenian scientists.
Journal of Documentation , 79(7):72–94, 2023.
T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word
representations in vector space. https://arxiv.org/abs/1301.3781 , 2013.
D. Mishkin, A. Tabb, and J. Matas. ArXiving before submission helps everyone.
https://arxiv.org/abs/2010.05365 , 2020.
K. Okamura. Scientometric engineering: Exploring citation dynamics via arXiv
eprints. Quantitative Science Studies , 3(1):122–146, 2022.
M. R¨ udiger, D. Antons, A. M Joshi, and T.O. Salge. Topic modeling revisited:
New evidence on algorithm performance and quality metrics. Plos one , 17(4):
e0266325, 2022.
K. Sethia, M. Saxena, M. Goyal, and R.K. Yadav. Framework for topic modeling
using BERT, LDA and K-Means. In 2022 2nd International Conference on
Advance Computing and Innovative Technologies in Engineering (ICACITE) ,
pages 2204–2208. IEEE, 2022.
P. Sharma, D.K. Sharma, and P. Gupta. Review of research on option pricing: A
bibliometric analysis. Qualitative Research in Financial Markets , 2023.
20",2024-01-03T14:06:06Z,eb nez er transform  cui eli note gu re vy predii   er yu tvec topic twier frontiers tima basi wa fatal rievi  soc ar  informatsoftware teroot end or st topic neural hu is masmith duratsci eto metrics ip polite matmatics some to poi le mike lov distributed iinternatnal liyu hou hou shi how  sci eto metrics met  oelori  slovenia journal documentatmike lov ornado aefcient mis hk itab mat as ar  i oka  sci eto metric elori  quantitative science studiantojoshi lge topic new plo seth xeno goa adam framework means iinternatnal conference advance uti innovative technologieineeri sharma sharma gupta review qualitative researnancial markets
paper_qf_10.pdf,21,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","K. Tan, A. Munster, and A. Mackenzie. Images of the arXiv: Reconfiguring large
scientific image datasets. Journal of Cultural Analytics , 3:1–41, 2021.
S. Terragni, E. Fersini, B.G. Galuzzi, P. Tropeano, and A. Candelieri. OCTIS:
comparing and optimizing topic models is simple! In Proceedings of the 16th
Conference of the European Chapter of the Association for Computational Lin-
guistics: System Demonstrations , pages 263–270, 2021.
N. Van Eck and L. Waltman. Software survey: VOSviewer, a computer program
for bibliometric mapping. Scientometrics , 84(2):523–538, 2010.
N.T. Viet and A.G. Kravets. Analyzing recent research trends of computer science
from academic open-access digital library. In 2019 8th International Conference
System Modeling and Advancement in Research Trends (SMART) , pages 31–36,
2019.
M. Vogl. Quantitative modelling frontiers: a literature review on the evolution in
financial and risk modelling after the financial crisis (2008–2019). SN Business
& Economics , 2(12):183, 2022.
Z. Wang, Y. Chen, and W. Gl¨ anzel. Preprints as accelerator of scholarly commu-
nication: An empirical analysis in Mathematics. Journal of Informetrics , 14(4):
101097, 2020.
D. Westergaard, H.H. Stærfeldt, C. Tønsberg, L.J. Jensen, and S. Brunak. A
comprehensive and quantitative comparison of text-mining in 15 million full-
text articles versus their corresponding abstracts. PLoS Computational Biology ,
14(2), 2018.
21",2024-01-03T14:06:06Z,tamunster mackenzie imag re conuri journal cultural analytics terra gni fer a luz zi trope no cali eri iproceedis conference achapter associattnal litem monstratns vaeck artmasoftware vier sci eto metrics viet kr vets analyzi iinternatnal conference tem moli advancement researtrends vo gl quantitative business economics wa cgl pre prints amatmatics journal ifor metrics sterga ard st jensebruno lo tnal blogy
paper_qf_10.pdf,22,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","number label title of the most representative paper
0 diverse perspectives in education, innovation, and economic development Perspectives in public and university sector co-operation in the change of higher education model in Hungary, in light of China’s experience
1 modeling financial market dynamics Comment on: Thermal model for adaptive competition in a market
2 decentralized finance and blockchain technology Understanding the maker protocol
3 correlation analysis in financial markets and networks Random matrix theory and cross-correlations in global financial indices and local stock market indices
4 deep reinforcement learning in stock trading and portfolio management Practical deep reinforcement learning approach for stock trading optimal market making by reinforcement learning
5 optimal trading and portfolio liquidation strategies in financial markets An FBSDE approach to market impact games with stochastic parameters
6 portfolio optimization techniques and strategies Seven sins in portfolio optimization
7 stochastic volatility modeling and option pricing On the uniqueness of classical solutions of Cauchy problems
8 asset pricing, investment, and arbitrage in financial markets Characterization of arbitrage-free markets
9 network analysis of financial contagion and systemic risk Clearing algorithms and network centrality
10 counterparty risk and valuation adjustments in financial derivatives Collateral margining in arbitrage-free counterparty valuation adjustment including re-hypotecation and netting
11 quantum models in finance and option pricing Sornette-Ide model for markets: Trader expectations as imaginary part
12 valuation and risk management in annuity and insurance products A policyholder’s utility indifference valuation model for the guaranteed annuity option
13 optimal dividend strategies in stochastic control and risk management Optimal dividends problem with a terminal value for spectrally positive Levy processes
14 risk measures and utility maximization under model uncertainty On the C-property and w∗-representations of risk measures
15 economic complexity, networks, and trade patterns Economic complexity and growth: Can value-added exports better explain the link?
16 health, policy, and social impact studies Ramadan and infants health outcomes
17 statistical analysis of financial markets and volatility Volatility distribution in the S&P500 Stock Index
18 renewable energy economics and electricity market dynamics On wholesale electricity prices and market values in a carbon-neutral energy system
19 game theory and strategic decision-making Simultaneous auctions for complementary goods
20 stock price prediction with deep learning and news sentiment analysis Stock Prediction: a method based on extraction of news features and recurrent neural networks
21 kinetic wealth exchange models in economics Gibbs versus non-Gibbs distributions in money dynamics
22 market order flow and price impact Order flow and price formation
23 high-order numerical methods for option pricing in finance High-order compact finite difference scheme for option pricing in stochastic volatility with contemporaneous jump models
24 optimal investment and consumption in financial models with constraints Recursive utility optimization with concave coefficients
25 environmental and economic impacts of mobility technologies A review on energy, environmental, and sustainability implications of connected and automated vehicles
26 advanced risk measures in financial modeling Generating unfavourable VaR scenarios with patchwork copulas
27 Bayesian models for financial tail risk forecasting A semi-parametric realized joint value-at-risk and expected shortfall regression framework
28 pricing and modeling options in stochastic volatility models with jumps Semi-analytical pricing of barrier options in the time-dependent Heston model
29 economic growth and market dynamics Uncovering volatility dynamics in daily REIT returns
Table 2: For each topic we report the label extracted from ChatGPT and the title of the most representative paper.
22",2024-01-03T14:06:06Z,speivhuary  coent trmal unrstandi random praical aseveocatc charaerizateari collatl corvee i trar optimal levy oeconomic caarmada volatity stock inx osimultaneous stock predigibbs gibbs orr hh recursive genti va bayesiasemi stouncoveri table for 
paper_qf_10.pdf,23,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","author id occurences name topics citations h-index i10-index
mG07 6k 4505.0 Ioannis Karatzas [’stochastic analysis’, ’stochastic control’, ’mathematical finance’] 35724 60 127
58amEmw 4441.0 Jean-Philippe Bouchaud [’statistical mechanics’, ’disordered systems’, ’random matrices’, ’quantitative finance’, ’agent based models’] 49794 105 351
ahLm1v0 3256.0 Walter Schachermayer [] 15891 56 124
HGsSmMA 3135.0 Didier Sornette [’cooperation’, ’organization’, ’patterns’, ’prediction’] 53371 112 587
CqFCQVE 2794.0 Alexander Schied [’probability theory’, ’stochastic processes’, ’mathematical finance’] 16780 35 71
2QOp9 M 2457.0 Peter Carr [’financial engineering’, ’quantitative finance’, ’mathematical finance’, ’derivatives’, ’volatility’] 24060 62 117
mVF1X U 2371.0 Freddy Delbaen [’mathematik’, ’¨ okonomie’] 25557 50 90
ElAtiUs 2258.0 Darrell Duffie [’finance’, ’central banking’] 58442 88 142
fq7BQos 2042.0 Dilip B. Madan [’mathematical finance’, ’general equilibrium theory’] 25131 55 156
8abFiFM 1774.0 Robert Engle [’finance and econometrics’] 189678 118 229
GU9HgNA 1743.0 Quanquan Gu [’statistical machine learning’, ’nonconvex optimization’, ’deep learning theory’, ’reinforcement learning’, ’ai for science’] 14915 56 161
Q7N-rCk 1672.0 Rosario Nunzio Mantegna [’econophysics’, ’statistical physics’, ’complex systems’, ’financial markets’, ’information filtering’] 28269 67 139
QsYYhSE 1629.0 Søren Johansen [’matamatical statistics’, ’econometrics’] 97728 65 132
vZA2pjw 1627.0 Benoˆ ıt B. Mandelbrot [’mathematics’, ’fractals’, ’economics’, ’information theory’, ’fluid dynamics’] 142895 96 319
Lf1kf1Q 1505.0 Mario Coccia [’evolution of technology’, ’scientific change’, ’social dynamics’, ’complex adaptive systems’, ’environment & covid-19’] 19376 106 228
9HXRjPk 1444.0 Damir Filipovic [’quantitative finance’, ’quantitative risk management’] 6910 39 73
6INHZI 1389.0 Fabrizio Lillo [’quantitative finance’, ’statistical mechanics’, ’data science’] 10900 51 121
mGpnlA8 1218.0 Touzi Nizar [’stochastic control’, ’mathematical finance’, ’monte carlo methods’] 11831 56 120
rp-3Yoo 1187.0 Barry Williams [’banks and banking’, ’bank risk’, ’multinational’, ’banking’] 2342 17 23
MZNxzRY 1161.0 Huyˆ en Pham [’mathematical finance’, ’stochastic control’, ’numerical probabilities’] 9967 54 135
3HhvEUc 1147.0 Yuri Kabanov [’mathematical finance’, ’mathematics’] 6297 38 77
-YEPo1E 1143.0 Wing-Keung Wong [’financial economics’, ’econometrics’, ’investment theory’, ’risk management’, ’operational research’] 14440 65 274
GyPrRgc 1138.0 Swarn Chatterjee [’financial planning’, ’wealth management’, ’financial literacy’, ’household finance’, ’behavioral finance’] 2719 28 54
RZid9X8 1075.0 Guido Caldarelli [’network theory’, ’network science’, ’statistical physics’, ’complex systems’] 24165 71 191
ImhakoA 1075.0 Daniel Kahneman [] 519507 158 369
zOtShM 1050.0 Marek Rutkowski [’mathematical finance’, ’stochastic processes’] 7559 30 67
7NJ7Ax8 1039.0 Patrick Cheridito [] 5400 34 59
nyfza90 1019.0 Volker Schmidt [’virtual materials testing’, ’statistical learning’, ’image analysis’, ’spatial stochastic modeling’, ’monte carlo simulation’] 11896 52 230
x4vtSxI 1017.0 Rene Carmona [’stochastic analysis’, ’financial mathematics’, ’financial engineering’] 17474 59 139
kukA0Lc 999.0 Yoshua Bengio [’machine learning’, ’deep learning’, ’artificial intelligence’] 656874 222 763
vQ0 nz8 989.0 Emmanuel Bacry [’self-similarity’, ’multifractal’, ’stochastic modeling’, ’statistical finance’, ’financial time-series modelization’] 11937 47 69
1XwLUrc 980.0 Jim Gatheral [’volatility modeling’, ’market microstructure’, ’algorithmic trading’] 6126 30 42
3HwRbiQ 955.0 Jerome Friedman [] 283058 95 197
e2Xowj0 900.0 Neil Shephard [’econometrics’, ’economics’, ’statistics’, ’financial econometrics’, ’finance’] 42035 69 140
pEnxwCM 887.0 Victor M. Yakovenko [’condensed matter theory’, ’econophysics’] 8891 44 101
a11vssU 845.0 Constantinos Kardaras [’stochastic analysis’, ’probability’, ’mathematical finance’] 1557 20 31
79htA7g 838.0 Bent Flyvbjerg [’project management’, ’management’, ’infrastructure’, ’planning’, ’cities’] 73264 70 152
zH1qBSo 834.0 Albert Shiryaev [’probability theory’] 35521 59 163
QVb4LGI 815.0 Andrey Itkin [’mathematical finance’, ’computational finance’, ’derivatives’, ’quantitative finance’, ’machine learning’] 709 14 19
Zuhod6s 813.0 Yong Deng [’uncertainty’, ’deng entropy’, ’information volume’, ’random permutation set’, ’chaos and fractal’] 23189 81 335
bWlZ3-Y 810.0 Eric Jacquier [] 4458 19 25
GKthQJQ 804.0 Peter K. Friz [’rough path theory’, ’stochastic analysis’, ’pdes’, ’finance’] 5678 39 82
2qTa 4U 794.0 Francis Diebold [’economics’, ’econometrics’, ’time series’, ’statistics’] 76159 97 175
utY1nTo 794.0 Matteo Marsili [’statistical mechanics’, ’stochastic processes’, ’collective phenomena in socio-economic systems’, ’networks’, ’complex systems’] 10093 50 139
ZpG cJw 783.0 Robert Tibshirani [’statistics’, ’data science’, ’machine learning’] 460493 172 525
65wdZxA 780.0 Damiano Brigo [’probability’, ’mathematical finance’, ’stochastic analysis’, ’signal processing’, ’differential geometry and statistics’] 9663 42 114
bxJe87s 780.0 Marco Frittelli [’financial mathematics’, ’mathematical finance’, ’probability’] 3795 24 33
aVju7cI 771.0 Monique Jeanblanc [’math´ ematiques financi` eres’] 10256 51 110
-iOn6uI 769.0 Aur´ elien Alfonsi [] 2731 22 30
PLECrk 750.0 Tomasz R. Bielecki [’mathematical finance’, ’stochastic processes’, ’stochastic control’, ’stochastic analysis’, ’probability’] 6901 39 89
5sQ0Fag 729.0 Ajit Singh [] 21592 60 360
6quAJUE 706.0 Josef Teichmann [’mathematical finance’, ’machine learning in finance’, ’rough analysis’] 3019 30 67
58amEmw 705.0 Jean-Philippe Bouchaud [’statistical mechanics’, ’disordered systems’, ’random matrices’, ’quantitative finance’, ’agent based models’] 49794 105 351
JicYPdA 691.0 Geoffrey Hinton [’machine learning’, ’psychology’, ’artificial intelligence’, ’cognitive science’, ’computer science’] 687453 180 436
i2MC67A 679.0 J.F. Muzy [’multifractal analysis’, ’econophysics’, ’turbulence’] 13417 55 83
K9yGky8 678.0 Andreas Kyprianou [’probability theory’, ’applied mathematics’] 7946 44 99
aCSds20 670.0 Xavier Gabaix [’economics’, ’finance’] 30926 56 75
23",2024-01-03T14:06:06Z,loais kara tz as mw  phippe  chalm walter scr mayer gs sm rir corvee cq alexanr sc peter carr reddy l beeel at us barrel duff ie qo di lip madma robert eagle hg quaquagu ck rosar juantenna qs sobe no manlbrot lf mar coarj pk amir lipovic fabriz lle gp nl to ui nica yoo barry wliams nx buy ham hv uc yuri kabanov po wi kyu wo gy pr rg warterjee zi gui cal dare lli im  daniel kane maot sh mark rut   patrick cr di to oschmidt sx rene car mona lc yo sha be   eanuel ba y url jim gatr al rbi jerome friedmaxo ne sprd evior v e constant os karla ras bent fly vb erg so albert shi rya ev vb andrew it kizu had yo e eric jac er th peter fri ta francis die bold to maer mars li zp robert tib shi rani zx amino br je marco fr it tell ju ronique  blanc oaur alo rk tomas bl eck fag it sih josef eichmanmw  phippe  chaji pd geoffrey htomu  ky s ky pri nou sds xier ga articial intellence
paper_qf_10.pdf,24,Text mining arXiv: a look through quantitative finance papers,"  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
","author id occurences name topics citations h-index i10-index
G-WPCrM 667.0 Diego Garlaschelli [’network theory’, ’econophysics’, ’sociophysics’, ’statistical physics’] 7394 41 73
YTCnA4E 664.0 Eduardo Schwartz [’finance’] 44652 81 141
A0ISJPU 664.0 Steven Shreve [’probability’, ’financial mathematics’] 35605 42 70
fFFOHec 660.0 Alexander McNeil [] 24473 41 60
dYwbc9s 659.0 Guido Imbens [’causal inference’, ’econometrics’] 90765 95 169
OQK4DDY 657.0 Peter Forsyth [’scientific computing’, ’computational finance’, ’numerical solution of pdes’] 10617 58 143
c1wQ9 k 655.0 Daojian Zeng [’natural language processing’] 4877 13 17
Vs7kOf4 645.0 Marcel Nutz [’optimal transport’, ’mathematical finance’, ’game theory’] 2339 30 43
vjc1kF0 640.0 Francesca Biagini [’financial and insurance mathematics’, ’stochastic calculus’, ’probability’] 2598 20 42
zGJKZpk 629.0 Marianne Bertrand [] 64693 66 119
nEfnJZM 628.0 Vadim Linetsky [’mathematical finance’, ’financial economics’] 5260 39 63
Bekg2Qo 621.0 Joel Shapiro [’financial intermediation’, ’regulation of financial institutions’, ’corporate governance’, ’industrial organization’] 3449 15 16
KDhGvNQ 611.0 Johanna Ziegel [’statistical forecasting’, ’risk measures’, ’postitive definite functions’, ’stereology’, ’copulas’] 2088 19 31
r5PHkCs 610.0 Thomas Guhr [’theoretical physics’] 8395 39 104
Table 3: Number of authors occurrences and Google Scholar metrics as of May 2023.
24",2024-01-03T14:06:06Z, diego gar las sll  eduardo schwartz stevehr eve  alexanr mc ne wbc gui im bepeter forth dao jae vs of marcel nut france bia izp marine bertrand ef rad linet be kg qo joel shapi dh ny zi gel hk cs thomas gu hr table number  scholar may
paper_qf_11.pdf,1,"Statistical properties of stock order books: empirical results and
  models","  We investigate several statistical properties of the order book of three
liquid stocks of the Paris Bourse. The results are to a large degree
independent of the stock studied. The most interesting features concern (i) the
statistics of incoming limit order prices, which follows a power-law around the
current price with a diverging mean; and (ii) the humped shape of the average
order book, which can be quantitatively reproduced using a `zero intelligence'
numerical model, and qualitatively predicted using a simple approximation.
","arXiv:cond-mat/0203511v2  18 Jun 2002Statistical properties of stock order
books: empirical results and models
Jean-Philippe Bouchaud†,∗, Marc M´ ezard∗∗,∗, Marc Potters∗
February 6, 2008
†Commissariat ` a l’Energie Atomique, Orme des Merisiers
91191 Gif-sur-Yvette cedex , France
∗Science & Finance, CFM, 109-111 rue Victor Hugo
92 353 Levallois cedex , France
∗∗Laboratoire de Physique Th´ eorique et Mod` eles Statistiqu es
Universit´ e Paris Sud, Bat. 100, 91 405 Orsay cedex , France
February 6, 2008
Abstract
We investigate several statistical properties of the order book of three
liquid stocks of the Paris Bourse. The results are to a large d egree inde-
pendent of the stock studied. The most interesting features concern (i)
the statistics of incoming limit order prices, which follow s a power-law
around the current price with a diverging mean; and (ii) the s hape of the
average order book, which can be quantitatively reproduced using a ‘zero
intelligence’ numerical model, and qualitatively predict ed using a simple
approximation.
Financial markets oﬀer an amazing source of detailed data on the collective
behaviour of interacting agents. It is possible to ﬁnd many r eproducible patterns
and even to perform experiments, which bring this atypical s ubject into the realm
of experimental science. The situation is simple and well de ﬁned, since many
agents, with all the same goal, trade the very same asset. As s uch, the statistical
analysis of ﬁnancial markets also oﬀers an interesting test ing ground not only for
economic theories, but also for more ambitious theories of h uman activities. One
may indeed wonder to what extent it is necessary to invoke hum an intelligence
orrationality to explain the various universal statistical laws which hav e been
recently unveiled by the systematic analysis of very large d ata sets.
Many statistical properties of ﬁnancial markets have alrea dy been explored,
and have revealed striking similarities between very diﬀer ent markets (diﬀerent
1",2002-03-25T12:53:09Z, justatistical  phippe  chamarc marc ters ruary coissariat energie atom que or me merit tier g veed france science nance vior hugo levallois france labor to ire psique th mod status qu  paris sud bat say france ruary abstra  paris course t t nancial it t as one many
paper_qf_11.pdf,2,"Statistical properties of stock order books: empirical results and
  models","  We investigate several statistical properties of the order book of three
liquid stocks of the Paris Bourse. The results are to a large degree
independent of the stock studied. The most interesting features concern (i) the
statistics of incoming limit order prices, which follows a power-law around the
current price with a diverging mean; and (ii) the humped shape of the average
order book, which can be quantitatively reproduced using a `zero intelligence'
numerical model, and qualitatively predicted using a simple approximation.
","traded assets, diﬀerent geographical zones, diﬀerent epoc hs) [1, 3, 2]. For exam-
ple, the distribution of price changes exhibits a power-law tail with an apparently
universal exponent [4, 5, 6, 8]. The volatility of most asset s shows random vari-
ations in time, with a correlation function which decays as a small power of the
time lag, again quite universal across diﬀerent markets [7] .
Here, we study the statistics of the order book, which is the u ltimate ‘mi-
croscopic’ level of description of ﬁnancial markets. The or der book is the list of
all buy and sell ‘limit’ orders, with their corresponding pr ice and volume, at a
given instant of time. A limit order speciﬁes the maximum (re sp. minimum)
price at which an investor is willing to buy (resp. sell) a cer tain number of shares
(volume). At a given instant of time, all limit buy orders are below the best buy
order called the bid price , while all sell orders are above the best sell order called
theask price . When a new order appears (say a buy order), it either adds to
the book if it is below the ask price, or generates a trade at th e ask if it is above
(or equal to) the ask price (we call all these ‘market orders’ even if technically
they could also be ‘marketable limit orders’). The price dyn amics is therefore
the result of the interplay between the order book and the ord er ﬂow. The study
of the order book is very interesting both for academic and pr actical reasons. It
provides intimate information on the processes of trading a nd price formation,
and reveals a highly non trivial structure of the agents’ exp ectations (see below):
as such, it is of importance to test some basic notions of econ omics and models
of market microstructure. The practical motivations are al so clear. Regulatory
bodies want to set the rules of exchange markets such as to pro duce fair, orderly
trading that maximize ﬂow. For market participants, issues such as the market
impact or the relative merit of limit versus market orders ar e determined by the
structure and dynamics of the order book.
Complete data on the order book of certain markets, such as th e Paris Bourse
(now Euronext), is now available, and contains a particular ly abundant infor-
mation. Allorders (representing hundreds of thousands orders per mont h on
liquid stocks) on allstocks are stored, which makes possible the reconstitution of
the full order book at any instant of time, including orders t hat are not directly
observable on traders’ screens.
Many questions can be studied; the systematic investigatio n of these data sets
is only very recent [9, 10, 11] and has motivated a number of in teresting theo-
retical work [12, 13, 10, 14, 15, 16]. Here, we mostly focus on ‘static’ properties
of the order book, such as the distribution of incoming limit orders, the average
shape of the order book in the moving reference frame of the pr ice, or the distri-
bution of volume at the bid/ask. Many other ‘dynamical’ prop erties can also be
analyzed, such as the response of the price to order ﬂow, the f ull temporal corre-
lation of the book, etc. In order to keep our study reasonably focused, we leave
these dynamical aspects, that we have not fully understood y et, for a subsequent
publication.
Our main results are as follows: (a) the price at which new lim it orders are
2",2002-03-25T12:53:09Z,for t re t at  t it t regulatory for e paris course euronext all orrs many re many iour
paper_qf_11.pdf,3,"Statistical properties of stock order books: empirical results and
  models","  We investigate several statistical properties of the order book of three
liquid stocks of the Paris Bourse. The results are to a large degree
independent of the stock studied. The most interesting features concern (i) the
statistics of incoming limit order prices, which follows a power-law around the
current price with a diverging mean; and (ii) the humped shape of the average
order book, which can be quantitatively reproduced using a `zero intelligence'
numerical model, and qualitatively predicted using a simple approximation.
","placed is, somewhat surprisingly, very broadly (power-law ) distributed around
the current bid/ask; (b) the average order book has a maximum away from the
current bid/ask, and a tail reﬂecting the statistics of the i ncoming orders; (c)
the distribution of volume at the bid (or ask) follows a Gamma distribution.
Notably, we ﬁnd exactly the same statistical features, for t he three liquid stocks
studied. We then study numerically a ‘zero intelligence’ mo del of order book
which reproduces most of these empirical results. Finally, we show how the
characteristic shape of the average order book can be analyt ically predicted,
using a simple approximation.
The data provided by Paris Bourse gives the history of all tra nsactions, with
their price, volume and time stamp, of all quotes (bid and ask prices) with the cor-
responding volumes, and of all orders, with their price, vol ume and time stamp.
This information allows in principle to reconstruct the who le order book at any
instant of time. However, some limit orders (roughly 10% of t hem) are modiﬁed
or cancelled before being executed; unfortunately the data base only contains
this qualitative information, but not the time at which a giv en order is modi-
ﬁed/cancelled, nor the resulting new price. When reconstru cting the order book
at a given instant of time, we have therefore made two extreme assumptions,
which lead to nearly identical conclusions. Either we disca rd these orders alto-
gether, or we keep them until the time where we can be sure that they have
been previously modiﬁed/cancelled, otherwise they would h ave been executed
since the transaction price was observed to be below (for buy orders) or above
(for sell orders) the corresponding limit price. The result s given below uses the
second procedure. We had available the data corresponding t o all stocks during
February 2001, from which we have extracted three of the most liquid stocks:
France-Telecom (F.T.), Vivendi and Total. Some basic infor mation is summa-
rized in Table I for France-Telecom and Total. We expect that our ﬁndings will
not depend on the particular month considered, but that some diﬀerences may
appear when one studies stocks with smaller capitalisation (for example the av-
erage bid-ask spread is much larger): these questions will b e investigated in the
near future. Let us call a(t) the ask price at time tandb(t) the level of bid
price at time t. The midpoint m(t) is the average between the bid and the ask:
m(t) = [a(t) +b(t)]/2. The smallest possible change of these quantities, called
the ‘tick’, was 0 .05 Euros for France-Telecom and Vivendi, and 0 .10 Euros for
Total.
We denote by b(t)−∆ the price of a new buy limit order, and a(t) + ∆ the
price of a new sell limit order. Notice that ∆ can be negative ( this is the only
case where the spread g(t) =a(t)−b(t) can be narrowed), but is always larger
than−g(t) (otherwise it would be a market order). A ﬁrst interesting q uestion
concerns the distribution density of ∆, i.e. the distance be tween the current price
and the incoming limit order. We ﬁnd that P(∆) is identical for buy and sell
orders (up to statistical ﬂuctuations); the shape of P(∆) is found to be very well
3",2002-03-25T12:53:09Z,gaa notably  nally t paris course  itr t  ruary france telecom vivendi total some table france telecom total   t t euros france telecom vivendi euros total  notice 
paper_qf_11.pdf,4,"Statistical properties of stock order books: empirical results and
  models","  We investigate several statistical properties of the order book of three
liquid stocks of the Paris Bourse. The results are to a large degree
independent of the stock studied. The most interesting features concern (i) the
statistics of incoming limit order prices, which follows a power-law around the
current price with a diverging mean; and (ii) the humped shape of the average
order book, which can be quantitatively reproduced using a `zero intelligence'
numerical model, and qualitatively predicted using a simple approximation.
","Quantity F.-T. Total
Initial/ﬁnal price (Euros) 90-65 157-157
Tick size (Euros) 0.05 0.10
Total # orders 270,000 94,350
# market orders 28,600 9,300
# trades 176,000 60,000
Transaction volume 75,6 10623,4 106
Average bid-ask (ticks) 2.0 1.4
Average volume at bid/ask 2700 2400
Table 1: Some useful data for the studied stocks (February 2001). The transaction
volume is in number of shares. The ﬁgures for Vivendi are near ly identical to those for
France-Telecom.
ﬁtted (see Fig. 1) by a power-law:
P(∆)∝∆µ
0
(∆1+ ∆)1+µ,∆≥1 (1)
with an exponent µ≈0.6 for all three stocks. This power-law extends from 1 tick
to over 100 ticks (sometimes even 1000 ticks), correspondin g to a relative change
of price of 5% to 50% [17]. There are also orders placed at the b id (or ask) or
within the spread. One ﬁnds that P(∆ = −1)∼P(∆ = 0) ∼P(∆ = 1), and
these orders add up to roughly half of the total number of orde rs. The distribution
of orders Eq. (1) has a maximum around the current price (this was already noted
in [9]). But the fact that µ <1 means that the average ∆ is formally inﬁnite
(although of course the distribution is ultimately cut-oﬀ f or large ∆’s).
It is quite surprising to observe such a broad distribution o f limit order prices,
which tells us that the opinion of market participants about the price of the
stock in a near future could be anything from its present valu e to 50% above
or below this value, with all intermediate possibilities. T his means that market
participants believe that large jumps in the price of stocks are always possible,
and place orders very far from the current price in order to ta ke advantage of
these large potential ﬂuctuations. (Note that placing a lim it order is free of
charge.) A naive argument would then suggest that the probab ility to place an
order at distance ∆ should be proportional to the probabilit y that the price moves
more than ∆ in order to meet the order. Since the tail of the dis tribution of price
increments is a power-law with an exponent µδp≈3 [4], this would indeed lead to
a power-law for P(∆), but with a value µ=µδp−1≈2 larger than the observed
one. This however does not take into account the fact that mar ket participants
have diﬀerent time horizons – large ∆’s presumably correspo nd to more patient
investors. The value of µshould result from an interplay between the perceived
distribution of future price changes and the distribution o f time horizons, which
4",2002-03-25T12:53:09Z,quantity total initial euros tick euros total transage ge table some ruary t t vivendi france telecom   tre one t eq but it note since  t
paper_qf_11.pdf,5,"Statistical properties of stock order books: empirical results and
  models","  We investigate several statistical properties of the order book of three
liquid stocks of the Paris Bourse. The results are to a large degree
independent of the stock studied. The most interesting features concern (i) the
statistics of incoming limit order prices, which follows a power-law around the
current price with a diverging mean; and (ii) the humped shape of the average
order book, which can be quantitatively reproduced using a `zero intelligence'
numerical model, and qualitatively predicted using a simple approximation.
","1 10 100
∆+∆10110010000Ρ(∆) Total
 Vivendi
 F.T.
 Power−law, µ=0.6
Figure 1: Number of incoming orders arriving at a distance ∆ from the cu rrent best
price, as a function of ∆ 1+ ∆ (in ticks), in log-log coordinates. We chose ∆ 1= 1
for F.T., 0 .5 for Vivendi and 0 for Total. We also found P(∆ = −1)∼P(∆ = 0) ∼
P(∆ = 1) (not shown). The symbols correspond to buy orders, but sell orders show
an identical distribution. The straight lines correspond t oµ= 0.6. Note that the
power-law crosses over to a faster decay beyond ∆ ≈100.
5",2002-03-25T12:53:09Z,total vivendi   number  vivendi total  t t note
paper_qf_11.pdf,6,"Statistical properties of stock order books: empirical results and
  models","  We investigate several statistical properties of the order book of three
liquid stocks of the Paris Bourse. The results are to a large degree
independent of the stock studied. The most interesting features concern (i) the
statistics of incoming limit order prices, which follows a power-law around the
current price with a diverging mean; and (ii) the humped shape of the average
order book, which can be quantitatively reproduced using a `zero intelligence'
numerical model, and qualitatively predicted using a simple approximation.
","could well be itself a power-law. We feel that more theoretic al work is needed to
fully account for this striking result.
Limit orders strongly vary in volume. We ﬁnd that the uncondi tional limit
order size, φ, is distributed uniformly in log-size, between 10 and 50 ,000 (both
for buy or sell orders). One can study the correlation betwee n the incoming
volume φ(number of shares for a given order) and the distance ∆ betwee n the
order and the current price. We ﬁnd that the conditional aver aged volume ∝an}bracketle{tφ∝an}bracketri}ht|∆
is roughly independent of ∆ between 1 and 20 ticks, but decays as a power law
∆−ν, with ν∼1.5 beyond ∆∗(with ∆∗≈20 ticks for FT, and ≈50 ticks for
the other two stocks). Not unexpectedly, extremely far limi t orders tend to be of
smaller volumes. Since the average volume arriving at ∆ is eq ual to the number
of orders at ∆ multiplied by ∝an}bracketle{tφ∝an}bracketri}ht|∆, one predicts, using the shape of P(∆), that
the distribution of incoming volume should then decay as ∆−1−µfor ∆ <∆∗and
as ∆−1−µ−νfor large ∆ >∆∗, a feature that we have conﬁrmed directly.
We now turn to the shape of the order book. The order ﬂow is maxi mum
around the current price, but an order very close to the curre nt price has a larger
probability of being executed and thus disappear from the bo ok. It is thus not a
priori clear what will be the average shape of the order book. Quite interestingly,
we ﬁnd that the (time-averaged) volume of the queue in the ord er book (that
we will call for simplicity the ‘average order book’) is symm etrical, and has a
maximum away from the current bid (ask): see Fig. 2. This was a lso noted in
[9]. This shape furthermore appears to be universal , up to a rescaling of both the
∆ axis and the volume axis, at least for the three stocks studi ed. In order to test
the universality of this result, we are currently studying l ess liquid stocks, and in
a diﬀerent time interval. The empirical determination of th e average order book
is the central result of this paper, and simple models that ex plain this shape will
be discussed below.
The next question concerns the volume ﬂuctuations around th is average shape.
We ﬁrst study the distribution R(V) of volume at the bid (or ask). Again, the
two are identical, and can be ﬁtted by a Gamma distribution fo r the volume (Fig.
3):
R(V)∝Vγ−1exp/parenleftbigg
−V
V0/parenrightbigg
. (2)
We ﬁnd γ≃0.7−0.8 for all three stocks. A Gamma distribution with γ≤1
has its maximum for V= 0. This shows that the most probable value of the
volume at bid is very small, although its typical value is qui te large ( V0≃2700
for France-Telecom).
Therefore, the order book has strong ﬂuctuations and at a giv en instant of
time can look rather diﬀerent from its average, Fig. 2. We hav e also studied the
ﬂuctuations of volume in the book as a function of ∆, deﬁned as :
σV(∆) =/radicalBig
∝an}bracketle{tV2∝an}bracketri}ht|∆− ∝an}bracketle{tV∝an}bracketri}ht|2
∆. (3)
6",2002-03-25T12:53:09Z, limit  one  not since  t it quite    it t  ag articial intellence gaa   gaa  france telecom trefore   b
paper_qf_11.pdf,7,"Statistical properties of stock order books: empirical results and
  models","  We investigate several statistical properties of the order book of three
liquid stocks of the Paris Bourse. The results are to a large degree
independent of the stock studied. The most interesting features concern (i) the
statistics of incoming limit order prices, which follows a power-law around the
current price with a diverging mean; and (ii) the humped shape of the average
order book, which can be quantitatively reproduced using a `zero intelligence'
numerical model, and qualitatively predicted using a simple approximation.
","1 10 100 1000050010001500
 Vivendi
 Total
 France Telecom
 Numerical model0 1 10 1000110100100010000
Figure 2: Average volume of the queue in the order book for the three sto cks, as a
function of the distance ∆ from the current bid (or ask) in a lo g-linear scale. Both axis
have been rescaled in order to collapse the curves correspon ding to the three stocks.
The thick dots correspond to the numerical model explained b elow, with Γ = 10−3and
pm= 0.25. Inset: same data in log-log coordinates.
0 1 2 3 4 5
 Log10(V)050001000015000R(V) France Telecom (Asks)
 Gamma distribution, γ=0.7
Figure 3: Histogram of the log-volume at the ask (same for bids), for Fr ance Telecom.
The ﬁt corresponds to Gamma distribution, Eq. (2) (after a ch ange of variables to
logV), with γ= 0.7.
7",2002-03-25T12:53:09Z,vivendi total france telecom numerical  ge both t iset log france telecom asks gaa  piogram fr telecom t gaa eq
paper_qf_11.pdf,8,"Statistical properties of stock order books: empirical results and
  models","  We investigate several statistical properties of the order book of three
liquid stocks of the Paris Bourse. The results are to a large degree
independent of the stock studied. The most interesting features concern (i) the
statistics of incoming limit order prices, which follows a power-law around the
current price with a diverging mean; and (ii) the humped shape of the average
order book, which can be quantitatively reproduced using a `zero intelligence'
numerical model, and qualitatively predicted using a simple approximation.
","We ﬁnd that σV(∆) is of order 1 for ∆ = 1 (as expected from the above Gamma
distribution), and is roughly constant up to ∆ = 50. This show s that far from
the current price, the instantaneous order book shows large relative ﬂuctuations,
which is indeed reasonable. We have also studied the full cov ariance matrix of
the ﬂuctuations CV(∆,∆′) (note that CV(∆,∆)≡σ2
V(∆)). For France-Telecom,
its ﬁrst eigenvalue corresponds to a dilation of the book, wh ereas the second one
reﬂects an even/odd oscillation (orders are more numerous a t prices in tenth of
Euros than in twentieth of Euros). Higher eigenvalues are st ructureless, suggest-
ing that the orderbook has no ‘elasticity’. This is expected , since most investors
do not have access to the order book in real time.
We now turn to a quantitative interpretation of the data. Fir st, we have
simulated an artiﬁcial ‘zero intelligence’ order book, muc h in the same spirit as
in [11, 16], but more directly inspired from our empirical re sults. Limit orders,
with a size distributed uniformly on a log scale, are launche d at random with an
a priori ‘sprinkling’ distribution P(∆), which we take identical to the empirical
one. Here we depart from [16], where all orders have unit size and the ‘sprinkling’
distribution is uniform between 0 and a certain ∆ max. The size distribution was
actually found to play a minor role except in the far tail of th e average order
book, where the exponent νintroduced above becomes important. For example,
we also studied the case of orders of equal size with very simi lar results.
On the other hand, choosing the correct ‘rain’ distribution P(∆) is crucial to
reproduce the shape of the order book. We also launch a certai n fraction pmof
market orders (chosen to be typically 1 /6−1/4th of the total) in order to trigger
trades. The results we ﬁnd (in particular the averge size of t he bid-ask spread) are
quite sensitive to the value of pm. Finally, with a constant probability Γ (typically
of order 10−3) per unit time, independently of both size and position in th e book,
an order is cancelled. We have found that this simple model is able to reproduce
quantitatively many of the features observed in the empiric al data, such a the
shape of the order book (see Fig. 2), or the Gamma distributio n of volume at the
bid/ask R(V), with a similar value for the exponent γ. This model also reproduces
other empirical properties, such as the short time dynamics of the ‘midpoint’ and
the sublinear volume dependence of the response to an incomi ng market order
[18, 16, 19]. However, we note that the above simple model ove restimates the
average size of the queue close to ∆ = 0. This might be due eithe r to the fact that
our procedure to treat modiﬁed orders leads to an systematic bias which is large
near the best price, or that orders close to the best price hav e a non negligible
strategic content that our model fails to capture.
Finally, we discuss a simple analytical approximation whic h allows us to com-
pute the average order book from the ingredients of the numer ical model. This
was also attempted in [16]. Although very few details were gi ven in [16], our
method appears to be quite diﬀerent from theirs. Consider se ll orders. Those at
distance ∆ from the current ask at time tare those which were placed there at a
timet′< t, and have survived until time t, that is, (i) have not been cancelled;
8",2002-03-25T12:53:09Z, gaa   for france telecom euros euros hr   r limit re t for o t nally   gaa   nally  although consir those
paper_qf_11.pdf,9,"Statistical properties of stock order books: empirical results and
  models","  We investigate several statistical properties of the order book of three
liquid stocks of the Paris Bourse. The results are to a large degree
independent of the stock studied. The most interesting features concern (i) the
statistics of incoming limit order prices, which follows a power-law around the
current price with a diverging mean; and (ii) the humped shape of the average
order book, which can be quantitatively reproduced using a `zero intelligence'
numerical model, and qualitatively predicted using a simple approximation.
","(ii) have not been touched by the price at any intermediate ti met′′between t′
andt. An order at distance ∆ at time tin the reference frame of the ask a(t)
appeared in the order book at time t′at a distance ∆ + a(t)−a(t′). The average
order book can thus be written, in the long time limit, as:
ρ(∆, t) =/integraldisplayt
−∞dt′/integraldisplay
duP(∆ +u)P(u|C(t, t′))e−Γ(t−t′), (4)
where P(u|C(t, t′)) is the conditional probability that the time evolution of the
price produces a given value of the ask diﬀerence u=a(t)−a(t′), given the
condition that the path always satisﬁes ∆ + a(t)−a(t′′)≥0 at all intermediate
times t′′∈[t′, t]. The evaluation of Prequires the knowledge of the statistics of
the price process. Because of the exponential cut-oﬀ e−Γ(t−t′), only the short time
behaviour of the process is relevant where the conﬁnement eﬀ ects of the order
book on the price are particularly important [10, 16, 19]. Ne vertheless, to make
progress, we will assume that the process is purely diﬀusive . In this case, Pcan
be calculated using the method of images. One ﬁnds:
P(u|C(t, t′)) =1√
2πDτ/bracketleftBigg
exp/parenleftBigg
−u2
2Dτ/parenrightBigg
−exp/parenleftBigg
−(2∆ + u)2
2Dτ/parenrightBigg/bracketrightBigg
, (5)
where τ=t−t′andDis the diﬀusion constant of the price process.
After a simple computation, one ﬁnally ﬁnds, up to a multipli cative constant
which only aﬀects the overall normalisation of ρst(∆) = ρ(∆, t→ ∞):
ρst(∆) = e−α∆/integraldisplay∆
0duP(u) sinh( αu) + sinh( α∆)/integraldisplay∞
∆duP(u)e−αu, (6)
where α−1=/radicalBig
D/2Γ measures the typical variation of price during the lifeti me
of an order, and ﬁxes the scale over which the order book varie s. When the
distribution of order ﬂows P(∆) has the power-law shape Eq. (1) with µ <1, the
parameter αcan be rescaled away in the ‘continuous’ limit where α−1is much
larger than the tick size (which is the relevant limit for sto cks, where α−1∼10).
In this case, the shape of the average order book only depends onµandˆ∆ =α∆,
and is given by the following convergent integral:
ρst(ˆ∆) = e−ˆ∆/integraldisplayˆ∆
0du u−1−µsinh(u) + sinh( ˆ∆)/integraldisplay∞
ˆ∆du u−1−µe−u. (7)
For ∆ →0, the average order book vanishes in a singular way, as ρst(∆)∝
∆1−µ, whereas for ∆ → ∞, the average order book reﬂects the incoming ﬂow
of orders: ρst(∆)∝∆−1−µ. We have plotted in Fig.4 the average order book
obtained numerically from the ‘zero intelligence’ model (w hich is very close to
empirical data, see Fig. 2) and compared it with Eq. (7), with µ= 0.6 and var-
ious choices of parameters. After rescaling the two axes, th e various numerical
9",2002-03-25T12:53:09Z,at t requirbecause ne icaone b b b b b b is after b q ifor    eq after
paper_qf_11.pdf,10,"Statistical properties of stock order books: empirical results and
  models","  We investigate several statistical properties of the order book of three
liquid stocks of the Paris Bourse. The results are to a large degree
independent of the stock studied. The most interesting features concern (i) the
statistics of incoming limit order prices, which follows a power-law around the
current price with a diverging mean; and (ii) the humped shape of the average
order book, which can be quantitatively reproduced using a `zero intelligence'
numerical model, and qualitatively predicted using a simple approximation.
","0 2 4 6 8 10
∆00.511.5
0 1 10
∆00.511.52
Figure 4: The average order book of the numerical model with various ch oices of
parameters ( µ=.6,pm∈ {1/4,1/6}, and Γ ∈ {10−3,510−4}is compared to the
approximate analytical prediction, (full curve), Eq. (7). After rescaling the axes, the
various results roughly scale on the same curve, which is wel l reproduced by our simple
analytic argument. The inset shows the same data in a log-lin ear representation.
10",2002-03-25T12:53:09Z, t eq after t
paper_qf_11.pdf,11,"Statistical properties of stock order books: empirical results and
  models","  We investigate several statistical properties of the order book of three
liquid stocks of the Paris Bourse. The results are to a large degree
independent of the stock studied. The most interesting features concern (i) the
statistics of incoming limit order prices, which follows a power-law around the
current price with a diverging mean; and (ii) the humped shape of the average
order book, which can be quantitatively reproduced using a `zero intelligence'
numerical model, and qualitatively predicted using a simple approximation.
","models lead to very similar average order books, and the anal ytic approxima-
tion appears rather satisfactory, bearing in mind the rough ness of our ‘diﬀusive’
approximation.
The shape of the average order book therefore reﬂects the com petition between
a power-law ﬂow of limit orders with a ﬁnite lifetime, and the price dynamics that
removes the orders close to the current price. These eﬀects l ead to a universal
shape which will presumably hold for many diﬀerent markets, provided the life-
time of orders is suﬃciently long compared to the typical tim e between trades,
and the volatility on the scale of the orders lifetime is some what larger than the
ticksize.1On the other hand, the detailed shape of the book in the immedi ate
vicinity of the best price (i.e., around ∆ = 0) is not expected to be universal.
In conclusion, we have investigated several ‘static’ prope rties of the order
book. We have found that the results appear to be independent of the stock
studied. The most interesting features concern (i) the stat istics of incoming limit
order prices, which follows a power-law around the current p rice with a diverging
mean – suggesting, quite surprisingly, that market partici pants believe that very
large variations of the price are possible within a rather sh ort time horizon; and
(ii) the shape of the average order book, which can be quantit atively reproduced
using a ‘zero intelligence’ numerical model, and qualitati vely predicted using a
simple approximation. One of the most interesting open prob lems is, in our
view, to explain the clear power-law behaviour of the incomi ng orders and the
diﬀerence between our results and those of [17]. The dynamic al properties of the
order book, and the way the price reacts to order ﬂow, will be t he subject of a
further study [19].
Acknowledgements: We thank Jean-Pierre Aguilar, Jelle Boe rsma, Damien Chal-
let, J. Doyne Farmer, Laurent Laloux, Andrew Matacz, Philip Seager and Denis
Ullmo for stimulating and useful discussions.
References
[1] for a recent review, see: R. Cont, Quantitative Finance, 1, 223 (2001), and
refs. therein.
[2] J.-P. Bouchaud and M. Potters, Th´ eorie des Risques Financiers , Al´ ea-Saclay,
1997;Theory of Financial Risks , Cambridge University Press, 2000.
[3] R. Mantegna & H. E. Stanley, An Introduction to Econophysics , Cambridge
University Press, 1999.
1Preliminary results indeed show that the average order book in futures markets has a
similar hump away from the current midpoint.
11",2002-03-25T12:53:09Z,t tse oi t one t ackledgement   pierre guitar elle boe admichal do ne farmer laurent halo ux andrew mata cz phip eager nis url mo referenccont quantitative nance  chaters th rique nanciers al ay tory nancial risks cambridge   antenna stanley aintrodueco no psics cambridge   preliminary
paper_qf_11.pdf,12,"Statistical properties of stock order books: empirical results and
  models","  We investigate several statistical properties of the order book of three
liquid stocks of the Paris Bourse. The results are to a large degree
independent of the stock studied. The most interesting features concern (i) the
statistics of incoming limit order prices, which follows a power-law around the
current price with a diverging mean; and (ii) the humped shape of the average
order book, which can be quantitatively reproduced using a `zero intelligence'
numerical model, and qualitatively predicted using a simple approximation.
","[4] V. Plerou, P. Gopikrishnan, L.A. Amaral, M. Meyer, H.E. S tanley, Phys.
Rev.E606519 (1999).
[5] T. Lux, Applied Financial Economics, 6, 463, (1996).
[6] D. M. Guillaume, M. M. Dacorogna, R. D. Dav´ e, U. A. M¨ ulle r, R. B. Olsen
and O. V. Pictet, Finance and Stochastics 195 (1997).
[7] for a review, see e.g. J.-F. Muzy, J. Delour, E. Bacry, Eur . Phys. J. B 17,
537-548 (2000), and refs. therein.
[8] F. Longin, Journal of Business, 69383 (1996)
[9] B. Biais, P. Hilton, C. Spatt, An empirical analysis of the limit order book
and the order ﬂow in the Paris Bourse , Journal of Finance, 50, 1655 (1995)
[10] S. Maslov, Simple model of a limit order-driven market , Physica A 278,
571 (2000); S. Maslov, M. Millis, Price ﬂuctuations from the order book
perspective – empirical facts and a simple model , Physica A 299, 234 (2001)
[11] D. Challet, R. Stinchcombe, Analyzing and modelling 1+1d markets , Physica
A300, 285 (2001)
[12] P. Bak, M. Paczuski, and M. Shubik, Physica A 246, 430 (1997)
[13] David L.C. Chan, David Eliezer, Ian I. Kogan, Numerical analysis of the
Minimal and Two-Liquid models of the Market Microstructure , preprint
cond-mat/0101474
[14] H. Luckock, A statistical model of a limit order market , Sidney University
preprint (September 2001).
[15] F. Slanina, Mean-ﬁeld approximation for a limit order driven market mod el,
preprint cond-mat/0104547
[16] Marcus G. Daniels, J. Doyne Farmer, Giulia Iori, Eric Sm ith,How storing
supply and demand aﬀects price diﬀusion , preprint cond-mat/0112422.
[17] Such a broad distribution, with very far away limit orde rs, was also found
for British stocks, J.D. Farmer, private communication and I. Zovko, J. D.
Farmer, preprint cond-mat/0206280. Note that these author s ﬁnd µ≈1.5.
[18] P. Gopikrishnan, V. Plerou, X. Gabaix, H.E. Stanley, Statistical Properties
of Share volume traded in Financial Markets , Phys. Rev E 62, R4493 (2000).
[19] J.P. Bouchaud, M. M´ ezard, M. Potters, in preparation.
12",2002-03-25T12:53:09Z,le rou go pi krishna moral meyer ph ys rev lux applied nancial economics gulaume da core gd olsepic tet nance stochastic mu  l our ba y eur ph ys lo ijournal business bia is htospat aparis course journal nance mas lov simple psics mas lov ml is price psics chal  stinalyzi psics bak pac zu  sh ubi psics did chadid belieiayoga numerical minimal two liquid market m struure luck ock sydney  september spaimeamarcus daniels do ne farmer giuliano ori eric sm how subritish farmer zo  farmer note go pi krishna le rou ga articial intellence stanley statistical protishare nancial markets ph ys rev  chaters
paper_qf_12.pdf,1,The US 2000-2003 Market Descent: Clarifications,"  In a recent comment (Johansen A 2003 An alternative view, Quant. Finance 3:
C6-C7, cond-mat/0302141), Anders Johansen has criticized our methodology and
has questioned several of our results published in [Sornette D and Zhou W-X
2002 The US 2000-2002 market descent: how much longer and deeper? Quant.
Finance 2: 468-81, cond-mat/0209065] and in our two consequent preprints
[cond-mat/0212010, physics/0301023]. In the present reply, we clarify the
issues on (i) the analogy between rupture and crash, (ii) the Landau expansion,
``double cosine'' and Weierstrass-type solutions, (iii) the symmetry between
bubbles and anti-bubbles and universality, (iv) the condition of criticality,
(v) the meaning of ``bullish anti-bubbles'', (vi) the absolute value of t_c-t,
(vii) the fractal log-periodic power law patterns, (viii) the similarity
between the Nikkei index in 1990-2000 and the S&P500 in 2000-2002 and (ix) the
present status of our prediction.
","arXiv:cond-mat/0305004v1  [cond-mat.stat-mech]  30 Apr 2003TheUS2000-2003MarketDescent: Clariﬁcations
DidierSornette1,2,3and Wei-XingZhou1
1. Institute of Geophysics and Planetary Physics, Universi ty of California, Los Angeles, CA
90095
2. Department of Earth and SpaceSciences, University of Cal ifornia, Los Angeles, CA90095
3. LaboratoiredePhysiquedelaMati` ereCondens´ ee,CNRSU MR6622andUniversit´ edeNice-
Sophia Antipolis, 06108 NiceCedex 2, France
October 31, 2018
Abstract
In a recent comment [Johansen A 2003 An alternative view, Qua nt. Finance 3: C6-C7,
cond-mat/0302141],AndersJohansenhas criticized our met hodologyand has questionedsev-
eral of our results published in [Sornette D and Zhou W-X 2002 The US 2000-2002 market
descent: how much longer and deeper? Quant. Finance 2: 468-8 1, cond-mat/0209065] and
in our two consequent preprints [cond-mat/0212010, physic s/0301023]. In the present reply,
we clarify the issues on (i) the analogy between rupture and c rash, (ii) the Landau expan-
sion, “double cosine” and Weierstrass-type solutions, (ii i) the symmetry between bubbles and
anti-bubblesand universality, (iv) the condition of criti cality, (v) the meaning of “bullish anti-
bubbles”,(vi)theabsolutevalueof tc−t,(vii)thefractallog-periodicpowerlawpatterns,(viii)
the similarity between the Nikkei index in 1990-2000and the S&P500 in 2000-2002and (ix)
thepresentstatusofourprediction.
In a recent comment [5], Anders Johansen has criticized our m ethodology and has questioned
several of our results published in this journal [17] and in o ur two consequent preprints [19, 20].
We regret the controversial tone adopted in [5] but welcome t his opportunity to clarify our work
further.
In a series of works starting with [16] (see [13] and referenc es therein), ﬁnancial bubbles have
been deﬁned as regimes in which the stock market exhibits an u nsustainable super-exponential
growth, that can be characterized quantitatively as a genui ne critical phenomenon with speciﬁc
log-periodic power-law (LPPL) signatures. The underlying mechanism is proposed to be found in
imitation between investors and their herding behavior, wh ich lead to self-reinforcement positive
feedbacks.
In [7], Johansen and Sornette introduced the concept of “ant i-bubbles” to describe decaying
LPPLpricetrajectories that aresometimesfoundtofollow v erylargemarket highs. Basedonmod-
els of imitation between investors and their cooperative he rding behavior [7, 4, 14], it was realized
that speculation and imitation also occur during bearish ma rkets, leading to price trajectories that
seem approximately symmetric to the accelerating speculat ive bubbles ending in crashes, under a
time reversal transformation ( tc−t→t−tcwheretcis a critical time corresponding to the end
of the bubble or the start of the anti-bubble). The two ﬁrst ex amples of anti-bubbles were found
[7] in the Japanese Nikkei stock index from 1990 to 1998, whos e analysis led to the successful
prediction of two trend reversals [9, 13], and in the Gold fut ure prices after 1980, both after their
1",2003-04-30T21:23:23Z, apr t market scent car rir corvee i ki hou institute geopsical planetary psics universe calornia los aelpartment earth space scienc cal los aellabor to red psique la math cons  nice sophia anti po nice ce france ober abstra isoaqua nance anrs sohas corvee hou t quant nance iland ierstrass  kei ianrs so it isocorvee price trajeoribased omod t ese  kei gold
paper_qf_12.pdf,2,The US 2000-2003 Market Descent: Clarifications,"  In a recent comment (Johansen A 2003 An alternative view, Quant. Finance 3:
C6-C7, cond-mat/0302141), Anders Johansen has criticized our methodology and
has questioned several of our results published in [Sornette D and Zhou W-X
2002 The US 2000-2002 market descent: how much longer and deeper? Quant.
Finance 2: 468-81, cond-mat/0209065] and in our two consequent preprints
[cond-mat/0212010, physics/0301023]. In the present reply, we clarify the
issues on (i) the analogy between rupture and crash, (ii) the Landau expansion,
``double cosine'' and Weierstrass-type solutions, (iii) the symmetry between
bubbles and anti-bubbles and universality, (iv) the condition of criticality,
(v) the meaning of ``bullish anti-bubbles'', (vi) the absolute value of t_c-t,
(vii) the fractal log-periodic power law patterns, (viii) the similarity
between the Nikkei index in 1990-2000 and the S&P500 in 2000-2002 and (ix) the
present status of our prediction.
","all-time highs. Several other examples have been described in the Russian stock market [12] and
in emergent and western markets [8]. Our recent work [17, 19, 20] adds many other cases that all
started in the summer of 2000.
Status of the rupture analogy . More precise and probably more relevant than the analogy
withmaterial rupture istheconcept of aﬁnite-timesingula rity asdeveloped in[4,14,10,2],which
emerges from positive feedbacks. The concept of a ﬁnite-tim e singularity is the counterpart in the
time-domain of the concept of criticality. The ﬁght between positive and negative feedbacks is the
key concept underlying the proposal of LPPLsignatures of sp eculative bubbles and anti-bubbles in
stock markets [13].
Landau expansion, “double cosine” and Weierstrass-type so lutions. A. Johansen criticizes
our useof the “double cosine” function onthebasis that asou nd theoretical justiﬁcation islacking,
while he puts his faith in the Landau expansion introduced in [15] and extended up to third order
in [7]. Actually, the full solution of the simplest renormal ization group equation for acritical point
has been analyzed in depth in [3] and provides an improvement of these approaches in the form of
Weierstrass-type functions of the form
ln[p(t)] =A+Bτm+ℜ/parenleftBiggN/summationdisplay
n=1Cneiψnτ−sn/parenrightBigg
, (1)
whereτ=|t−tc|,sn=−m+inωandℜ()is the real part operator. The existence of different
phasesψnincriminated by A. Johansen can be seen to derive naturally f rom the Mellin transform
of the regular part of the renormalization group equation. I n simple words, the different phases ψn
embodyaninformation onthemechanisms ofinteractions bet weeninvestors. Thereisthusasound
theoretical justiﬁcation forsuchaphaseshift (understan dψ2−ψ1)betweentheﬁrstandthesecond
harmonics (understand theﬁrsttwoterms n= 1and2oftheexpansion (1)). Whenthephaseshave
certain relationships (phase locking), a discrete hierarc hy of critical times emerge, which has been
found to describe very well the US stock market since the summ er of 2000 [20]. A. Johansen’s
misconception can probably be traced to the incorrect idea t hat the phase of the simple cosine
formula (case N= 1) has no ﬁnancial meaning because it can be gauged away in a red eﬁnition of
the time scale.
Symmetry between bubbles and anti-bubbles and universalit y. From a mechanistic view
point, we advocate the existence of anti-bubbles from the id ea that the ﬁght between positive and
negativefeedbacksisoperativebothinbullishaswellasin bearishmarkets[13]. Fromadescriptive
view point, our recent works [17, 19,20] just follow Johanse n and Sornette’s previous works [7, 9,
12,8],whichintroducedtheconceptofan“anti-bubble” fro masymmetryperspective. Asymmetry
may have distinct consequences. It can be used to justify the same functional expressions both for
bubbles and anti-bubbles. Thus, in the mathematical expres sions, the symmetry between bubbles
andanti-bubbles amountstochanging tc−tfor bubbles to t−tcforanti-bubbles. Here,weshould
stressthat, ifaLPPLanti-bubble followsaLPPLbubble (whi chisnot thegeneral case), thecritical
timetcisnot generally the same. Anoteworthy exception isthe Russ ian stock market around 1997
[12]. Wereportin[17,19]dozensofanti-bubbles inmanydif ferentstockmarketsworldwidewhich
started almost all inAugust 2000, that is,4months later tha ntheendofthe“neweconomy” bubble
and its crash in April 2000. Another case is Chile around 1994 -1995, where the bubble ended in
February 1994 while theanti-bubble started in July 1995 [8] .
A.Johansenwouldlikethatthesymmetrybetweenbubblesand anti-bubblesshouldbeextended
so that thesame log-periodic angular frequency ωdescribes both cases. Hethus invokes morethan
2",2003-04-30T21:23:23Z,sevl our status  t t snaturland ierstrass soland aually ierstrass b nei b t soll itre is  sound  phashe sosyetry from from siptive  se corvee asyetry it  re anti bubble notewort russ re t iaugust apr anotr c ruary july sowould like that t syetry betebubbland  
paper_qf_12.pdf,3,The US 2000-2003 Market Descent: Clarifications,"  In a recent comment (Johansen A 2003 An alternative view, Quant. Finance 3:
C6-C7, cond-mat/0302141), Anders Johansen has criticized our methodology and
has questioned several of our results published in [Sornette D and Zhou W-X
2002 The US 2000-2002 market descent: how much longer and deeper? Quant.
Finance 2: 468-81, cond-mat/0209065] and in our two consequent preprints
[cond-mat/0212010, physics/0301023]. In the present reply, we clarify the
issues on (i) the analogy between rupture and crash, (ii) the Landau expansion,
``double cosine'' and Weierstrass-type solutions, (iii) the symmetry between
bubbles and anti-bubbles and universality, (iv) the condition of criticality,
(v) the meaning of ``bullish anti-bubbles'', (vi) the absolute value of t_c-t,
(vii) the fractal log-periodic power law patterns, (viii) the similarity
between the Nikkei index in 1990-2000 and the S&P500 in 2000-2002 and (ix) the
present status of our prediction.
","just afunctional but anumerical symmetry. Wethink that thi s belief maybetoorigid atthepresent
time when we still have a rather limited understanding of thi s complex problem. We propose an
open-minded approach moreadapted toalearning phase. It is correct that, for LPPLbubbles, there
israther well-deﬁned cluster of values for ω≈6.36±1.56andform≈0.33±0.18asreported in
[11] (see equation (4) of [6]). For anti-bubbles in the USA S& P and in many EU markets, we ﬁnd
almost the same value ω≈12. This value is comparable with those obtained for the anti-b ubbles
in the Latin-American markets and Western markets in the 199 0’s [8]. It is interesting that this
valueω≈12is approximately twice the most probably value ωfound for LPPL bubbles. Does
it correspond to a log-periodicity different from that of bu bbles? Probably not for the following
reason: we have found in [17, 19] that both ωand2ωwere quite signiﬁcant in the anti-bubbles,
including the Nikkei case that started in 1990. Due to the pro bable variation of the strength of
nonlinear processes in the stock markets, it can be expected that the amplitudes of the ﬁrst and
second harmonics can be different from one realization to th e next. Within the renormalization
groupframework,therelativestrengthoftheﬁrstandsecon dharmonicsiscontrolled bytheregular
part[3]whichdescribesthespeciﬁcinteractions oftheinv estorsthatledtoagivenrealization ofthe
market. Letusaddthattheimportance oftheroleoflog-peri odic harmonicshasbeendemonstrated
for turbulence [18, 21], where the evidence is much stronger . For the emergent markets, the LPPL
signatures are not as signiﬁcant as for the major western mar kets, as noted already in [8]. A.
Johansen also notes the ω’s found for different worldwide markets are not peaked and m ay be due
tonoise. Instead,wethinkthatthisisduetoapossiblelack ofsufﬁcientrobustnessoftheﬁts,which
does not diminish the evidence for log-periodicity but sugg ests to interpret with care the speciﬁc
reported values. Thiscanbeseenfromthefactthat,ifweimp osetheadditional conditioninourﬁts
that the different worldwide markets exhibit an anti-bubbl e with the same critical time tc, we ﬁnd
that their angular log-periodic frequency ωare very close to each other. The quasi-simultaneity of
the starting time and the ensuing strong synchronization of the anti-bubbles exhibited by the major
stock marketsintheworld, whichhasbeendocumented in[19] ,provides anadditional justiﬁcation
for the use of the same critical time tc.
Criticality . A. Johansen criticizes our abandoning of the constraint m <1as a necessary
condition toqualifytheexistence ofabubbleoranti-bubbl e, suggesting thatwehaverenounced the
concept of criticality. There are several issues here that n eed to be distinguished. First, our many
tests performed bythepresent authors andpreviously byA.J ohansen withD.Sornette (reported as
the work that Johansen performed with Matt Lee in [5]) show th at the condition on the exponent
mis much less effective in the detection of bubbles than a cond ition onωfor instance (see also
discussion in Chapter 9 of [13]). This is one justiﬁcation fo r abandoning any constraint on this
rather sensitiveparameter to“letthedataspeak.” Second, ﬁndingvaluesof m≥1doesnotamount
toanabsence ofcriticality, because theequation isstill c ritical (that is,itexhibits asingularity) due
to the presence of the theoretically inﬁnite hierarchy of lo g-periodic oscillations. In other words,
criticality remains present due to the imaginary part ωof the exponent sn=−m+inωof the
LPPL (see equation (1)) as long as it is non-zero, whatever th e valuemof its real part. Third, we
can relax the condition 0< m <1for the present purpose because our LPPL formulas describe
only aﬁnite range of the timeinterval: it iswell-known that true singularities donot exist innature
as friction, ﬁnite-size effects and other regularization m echanisms come into play close enough to
the theoretical mathematical singularity. What is importa nt is the ability of the LPPL formula to
describe with good accuracy a large range of the data, not nec essarily the very close proximity to
the phantom singularity. In this respect, we refer to the rat her detailed discussion of the effect of
ﬁnite-size effects on singularities presented in [10].
3",2003-04-30T21:23:23Z, think  it bubblfor   sterit doprobably  kei due withi us add that t imtance for soinstead  cabe from t fa that t itical it sotre rst corvee soma  chapter  second ithird what in
paper_qf_12.pdf,4,The US 2000-2003 Market Descent: Clarifications,"  In a recent comment (Johansen A 2003 An alternative view, Quant. Finance 3:
C6-C7, cond-mat/0302141), Anders Johansen has criticized our methodology and
has questioned several of our results published in [Sornette D and Zhou W-X
2002 The US 2000-2002 market descent: how much longer and deeper? Quant.
Finance 2: 468-81, cond-mat/0209065] and in our two consequent preprints
[cond-mat/0212010, physics/0301023]. In the present reply, we clarify the
issues on (i) the analogy between rupture and crash, (ii) the Landau expansion,
``double cosine'' and Weierstrass-type solutions, (iii) the symmetry between
bubbles and anti-bubbles and universality, (iv) the condition of criticality,
(v) the meaning of ``bullish anti-bubbles'', (vi) the absolute value of t_c-t,
(vii) the fractal log-periodic power law patterns, (viii) the similarity
between the Nikkei index in 1990-2000 and the S&P500 in 2000-2002 and (ix) the
present status of our prediction.
","“Bullish anti-bubbles” . In our analysis of the largest stock markets in the world, we have
identiﬁed six examples which give a positive coefﬁcient Bin (1). In particular, the statistical sig-
niﬁcance of this result is very high for Australia, Mexico an d Indonesia. This regime B >0is
different from the normal bubble and anti-bubble cases prev iously reported for which B <0. This
regimeB >0has been coined “bullish anti-bubbles” [19] to describe the joint features of deceler-
ating log-periodic oscillations and of an overall increasi ng price. In contradiction with Johansen’s
remark, this regime B >0does not lead to inﬁnite prices in a ﬁnite time but describes a long-term
growth which turns out to be slower than standard exponentia l growth. The same remark applies
form>1.
Absolute value of tc−t. In complete disagreement with A. Johansen’s remark, our us e of
|tc−t|in our ﬁts to locate the critical time tcdoes not abandon “another restriction coming from
the data.” Rather than adding a degree of freedom, this appro ach instead removes an arbitrariness
previously present in the ﬁtting procedure in choosing the t ime interval over which the ﬁt is per-
formed. Rather than determining an approximate starting ti me and/or estimating the critical time
tcby the location of the largest market peak, using |tc−t|makes the ﬁts almost independent of
the chosen starting time. This improved robustness has been documented in details by our many
numerical tests presented in [17,19].
Fractal LPPLpatterns . Aswequoted in[17],Drozdz et al. [1] havereported theexis tence of
LPPLwithin LPPLwithin LPPL,using eye-balling in asingle c ase. Asmentioned by A. Johansen
[5], he with D. Sornette studied this phenomenon rather syst ematically about a year earlier but
never published due to the rather marginal quality level of t he results. In [17], we mentioned that
the worldwide anti-bubble started in the summer of 2000 has a lso left its imprint on the Japanese
market, leading to an anti-bubble within the large scale ant i-bubble that started in January 1990.
This possibility of structures within structures is expect ed on general grounds from the renormal-
ization group model of LPPLsingularities leading to Weiers trass-like solutions (see [20, 13]). The
problemisthatsuchobservationisnotveryrobustwhenoneg oestosmalltimescales,probablydue
to the fact that “noise” and idiosyncratic news affect more a nd more strongly the price time series,
thesmalleristhetimescaleofobservation. However,wenot ethatourreport[17]ofa 2.5yearlong
anti-bubble decorating a 13year long anti-bubble of the Nikkei index should have a speci al status
because both time scales are sufﬁciently long to compare wit h the time span over which previous
LPPL have been qualiﬁed. A. Johansen himself acknowledges t hat “the real success was with a
LPPL analysis on time scales of one to two years of data.” Our r eport in [17] passes this criterion
and should thus be considered at a level different from the pu blished [1] and unpublished analyses
at smaller timescales.
Similarity between theNikkeiindexin1990-2000 andtheS&P 500in2000-2002 . Johansen
downplays the “remarkable similarity” we as well as many obs ervers noticed between these two
markets. First, the factor of 2in the value of the log-periodic frequency is explained by th e com-
petition between the two ﬁrst harmonics n= 1andn= 2in (1), as we explained above. In [17],
westress the remarkable similarity inthetwomarkets withr espect totheexistence oftwoharmon-
ics in both cases. Second, the Nikkei did go through a now well -recognized speculative bubble
culminating at the end of December 1989, even if its price tra jectory does not qualify as a very
good LPPL. We note in this vain that an anti-bubble is usually the follow-up of very high prices,
not necessarily of a LPPL bubble. Even in the case of the US mar ket, we stressed above that the
critical time of the bubble occurred 4months before the critical time of the following anti-bubbl e.
This again stresses that one should exercise caution in twin ning rigidly in time the occurrence of
4",2003-04-30T21:23:23Z,pubh ibiiaustralia me indonesia   isot absolute isoratr ratr  fraal paerns as  quoted dr oz dz withiwithias mentned socorvee iese uary  siularitii ers t  kei soour simarity  kei inx isorst isecond  kei cember  eve
paper_qf_12.pdf,5,The US 2000-2003 Market Descent: Clarifications,"  In a recent comment (Johansen A 2003 An alternative view, Quant. Finance 3:
C6-C7, cond-mat/0302141), Anders Johansen has criticized our methodology and
has questioned several of our results published in [Sornette D and Zhou W-X
2002 The US 2000-2002 market descent: how much longer and deeper? Quant.
Finance 2: 468-81, cond-mat/0209065] and in our two consequent preprints
[cond-mat/0212010, physics/0301023]. In the present reply, we clarify the
issues on (i) the analogy between rupture and crash, (ii) the Landau expansion,
``double cosine'' and Weierstrass-type solutions, (iii) the symmetry between
bubbles and anti-bubbles and universality, (iv) the condition of criticality,
(v) the meaning of ``bullish anti-bubbles'', (vi) the absolute value of t_c-t,
(vii) the fractal log-periodic power law patterns, (viii) the similarity
between the Nikkei index in 1990-2000 and the S&P500 in 2000-2002 and (ix) the
present status of our prediction.
","bubbles and of anti-bubbles. Third, Johansen argues that th e analysis of the Nikkei was based on
9years of data compared with less than 3years for the US market which, he argues, makes these
two cases apart. Johansen forgets to mention is that the 9years of Nikkei data required the use of
alog-periodic formula extended tothird-order inthe Landa u expansion mentioned above while the
analysis in [17] of the US market used only the ﬁrst-order for mula and its extension with a second
harmonics. Johansen and Sornette’s initial analysis of the Nikkei data in [7] showed that, similarly
to the US market, the ﬁrst three years of the Nikkei time serie s could be adequately described by
the ﬁrst-order formula. It is by extending to large time hori zon that it was necessary to use the
higher-order terms in the Landau expansion. It is also inter esting to note that there was a global
anti-bubble starting in January 1994 in the major western st ock markets [8], which also bears sim-
ilarities to the present worldwide 2000-2003 anti-bubble c ase [19]. The global anti-bubble in the
mid-1990’s lasted less than one year, while the 2000-2003 an ti-bubble is still alive on many more
markets, resulting in a much higher statistical signiﬁcanc e level. There is thus no qualitative nor
quantitative difference betweentheJapanese andUSAdatas ets. Wewouldliketoaddthat thesim-
ilarity between theNikkei in1990-2000 and theS&P500in200 0-2002 canbefurther strengthened
by paralleling theeconomic and ﬁnancial distresses of the t wocountries, asexplained in[17].
Status of the prediction . Finally, “The proof of the pudding is in the eating.” The ult imate
evidence attesting the true nature of something lies in the v eriﬁcation of ex-ante predictions by
future data. We have offered the predictions for the future o f the US market in [17, 20] and of
many worldwide markets in [19] as an important additional st ep for testing the LPPL hypothesis.
These predictions for the S&P500 US market are compared with the realized values and are also
updated monthly (go to the URL http://www.ess.ucla.edu/fa culty/sornette/ and then click on “The
future of the USAstock market”). Recall that the prediction published in [17] was made at the end
ofAugust 2002. Atthetimeofthelatest comparison, March18 2003, onecanseethat wecorrectly
predicted therecovery ofthemarket until theendof2002 but missedtheseveredropthat followed,
which was probably due to the uncertainties associated with the coming war with Iraq. We should
also stress that these last months have exhibited a very larg e volatility, leading to deviations from
our prediction that arehowever comparable inmagnitude wit hprevious deviations inthein-sample
period. Our predictions arefundamentally “low-frequency ” innature andcannot obviously capture
the detailed idiosyncratic volatility. Thecomparison bet ween our predictions and therealized price
should thus be made at the time scale of the prediction horizo n, that is, from August 2002 till
summer 2004. We stand by our prediction that the market shoul d appreciate somewhat and then
resume itsoverall bearish anti-bubble descent.
Acknowledgments: ThisworkwaspartiallysupportedbytheJamesS.McDonnellF oundation
21st century scientist award/studying complex system.
References
[1] Drozdz S, Ruf F, Speth J and Wojcik M 1999 Imprints of log-p eriodic self-similarity in the
stock market Eur. Phys. J. B 10589-93
[2] GluzmanSandSornetteD2002Classiﬁcationofpossibleﬁ nite-timesingularitiesbyfunctional
renormalization Phys. Rev. E 66016134
[3] GluzmanSandSornetteD2002Log-periodic routetofract al functions Phys.Rev.E 65036142
5",2003-04-30T21:23:23Z,third so kei so kei land socorvee  kei  kei it land it uary t tre ese data  would like to add that  kei status nally t t  tse t stock recall august at t time of t latest mariraq  our t arisoaugust  ackledgment  work was partially supted by t jammc cor referencdr oz dz ru pe th wocik imprints eur ph ys luz maand corvee ass ph ys rev luz maand corvee log ph ys rev
paper_qf_12.pdf,6,The US 2000-2003 Market Descent: Clarifications,"  In a recent comment (Johansen A 2003 An alternative view, Quant. Finance 3:
C6-C7, cond-mat/0302141), Anders Johansen has criticized our methodology and
has questioned several of our results published in [Sornette D and Zhou W-X
2002 The US 2000-2002 market descent: how much longer and deeper? Quant.
Finance 2: 468-81, cond-mat/0209065] and in our two consequent preprints
[cond-mat/0212010, physics/0301023]. In the present reply, we clarify the
issues on (i) the analogy between rupture and crash, (ii) the Landau expansion,
``double cosine'' and Weierstrass-type solutions, (iii) the symmetry between
bubbles and anti-bubbles and universality, (iv) the condition of criticality,
(v) the meaning of ``bullish anti-bubbles'', (vi) the absolute value of t_c-t,
(vii) the fractal log-periodic power law patterns, (viii) the similarity
between the Nikkei index in 1990-2000 and the S&P500 in 2000-2002 and (ix) the
present status of our prediction.
","[4] Ide K and Sornette D 2002 Oscillatory ﬁnite-time singula rities in ﬁnance, population and rup-
turePhysica A 30763-106
[5] Johansen A2003 Analternative view Quant. Finance 3C6-C7
[6] Johansen A 2003 Characterization of large price variati ons in ﬁnancial markets Physica A in
press
[7] Johansen A and Sornette D 1999 Financial “anti-bubbles” : log-periodicity in Gold and Nikkei
collapses Int. J.Mod. Phys. C 10563-75
[8] JohansenAandSornetteD2000Bubblesandanti-bubbles i nLatin-American, AsianandWest-
ern stock markets: an emprical study, Int. J.Theor. Appl. Fin. 4853-920
[9] Johansen AandSornetteD2000Evaluationofthequantita tive prediction ofatrendreversal on
the Japanese stock market in1999 Int. J. Mod. Phys. C 11359-64
[10] Johansen Aand Sornette D2001 Finite-time singularity inthe dynamics of the worldpopula-
tion and economic indices Physica A 294465-502
[11] Johansen A and Sornette D 2002 Endogenous versus exogen ous crashes in ﬁnancial markets
preprinthttp://arXiv.org/abs/cond-mat/0210509
[12] Johansen A, Sornette D and Ledoit O 1999 Predicting ﬁnan cial crashes using discrete scale
invariance Journal of Risk 15-32
[13] Sornette D 2003 Why Stock Markets Crash? (Critical Events in Complex Financ ial Systems)
(Princeton, NJ: Princeton University Press)
[14] Sornette D and Ide K 2003 Theory of self-similar oscilla tory ﬁnite-time singularities in Fi-
nance, Population and Rupture Int. J.Mod. Phys. C 14in press
[15] Sornette Dand Johansen A 1997 Large ﬁnancial crashes Physica A 245411-22
[16] SornetteD,JohansenAandBouchaudJ-P1996Stockmarke tcrashes,PrecursorsandReplicas
J.Phys.I France 6167-75
[17] Sornette D and Zhou W-X 2002 The US 2000-2002 market desc ent: how much longer and
deeper?Quant. Finance 2468-81
[18] Zhou W-X and Sornette D 2002 Evidence of intermittent ca scades from discrete hierarchical
dissipation in turbulence Physica D 16594-125
[19] Zhou W-X and Sornette D 2003 Evidence of a worldwide stoc k market log-periodic anti-
bubble since mid-2000 preprinthttp://arXiv.org/abs/cond-mat/0212010
[20] ZhouW-XandSornetteD2003Renormalization groupanal ysisofthe2000-2002 anti-bubble
in the US S&P 500 index: explanation of the hierarchy of 5 cras hes and prediction preprint
http://arXiv.org/abs/physics/0301023
[21] Zhou W-X, Sornette D and Pisarenko V 2003 New evidence of discrete scale invariance in
the energy dissipation of three-dimensional turbulence: c orrelation approach and direct spectral
detection Int. J.Modern Phys. C 14in press
6",2003-04-30T21:23:23Z,i corvee osclator psics soaalternative quant nance socharaerizatpsics socorvee nancial gold  kei imod ph ys soand corvee bubbland anti   and st it or ppl soand corvee evaluatof t quant it ese imod ph ys soand corvee nite psics socorvee endogenous  socorvee le do it predii journal risk corvee w stock markets ash itical events lex anc tems princetoprinceto  corvee i tory  populatrupture imod ph ys corvee and solarge psics corvee soand  chastock make precursors and replicas ph ys france corvee hou t quant nance hou corvee evince psics hou corvee evince  hou and corvee re normalizat hou corvee visa re new imorph ys
paper_qf_13.pdf,1,Sig-SDEs model for quantitative finance,"  Mathematical models, calibrated to data, have become ubiquitous to make key
decision processes in modern quantitative finance. In this work, we propose a
novel framework for data-driven model selection by integrating a classical
quantitative setup with a generative modelling approach. Leveraging the
properties of the signature, a well-known path-transform from stochastic
analysis that recently emerged as leading machine learning technology for
learning time-series data, we develop the Sig-SDE model. Sig-SDE provides a new
perspective on neural SDEs and can be calibrated to exotic financial products
that depend, in a non-linear way, on the whole trajectory of asset prices.
Furthermore, we our approach enables to consistently calibrate under the
pricing measure $\\\\mathbb Q$ and real-world measure $\\\\mathbb P$. Finally, we
demonstrate the ability of Sig-SDE to simulate future possible market scenarios
needed for computing risk profiles or hedging strategies. Importantly, this new
model is underpinned by rigorous mathematical analysis, that under appropriate
conditions provides theoretical guarantees for convergence of the presented
algorithms.
","Sig-SDEs model for quantitative finance.
Imanol Perez Arribas
imanol.perez@maths.ox.ac.uk
University of Oxford
Alan Turing Institute
United KingdomCristopher Salvi
cristopher.salvi@maths.ox.ac.uk
University of Oxford
Alan Turing Institute
United KingdomLukasz Szpruch
l.szpruch@ed.ac.uk
University of Edinburgh
Alan Turing Institute
United Kingdom
ABSTRACT
Mathematical models, calibrated to data, have become ubiquitous
to make key decision processes in modern quantitative finance. In
this work, we propose a novel framework for data-driven model
selection by integrating a classical quantitative setup with a genera-
tive modelling approach. Leveraging the properties of the signature,
a well-known path-transform from stochastic analysis that recently
emerged as leading machine learning technology for learning time-
series data, we develop the Sig-SDE model. Sig-SDE provides a
new perspective on neural SDEs and can be calibrated to exotic
financial products that depend, in a non-linear way, on the whole
trajectory of asset prices. Furthermore, we our approach enables to
consistently calibrate under the pricing measure Qand real-world
measure P. Finally, we demonstrate the ability of Sig-SDE to sim-
ulate future possible market scenarios needed for computing risk
profiles or hedging strategies. Importantly, this new model is under-
pinned by rigorous mathematical analysis, that under appropriate
conditions provides theoretical guarantees for convergence of the
presented algorithms.
CCS CONCEPTS
•Mathematics of computing →Probability and statistics ;•
Applied computing ;•Computing methodologies →Machine
learning ;
KEYWORDS
market simulation, pricing, signatures, rough path theory
ACM Reference Format:
Imanol Perez Arribas, Cristopher Salvi, and Lukasz Szpruch. 2018. Sig-SDEs
model for quantitative finance. . In 2020 ACM International Conference on
AI in Finance, October 15–16, 2020, NY. ACM, New York, NY, USA, 8 pages.
https://doi.org/10.1145/1122445.1122456
1 INTRODUCTION
The question of finding a parsimonious model that well represents
empirical data has been of paramount importance in quantitative
finance. The modelling choice is dictated by the desire to fit and
explain the available data, but is also subject to computational
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ICAIF-2020, October 15–16, 2020, NY
©2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/10.1145/1122445.1122456considerations. Inevitably, all models can only provide an approxi-
mation to reality, and the risk of using inadequate ones is hard to
detect. A classical approach consists in fixing a class of parametric
models, with a number of parameters that is significantly smaller
than the number of available data points. Next, in the process
called calibration, the goal is to solve a data-dependent optimiza-
tion problem yielding an optimal choice of model parameters. The
main challenge, of course, is to decide what class of models one
should choose from. The theory of statistical learning [ 28] tell us
that to simple models cannot fit the data, and to complex one are
not expected to generalise to unseen observations. In modern ma-
chine learning approaches, one usually starts by defining a highly
oveparametrised model from some universality class, exhibiting a
number of parameters often exceeding the number of data points,
and let (stochastic) gradient algorithms find the best configuration
of parameters yielding a calibrated model. In this work, we find
a middle ground between the two approaches. We develop a new
framework for systematic model selection that exhibits universal
approximation properties, and we provide a explicit solution to
the optimization used in its calibration, that completely removes
the need to deploy expensive gradient descent algorithms. Impor-
tantly the class of models that we consider builds upon classical
risk models that are well underpinned by research on quantitative
finance.
The mathematical object at the core of this work is the expected
signature of a path, whose properties are well-understood in the
field of stochastic analysis. It allows to identify a linear structure
underpinning the high non-linearity of the sequential data we work
with. This linear structure leads to a massive speed-up of calibra-
tion, pricing, and generation of future scenarios. Our approach
provides a new systematic model selection mechanism, that can
also be deployed to calibrate classical non-Markovian models in a
computationally efficient way. Signatures have been deployed to
solve various tasks in mathematical finance, such as options pricing
and hedging [ 22,23], high frequency optimal execution [ 4,14] and
others [ 12,24]. They have also been applied in several areas of
machine learning [6, 16, 19, 21, 29–34].
1.1 Sig-SDE Model
LetX:[0,T]→Rddenote the price process of an arbitrary finan-
cial asset under the pricing measure Q. To ensure the no-arbitrage
assumption is not violated, Xtypically is given by the solution of
the following Stochastic Differential Equation (SDE)
dXt=ΣtdWt,X0=x, (1)
where Wis a one-dimensional Brownian motion and Σtis an
adapted process (the volatility process). Model (1)accommodates
many standard risk models used e.g: the classical Black–ScholesarXiv:2006.00218v2  [q-fin.CP]  3 Jun 2020",2020-05-30T08:14:49Z,s maez arriva  oxford ala institute united kidom isis r sal vi  oxford ala institute united kidom ukasz zp su edinburgh ala institute united kidom matmatical i s s furtr and nally s imtantly matmatics probabity applied uti machine reference format maez arriva isis r sal vi ukasz zp sus iinternatnal conference nance ober new york t t misscopyrhts arai to  ober associatuti machinery inevitably next t t ii imp or t it  our mark iasnaturty s m rd note to typically stochastic dferential equatxt  wis brownish m schools ar  jun
paper_qf_13.pdf,2,Sig-SDEs model for quantitative finance,"  Mathematical models, calibrated to data, have become ubiquitous to make key
decision processes in modern quantitative finance. In this work, we propose a
novel framework for data-driven model selection by integrating a classical
quantitative setup with a generative modelling approach. Leveraging the
properties of the signature, a well-known path-transform from stochastic
analysis that recently emerged as leading machine learning technology for
learning time-series data, we develop the Sig-SDE model. Sig-SDE provides a new
perspective on neural SDEs and can be calibrated to exotic financial products
that depend, in a non-linear way, on the whole trajectory of asset prices.
Furthermore, we our approach enables to consistently calibrate under the
pricing measure $\\\\mathbb Q$ and real-world measure $\\\\mathbb P$. Finally, we
demonstrate the ability of Sig-SDE to simulate future possible market scenarios
needed for computing risk profiles or hedging strategies. Importantly, this new
model is underpinned by rigorous mathematical analysis, that under appropriate
conditions provides theoretical guarantees for convergence of the presented
algorithms.
","ICAIF-2020, October 15–16, 2020, NY Perez Arribas, Salvi and Szpruch
model assumes that volatility is proportional to the spot price, i.e.
Σt:=σXtwithσ∈Rconstant; the local volatility model assumes
thatΣt:=σ(t,Xt)Xt, whereσ(·,·)(called local volatility surface)
depends on both time and spot. Hence, it is a generalisation of the
Black–Scholes model; various stochastic volatility model assume
thatΣt:=σtXtwithσ2
tfollowing some diffusion process; the
SABR model chooses Σt:=σtXβ
t, withβ∈[0,1]and where σt
follows a diffusion process.
A natural question would be whether one can find a model for the
volatility process Σtthat is large enough to include all the classical
models such as the ones mentioned above and that would allow for
systematic a data driven model selection. We will require such a
model to satisfy the following requirements:
(1)Universality . The model should be able to approximate ar-
bitrarily well the dynamics of classical models.
(2)Efficient calibration . Given market prices for a family of
options, it should be possible to efficiently calibrate the model
so that it correctly prices the family of options.
(3)Fast pricing . Ideally, it should be possible to quickly price
(potentially exotic) options under the model without using
Monte Carlo techniques.
(4)Efficient simulation . Sampling trajectories from the model
should be computationally cheap and efficient.
An example of a model that satisfies point 1. above is a neural
network model , where the volatility process Σtis approximated by
a neural networkNNθ(t,(Ws)s∈[0,t])with parameters θ. Such a
model would be able to approximate a rich class of classical models.
However, the calibration and pricing of such models would involve
performing multiple Monte Carlo simulations on each epoch, which
might be expensive if done naively. See however, [7, 10].
The aim of this paper is to propose a model for asset price dynam-
ics that, we believe, satisfies all four points above. Our technique
models the volatility process Σtas
Σt=⟨ℓN,bW0,t⟩ (2)
where ℓNis the model parameters and bW0,tis the signature (c.f def-
inition 2.6) of the stochastic process bWt:=(t,Wt). The motivation
for choosing the signature as the main building block of this paper
is anchored in a very powerful result for universal approximation
of functions based on the celebrated Stone-Weierstrass Theorem that
we present next in an informal manner (for more technical details
see [8, Proposition 3])
Theorem 1.1. Consider a compact set Kof continuous Rd-valued
paths. Denote by Sthe function that maps a path Xfrom Kto its
signature X. Let f:K→Rbe any any continuous functions. Then,
for anyϵ>0and any path X∈K, there exists a linear function l∞
acting on the signature such that
||f(X)−⟨l∞,X⟩||∞<ϵ (3)
In other words, any continuous function on a compact set of paths
can be uniformly well approximated by a linear combination of
terms of the signature. This universal approximation property is
similar to the one provided by Neural Networks (NN). However, as
we will discuss below, NN models depend on a very large collec-
tion of parameters that need to be optimized via expensive back-
propagation-based techniques, whilst the optimization needed inour Sig-SDE model consists of a simple linear regression on the
terms of the signature. In this way, the signature can be thought of
as a feature map for paths that provides a linear basis for the space
of continuous functions on paths. In the setting of SDEs, sample
paths are Brownian and solutions are images of these sample tra-
jectories by a continuous functions that one wishes to approximate
from a set of observations. Our Sig-SDE model will rely upon the
universality of the signature to approximate such functions acting
on Brownian trajectories. Importantly, the signature of a realisa-
tion of a semimartingale provides a unique representation of the
sample trajectory [ 2,13]. Similarly, the expected signature – i.e. the
collection of the expectations of the iterated integrals – provides a
unique representation of the law of the semimartingale [5].
Note that model calibration is an example of generative modelling
[11,18]. Indeed, recall that if one knew prices of traded liquid
derivatives, then one can approximate the pricing measure from
market data [3, 22]. We denote this measure by Qreal.
We know that when equation (1)admits a strong solution then
there exists a measurable map G:R×C([0,T])→ C([0,T])such
that
X=G(x,(Ws)s∈[0,T]) (4)
as shown in [ 15, Corollary 3.23]. If Gtdenotes the projection of
Ggiven by Xt:=Gt(ξ,(Ws)s∈[0,t]), then one can view (1)as a
generative model that maps µ0supported on Rdinto(Gt)#µ0=
Qθ
t. Note that by construction Gis a casual transport map i.e a
transport map that is adapted to the filtration Ft[1]. In practice,
one is interested in finding such a transport map from a family of
parametrised functions Gθ. One then looks for a θsuch that Gθ
#µ0
is a good approximation of Qrealwith respect to a metric specified
by the user. In this paper the family of transport maps Gθis given
by linear functions on signatures (or linear functionals below).
2 NOTATION AND PRELIMINARIES
We begin by introducing some notation and preliminary results
that are used in this paper.
2.1 Multi-indices
Definition 2.1. Letd∈N. For any n≥0, we call an n-dimensional
d-multi-index anyn-tuple of non-negative integers of the form
K=(k1, . . . , kn)such that ki∈{1, . . . , d}for all i∈{1, . . . , n}. We
denote its length by |K|=n. The empty multi-index is denoted by
ø. We denote byIdthe set of all d-multi-indices, and by In
d⊂Id
the set of all d-multi-indices of length at most n∈N.
Definition 2.2 (Concatenation of multi-indices). LetI=(i1, . . . , ip)
andJ=(j1, . . . , jq)be any two multi-indices in Id. Their concate-
nation product⊗as the multi-index I⊗J=(i1, . . . , im,j1, . . . , jn)∈
Id.
Example 2.3.
(1)(1,3)⊗(2,2)=(1,3,2,2).
(2)(2,1,3)⊗(1)=(2,1,3,1).
(3)(2,2)⊗ø=(2,2).
2.2 Linear functionals
Definition 2.4 (Linear functional). For a given d≥1, alinear func-
tional is a (possibly infinite) sequence of real numbers indexed by",2020-05-30T08:14:49Z,ober ez arriva sal vi zp suxt with constant xt xt nce  schools xt with  universality t efcient givefast ially   efcient sampli aws su   t our is   t stone ierstrass torem proposittorem consir of rd note t from to  be ti neural networks s iibrownish our s brownish imtantly simarly note ined  real  ws corollary  gt notgivext gt ws rd into gt note is ft ione real with i multi nit for  t  id t iid nitconcatenat id tir id  linear nitlinear for
paper_qf_13.pdf,3,Sig-SDEs model for quantitative finance,"  Mathematical models, calibrated to data, have become ubiquitous to make key
decision processes in modern quantitative finance. In this work, we propose a
novel framework for data-driven model selection by integrating a classical
quantitative setup with a generative modelling approach. Leveraging the
properties of the signature, a well-known path-transform from stochastic
analysis that recently emerged as leading machine learning technology for
learning time-series data, we develop the Sig-SDE model. Sig-SDE provides a new
perspective on neural SDEs and can be calibrated to exotic financial products
that depend, in a non-linear way, on the whole trajectory of asset prices.
Furthermore, we our approach enables to consistently calibrate under the
pricing measure $\\\\mathbb Q$ and real-world measure $\\\\mathbb P$. Finally, we
demonstrate the ability of Sig-SDE to simulate future possible market scenarios
needed for computing risk profiles or hedging strategies. Importantly, this new
model is underpinned by rigorous mathematical analysis, that under appropriate
conditions provides theoretical guarantees for convergence of the presented
algorithms.
","Sig-SDEs model for quantitative finance. ICAIF-2020, October 15–16, 2020, NY
multi-indices inIdof the following form
F={F(K)∈R:K∈Id}. (5)
We note that a multi-index K∈Idis always a linear functional.
Both concatenation ⊗andxcan be extended by linearity to opera-
tions on linear functionals. We will now define two basic operations
on linear functionals that will be used throughout the paper.
Definition 2.5. For any two linear functionals F,Gand any real
numbersα,β∈Rdefine
αF+βG={αF(K)+βG(K)∈R:K∈Id} (6)
and
⟨F,G⟩=Õ
K∈IdF(K)G(K)∈R (7)
2.3 Signatures
Rough paths theory can be briefly described as a non-linear ex-
tension of the classical theory of controlled differential equations
which is robust enough to allow a deterministic treatment of sto-
chastic differential equations controlled by much rougher signals
than semi-martingales [26].
Definition 2.6 (Signature). LetX:[0,T]→Rdbe a continuous
semimartingale. The Signature of Xover a time interval [s,t]⊂
[0,T]is the linear functional Xs,t:={X(K)
s,t∈R:K∈Id}, such
thatX(ø)
s,t=1 and so that for any n≥1 and K=bK⊗a∈In
d, with
a∈{1, . . . , d}andbK∈In−1
dwe have
X(K)
s,t=∫t
sX(bK)
s,u◦dX(a)
u (8)
where the integral is to be interpreted in the Stratonovich sense.
Example 2.7. LetX:[0,T]→R2be a semimartingale.
(1)X(1)
s,t=X(1)
t−X(1)
s.
(2)X(1,2)
s,t=∫t
sX(1)
s,u◦dX(2)
u.
(3)X(2,2)
s,t=1
2(X(2)
s−X(2)
t)2.
A more detailed overview of signatures is included in Appendix A.
3 SIGNATURE MODEL
In this section we define the Signature Model for asset price dy-
namics that we propose in this paper. The goal is to approximate
the volatility process Σt(that is a continuous function on the driv-
ing Brownian path) by a linear functional on the signature of the
Brownian path.
Definition 3.1 (Signature Model). LetWbe a one-dimensional Brow-
nian motion. Let N∈Nbe the order of the Signature Model. The
Signature Model of parameter ℓ={ℓ(K):K∈IN
2}is given by
Σt:=⟨ℓ,bW0,t⟩, where bWdenotes the signature of Wadd-time. In
other words, the asset price dynamics are given by
dXt=⟨ℓ,bW0,t⟩dWt,X0=x∈R. (9)
We note that the Signature Model has two components: the hy-
perparameter N∈N, and the model parameter ℓ. Intuitively, the
hyperparameter Nplays a similar role to the width of a layer in aneural network. The larger this value is, the richer the range of mar-
ket dynamics the Signature Models can generate. Once the value
ofNis fixed, the challenge is to find a suitable model parameter
ℓ. Again, in analogy with neural networks, ℓplays the role of the
weights of the network.
The Signature Model possesses the universality property , in the
sense that given a classical model, there exists a Signature Model
that can approximate its dynamics to a given accuracy [20].
We show in the upcoming Sections 5-7 that (a) the Signature Model
is efficient to simulate, (b) it is efficient to calibrate, and (c) exotic
options can be priced fast under the Signature Model.
Remark 1. The Signature Model introduced in Definition 3.1 as-
sumes that the source of noise (i.e. the Brownian motion W) is one-
dimensional. This was done for simplicity, but the authors would
like to emphasise that the model generalises in a straightforward
way to multi-dimensional Brownian motion.
4 NUMERICAL EXPERIMENTS
We now demonstrate the feasibility of our methodology as outlined
in Sections 5-7. Throughout this section, we work with the Signature
Model
dXt=⟨ℓ,bW0,t⟩dWt,X0=1
with ℓ={ℓ(K):K∈IN
2}. We fix N=4. Therefore, the model has
1+2+22+23+24=31 parameters that need to be calibrated. We
also fix the terminal maturity T=1.
In this section we will show experiments for the calibration of the
model, pricing of options under the signature model and simula-
tion. Sections 5-7 will then include the technical details of how
calibration, pricing and simulation of signatures model are done.
4.1 Calibration
We assume that the family of options available on the market are a
mixture of vanilla and exotic options, given as follows:
•Vanilla call options with strikes K=0.5,0.6, . . . , 1,1.and
maturities t=0.4,0.45,0.5, . . . , 0.9,0.95,1:
Φ:=max(Xt−K,0).
•Variance options with strikes K=0.01,0.015, . . . , 0.035,0.04
and maturities t=0.4,0.45,0.5, . . . , 0.9,0.95,1:
Φ:=max(⟨X⟩t−K,0).
where⟨X⟩is the quadratic variation of X.
•Down-and-Out barrier call options with maturity 1, strikes
K=0.9,0.92,0.94, . . . , 1.01,1.03 and barrier levels L=
0.6,0.62,0.64, . . . , 0.88,0.9:
Φ:=(
max(Xt−K,0)if min s∈[0,t]Xs>L
0 else .
The option prices are generated from a Black-Scholes model with
volatilityσ=0.2:
dXt=σXtdWt.
The optimisation (14)was then solved to calibrate the model pa-
rameters ℓ={ℓ(K):K∈IN
2}.
Figure 1 shows the absolute error between the real option prices and
the option prices of the calibrated model, for the different option
types.",2020-05-30T08:14:49Z,s ober id of id  id is both  nitfor and ne id id snaturrough nitsnature  rd be t snature oxs id iistrata no ri   isnature mt brownish brownish nitsnature m be brow  be snature mt snature mnotadd ixt   snature mintuitively plays t snature mols once is ag articial intellence t snature msnature m sens snature msnature mremark t snature mnitbrownish  brownish  sens throughout snature mxt   trefore  isens calibrat vanla xt variance dowout xt xs t  schools xt td  t 
paper_qf_13.pdf,4,Sig-SDEs model for quantitative finance,"  Mathematical models, calibrated to data, have become ubiquitous to make key
decision processes in modern quantitative finance. In this work, we propose a
novel framework for data-driven model selection by integrating a classical
quantitative setup with a generative modelling approach. Leveraging the
properties of the signature, a well-known path-transform from stochastic
analysis that recently emerged as leading machine learning technology for
learning time-series data, we develop the Sig-SDE model. Sig-SDE provides a new
perspective on neural SDEs and can be calibrated to exotic financial products
that depend, in a non-linear way, on the whole trajectory of asset prices.
Furthermore, we our approach enables to consistently calibrate under the
pricing measure $\\\\mathbb Q$ and real-world measure $\\\\mathbb P$. Finally, we
demonstrate the ability of Sig-SDE to simulate future possible market scenarios
needed for computing risk profiles or hedging strategies. Importantly, this new
model is underpinned by rigorous mathematical analysis, that under appropriate
conditions provides theoretical guarantees for convergence of the presented
algorithms.
","ICAIF-2020, October 15–16, 2020, NY Perez Arribas, Salvi and Szpruch
Figure 1: Error analysis between the option prices of the real
model and the calibrated Signature Model.
4.2 Simulation
Once the Signature Model has been calibrated to the available
option prices, we can use Algorithm 1 to simulate realisations of
the calibrated Signature Model. Figure 2 shows 1,000 realisations
of the Signature Model.
4.3 Pricing
We will now use the calibrated Signature Model to price a new set of
options that was not used in the calibration step. This set of option
consists of Down-and-In barrier put options with barriers levels
L=0.7,0.71, . . . , 0.81,0.82 and strikes K=0.9,0.92, . . . , 1.01,1.03:
Φ:=(
max(K−Xt,0)if min s∈[0,t]Xs<L
0 else .
Figure 2: 1,000 realisations of the calibrated Signature
Model.
Figure 3: Error analysis between the option prices of the real
model and the calibrated Signature Model.
Figure 3 shows the absolute error of the prices under the Signature
Model, compared to the real prices.
As we see, the calibrated model is able to generate accurate prices
for these new exotic options. The error is highest when the barrier
is close to the strike price, as expected.
5 SIMULATION
This section will address the question of simulation efficiency of
Signature Models. We begin by stating the following two results.
The first result rewrites the differential equation (9)solely in terms
of the lead-lag signature of the Brownian motion, bWLL
0,t. HerebWLL
denotes the lead-lag transformation of bW, see Appendix B. We
use the lead-lag transformation because it allows us to rewrite
Itˆo integrals as certain Stratonovich integrals, which in turn can
be written as linear functions on signatures. The second result
guarantees that the computational cost of computing bWLL
0,tis the
same as the cost of computing {bWLL
0,s; 0≤s≤t}. These two
results lead to Algorithm 1, which provides an efficient algorithm
to sample from a Signature Model.",2020-05-30T08:14:49Z,ober ez arriva sal vi zp su error snature msimulatonce snature malgorithm snature m snature mprici  snature m dowixt xs  snature m error snature m snature mas t  snature mols  t brownish re   it strata no rit tse algorithm snature mol
paper_qf_13.pdf,5,Sig-SDEs model for quantitative finance,"  Mathematical models, calibrated to data, have become ubiquitous to make key
decision processes in modern quantitative finance. In this work, we propose a
novel framework for data-driven model selection by integrating a classical
quantitative setup with a generative modelling approach. Leveraging the
properties of the signature, a well-known path-transform from stochastic
analysis that recently emerged as leading machine learning technology for
learning time-series data, we develop the Sig-SDE model. Sig-SDE provides a new
perspective on neural SDEs and can be calibrated to exotic financial products
that depend, in a non-linear way, on the whole trajectory of asset prices.
Furthermore, we our approach enables to consistently calibrate under the
pricing measure $\\\\mathbb Q$ and real-world measure $\\\\mathbb P$. Finally, we
demonstrate the ability of Sig-SDE to simulate future possible market scenarios
needed for computing risk profiles or hedging strategies. Importantly, this new
model is underpinned by rigorous mathematical analysis, that under appropriate
conditions provides theoretical guarantees for convergence of the presented
algorithms.
","Sig-SDEs model for quantitative finance. ICAIF-2020, October 15–16, 2020, NY
Algorithm 1: Sampling from a Signature Model.
Parameters: D={ti}n
i=1with
0=t0<t1< . . . < tn−1<tn=T: sampling
times.
ℓ={ℓ(K):K∈IN
4}: Signature Model parameter.
x∈R: initial spot price.
Output: A sample path{Xtk}n
k=0from the Signature Model.
1Simulate a one-dimensional Brownian motion at the
sampling times{Wti}n
i=0.
2Apply the lead-lag transformation (25) to bWto obtain bWLL.
3bWLL
0,0←{F(K):K∈IN+1
4}with F(ø)=1 and F(K)=0 for K,ø.
4X0←x.
5fork=1, . . . , ndo
6 Compute the signature bWLL
tk−1,tk={bWLL ,(K)
tk−1,tk:K∈IN+1
4}.
7 Use Chen’s identity (Theorem 5.2) to compute the
signature bWLL
0,tk←{bWLL,K
0,tk:K∈IN+1
4}
8 Use proposition 5.1 to get Xtk←⟨x(ø)+ℓ⊗(4),bWLL
0,tk⟩.
9end
10return{Xtk}n
k=0.
Proposition 5.1 ([ 22, Lemma 3.11]). LetXfollow a Signature Model
with parameter ℓ={ℓ(K):K∈IN
2}. Then, Xis given by
Xt=⟨x(ø)+ℓ⊗(4),bWLL
0,t⟩ (10)
where ℓ⊗(4)={K⊗(4):K∈ℓ},x=X0∈R, andbWLLdenotes
the lead-lag transformation, introduced in Definition B.1, of the 2-
dimensional process bW=(t,Wt).
Theorem 5.2 (Chen’s identity, [ 25, Theorem 2.12]). Let0≤s≤t.
Then, for each multi-index K∈Idwe have
bWLL,(K)
0,t=Õ
I,J∈Id
I⊗J=KbWLL,(I)
0,t·bWLL,(J)
0,t(11)
where for any multi-index K∈Idwe used the notation bWLL,(K)
0,t=
⟨K,bWLL
0,t⟩.
These two results lead to Algorithm 1. We note there are a number
of publicly available software packages to compute signatures, such
asesig1,iisignature2andsignatory3.
6 PRICING
This section will show that exotic options can be priced fast under
a Signature Model. This will be done via a two step procedure.
First, it was shown in [ 22,23] that prices of exotic options can be
approximated with arbitrary precision by a special class of payoffs
called signature payoffs , defined below. Hence, we will assume that
the exotic option to be priced is a signature payoff, defined as
follows.
1https://pypi.org/project/esig/
2https://github.com/bottler/iisignature, [27]
3https://github.com/patrick-kidger/signatory, [17]Definition 6.1 (Signature payoffs). A signature payoff of maturity
T>0 and parameter f={f(K):K∈IN
3}is a payoff that pays at
time Tan amount given by ⟨f,bX0,T⟩.
Second, the price of a signature payoff is ⟨f,E[bX0,T]⟩. To price a
signature payoff, all we need is E[[]bX0,T], which doesn’t depend
on the signature payoff itself. In particular, it may be reused to price
other signature payoffs.
We now explicitly derive the expected signature E[[]bX0,T]in terms
of the model parameters and the expected signature of the lead-lag
Brownian motion E[[]bWLL
0,T].
Proposition 6.2. LetXbe a Signature Model of order N∈Nwith
parameter ℓ={ℓ(K):K∈ IN
2}. Consider the following linear
functionals P1=(1)andP2=ℓ⊗(4). Consider any multi-index
I=(i1, . . . , in)∈In
2such that n≤N. Then
X(I)
s,t=⟨CI(ℓ),bWLL
s,t⟩ (12)
where CI(ℓ)is given explicitly in closed-form by
CI(ℓ)=(. . .((Pi1≻Pi2)≻Pi3)≻. . .≻Pin) (13)
Proof. By Proposition 5.1 we know that if Xfollows a Signature
Model with parameter ℓ={ℓ(K):K∈IN
2}then
Xt=⟨x(ø)+ℓ⊗(4),bWLL
0,t⟩
LetI=(i1, . . . , in)be any multi-index in In
2such that n≤N. If
n=1 then I=(i1)and we necessarily one of the following two
options must hold
•Ifi1=1 then X(i1)
s,t=t−s=bWLL,(1)
s,t=⟨P1,bWLL
s,t⟩
•Ifi1=2 then X(i1)
s,t=Xt−Xs=⟨ℓ⊗(4),bWLL
s,t⟩=⟨P2,bWLL
s,t⟩.
Hence the statement holds for n=1. Let’s assume by induction that
the statement holds for any 1 ≤n≤N. We write I=J⊗(in)with
in∈{1,2}andJ=(i1, . . . , in−1)∈In−1
2. Clearly|(in)|,|J|<n,
therefore by induction hypothesis
X(in)
s,t=⟨C(in)(ℓ),bWLL
s,t⟩=⟨Pin,bWLL
s,t⟩
and
X(J)
s,t=⟨CJ(ℓ),bWLL
s,t⟩=⟨(. . .(Pi1≻Pi2)≻. . .≻Pin−1),bWLL
s,t⟩
By definition of the signature (see 2.6) we know that
X(I)
s,t=∫t
sX(J)
s,u◦dX(in)
u
=∫t
s⟨CJ(ℓ),bWLL
s,t⟩◦d⟨C(in)(ℓ),bWLL
s,t⟩
=⟨CJ(ℓ)≻C(in)(ℓ),bWLL
s,t⟩
=⟨(. . .(Pi1≻Pi2)≻. . .≻Pin−1)≻Pin,bWLL
s,t⟩
which concludes the induction. □",2020-05-30T08:14:49Z,s ober algorithm sampli snature mmeters snature moutput tk snature msimulate brownish  apply to ute use orem use tk tk propositlea  follow snature mtis xt notnit torem orem  tid  id kb id  tse algorithm   snature m rst nce nitsnature tasecond to i brownish proposit be snature mwith consir consir itpi pi pi piproof by propositfollows snature mxt  i   xt xs nce   i pipi pi piby pi pi pipin
paper_qf_13.pdf,6,Sig-SDEs model for quantitative finance,"  Mathematical models, calibrated to data, have become ubiquitous to make key
decision processes in modern quantitative finance. In this work, we propose a
novel framework for data-driven model selection by integrating a classical
quantitative setup with a generative modelling approach. Leveraging the
properties of the signature, a well-known path-transform from stochastic
analysis that recently emerged as leading machine learning technology for
learning time-series data, we develop the Sig-SDE model. Sig-SDE provides a new
perspective on neural SDEs and can be calibrated to exotic financial products
that depend, in a non-linear way, on the whole trajectory of asset prices.
Furthermore, we our approach enables to consistently calibrate under the
pricing measure $\\\\mathbb Q$ and real-world measure $\\\\mathbb P$. Finally, we
demonstrate the ability of Sig-SDE to simulate future possible market scenarios
needed for computing risk profiles or hedging strategies. Importantly, this new
model is underpinned by rigorous mathematical analysis, that under appropriate
conditions provides theoretical guarantees for convergence of the presented
algorithms.
","ICAIF-2020, October 15–16, 2020, NY Perez Arribas, Salvi and Szpruch
7 CALIBRATION
We will now address the task of calibrating a Signature Model.
We assume that the market has a family of options {Φi}n
i=1whose
market prices{pi}n
i=1are observable. Typically {Φi}n
i=1will contain
vanilla options, together with some exotic options such as various
variance or barrier products. Fix N∈Nbe the order of the Signature
Model. The challenge here is to find the model parameter ℓ={ℓ(K):
K∈IN
2}that best fits the data, in the sense that the prices of Φi,
under the Signature Model with parameter ℓ, are approximately
given by the observed market prices pi.
Following Section 6, we assume that the options Φiare given by
signature options. Therefore, we assume that we can write Φiby
Φi=⟨φi,bX0,T⟩,φi={φ(K)
i:K∈IN
2}.
The minimisation problem we aim to solve now is the following:
min
ℓ={ℓ(K):K∈IN
2}nÕ
i=1
⟨φi,E[bX0,T]⟩−pi2
. (14)
where E[bX0,T]is the expected signature of the Signature Model
with parameter ℓ={ℓ(K):K∈IN
2}.
By Proposition 6.2, the price of Φi, which is given by ⟨φi,E[bX0,T]⟩,
can be written as a polynomial on ℓ(K). Hence, the optimisation
(14)is rewritten as a minimisation of a polynomial of variables ℓ(K),
forK∈IN
2.
If the number of parameters ℓ(K)is large compared to the number
of available option prices, the optimisation problem might be over-
parametrised and there will be multiple solutions to (14). In this
case, we are in the robust finance setting where there are multiple
equivalent martingale measures that fit to the data. If the number
of parameters ℓ(K)is small, however, we are in the setting of classi-
cal mathematical finance modeling and there will in general be a
unique solution to (14).
8 CONCLUSION
In this paper we have proposed a new model for asset price dy-
namics called the signature model . This model was develop with the
objective of satisfying the following properties:
(1) Universality.
(2) Efficiency of calibration to vanilla and exotic options.
(3) Fast pricing of vanilla and exotic options.
(4) Efficiency of simulation.
Due to the rich properties of signatures, the signature model sat-
isfies all four properties and is, therefore, capable of generating
realistic paths without sacrificing the computational feasibility of
calibration, pricing and simulation.
Although this paper has focused on the risk-neutral measure Q, it
can also be used to learn the real-world measure P. One would first
calibrate to the risk-neutral measure Qand then learn the drift.
ACKNOWLEDGMENTS
This work was supported by The Alan Turing Institute under the
EPSRC grant EP/N510129/1.REFERENCES
[1]Beatrice Acciaio, Julio Backhoff-Veraguas, and Anastasiia Zalashko. 2019. Causal
optimal transport and its links to enlargement of filtrations and continuous-time
stochastic optimization. Stochastic Processes and their Applications (2019).
[2]Horatio Boedihardjo, Xi Geng, Terry Lyons, and Danyu Yang. 2016. The signature
of a rough path: uniqueness. Advances in Mathematics 293 (2016), 720–737.
[3]Douglas T Breeden and Robert H Litzenberger. 1978. Prices of state-contingent
claims implicit in option prices. Journal of business (1978), 621–651.
[4] ´Alvaro Cartea, Imanol Perez Arribas, and Leandro S ´anchez-Betancourt. 2020.
Optimal Execution of Foreign Securities: A Double-Execution Problem with
Signatures and Machine Learning. Available at SSRN (2020).
[5]Ilya Chevyrev, Terry Lyons, et al .2016. Characteristic functions of measures on
geometric rough paths. The Annals of Probability 44, 6 (2016), 4049–4082.
[6]Ilya Chevyrev and Harald Oberhauser. 2018. Signature moments to characterize
laws of stochastic processes. arXiv preprint arXiv:1810.10971 (2018).
[7]Christa Cuchiero, Wahid Khosrawi, and Josef Teichmann. 2020. A generative
adversarial network approach to calibration of local stochastic volatility models.
arXiv preprint arXiv:2005.02505 (2020).
[8]Adeline Fermanian. 2019. Embedding and learning with signatures. arXiv preprint
arXiv:1911.13211 (2019).
[9]Guy Flint, Ben Hambly, and Terry Lyons. 2016. Discretely sampled signals and
the rough Hoff process. Stochastic Processes and their Applications 126, 9 (2016),
2593–2614.
[10] M. Siska D. Szpruch L. Gierjatowicz P., Sabate-Vidales. 2020. Robust Pricing and
Hedging with neural SDEs. to appear (2020).
[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
nets. In Advances in neural information processing systems . 2672–2680.
[12] Lajos Gergely Gyurk ´o, Terry Lyons, Mark Kontkowski, and Jonathan Field. 2013.
Extracting information from the signature of a financial data stream. arXiv
preprint arXiv:1307.7244 (2013).
[13] Ben Hambly and Terry Lyons. 2010. Uniqueness for the signature of a path of
bounded variation and the reduced path group. Annals of Mathematics (2010),
109–167.
[14] Jasdeep Kalsi, Terry Lyons, and Imanol Perez Arribas. 2020. Optimal execution
with rough path signatures. SIAM Journal on Financial Mathematics 11, 2 (2020),
470–493.
[15] Ioannis Karatzas and Steven Shreve. 2012. Brownian motion and stochastic calculus.
Vol. Vol. 113 . Springer Science & Business Media.
[16] Patrick Kidger, Patric Bonnier, Imanol Perez Arribas, Cristopher Salvi, and Terry
Lyons. 2019. Deep Signature Transforms. In Advances in Neural Information
Processing Systems . 3099–3109.
[17] Patrick Kidger and Terry Lyons. 2020. Signatory: differentiable computations of
the signature and logsignature transforms, on both CPU and GPU. arXiv preprint
arXiv:2001.00706 (2020).
[18] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.
arXiv preprint arXiv:1312.6114 (2013).
[19] Franz J Kir ´aly and Harald Oberhauser. 2016. Kernels for sequentially ordered
data. arXiv preprint arXiv:1601.08169 (2016).
[20] Daniel Levin, Terry Lyons, and Hao Ni. 2013. Learning from the past, predict-
ing the statistics for the future, learning an evolving system. arXiv preprint
arXiv:1309.0260 (2013).
[21] Chenyang Li, Xin Zhang, and Lianwen Jin. 2017. Lpsnet: A novel log path
signature feature based hand gesture recognition framework. In Proceedings of
the IEEE International Conference on Computer Vision Workshops . 631–639.
[22] Terry Lyons, Sina Nejad, and Imanol Perez Arribas. 2019. Nonparametric pricing
and hedging of exotic derivatives. arXiv preprint arXiv:1905.00711 (2019).
[23] Terry Lyons, Sina Nejad, and Imanol Perez Arribas. 2020. Numerical Method
for Model-free Pricing of Exotic Derivatives in Discrete Time Using Rough Path
Signatures. Applied Mathematical Finance (2020), 1–15.
[24] Terry Lyons, Hao Ni, and Harald Oberhauser. 2014. A feature set for streams and
an application to high-frequency financial tick data. In Proceedings of the 2014
International Conference on Big Data Science and Computing . 1–8.
[25] Terry J Lyons. 1998. Differential equations driven by rough signals. Revista
Matem ´atica Iberoamericana 14, 2 (1998), 215–310.
[26] Terry J Lyons, Michael Caruana, and Thierry L ´evy. 2007. Differential equations
driven by rough paths . Springer.
[27] Jeremy F Reizenstein and Benjamin Graham. 2020. Algorithm 1004: The iisigna-
ture library: Efficient calculation of iterated-integral signatures and log signatures.
ACM Transactions on Mathematical Software (TOMS) 46, 1 (2020), 1–21.
[28] Vladimir Vapnik. 2013. The nature of statistical learning theory . Springer science
& business media.
[29] Zecheng Xie, Zenghui Sun, Lianwen Jin, Hao Ni, and Terry Lyons. 2017. Learning
spatial-semantic context with fully convolutional recurrent network for online
handwritten chinese text recognition. IEEE transactions on pattern analysis and
machine intelligence 40, 8 (2017), 1903–1917.",2020-05-30T08:14:49Z,ober ez arriva sal vi zp su snature m typically x be snature mt snature mfollowi setrefore t snature mby propositnce  i i universality efciency fast efciency due although one and  t ala institute atrice cci articial intellence jul back off v guas astase zal ask causal stochastic processapplicatns rat boe di hard jo xi e terry lns any ya t advancmatmatics douglas breed erobert litzenberger pricjournal lvaro carte maez arriva aledro betancourt optimal executforesecuritidouble executproblem snaturmachine learni  articial intellence  a ty rev terry lns charaeristic t annals probabity a ty rev rald ouser snature   christ uzero said  raw josef eichman  anine germaiaembeddi   guy flint beham by terry lns disete ly off stochastic processapplicatns risk zp sutier at owicz bate vida lrobust prici edgi iagoodfellow  po get aba die media mira bi xu did war  barley r o oz articial intellence oco vle yo sha be   gentive iadvancla jos ger ely gyu rk terry lns mark t  jonathaeld extrai   beham by terry lns uniqueness annals matmatics as ep kal si terry lns maez arriva optimal journal nancial matmatics loais kara tz as stevehr eve brownish vv science business media patrick sier parti bonnie maez arriva isis r sal vi terry lns ep snature transforms iadvancneural informatprocessi tems patrick sier terry lns snatory   died erik ki ma max selli auto   franz kir rald ouser kernels   daniel  terry lns hao ni learni   cya li i liailps net iproceedis internatnal conference uter visworkshops terry lns ine ad maez arriva nometric   terry lns ine ad maez arriva numerical method mprici exotic rivativdisete time usi rough path snaturapplied matmatical nance terry lns hao ni rald ouser iproceedis internatnal conference b data science uti terry lns dferential revista mate libero terry lns michael car athierry dferential   rei zesteibenjamraham algorithm t efcient transans matmatical software vladimir va  t  te die e hui suliaihao ni terry lns learni
paper_qf_13.pdf,7,Sig-SDEs model for quantitative finance,"  Mathematical models, calibrated to data, have become ubiquitous to make key
decision processes in modern quantitative finance. In this work, we propose a
novel framework for data-driven model selection by integrating a classical
quantitative setup with a generative modelling approach. Leveraging the
properties of the signature, a well-known path-transform from stochastic
analysis that recently emerged as leading machine learning technology for
learning time-series data, we develop the Sig-SDE model. Sig-SDE provides a new
perspective on neural SDEs and can be calibrated to exotic financial products
that depend, in a non-linear way, on the whole trajectory of asset prices.
Furthermore, we our approach enables to consistently calibrate under the
pricing measure $\\\\mathbb Q$ and real-world measure $\\\\mathbb P$. Finally, we
demonstrate the ability of Sig-SDE to simulate future possible market scenarios
needed for computing risk profiles or hedging strategies. Importantly, this new
model is underpinned by rigorous mathematical analysis, that under appropriate
conditions provides theoretical guarantees for convergence of the presented
algorithms.
","Sig-SDEs model for quantitative finance. ICAIF-2020, October 15–16, 2020, NY
[30] Weixin Yang, Lianwen Jin, and Manfei Liu. 2015. Chinese character-level writer
identification using path signature feature, DropStroke and deep CNN. In 2015
13th International Conference on Document Analysis and Recognition (ICDAR) .
IEEE, 546–550.
[31] Weixin Yang, Lianwen Jin, and Manfei Liu. 2016. Deepwriterid: An end-to-end
online text-independent writer identification system. IEEE Intelligent Systems 31,
2 (2016), 45–53.
[32] Weixin Yang, Lianwen Jin, Hao Ni, and Terry Lyons. 2016. Rotation-free online
handwritten character recognition using dyadic path signature features, hanging
normalization, and deep neural network. In 2016 23rd International Conference on
Pattern Recognition (ICPR) . IEEE, 4083–4088.
[33] Weixin Yang, Lianwen Jin, Dacheng Tao, Zecheng Xie, and Ziyong Feng. 2016.
DropSample: A new training method to enhance deep convolutional neural net-
works for large-scale unconstrained handwritten Chinese character recognition.
Pattern Recognition 58 (2016), 190–203.
[34] Weixin Yang, Terry Lyons, Hao Ni, Cordelia Schmid, Lianwen Jin, and Jiawei
Chang. 2017. Leveraging the path signature for skeleton-based human action
recognition. arXiv preprint arXiv:1707.03993 (2017).
A OVERVIEW OF SIGNATURES
In this section we state some of the main properties of signatures
that are used in this paper.
Definition A.1 (Shuffle of multi-indices). For any two multi-indices
I,J∈Idand 1-dimensional multi-indices a,b∈I1
d={1, . . . , d}
we define the shuffle product xrecursively as follows:
øxI=Ixø=I (15)
and
(I⊗a)x(J⊗b)=((I⊗a)xJ)⊗b+(Ix(J⊗b))⊗a (16)
Example A.2. We have the following examples for I4:
(1)(1,2)x(3)=(1,2,3)+(1,3,2)+(3,1,2).
(2)(1,2)x(3,4)=2·(1,2,2,4)+(1,2,4,2)+(2,1,2,4)+(2,1,4,2)+
(2,4,1,2).
(3)(2,1)xø=(2,1).
(4) øx(2,1)=(2,1).
Proposition A.3 (Shuffle identity). LetX:[0,T]→Rdbe a
continuous semimartingale. For any two multi-indices I,J∈Idthe
following identity on the Signature of Xholds
⟨IxJ,Xs,t⟩=⟨I,Xs,t⟩·⟨J,Xs,t⟩:=X(I)
s,t·X(J)
s,t(17)
Proof. Theorem 2.15 in [26]. □
Proposition A.4 (Uniqeness of the Signature). LetX:[0,T]→
Rd,Y:[0,T]→Rdbe two continuous semimartingales. Then
∀t∈[0,T],Xt=Yt⇐⇒ ∀K∈Id,X(K)
s,t=Y(K)
s,t(18)
Proof. See main result in [13]. □
Proposition A.5 (Factorial decay). Given a semimartingale X:
[0,T]→Rd, for any time interval [s,t]⊂[ 0,T]and any multi-index
K∈Idsuch that|K|=n
X(K)
s,t=O1
n!
(19)
Proof. Proposition 2.2 in [26]. □
Definition A.6. For a given time interval [0,T]we call a contin-
uous, surjective, increasing function ψ:[0,T]→[ 0,T]a time-
reparametrization.Proposition A.7 (Invariance to time-reparametrizations). Let
X:[0,T]→Rdbe a semimartingale and ψ:[0,T]→[ 0,T]be a
time-reparametrization. Then the Signature of Xhas the following
invariance property
Xs,t=Xψ(s),ψ(t)∀s,t∈[0,T]such that s<t (20)
Definition A.8 (Half-Shuffle). LetFandGbe any two linear function-
als. We define their half-shuffle product ≻onXs,tas the following
(Stratonovich) iterated integral on the real line
⟨F≻G,Xs,t⟩=∫t
s⟨F,Xs,u⟩◦d⟨G,Xs,u⟩ (21)
LetBbe a 2-dimensional Brownian motion, defined for example
on the interval[0,1]. Consider two linear functionals F={F(K):
K∈Id}andG={G(K):K∈Id}defined as
F(K)=1 if K=(1,2)
0 otherwise(22)
and
G(K)=1 if K=(2,1)
0 otherwise(23)
Then the following quantity
As,t=1
2
F≻G−G≻F,Bs,t
(24)
is the Levy area of the Brownian motion Bon[s,t]⊂[ 0,1].
A.0.1 Expected signature. We will now define the expected signa-
ture of a semimartingale.
Definition A.9 (Expected signature). LetX:[0,T]→Rdbe a con-
tinuous semimartingale, and let Xs,t={X(K)
s,t∈R:K∈Id}be its
signature. The expected signature of Xis defined by
E[Xs,t]:={E[X(K)
s,t]∈R:K∈Id}.
The expected signature – i.e. the expectation of the iterated integrals
(8)– behaves analogously to the moments of random variables, in
the sense that under certain assumptions it characterises the law
of the stochastic process:
Theorem A.10 ([ 5]).LetX:[0,T]→Rdbe a semimartingale. Then,
under certain assumptions (see [ 5]) the expected signature E[X0,T]
characterises the law of X.
B TIME AND LEAD-LAG TRANSFORMATION
The invariance of the signature of a semimartingale to time
reparametrizations allows to handle irregularly sampled sample
paths (prices etc.) by completely eliminating the need to retain in-
formation about the original time-parametrization. Nonetheless, for
the pricing of many options, especially ones resulting from payoffs
calculated pathwise (such as integrals for American options), the
time represents an important information that we are required to
retain. To do so it suffices to augment the state space of the input
semimartingale Xby adding time tas an extra dimension to get
Xadd-time
t=(t,Xt).
We report another basic transformation that can be applied to
semimartingales and that will be useful in the sequel of the paper:
the lead-lag transformation. This transformation allows us to write
Itˆo integrals as linear functions on the signature of the lead-lag
transformed path.",2020-05-30T08:14:49Z,s ober  ix iya liaima  chinese drstroke iinternatnal conference document analysis renit ix iya liaima  ep write rid aintellent tems  ix iya liaihao ni terry lns rotatiinternatnal conference paerrenit ix iya liaida c tao te die zi yo fe drsample chinese paerrenit ix iya terry lns hao ni corlia schmidt liaijia i  gi   initshuffle for id and ix ix   propositshuffle  rd be for id t snature holds ix xs xs xs proof torem proposituni qe ness snature  rd rd be txt id proof  propositfaoriverd id suproof propositnitfor propositivariance  rd be tsnature has xs nithalf shuffle  and be  xs strata no rixs xs xs  be brownish consir id id tas bs levy brownish boeeed  niteeed  rd be xs id t is xs id t torem  rd be tt nonetless to by add xt   it
paper_qf_13.pdf,8,Sig-SDEs model for quantitative finance,"  Mathematical models, calibrated to data, have become ubiquitous to make key
decision processes in modern quantitative finance. In this work, we propose a
novel framework for data-driven model selection by integrating a classical
quantitative setup with a generative modelling approach. Leveraging the
properties of the signature, a well-known path-transform from stochastic
analysis that recently emerged as leading machine learning technology for
learning time-series data, we develop the Sig-SDE model. Sig-SDE provides a new
perspective on neural SDEs and can be calibrated to exotic financial products
that depend, in a non-linear way, on the whole trajectory of asset prices.
Furthermore, we our approach enables to consistently calibrate under the
pricing measure $\\\\mathbb Q$ and real-world measure $\\\\mathbb P$. Finally, we
demonstrate the ability of Sig-SDE to simulate future possible market scenarios
needed for computing risk profiles or hedging strategies. Importantly, this new
model is underpinned by rigorous mathematical analysis, that under appropriate
conditions provides theoretical guarantees for convergence of the presented
algorithms.
","ICAIF-2020, October 15–16, 2020, NY Perez Arribas, Salvi and Szpruch
Figure 4: Lead-lag transformation of a Brownian motion.
Definition B.1 (Lead-lag transformation). LetZ:[0,T]→Rdbe a
semimartingale. For each partition D={ti}i⊂[0,T]of mesh size
|D|, define the piecewise linear path ZD:[0,T]→R2dgiven by
ZD
2kT/2n:=(Ztk,Ztk), (25)
ZD
(2k+1)T/2n:=(Ztk,Ztk+1) (26)
and linear interpolation in between. Figure 4 shows the lead-lag
transformation of a Brownian motion. As we see, the lead compo-
nent leads the lag component, hence the name. The lead component
can be seen as the future of the path, and the lag component as the
past.
Denote by ZDthe signature of ZD. Then, we define the lead-lag
transformation of Z, denoted by ZLL, as the limit of signatures of
ZD:
ZLL:=lim
|D|→0ZD.
The work in [ 9] showed the convergence of this limit and studied
some of its properties.
B.1 Expected signature of the lead-lag
Brownian motion
Definition B.2. LetI=(i1, . . . , in)∈In
3be a multi-index. We denote
byP(I)the set of all possible tuples of non-empty multi-indices
fromIn−1
3such that their concatenation is equal to Iand their
length doesn’t exceed 2, i.e.
P(I)={(I1, . . . , Ik)∈(In−1
3)k:I1⊗. . .⊗Ik=Iand|Ij|∈{1,2}}
Example B.3.
(1)P((1,2,3))={(1,2,3),(1,(2,3)),((1,2),3)}.
(2)P((1,3,2,2))={(1,3,2,2),(1,(3,2),2),(1,3,(2,2)),
((1,3),2,2),((1,3),(2,2))}(3)P((3,2))={(3,2),((3,2))}.
Definition B.4 (Exponential of a linear functional). LetF={F(K)∈
R:K∈Id}be a linear functional. We define the exponential ofF
as the following linear functional
exp(F)=(ø)+∞Õ
k=11
k!F⊗k(27)
where for any k≥1,F⊗k=F⊗. . .⊗Fforktimes.
Proposition B.5. Define the function α:I3→I 3that maps a
multi-index to another multi-index in the following way: ∀I∈I3
α(I)= 
(1) ifI=(1)
(2) ifI∈{(2),(3)}
−1
2(1)ifI=(2,3)
1
2(1) ifI=(3,2)
0·(ø)otherwise(28)
Given a final time Tdefine the linear functional ET:=exp(T+
T
2(2,2)). Then we have the explicit closed-form expression for the
Expected Signature of the lead-lag Brownian motion: given any multi-
index I∈I3
Eh
bWLL,(I)
0,Ti
=Õ
(I1,...,Ik)∈P( I)⟨α(I1)⊗. . .⊗α(Ik),ET⟩ (29)
Proof. Follows from [ 22, Lemma B.1] and the fact that E[bW0,T]=
ET. □
Example B.6. IfI=(3,2,3),P(I)={(3,2,3),((3,2),3),(3,(2,3))}.
Hence,
Eh
bWLL,(I)
0,Ti
=⟨α(3)⊗α(2)⊗α(3),ET⟩
+α((3,2))⊗α(3),ET⟩
+α(3)⊗α((2,3)),ET⟩
=E(2,2,2)
T+1
2E(1,2)
T−1
2E(2,1)
T.",2020-05-30T08:14:49Z,ober ez arriva sal vi zp su lead brownish nitlead  rd be for tk tk tk tk  brownish as t note t tt eeed brownish nit i iand ik iik and ij  niteonential  id  fork timpropositne givene teeed snature brownish eh ti ik ik proof follows lea   nce eh ti
paper_qf_14.pdf,1,"The Transport-based Mesh-free Method (TMM) and its applications in
  finance: a review","  We review a numerical technique, referred to as the Transport-based Mesh-free
Method (TMM), and we discuss its applications to mathematical finance. We
recently introduced this method from a numerical standpoint and investigated
the accuracy of integration formulas based on the Monte-Carlo methodology:
quantitative error bounds were discussed and, in this short note, we outline
the main ideas of our approach. The techniques of transportation and
reproducing kernels lead us to a very efficient methodology for numerical
simulations in many practical applications, and provide some light on the
methods used by the artificial intelligence community. For applications in the
finance industry, our method allows us to compute many types of risk measures
with an accurate and fast algorithm. We propose theoretical arguments as well
as extensive numerical tests in order to justify sharp convergence rates,
leading to rather optimal computational times. Cases of direct interest in
finance support our claims and the importance of the problem of the curse of
dimensionality in finance applications is briefly discussed.
","The Transport-based Mesh-free Method (TMM): a review
Philippe G. LeFlochand Jean-Marc Merciery
October 2019
Abstract
We review a numerical technique, referred to as the Transport-based Mesh-free Method (TMM),
and we discuss its applications. We recently introduced this method from a numerical standpoint and
investigated the accuracy of integration formulas based on the Monte-Carlo methodology: quantitative
error bounds were discussed and, in this short note, we outline the main ideas of our approach.
The techniques of transportation and reproducing kernels lead us to a very ecient methodology for
numerical simulations in many practical applications, and provide some light on the methods used by
the articial intelligence community. For applications in the nance industry, our method allows us
to compute many types of risk measures with accurate and fast algorithms. We propose theoretical
arguments as well as extensive numerical tests in order to justify sharp convergence rates, leading to
rather optimal computational times. Cases arising in nance applications support our claims and,
nally, the problem of the curse of dimensionality in nance is briey discussed.
1 Introduction
Relying on our recent papers [11]{[13], we present
and discuss here a numerical technique, that we
refer to as the Transport-based Mesh-free Method
(TMM), which is of direct interest in numerical sim-
ulations. Our method is mesh-free (cf. for instance
[7, 16]) and somewhat similar to a Lagrangian mesh-
free method. Importantly, our method can handle
transport as well as diusive terms and was intro-
duced rst in [11].
Our motivation was to reduce as much as pos-
sible the algorithmic burden of solving partial dif-
ferential equations (PDEs) especially for problems
in large dimensions, met for instance in mathemat-
ical nance and machine learning. Computational
times reect, in a concrete manner, the complexity
of an algorithm. For PDEs solvers, the algorithmic
complexity can be measured by establishing suitable
error estimates. For our TMM approach, in [12] we
were able to establish some Monte-Carlo type error
estimates, at least via heuristic arguments, as we
outline below in Section 2.
This allowed to perform a precise error analysis
of this method in [13], which is outlined in Section 5.Finally, in Section 6 we discuss the limitations com-
ing from the curse of dimensionality for applications
to nance.
The TMM methodology has wide applications in
mathematical nance, since it allows one to compute
almost any risk measures, quite accurately and with
a fast algorithm. A risk measure is here understood
as a price, future prices, future sensitivities, Value
at Risk (VaR), or Counterparty Value Adjustment
(CVA), and may concern a simple asset, a complex
derivative or an investment strategy, as well as a big
portfolio of such instruments; they can be written
on any number of underlyings, themselves depend-
ing on any Markov-type stochastic processes.
The proposed method was extensively tested in
mathematical nance ones; see [11]-[13] as well as
[14] for a business case in asset and liability man-
agement using the so-called Libor market model [2].
Another business case, for front-oce equity deriva-
tives, was treated using this method: it consists in
computing metrics for specic customer needs for
big portfolios of autocalls, that are useful for pre-
sales purposes. The modeling of shares uses the
Buelher dividend models [3], and the algorithm de-
scribed in [11] for local volatility calibration. For an
Laboratoire Jacques-Louis Lions, Centre National de la Recherche Scientique, Sorbonne Universit e, 4 Place Jussieu,
75252 Paris, France. Email: contact@philippelefloch.org .
yMPG-Partners, 136 Boulevard Haussmann, 75008 Paris, France. Email: jean-marc.mercier@mpg-partners.com.
1arXiv:1911.00992v2  [math.AP]  17 Nov 2019",2019-11-03T23:38:32Z,t transt mesh method phippe le lo marc mercer ober abstra  transt mesh method    t for  casintrodurelyi transt mesh method our lagraiaimtantly our tnal for for   se senally set value risk va counter party value adjustment mark t lib or anotr t uel r for labor to ire jac louis lns centre natnal recrc cie nti sorbonne  place aussie paris france em articial intellence partners levard haussmanparis france em articial intellence  nov
paper_qf_14.pdf,2,"The Transport-based Mesh-free Method (TMM) and its applications in
  finance: a review","  We review a numerical technique, referred to as the Transport-based Mesh-free
Method (TMM), and we discuss its applications to mathematical finance. We
recently introduced this method from a numerical standpoint and investigated
the accuracy of integration formulas based on the Monte-Carlo methodology:
quantitative error bounds were discussed and, in this short note, we outline
the main ideas of our approach. The techniques of transportation and
reproducing kernels lead us to a very efficient methodology for numerical
simulations in many practical applications, and provide some light on the
methods used by the artificial intelligence community. For applications in the
finance industry, our method allows us to compute many types of risk measures
with an accurate and fast algorithm. We propose theoretical arguments as well
as extensive numerical tests in order to justify sharp convergence rates,
leading to rather optimal computational times. Cases of direct interest in
finance support our claims and the importance of the problem of the curse of
dimensionality in finance applications is briefly discussed.
","application to nonlinear propagation, see [10]-[13].
2 Monte-Carlo-type strategy
We postpone the discussion of earlier references at
end of this section and outline now our strategy for
deriving a priori error estimates on multidimensional
integrals. One of our task is to investigate the valid-
ity of Monte-Carlo-type error estimates of the form
Z
RD'(x)d",2019-11-03T23:38:32Z,   one  
paper_qf_14.pdf,3,"The Transport-based Mesh-free Method (TMM) and its applications in
  finance: a review","  We review a numerical technique, referred to as the Transport-based Mesh-free
Method (TMM), and we discuss its applications to mathematical finance. We
recently introduced this method from a numerical standpoint and investigated
the accuracy of integration formulas based on the Monte-Carlo methodology:
quantitative error bounds were discussed and, in this short note, we outline
the main ideas of our approach. The techniques of transportation and
reproducing kernels lead us to a very efficient methodology for numerical
simulations in many practical applications, and provide some light on the
methods used by the artificial intelligence community. For applications in the
finance industry, our method allows us to compute many types of risk measures
with an accurate and fast algorithm. We propose theoretical arguments as well
as extensive numerical tests in order to justify sharp convergence rates,
leading to rather optimal computational times. Cases of direct interest in
finance support our claims and the importance of the problem of the curse of
dimensionality in finance applications is briefly discussed.
","Throughout, we are given a convex and open
set 
RDwhich is assumed to have a piecewise
smooth boundary and, typically, we will take [0 ;1]D.
We observe that, using a transportation argument,
it is sucient take in (2.1) the Lebesgue measure
=dx
on 
. Namely, if S: 
7!RDis a transport
map for a general measure , that is, the unique map
satisfyingR
RD'd=R

('S)dxfor any continuous
'2L1
(RD), together with S=rh,hconvex and
rthe gradient operator. Indeed, using such a map,
(2.1) can be written as
Z

('S)dx",2019-11-03T23:38:32Z,throughout whi le be gue namely is ined
paper_qf_14.pdf,4,"The Transport-based Mesh-free Method (TMM) and its applications in
  finance: a review","  We review a numerical technique, referred to as the Transport-based Mesh-free
Method (TMM), and we discuss its applications to mathematical finance. We
recently introduced this method from a numerical standpoint and investigated
the accuracy of integration formulas based on the Monte-Carlo methodology:
quantitative error bounds were discussed and, in this short note, we outline
the main ideas of our approach. The techniques of transportation and
reproducing kernels lead us to a very efficient methodology for numerical
simulations in many practical applications, and provide some light on the
methods used by the artificial intelligence community. For applications in the
finance industry, our method allows us to compute many types of risk measures
with an accurate and fast algorithm. We propose theoretical arguments as well
as extensive numerical tests in order to justify sharp convergence rates,
leading to rather optimal computational times. Cases of direct interest in
finance support our claims and the importance of the problem of the curse of
dimensionality in finance applications is briefly discussed.
","xyzMatern xykMaternFigure 3.1: Periodic/transported Mat ern kernels
For both localization techniques, we can approx-
imate the sharp discrepancy sequences in view of
(2.5). For instance, Figure (3.2) shows three distri-
butions in the two-dimensional case: the rst one
is a random Mersenne Twister sequece (MT19997);
the second one is a sequence approximating the
sharp discrepancy one for the lattice-based Mat ern
kernel (from the left-hand side of Figure (3.1)); the
third one corresponds to the transported Gaussian
kernel and is designed from the Gaussian kernel
K(x;y) = exp(",2019-11-03T23:38:32Z,mater mater  dic mat for for  sonne twier mat  n
paper_qf_14.pdf,5,"The Transport-based Mesh-free Method (TMM) and its applications in
  finance: a review","  We review a numerical technique, referred to as the Transport-based Mesh-free
Method (TMM), and we discuss its applications to mathematical finance. We
recently introduced this method from a numerical standpoint and investigated
the accuracy of integration formulas based on the Monte-Carlo methodology:
quantitative error bounds were discussed and, in this short note, we outline
the main ideas of our approach. The techniques of transportation and
reproducing kernels lead us to a very efficient methodology for numerical
simulations in many practical applications, and provide some light on the
methods used by the artificial intelligence community. For applications in the
finance industry, our method allows us to compute many types of risk measures
with an accurate and fast algorithm. We propose theoretical arguments as well
as extensive numerical tests in order to justify sharp convergence rates,
leading to rather optimal computational times. Cases of direct interest in
finance support our claims and the importance of the problem of the curse of
dimensionality in finance applications is briefly discussed.
","(4.2) dened in the sense of distributions must be
considered, since the initial data is a Dirac mass.
The (vector-valued) dual of the Fokker-Planck
equation is the Kolmogorov equation , also known
in mathematical nance as the Black and Scholes
equations . For an unknown P=P(t;x) withts
reads
@tP",2019-11-03T23:38:32Z,dfrac t former  lmogorov  schools for
paper_qf_14.pdf,6,"The Transport-based Mesh-free Method (TMM) and its applications in
  finance: a review","  We review a numerical technique, referred to as the Transport-based Mesh-free
Method (TMM), and we discuss its applications to mathematical finance. We
recently introduced this method from a numerical standpoint and investigated
the accuracy of integration formulas based on the Monte-Carlo methodology:
quantitative error bounds were discussed and, in this short note, we outline
the main ideas of our approach. The techniques of transportation and
reproducing kernels lead us to a very efficient methodology for numerical
simulations in many practical applications, and provide some light on the
methods used by the artificial intelligence community. For applications in the
finance industry, our method allows us to compute many types of risk measures
with an accurate and fast algorithm. We propose theoretical arguments as well
as extensive numerical tests in order to justify sharp convergence rates,
leading to rather optimal computational times. Cases of direct interest in
finance support our claims and the importance of the problem of the curse of
dimensionality in finance applications is briefly discussed.
","the volatility process tandx-axis the values of the
interest rates Ft.
Figure 5.1: SABR at time 0.02, 2 and 12. N=200.
Step 2: the backward computation. Once
the sharp discrepancy sequence is computed, we are
in a position to solve the Kolmogorov equation (4.4),
usingt7!yn(t) (withn= 1;:::;N ) as a moving
transported grid, again using the numerical scheme
in [11]. This scheme provides us with an approxima-
tion which is consistent with the Kolmogorov equa-
tion and, in view of (5.1) we see that this scheme
enjoys the error estimate
Z
RDP(t;)d(t;)",2019-11-03T23:38:32Z,ft  step once lmogorov  lmogorov
paper_qf_14.pdf,7,"The Transport-based Mesh-free Method (TMM) and its applications in
  finance: a review","  We review a numerical technique, referred to as the Transport-based Mesh-free
Method (TMM), and we discuss its applications to mathematical finance. We
recently introduced this method from a numerical standpoint and investigated
the accuracy of integration formulas based on the Monte-Carlo methodology:
quantitative error bounds were discussed and, in this short note, we outline
the main ideas of our approach. The techniques of transportation and
reproducing kernels lead us to a very efficient methodology for numerical
simulations in many practical applications, and provide some light on the
methods used by the artificial intelligence community. For applications in the
finance industry, our method allows us to compute many types of risk measures
with an accurate and fast algorithm. We propose theoretical arguments as well
as extensive numerical tests in order to justify sharp convergence rates,
leading to rather optimal computational times. Cases of direct interest in
finance support our claims and the importance of the problem of the curse of
dimensionality in finance applications is briefly discussed.
","7 Conclusions
In this note based on [11]{[13], we presented a new
analysis of Monte-Carlo-type integration formula,
which is relevant in a variety of applications and
leads to sharp error estimates of practical interest.
We also presented a new numerical method,
which we refer to as the Transport-based mesh-free
Method, and is designed for the numerical simula-
tions of PDEs and should be useful for a variety of
equations (hyperbolic and/or parabolic equations)
as well as applications such as articial intelligence.
The error analysis above applies and, importantly,
we can guarantee the v alidity of an a priori and
quantitative error bound . In many cases of in-
terest, depending upon the choice of the kernels, we
can check numerically or theoretically, that the error
rate is the optimal convergence rate.
We explored some industrial applications in
mathematical nance and non-linear hyperbolic-
parabolic equations. The overall algorithm we have
developped has been found to be robust, fast, accu-
rate and was quite ecient in order to compute stan-
dard risk measures for mathematical nance. In-
deed, since we can argue that these methods exhibit
a sharp convergence rate, they tend to minimize the
algorithmic work and computational time.
References
[1]A. Antonov, M. Konikov, and M.
Spector, The free boundary SABR:
natural extension to negative rates (Jan-
uary 28, 2015), 17 pages, available at
https://ssrn.com/abstract=2557046.
[2]A. Brace, D. Gatarek, and M. Musiela,
The market model of interest rate dynamics,
Mathematical Finance 7 (1997), 127{154.
[3]H. Buehler, Volatility and dividends:
volatility modelling with cash divi-
dends and simple credit risk (February
2, 2010), 37 pages, available at SSRN:
https://ssrn.com/abstract=1141877.
[4]H. Cohn and N. Elkies, New upper bounds
on sphere packings, Ann. of Math. 157 (2003),
689{714.
[5]G.E. Fasshauer, Mesh-free methods, in
\\\\Handbook of Theoretical and Computational
Nanotechnology"", Vol. 2, 2006.[6]F. Narcowich, J. Ward, and H. Wend-
land, Sobolev bounds on functions with scat-
tered zeros, with applications to radial basis
function surface tting, Math. of Comput. 74
(2005), 743-763.
[7]E.G. Fasshauer, mesh-free approximation
methods with Matlab, Interdisciplinary Math-
ematical Sciences, Vol. 6, World Scientic Pub-
lishing Co. Pte. Ltd., Hackensack, NJ, 2007.
[8]V. A. Menegatto, Strict positive deniteness
on spheres, Analysis 19 (1999), 217{233.
[9]H. Niederreiter ,Random number generation
and quasi-Monte Carlo methods, CBMS-NSF
Regional Conf. Series in Applied Math., Soc.
Industr. Applied Math., 1992.
[10]P.G. LeFloch and J.-M. Mercier , Revisit-
ing the method of characteristics via a convex
hull algorithm, J. Comput. Phys. 298 (2015),
95{112.
[11]P.G. LeFloch and J.-M. Mercier, A new
method for solving Kolmogorov equations in
mathematical nance, C.R. Math. Acad. Sci.
Paris 355 (2017), 680{686.
[12]P.G. LeFloch and J.-M. Mercier, Mesh-
free error integration in arbitrary dimensions:
a numerical study of discrepancy functions,
Preprint arXiv:1911.00795, October 2019.
[13]P.G. LeFloch and J.-M. Mercier, The
Transport-based Mesh-free Method (TMM)
and its applications in nance, in preparation.
[14]J.-M. Mercier and S. Miryusupov, Hedg-
ing strategies for net interest income and eco-
nomic values of equity,
http://dx.doi.org/10.2139/ssrn.3454813.
[15]R Opfer, Multiscale kernels, Adv. Comput.
Math. 25 (2006), 357{380.
[16]H. Wendland, Scattered data approximation,
Cambridge Monograph Appl. Compu. Math.,
Cambridge University, 2005.
[17]B. Zwicknagl, Power series kernels, Con-
structive Approx. 29 (2008), 61{84.
7",2019-11-03T23:38:32Z,conusns i   transt method t i t ireferencantov seor t jabrace gat are mus ie la t matmatical nance bu hler volatity ruary co inew anmath as haber mesh handbook toical tnal nano technology logo vnar cow iward end so bold math com put as haber mat lab interdisciplinary math sciencvworld cie nti pub co pte ltd hackensack meeg at to stri analysis wier center random   regnal conf seriapplied math soc inx tr applied math le lomercer revisit com put ph ys le lomercer lmogorov math acad sci paris le lomercer mesh pre print  ober le lomercer t transt mesh method mercer mir yusupov  offer multi scale adv com put math end land scaered cambridge monograph ppl  math cambridge  wick nag  coapprox
paper_qf_15.pdf,1,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","Theoretically Motivated Data Augmentation and Regularization for
Portfolio Construction
Liu Ziyin 1, Kentaro Minami 2, Kentaro Imajo 2
1Department of Physics, The University of Tokyo
2Preferred Networks, Inc.
December 23, 2022
Abstract
The task we consider is portfolio construction in a speculative market, a fundamental problem in
modern nance. While various empirical works now exist to explore deep learning in nance, the theory
side is almost non-existent. In this work, we focus on developing a theoretical framework for under-
standing the use of data augmentation for deep-learning-based approaches to quantitative nance. The
proposed theory claries the role and necessity of data augmentation for nance; moreover, our theory
implies that a simple algorithm of injecting a random noise of strength/radical.alt1
/divides.alt0rt−1/divides.alt0to the observed return rtis
better than not injecting any noise and a few other nancially irrelevant data augmentation techniques.1
1 Introduction
There is an increasing interest in applying machine learning methods to problems in the nance industry.
This trend has been expected for almost forty years (Fama, 1970), when well-documented and ne-grained
(minute-level) data of stock market prices became available. In fact, the essence of modern nance is fast
and accurate large-scale data analysis (Goodhart and O'Hara, 1997), and it is hard to imagine that machine
learning should not play an increasingly crucial role in this eld. In contemporary research, the central
theme in machine-learning based nance is to apply existing deep learning models to nancial time-series
prediction problems (Imajo et al., 2020; Buehler et al., 2019; Jay et al., 2020; Imaki et al., 2021; Jiang et al.,
2017; Fons et al., 2020; Lim et al., 2019; Zhang et al., 2020), which have demonstrated the hypothesized
usefulness of deep learning for the nancial industry.
However, one major existing gap in this interdisciplinary eld of deep-learning nance is the lack of a
theory relevant to justify nance-oriented algorithm design. The goal of this work is to propose such a
framework, where machine learning practices are analyzed in a traditional nancial-economic utility theory
setting. Our theory implies that a simple theoretically motivated data augmentation technique is better for
the task portfolio construction than not injecting any noise at all or some naive noise injection methods
that have no theoretical justication. To summarize, our main contributions are (1) to demonstrate how
we can use utility theory to analyze practices of deep-learning-based nance, and (2) to theoretically study
the role of data augmentation in the deep-learning-based portfolio construction problem. Organization :
the next section discusses the main related works; Section 3 provides the requisite nance background
for understanding this work; Section 4 presents our theoretical contributions, which is a framework for
understanding machine-learning practices in the portfolio construction problem; Section 5 describes how to
practically implement the theoretically motivated algorithm; section 6 validates the theory with experiments
on toy and real data.
1This is the full-length version of our work published at the 3rd ACM International Conference on AI in Finance (ICAIF'22).
Seehttps://doi.org/10.1145/3533271.3561720 for the shorter published version.
The code is available at: https://github.com/pfnet-research/Finance_data_augmentation_ICAIF2022
1arXiv:2106.04114v3  [cs.LG]  22 Dec 2022",2021-06-08T05:26:58Z,toically motivated data augmentatregular iz attfconstru zi yicentro miami centro major partment psics t   preferred networks inc cember abstra t w it introdutre  fam ood hart hara imajor bu hler jay make lia font lim  t our to organizatsesese internatnal conference nance  hps t nance  c
paper_qf_15.pdf,2,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","Figure 1: Performance (measured by the Sharpe ratio) of various algorithms on MSFT (Microsoft) from 2018-2020.
Directly applying generic machine learning methods, such as weight decay, fails to improve the vanilla model. The
proposed method show signicant improvement.
2 Related Works
Existing deep learning nance methods . In recent years, various empirical approaches to apply state-
of-the-art deep learning methods to nance have been proposed (Imajo et al., 2020; Ito et al., 2020; Buehler
et al., 2019; Jay et al., 2020; Imaki et al., 2021; Jiang et al., 2017; Fons et al., 2020). The interested readers
are referred to (Ozbayoglu et al., 2020) for detailed descriptions of existing works. However, we notice that
one crucial gap is the complete lack of theoretical analysis or motivation in this interdisciplinary eld of
AI-nance. This work makes one initial step to bridge this gap. One theme of this work is that nance-
oriented prior knowledge and inductive bias is required to design the relevant algorithms. For example,
Ziyin et al. (2020) shows that incorporating prior knowledge into architecture design is key to the success of
neural networks and applied neural networks with periodic activation functions to the problem of nancial
index prediction. Imaki et al. (2021) shows how to incorporate no-transaction prior knowledge into network
architecture design when transaction cost is incorporated.
In fact, most generic and popular machine learning techniques are proposed and have been tested for
standard ML tasks such as image classication or language processing. Directly applying the ML methods
that work for image tasks is unlikely to work well for nancial tasks, where the nature of the data is dierent.
See Figure 1, where we show the performance of a neural network directly trained to maximize wealth return
on MSFT during 2019-2020. Using popular, generic deep learning techniques such as weight decay or dropout
does not result in any improvement over the baseline. In contrast, our theoretically motivated method does.
Combining the proposed method with weight decay has the potential to improve the performance a little
further, but the improvement is much lesser than the improvement of using the proposed method over the
baseline. This implies that a generic machine learning method is unlikely to capture well the inductive
biases required to tackle a nancial task. The present work proposes to ll this gap by showing how nance
knowledge can be incorporated into algorithm design.
Data augmentation . Consider a training loss function of the additive form L=1
N∑i`(xi;yi)forNpairs
of training data points {(xi;yi)}N
i=1. Data augmentation amounts to dening an underlying data-dependent
distribution and generating new data points stochastically from this underlying distribution. A general way
to dene data augmentation is to start with a datum-level training loss and transform it to an expectation
over an augmentation distribution P(z/divides.alt0(xi;yi))(Dao et al., 2019), `(xi;yi)→E(zi;gi)∼P(z;g/divides.alt0(xi;yi))[`(zi;gi)],
and the total training loss function becomes
Laug=1
NN
/summation.disp
i=1E(zi;gi)∼P(z;g/divides.alt0(xi;yi))[`(zi;gi)]: (1)
One common example of data augmentation is injecting isotropic gaussian noise to the input (Shorten and
Khoshgoftaar, 2019; Fons et al., 2020), which is equivalent to setting P(z;g/divides.alt0(xi;yi))∼(g−yi)exp/bracketleft.alt1−(z−xi)T(z−xi)/slash.left(22)/bracketright.alt
for some specied strength 2. Despite the ubiquity of data augmentation in deep learning, existing works
are often empirical in nature (Fons et al., 2020; Zhong et al., 2020; Shorten and Khoshgoftaar, 2019; Antoniou
et al., 2017). For a relevant example, Fons et al. (2020) empirically evaluates the eect of dierent types
of data augmentation in a nancial series prediction task. Dao et al. (2019) is one major recent theoretical
work that tries to understand modern data augmentation theoretically; it shows that data augmentation is
2",2021-06-08T05:26:58Z, formance share msoft direly t related works existi imajor to bu hler jay make lia font t oz bay og lu  one for zi yimake idirely   usi icombini  t data consir articial intellence rs data dao aug one shorte hg of  font spite font ho shorte hg of  antofor font dao
paper_qf_15.pdf,3,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","approximately learning in a special kernel. He et al. (2019) argues that data augmentation can be seen as an
eective regularization. However, no theoretically motivated data augmentation method for nance exists
yet. One major challenge and achievement of this work is to develop a theory that bridges the traditional
nance theory and machine learning methods. In the next section, we introduce the portfolio theory.
3 Background: Markowitz Portfolio Theory
How to make optimal investment in a nancial market is the central concern of portfolio theory. One
unfamiliar with the portfolio theory may easily confuse the task the portfolio construction with wealth
maximization trading or future price prediction. Before we introduce the portfolio theory, we rst stress
that the task of portfolio construction is not equivalent to wealth maximization or accurate price prediction.
One can construct an optimal portfolio without predicting the price or maximizing the wealth increase.
Consider a market with an equity (a stock) and a xed-interest rate bond (a government bond). We
denote the price of the equity at time step tasSt, and the price return is dened as rt=St+1−St
St, which
is a random variable with variance Ct, and the expected return gt∶=E[rt]. Our wealth at time step tis
Wt=Mt+ntSt, whereMtis the amount of cash we hold, and nithe shares of stock we hold for the i-th
stock. As in the standard nance literature, we assume that the shares are innitely divisible. Usually, a
positivendenotes holding (long) and a negative ndenotes borrowing (short). The wealth we hold initially
isW0>0, and we would like to invest our money on the equity. We denote the relative value of the stock we
hold ast=ntSt
Wt.is called a portfolio . The central challenge in portfolio theory is to nd the best . At
timet, our wealth is Wt; after one time step, our wealth changes due to a change in the price of the stock
(setting the interest rate to be 0):  Wt∶=Wt+1−Wt=Wttrt. The goal is to maximize the wealth return
Gt∶=t⋅rtat every time step while minimizing risk2. The risk is dened as the variance of the wealth
change:
Rt∶=R(t)∶=Varrt[Gt]=/parenleft.alt1E[r2
t]−g2
t/parenright.alt12
t=2
tCt: (2)
The standard way to control risk is to introduce a \\\\risk regularizer"" that punishes the portfolios with a large
risk (Markowitz, 1959; Rubinstein, 2002).3Introducing a parameter for the strength of regularization (the
factor of 1 /slash.left2 appears for convention), we can now write down our objective:
∗
t=arg max
U()∶=arg max
/bracketleft.alt3TGt−
2R()/bracketright.alt3: (3)
Here,Ustands for the utility function; can be set to be the desired level of risk-aversion. When gtandCtis
known, this problem can be explicitly solved. However, one main problem in nance is that its data is highly
limited and we only observe one particular realized data trajectory, and gtandCtare hard to estimate.
This fact motivates for the necessity of data augmentation and synthetic data generation in nance (Assefa,
2020). In this paper, we treat the case where there is only one asset to trade in the market, and the task of
utility maximization amounts to nding the best balance between cash-holding and investment. The equity
we are treating is allowed to be a weighted combination of multiple stocks (a portfolio of some public fund
manager, for example), and so our formalism is not limited to single-stock situations. In section C.1, we
discuss portfolio theory with multiple stocks.
4 Portfolio Construction as a Training Objective
Recent advances have shown that the nancial objectives can be interpreted as training losses for an appro-
priately inserted neural-network model (Ziyin et al., 2019; Buehler et al., 2019). It should come as no surprise
that the utility function (3) can be interpreted as a loss function. When the goal is portfolio construction,
we parametrize the portfolio t=w(xt)by a neural network with weights w, and the utility maximization
problem becomes a maximization problem over the weights of the neural network. The time-dependence is
2It is important to not to confuse the price return rtwith the wealth return Gt.
3In principle, any concave function in Gtcan be a risk regularizer from classical economic theory (Von Neumann and
Morgenstern, 1947). One common alternative would be R(G)=log(G)(Kelly Jr, 2011), and our framework can be easily
extended to such cases.
3",2021-06-08T05:26:58Z, one ibackground marwitz tftory how one before one consir  st st st st  our  mt st mt is as usually t  st  t at      t gt t rt var rt gt  t marwitz rubinsteiintroduci gt re stands w is  are  asset it itfconstrutr articial intellence ni objeive recent zi yibu hler it  it gt t caewmamorgensterone kelly jr
paper_qf_15.pdf,4,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","modeled through the input to the network xt, which possibly consists of the available information at time
tfor determining the future price.4The objective function (to be maximized) plus a pre-specied data
augmentation transform xt→ztwith underlying distribution p(z/divides.alt0xt)is then
∗
t=arg max
w/braceleft.alt41
TT
/summation.disp
t=1Et[Gt(w(zt))]−Vart[Gt(w(zt))]/braceright.alt4; (4)
where Et∶=Ezt∼p(z/divides.alt0xt). In this work, we abstract away the details of the neural network to approximate .
We instead focus on studying the maximizers of this equation, which is a suitable choice when the underlying
model is a neural network because one primary motivation for using neural networks in nance is that they
are universal approximators and are often expected to nd such maximizers (Buehler et al., 2019; Imaki
et al., 2021).
The ultimate nancial goal is to construct ∗such that the utility function is maximized with respect to
the true underlying distribution of St, which can be used as the generalization loss (to be maximized):
∗
t=arg max
t{ESt[Gt()]−VarSt[Gt()]}: (5)
Note the dierence in taking the expectation between Eq (4) and (5) is that Etis computed with respect to
the training set we hold, while ESt∶=ESt∼p(St)is computed with respect to the underlying distribution of St
given its previous prices. We used the same short-hands for Var tand VarSt. Technically, the true utility we
dened is an in-sample counterfactual objective, which roughly evaluates the expected utility to be obtained
if we restart from yesterday, which is a relevant measure for nancial decision making. In Section 4.5, we
also analyze the out-of-sample performance when the portfolio is static.
4.1 Standard Models of Stock Prices
The expectations in the true objective Equation (5) need to be taken with respect to the true underlying
distribution of the stock price generation process. In general, the price follows the following stochastic process
St=f({Si}t
i=1)+g({Si}t
i=1)tfor a zero-mean and unit variance random noise t; the termfreects the
short-term predictability of the stock price based on past prices, and greects the extent of unpredictability
in the price. A key observation in nance is that gis non-stationary (heteroskedastic) and price-dependent
(multiplicative). One model is the geometric Brownian motion (GBM)
St+1=(1+r)St+tStt; (6)
which is taken as the minimal standard model of the motion of stock prices (Mandelbrot, 1997; Black
and Scholes, 1973); this paper also assumes the GBM model as the underlying model. Here, we note
that the theoretical problem we consider can be seen as a discrete-time version of the classical Merton's
portfolio problem (Merton, 1969). The more exible Heston model (Heston, 1993) takes the form dSt=
rStdt+√tStdWt, wheretis the instantaneous volatility that follows its own random walk, and dWtis
drawn from a Gaussian distribution. Despite the simplicity of these models, the statistical properties of
these models agree well with the known statistical properties of the real nancial markets (Dr agulescu and
Yakovenko, 2002). The readers are referred to (Karatzas et al., 1998) for a detailed discussion about the
meaning and nancial signicance of these models.
4.2 No Data Augmentation
In practice, there is no way to observe more than one data point for a given stock at a given time t. This
means that it can be very risky to directly train on the raw observed data since nothing prevents the model
from overtting to the data. Without additional assumptions, the risk is zero because there is no randomness
in the training set conditioning on the time t. To control this risk, we thus need data augmentation. One
can formalize this intuition through the following proposition, whose proof is given in Section C.3.
4It is helpful to imagine xtas, for example, the prices of the stocks in the past 10 days.
4",2021-06-08T05:26:58Z,t et gt art gt et ez i bu hler make t st st gt var st gt note eq et is st st st st  var var st technically isestandard mols stock pri equatist si si one brownish st st st manlbrot  schools re mortomortot stostost std std   is spite dr v e t kara tz as no data augmentati without to one seit
paper_qf_15.pdf,5,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","Proposition 1. (Utility of no-data-augmentation strategy.) Let the price trajectory be generated with GBM
in Eq. (6)with initial price S0, then the true utility for the no-data-augmentation strategy is
Uno−aug=[1−2(−r/slash.left)]r−
22(7)
whereU()is the utility function dened in Eq. (3);is the c.d.f. of a standard normal distribution.
This means that, the larger the volatility , the smaller is the utility of the no-data-augmentation strategy.
This is because the model may easily overt to the data when no data augmentation is used. In the next
section, we discuss the case when a simple data augmentation is used.
4.3 Additive Gaussian Noise
While it is still far from clear how the stock price is correlated with the past prices, it is now well-recognized
that VarSt[St/divides.alt0St−1]≠0 (Mandelbrot, 1997; Cont, 2001). This motivates a simple data augmentation tech-
nique to add some randomness to the nancial sequence we observe, {S1;:::;ST+1}. This section analyzes a
vanilla version of data augmentation of injecting simple Gaussian noise, compared to a more sophisticated
data augmentation method in the next section. Here, we inject random Gaussian noises t∼N(0;2)toSt
during the training process such that zt=St+. Note that the noisied return needs to be carefully dened
since noise might also appear in the denominator, which may cause divergence; to avoid this problem, we
dene the noisied return to be ~ rt∶=zt+1−zt
St, i.e., we do not add noise to the denominator. Theoretically, we
can nd the optimal strength ∗of the gaussian data augmentation to be such that the true utility function
is maximized for a xed training set. The result can be shown to be
(∗)2=2
2r∑t(rtS2
t)2
∑trtS2
t: (8)
The fact the ∗depends on the prices of the whole trajectory reects the fact that time-independent data
augmentation is not suitable for a stock price dynamics prescribed by Eq. (6), whose inherent noise Sttis
time-dependent through the dependence on St. Finally, we can plug in the optimal ∗to obtain the optimal
achievable strategy for the additive Gaussian noise augmentation. As before, the above discussion can be
formalized, with the true utility given in the next proposition (proof in Section C.4).
Proposition 2. (Utility of additive Gaussian noise strategy.) Under additive Gaussian noise strategy, and
let other conditions the same as in Proposition 1, the true utility is
UAdd=r2
22TESt/bracketleft.alt4(∑trtSt)2
∑t(rtSt)2/parenleft.alt4/summation.disp
trtS2
t/parenright.alt4/bracketright.alt4; (9)
where is the Heaviside step function.
4.4 Multiplicative Gaussian Noise
In this section, we derive a general kind of data augmentation for the price trajectories specied by the GBM
and the Heston model. From the previous discussions, one might expect that a better kind of augmentation
should have =0St, i.e., the injected noise should be multiplicative ; however, we do not start from imposing
→St; instead, we consider →t, i.e., a general time-dependent noise. In the derivation, one can nd an
interesting relation for the optimal augmentation strength:
(∗
t+1)2+(∗
t)2=2
2rrtS2
t: (10)
The following proposition gives the true utility of using this data augmentation (derivations in Section C.5).
Proposition 3. (Utility of general multiplicative Gaussian noise strategy.) Under general multiplicative
noise augmentation strategy, and let other conditions the same as in Proposition 1, then the true utility is
Umult=r2
22[1−(−r/slash.left)]: (11)
5",2021-06-08T05:26:58Z,propositutity  eq uno eq   iadditive noise w var st st st manlbrot cont   re st st note st toically t t eq st st nally as sepropositutity unr propositadd st st st isi multiplicative noise istofrom st st it sepropositutity unr propositmult
paper_qf_15.pdf,6,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","Combining the above propositions, we can prove the main theorem of this work ((Proof in Section C.6)),
which shows that the mean-variance utility of the proposed method is strictly higher than that of no data-
augmention and that of additive Gaussian noise.
Theorem 1. If≠0, thenUmult>UaddandUmult>Uno−augwith probability 1.
Heston Model and Real Price Augmentation . We also consider the more general Heston model. The
derivation proceeds similarly by replacing 2→2
t; one arrives at the relation for optimal augmentation:
(∗
t+1)2+(∗
t)2=1
2r2
trtS2
t. One quantity we do not know is the volatility t, which has to be estimated by
averaging over the neighboring price returns. One central message from the above results is that one should
add noises with variance proportional to rtS2
tto the observed prices for augmenting the training set.
4.5 Stationary Portfolio
In the previous sections, we have discussed the case when the portfolio is dynamic (time-dependent). One
slight limitation of the previous theory is that one can only compare the in-sample counterfactual performance
of a dynamic portfolio. Here, we alternatively motivate the proposed data augmentation technique when
the model is a stationary portfolio. One can show that, for a stationary portfolio, the proposed data
augmentation technique gives the overall optimal performance.
Theorem 2. Under the multiplicative data augmentation strategy, the in-sample counterfactual utility and
the out-of-sample utility is optimal among all stationary portfolios.
Remark. See Section C.8 for a detailed discussion and the proof. Stationary portfolios are important in
nancial theory and can be shown to be optimal even among all dynamic portfolios in some situations (Cover
and Thomas, 2006; Merton, 1969). While restricting to stationary portfolios allows us to also compare on
out-of-sample performance, the limitation is that a stationary portfolio is less relevant for a deep learning
model than the dynamical portfolios considered in the previous sections.
4.6 General Framework
So far, we have been analyzing the data augmentation for specic examples of the utility function and the
data augmentation distribution to argue that certain types of data augmentation is preferable. Now we
outline how this formulation can be generalized to deal with a wider range of problems, such as dierent
utility functions and dierent data augmentations. This general framework can be used to derive alternative
data augmentations schemes if one wants to maximize other nancial metrics other than the Sharpe ratio,
such as the Sortino ratio (Estrada, 2006), or to incorporate regularization eect that into account of the
heavy tails of the prices distribution.
For a general utility function U=U(x;)for some data point xthat describes the current state of the
market, and that describes our strategy in this market state, we would like to ultimately maximize
max
V();forV()=Ex[U(x;)] (12)
However, only observing nitely many data points, we can only optimize the empirical loss with respect to
some−parametrized augmentation distribution P:
^()=arg max
1
NN
/summation.disp
iEzi∼p(z/divides.alt0xi)[U(zi;i)]: (13)
The problem we would like to solve is to nd the eect of using such data augmentation on the true utility
V, and then, if possible, compare dierent data augmentations and identify the better one. Surprisingly,
this is achievable since V=V(^())is now also dependent on the parameter of the data augmentation.
Note that the true utility has to be found with respect to both the sampling over the test points and the
sampling over the N-sized training set:
V(^())=Ex∼p(x)E{xi}N∼pN(x)[U(x;^())] (14)
6",2021-06-08T05:26:58Z,combini proof setorem  mult add and mult uno stomreal price augmentat stot one one statnary tfione re one torem unr remark  sestatnary cothomas mortow genl framework so   share sort istrata for ex ez t surprisy note ex
paper_qf_15.pdf,7,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","Figure 2: Experiment on geometric brownian motion; S0=1,r=0:005,=0:04.Left: Examples of prices
trajectories in green; the black line shows the expected value of the price. Right : Comparison with other related
data augmentation techniques. The black dashed line shows the optimal achievable Sharpe ratio. We see that the
proposed method stay close to the optimality across a 600-step trading period as the theory predicts.
In principle, this allows one to identify the best data augmentation for the problem at hand:
∗=arg max
V(^())arg max
Ex∼p(x)E{xi}N∼pN(x)
/bracketleft.alt4U/parenleft.alt4x;arg max
1
NN
/summation.disp
iEzi∼p(z/divides.alt0xi)[U(zi;i)]/parenright.alt4/bracketright.alt4; (15)
and the analysis we performed in the previous sections is simply a special case of obtaining solutions to this
maximization problem. Moreover, one can also compare two dierent parametric augmentation distributions;
let their parameter be denoted as andrespectively, then we can say that data augmentation is better
thanif and only if max V(^())>maxV(^()):This general formulation can also have applicability
outside the eld of nance because one can interpret the utility Uas a standard machine learning loss function
andas the model output. This procedure also mimics the procedure of nding a Bayes estimator in the
statistical decision theory (Wasserman, 2013), with being the estimator we want to nd; we outline an
alternative general formulation to nd the \\\\minimax"" augmentation in Section C.2.
5 Algorithms
Our results strongly motivate for a specially designed data augmentation for nancial data. For a data point
consisting purely of past prices (St;:::;St+L;St+L+1)and the associated returns (rt;:::;rt+L−1;rt+L), we use
x=(St;:::;St+L)as the input for our model f, possibly a neural network, and use St+L+1as the unseen
future price for computing the training loss. Our results suggests that we should randomly noisify both the
inputxandSt+L+1at every training step by
/uni23A7/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23A9Si→Si+c/radical.alt1
^2
i/divides.alt0ri/divides.alt0S2
ii forSi∈x;
St+L+1→St+L+1+c/radical.alt1
^2
i/divides.alt0rt+L/divides.alt0S2
t+Lt+L+1;(16)
whereiare i.i.d. samples from N(0;1), andcis a hyperparameter to be tuned. While the theory suggests
thatcshould be 1 /slash.left2, it is better to make it a tunable-parameter in algorithm design for better exibility; ^ t
is the instantaneous volatility, which can be estimated using standard methods in nance (Degiannakis and
Floros, 2015). One might also assume ^ intoc.
5.1 Using return as inputs
Practically and theoretically, it is better and standard to use the returns x=(rt;:::;rt+L−1;rt+L)as the input,
and the algorithm can be applied in a simpler form:
/uni23A7/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23A9ri→ri+c/radical.alt1
^2
i/divides.alt0ri/divides.alt0i forri∈x;
rt+L→rt+L+c/radical.alt1
^2
i/divides.alt0rt+L/divides.alt0t+L+1:(17)
7",2021-06-08T05:26:58Z, eximent left s rht arisot share  iex ez o as  baswassermasealgorithms our for st st st st st st our st si si si st st w  giant is for os one usi praically
paper_qf_15.pdf,8,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","Table 1: Sharpe ratio on S&P 500 by sectors; the larger the better. Best performances in Bold .
Industry Sectors # Stock Merton no aug. weight decay additive aug. naive mult. proposed
Communication Services 9 −0:06±0:04−0:06±0:04−0:06±0:27 0:22±0:18 0:20±0:21 0:33±0:16
Consumer Discretionary 39 −0:01±0:03−0:07±0:03−0:06±0:10 0:48±0:10 0:41±0:09 0:64±0:08
Consumer Staples 27 0 :05±0:03 0:24±0:03 0:23±0:11 0:36±0:08 0:34±0:09 0:35±0:07
Energy 17 0 :07±0:03 0:03±0:03−0:02±0:12 0:70±0:09 0:52±0:10 0:91±0:10
Financials 46 −0:57±0:04−0:61±0:03−0:61±0:09−0:06±0:10−0:13±0:09 0:18±0:08
Health Care 44 0 :23±0:04 0:60±0:04 0:61±0:11 0:86±0:09 0:81±0:09 0:83±0:07
Industrials 44 −0:09±0:03−0:11±0:03−0:11±0:08 0:36±0:08 0:28±0:08 0:48±0:08
Information Technology 41 0 :41±0:04 0:41±0:04 0:41±0:11 0:67±0:10 0:74±0:11 0:79±0:09
Materials 19 0 :07±0:03 0:06±0:03 0:03±0:14 0:47±0:13 0:43±0:13 0:53±0:10
Real Estate 22 −0:14±0:04−0:39±0:03−0:40±0:12 0:05±0:10 0:05±0:09 0:19±0:07
Utilities 24 −0:29±0:02−0:29±0:02−0:28±0:07−0:01±0:06−0:00±0:06 0:15±0:04
S&P500 Avg. 365 −0:02±0:04−0:00±0:04−0:01±0:04 0:39±0:03 0:35±0:03 0:51±0:03
5.2 Equivalent Regularization on the output
One additional simplication can be made by noticing the eect of injecting noise to rt+Lon the training loss
is equivalent to a regularization. We show in Section B that, under the GBM model, the training objective
can be written as
arg max
bt/braceleft.alt41
TT
/summation.disp
t=1Ez[Gt()]−c2^2
t/divides.alt0rt/divides.alt02
t/braceright.alt4; (18)
where the expectation over xis now only taken with respect to the input. This means that the noise
injection on the rt+Lis equivalent to adding a L2regularization on the model output t. This completes
the main proposed algorithm of this work. We discuss a few potential variants in Section B. Also, it is well
known that the magnitude of /divides.alt0rt/divides.alt0has strong time-correlation (i.e., a large /divides.alt0rt/divides.alt0suggests a large /divides.alt0rt+1/divides.alt0) (Lux
and Marchesi, 2000; Cont and Bouchaud, 1997; Cont, 2007), and this suggests that one can also use the
average of the neighboring returns to smooth the /divides.alt0rt/divides.alt0factor in the last term for some time-window of width
:/divides.alt0rt/divides.alt0→/divides.alt0^rt/divides.alt0=1
∑
0/divides.alt0rt−/divides.alt0. In our S&P500 experiments, we use this smoothing technique with =20.
6 Experiments
We validate our theoretical claim that using multiplicative noise with strength√ris better than not using any
data augmentation or using a data augmentation that is not suitable for the nature of portfolio construction
(such as an additive Gaussian noise). We emphasize that the purpose of this section is for demonstrating
the relevance of our theory to real nancial problems, not for establishing the proposed method as a strong
competitive method in the industry. We start with a toy dataset that follows the theoretical assumptions and
then move on to real data with S&P500 prices. The detailed experimental settings are given in Section A.
Unless otherwise specied, we use a feedforward neural network with the number of neurons 10 →64→64→1
with ReLU activations. Training proceeds with the Adam optimizer with a minibatch size of 64 for 100 epochs
with the default parameter settings.5
We use the Sharpe ratio as the performance metric (the larger the better). Sharpe ratio is dened as
SRt=E[Wt]/radical.alt1
Var[Wt], which is a measure of the protability per risk. We choose this metric because, in the
framework of portfolio theory, it is the only theoretically motivated metric of success (Sharpe, 1966). In
particular, our theory is based on the maximization of the mean-variance utility in Eq. (3) and it is well-
known that the maximization of the mean-variance utility is equivalent to the maximization of the Sharpe
ratio. In fact, it is a classical result in classical nancial research that all optimal strategies must have the
same Sharpe ratio (Sharpe, 1964) (also called the ecient capital frontier). For the synthetic tasks, we
can generate arbitrarily many test points to compare the Sharpe ratios unambiguously. We then move to
experiments on real stock price series; the limitation is that the Sharpe ratio needs to be estimated and
involves one additional source of uncertainty.6
5In our initial experiments, we also experimented with dierent architectures (dierent depth or width of the FNN, RNN,
LSTM), and our conclusion that the proposed augmentation outperforms the specied baselines remain unchanged.
6We caution the readers to not to confuse the problem of portfolio construction with the problem of nancial price prediction.
Portfolio construction is the primary focus of our work and is fundamentally dierent from the problem of nancial price
prediction . Our method is not relevant and cannot be used directly for predicting future prices. As in real life, one does not
need to be able to predict prices to decide which stock to purchase.
8",2021-06-08T05:26:58Z,table share best bold industry seors stock mortocounicatservicconsumer disetnary consumer staplenergy nancial alth care industrial informattechnology materials real estate utit equivalent regular iz atone lo seez gt     sealso lux marcs cont  chacont ieximents    t seunless re tr articial intellence ni adam  share share rt  var   share ieq share ishare share for share  share i tfour as
paper_qf_15.pdf,9,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","Figure 3: Available portfolios and the market capital line (MCL). The black dots are the return-risk combinations
of the original stocks; the orange dots are the learned portfolios. The MCL of the proposed method is lower than
that of the original stocks, suggesting improved return and lower risk.
6.1 Geometric Brownian Motion
We rst start from experimenting with stock prices generated with a GBM, as specied in Eq. (6), and we
generate a xed price trajectory with length T=400 for training; each training point consists of a sequence
of past prices (St;:::;St+9;St+10)where the rst ten prices are used as the input to the model, and St+10is
used for computing the loss.
Results and discussion . See Figure 2. The proposed method is plotted in blue. The right gure compares
the proposed method with the other two baseline data augmentations we studied in this work. As the theory
shows, the proposed method is optimal for this problem, achieving the optimal Sharpe ratio across a 600-step
trading period. This directly conrms our theory.
6.2 S&P 500 Prices
This section demonstrates the relevance of the proposed algorithm to real market data. In particular, We
use the data from S&P500 from 2016 to 2020, with 1000 days in total. We test on the 365 stocks that
existed on S&P500 from 2000 to 2020. We use the rst 800 days as the training set and the last 200 days
for testing. The model and training setting is similar to the previous experiment. We treat each stock as a
single dataset and compare on all of the 365 stocks (namely, the evaluation is performed independently on
365 dierent datasets). Because the full result is too long, We report the average Sharpe ratio per industrial
sector (categorized according to GISC) and the average Sharpe ratio of all 365 datasets. See Section A.1
and A.4 for more detail.
Results and discussion . See Table 1. We see that, without data augmentation, the model works poorly
due to its incapability of assessing the underlying risk. We also notice that weight decay does not improve
the performance (if it is not deteriorating the performance). We hypothesize that this is because weight
decay does not correctly capture the inductive bias that is required to deal with a nancial series prediction
task. Using any kind of data augmentation seems to improve upon not using data augmentation. Among
these, the proposed method works the best, possibly due to its better capability of risk control. In this
experiment, we did not allow for short selling; when short selling is allowed, the proposed method also works
the best; see Section A.4. In Section A.5.1, we also perform a case study to demonstrate the capability of
the learned portfolio to avoid a market crash in 2020. We also compare with the Merton's portfolio (Merton,
1969), which is the classical optimal stationary portfolio constructed from the training data; this method
does not perform well either. This is because the market during the time 2019 −2020 is volatile and quite
dierent from the previous years, and a stationary portfolio cannot capture the nuances in the change of the
market condition. This shows that it is also important to leverage the exibility and generalization property
of the modern neural networks, along side the nancial prior knowledge.
6.3 Comparison with Data Generation Method
One common alternative to direct data augmentation in the eld is to generate additional realistic syn-
thetic data using a GAN. While it is not the purpose of this work to propose an industrial level method,
9",2021-06-08T05:26:58Z,  articial intellence  t t geometric brownish mot eq st st st st results   t t as share  pric i   t  because  share share  seresults  table    usi amo iseise mortomorto  arisodata gentmethod one w
paper_qf_15.pdf,10,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","nor do we claim that the proposed method outperforms previous methods, we provide one experimental
comparison in Section A.5 for the task of portfolio construction. We compare our theoretically motivated
technique with QuantGAN (Wiese et al., 2020), a major and recent technique in the eld of nancial data
augmentation/generation. The experiment setting is the same as the S&P500 experiment. The result shows
that directly applying QuantGAN to the portfolio construction problem in our setting does not signicantly
improve over the baseline without any augmentation and achieves a much lower Sharpe ratio than our sug-
gested method. This underperformance is possibly because QuantGAN is not designed for Sharpe ratio
maximization.
6.4 Market Capital Lines
In this section, we link the result we obtained in the previous section with the concept of market capital
line (MCL) in the capital asset pricing model (Sharpe, 1964), a foundational theory in classical nance. The
MCL of a set of portfolios denotes the line of the best return-risk combinations when these portfolios are
combined with a risk-free asset such as the government bond; an MCL with smaller slope means better return
and lower risk and is considered to be better than an MCL that is to the upper left in the return-risk plane.
See Figure 3. The risk-free rate r0is set to be 0 :01, roughly equal to the average 1-year treasury yield from
2018 to 2020. We see that the learned portfolios achieves a better MCL than the original stocks. The slope
of the SP500 MCL is roughly 0 :53, while that of the proposed method is 0 :35, i.e., much better return-risk
combinations can be achieved using the proposed method. For example, if we specify the acceptable amount
of risk to be 0 :1, then the proposed method can result in roughly 10% more gain in annual return than
investing in the best stock in the market. This example also shows that how tools in classical nance theory
can be used to visualize and better understand the machine learning methods that are applied to nance, a
crucial point that many previous works lack.
6.5 Case Study
For completeness, we also present the performance of the proposed method during the Market crush in Feb.
2020 for the interested readers. See Section A.5.1.
7 Outlook
In this work, we have presented a theoretical framework relevant to nance and machine learning to un-
derstand and analyze methods related to deep-learning-based nance. The result is a machine learning
algorithm incorporating prior knowledge about the underlying nancial processes. The good performance
of the proposed method agrees with the standard expectation in machine learning that performance can be
improved if the right inductive biases are incorporated. We have thus succeeded in showing that building
machine learning algorithms that is rooted rmly in nancial theories can have a considerable and yet-to-be
achieved benet. We hope that our work can help motivating for more works that approaches the theoretical
aspects of machine learning algorithms that are used for nance.
The limitation of the present work is obvious; we only considered the kinds of data augmentation that
takes the form of noise injection. Other kinds of data augmentation may also be useful to the nance;
for example, (Fons et al., 2020) empirically nds that magnify (Um et al., 2017), time warp (Kamycki
et al., 2020), and SPAWNER (Le Guennec et al., 2016) are helpful for nancial series prediction, and it is
interesting to apply our theoretical framework to analyze these methdos as well; a correct theoretical analysis
of these methods is likely to advance both the deep-learning based techniques for nance and our fundamental
understanding of the underlying nancial and economic mechanisms. Meanwhile, our understanding of the
underlying nancial dynamics is also rapidly advancing; we foresee better methods to be designed, and it
is likely that the proposed method will be replaced by better algorithms soon. There is potentially positive
social eects of this work because it is widely believed that designing better nancial prediction methods
can make the economy more ecient by eliminating arbitrage (Fama, 1970); the cautionary note is that
this work is only for the purpose of academic research, and should not be taken as an advice for monetary
investment, and the readers should evaluate their own risk when applying the proposed method.
10",2021-06-08T05:26:58Z,se quant wise t t quant share  quant share market capital linishare t   t  t for  case study for market   seoutlook it t   t otr font um ka my ck le gu enne meanw tre fam
paper_qf_15.pdf,11,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","References
Antoniou, A., Storkey, A., and Edwards, H. (2017). Data augmentation generative adversarial networks.
arXiv preprint arXiv:1711.04340 .
Assefa, S. (2020). Generating synthetic data in nance: opportunities, challenges and pitfalls. Challenges
and Pitfalls (June 23, 2020) .
Black, F. and Scholes, M. (1973). The pricing of options and corporate liabilities. Journal of political
economy , 81(3):637{654.
Bouchaud, J. P. and Potters, M. (2009). Financial applications of random matrix theory: a short review.
Buehler, H., Gonon, L., Teichmann, J., and Wood, B. (2019). Deep hedging. Quantitative Finance ,
19(8):1271{1291.
Cont, R. (2001). Empirical properties of asset returns: stylized facts and statistical issues. Quantitative
Finance .
Cont, R. (2007). Volatility clustering in nancial markets: empirical facts and agent-based models. In Long
memory in economics , pages 289{309. Springer.
Cont, R. and Bouchaud, J.-P. (1997). Herd behavior and aggregate uctuations in nancial markets. arXiv
preprint cond-mat/9712318 .
Cover, T. M. and Thomas, J. A. (2006). Elements of Information Theory (Wiley Series in Telecommunica-
tions and Signal Processing) . Wiley-Interscience, New York, NY, USA.
Dao, T., Gu, A., Ratner, A., Smith, V., De Sa, C., and R e, C. (2019). A kernel theory of modern data
augmentation. In International Conference on Machine Learning , pages 1528{1537. PMLR.
Degiannakis, S. and Floros, C. (2015). Methods of volatility estimation and forecasting. Modelling and
Forecasting High Frequency Financial Data , pages 58{109.
Dr agulescu, A. A. and Yakovenko, V. M. (2002). Probability distribution of returns in the heston model
with stochastic volatility. Quantitative nance , 2(6):443{453.
Estrada, J. (2006). Downside risk in practice. Journal of Applied Corporate Finance , 18(1):117{125.
Fama, E. F. (1970). Ecient capital markets: A review of theory and empirical work. The journal of
Finance , 25(2):383{417.
Fons, E., Dawson, P., jun Zeng, X., Keane, J., and Iosidis, A. (2020). Evaluating data augmentation for
nancial time series classication.
Goodhart, C. A. and O'Hara, M. (1997). High frequency data in nancial markets: Issues and applications.
Journal of Empirical Finance , 4(2-3):73{114.
He, Z., Xie, L., Chen, X., Zhang, Y., Wang, Y., and Tian, Q. (2019). Data augmentation revisited: Re-
thinking the distribution gap between clean and augmented data.
Heston, S. L. (1993). A closed-form solution for options with stochastic volatility with applications to bond
and currency options. The review of nancial studies , 6(2):327{343.
Imajo, K., Minami, K., Ito, K., and Nakagawa, K. (2020). Deep portfolio optimization via distributional
prediction of residual factors.
Imaki, S., Imajo, K., Ito, K., Minami, K., and Nakagawa, K. (2021). No-transaction band network: A neural
network architecture for ecient deep hedging. Available at SSRN 3797564 .
Ito, K., Minami, K., Imajo, K., and Nakagawa, K. (2020). Trader-company method: A metaheuristic for
interpretable stock price prediction.
11",2021-06-08T05:26:58Z,referencantostor key edwards data   asset genti challepitfalls june  schools t journal  chaters nancial bu hler go noeichmanwood ep quantitative nance cont emical quantitative nance cont volatity ilo  cont  chard  cothomas elements informattory rey seritelecom mu nica snal processi rey inter science new york dao gu ratr smith  iinternatnal conference machine learni  giant is for os methods molli forecasti hh frequency nancial data dr v e probabity quantitative strata downsi journal applied corate nance fam t nance font dawsoe kane osi evaluati good hart hara hh issujournal emical nance  die c wa thadata re stot major miami to nakagawa ep make major to miami nakagawa no  articial intellence  to miami major nakagawa trar
paper_qf_15.pdf,12,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","Jay, P., Kalariya, V., Parmar, P., Tanwar, S., Kumar, N., and Alazab, M. (2020). Stochastic neural networks
for cryptocurrency price prediction. IEEE Access , 8:82804{82818.
Jiang, Z., Xu, D., and Liang, J. (2017). A deep reinforcement learning framework for the nancial portfolio
management problem. arXiv preprint arXiv:1706.10059 .
Kamycki, K., Kapuscinski, T., and Oszust, M. (2020). Data augmentation with suboptimal warping for
time-series classication. Sensors , 20(1):98.
Karatzas, I., Shreve, S. E., Karatzas, I., and Shreve, S. E. (1998). Methods of mathematical nance , vol-
ume 39. Springer.
Kelly Jr, J. L. (2011). A new interpretation of information rate. In The Kelly capital growth investment
criterion: theory and practice , pages 25{34. World Scientic.
Le Guennec, A., Malinowski, S., and Tavenard, R. (2016). Data augmentation for time series classication
using convolutional neural networks. In ECML/PKDD workshop on advanced analytics and learning on
temporal data .
Lim, B., Zohren, S., and Roberts, S. (2019). Enhancing time-series momentum strategies using deep neural
networks. The Journal of Financial Data Science , 1(4):19{38.
Lux, T. and Marchesi, M. (2000). Volatility clustering in nancial markets: a microsimulation of interacting
agents. International journal of theoretical and applied nance , 3(04):675{702.
Mandelbrot, B. B. (1997). The variation of certain speculative prices. In Fractals and scaling in nance ,
pages 371{418. Springer.
Markowitz, H. (1959). Portfolio selection.
Merton, R. C. (1969). Lifetime portfolio selection under uncertainty: The continuous-time case. The review
of Economics and Statistics , pages 247{257.
Ozbayoglu, A. M., Gudelek, M. U., and Sezer, O. B. (2020). Deep learning for nancial applications: A
survey. Applied Soft Computing , page 106384.
Rubinstein, M. (2002). Markowitz's"" portfolio selection"": A fty-year retrospective. The Journal of nance ,
57(3):1041{1045.
Sharpe, W. F. (1964). Capital asset prices: A theory of market equilibrium under conditions of risk. The
journal of nance , 19(3):425{442.
Sharpe, W. F. (1966). Mutual fund performance. The Journal of business , 39(1):119{138.
Shorten, C. and Khoshgoftaar, T. M. (2019). A survey on image data augmentation for deep learning.
Journal of Big Data , 6(1):1{48.
Um, T. T., Pster, F. M., Pichler, D., Endo, S., Lang, M., Hirche, S., Fietzek, U., and Kuli c, D. (2017).
Data augmentation of wearable sensor data for parkinson's disease monitoring using convolutional neural
networks. In Proceedings of the 19th ACM International Conference on Multimodal Interaction , pages
216{220.
Von Neumann, J. and Morgenstern, O. (1947). Theory of games and economic behavior, 2nd rev.
Wasserman, L. (2013). All of statistics: a concise course in statistical inference . Springer Science & Business
Media.
Wiese, M., Knobloch, R., Korn, R., and Kretschmer, P. (2020). Quant gans: Deep generation of nancial
time series. Quantitative Finance , 20(9):1419{1440.
Zhang, Z., Zohren, S., and Roberts, S. (2020). Deep learning for portfolio optimization. The Journal of
Financial Data Science , 2(4):8{20.
12",2021-06-08T05:26:58Z,jay kala ri ya parma tawar kumar la ab stochastic  lia xu lia   ka my ck kap sc i os zu st data sensors kara tz as hr eve kara tz as hr eve methods  kelly jr it kelly world cie nti le gu enne mali   arena rd data ilim oh reroberts enhanci t journal nancial data science lux marcs volatity internatnal manlbrot t ifraal  marwitz tfmortoime t t economics statistics oz bay og lu gu  lek ser ep applied soft uti rubinsteimarwitz t journal share capital t share mutual t journal shorte hg of  journal b data um pic hler endo la irc  e tze kul data iproceedis internatnal conference multi modal intewmamorgenstertory wassermaall  science business media wise knob loborre temer quant ep quantitative nance  oh reroberts ep t journal nancial data science
paper_qf_15.pdf,13,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","Zhong, Z., Zheng, L., Kang, G., Li, S., and Yang, Y. (2020). Random erasing data augmentation. In
Proceedings of the AAAI Conference on Articial Intelligence , volume 34, pages 13001{13008.
Ziyin, L., Hartwig, T., and Ueda, M. (2020). Neural networks fail to learn periodic functions and how to x
it.arXiv preprint arXiv:2006.08195 .
Ziyin, L., Wang, Z., Liang, P., Salakhutdinov, R., Morency, L., and Ueda, M. (2019). Deep gamblers:
Learning to abstain with portfolio theory. In Proceedings of the Neural Information Processing Systems
Conference .
13",2021-06-08T05:26:58Z,ho c ka li ya random iproceedis conference art intellence zi yihart w uefa neural   zi yiwa lia salad hut dino  cy uefa ep learni iproceedis neural informatprocessi tems conference
paper_qf_15.pdf,14,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","A Experiments
This section describes the additional experiments and the experimental details in the main text. The
experiments are all done on a single TITAN RTX GPU. The S&P500 data is obtained from Alphavantage.7
The code will be released on github.
A.1 Dataset Construction
For all the tasks, we observe a single trajectory of a single stock prices S1;:::;ST. For the toy tasks, T=400;
for the S&P500 task, T=800. We then transform this into T−Linput-target pairs {(xi;yi)}T−L
i=1, where
/uni23A7/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23A9xi=(Si;:::;SL−1);
yi=SL:(19)
xiis used as the input to the model for training; yiis used as the unseen future price for calculating the loss
function. For the toy tasks, L=10; for the S&P500 task, L=15. In simple words, we use the most recent L
prices for constructing the next-step portfolio.
A.2 Sharpe Ratio for S&P500
The empirical Sharpe Ratios are calculated in the standard way (for example, it is the same as in (Ito et al.,
2020; Imajo et al., 2020)). Given a trajectory of wealth W1;:::;WTof a strategy , the empirical Sharpe
ratio is estimated as
Ri=Wi+1
Wi−1; (20)
^M=1
TT−1
/summation.disp
i=1Ri; (21)
/circumflexcmb.alt1SR=^M/radical.alt2
1
T∑T
i=1R2
i−^M2=average wealth return
std. of wealth return; (22)
and/circumflexcmb.alt1SRis the reported Sharpe Ratio for S&P500 experiments.
A.3 Variance of Sharpe Ratio
We do not report an uncertainty for the single stock Sharpe Ratios, but one can easily estimate the uncer-
tainties. The Sharpe Ratio is estimated across a period of Ttime steps. For the S&P500 stocks, T=200,
and by the law of large numbers, the estimated mean ^Mhas variance roughly 2/slash.leftT, whereis the true
volatility, and so is the estimated standard deviation. Therefore, the estimated Sharpe Ratio can be written
as
/circumflexcmb.alt1SR=^M/radical.alt2
1
T∑T
i=1R2
i−^M2(23)
=M+√
T
+c√
T(24)
≈M+√
T
=M
+1√
T (25)
whereandare zero-mean random variables with unit variance. This shows that the uncertainty in the
estimated /circumflexcmb.alt1SRis approximately 1 /slash.left√
T≈0:07 for each of the single stocks, which is often much smaller than
the dierence between dierent methods.
7https://www.alphavantage.co/documentation/
14",2021-06-08T05:26:58Z,eximents  t t alpha vantage t data set construfor for  input si for ishare rat t share rats to major giveof share ri wi wi ri is share rat variance share rat  share rats t share rat time for has trefore share rat  is
paper_qf_15.pdf,15,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","Figure 4: Case study of the performance of the model on MSFT from 2019 August to 2020 May. We see
that the model learns to invest less and less as the price of the stock rises to an unreasonable level, thus
avoiding the high risk of the market crash in February 2020.
A.4 S&P500 Experiment
This section gives more results and discussion of the S&P500 experiment.
A.4.1 Underperformance of Weight Decay
This section gives the detail of the comparison made in Figure 1. The experimental setting is the same as
theS&P500 experiments. For illustration and motivation, we only show the result on MSFT (Microsoft).
Choosing most of the other stocks would give a qualitatively similar plot.
See Figure 1, where we show the performance of directly training a neural network to maximize wealth
return on MSFT during 2018-2020. Using popular, generic deep learning techniques such as weight decay or
dropout does not improve the baseline. In contrast, our theoretically motivated method does. Combining
the proposed method with weight decay has the potential to improve the performance a little further, but
the improvement is much lesser than the improvement of using the proposed method over the baseline. This
implies that generic machine learning is unlikely to capture the inductive bias required to process a nancial
task.
In the plot, we did not interpolate the dropout method between a large pand a small p. The result is
similar to the case of weight decay in our experiments.
A.5 Comparison with GAN
Sector Q-GAN Aug. Ours
Comm. Services 0 :07±0:02 0:33±0:16
Consumer Disc. 0 :00±0:03 0:64±0:08
Consumer Staples 0 :10±0:02 0:35±0:07
Energy 0 :08±0:05 0:91±0:10
Financials −0:22±0:03 0:18±0:08
Health Care 0 :35±0:03 0:83±0:07
Industrials −0:12±0:03 0:48±0:08
Information Tech. 0 :28±0:04 0:79±0:09
Materials −0:03±0:03 0:53±0:10
Real Estate −0:35±0:03 0:19±0:07
Utilities −0:20±0:02 0:15±0:04
S&P500 Avg. 0 :05±0:03 0:51±0:03
Figure 5: Performance (Sharpe Ratio) of the
training set augmented with QuantGAN.We stress that it is not the goal of this paper to compare meth-
ods but to understand why certain methods work or do not
work from the perspective of the classical nance theory. With
this caveat clearly stated, we compare our theoretically moti-
vated technique with QuantGAN, a major and recent technique
in the eld of nancial data augmentation/generation. We
rst train a QuantGAN on each stock trajectory and use the
trained QuantGAN to generate 10000 additional data points
to augment the original training set (containing roughly 1000
data points for each stock) and train the same feedforward net
as described in the main text. This feedforward net is then
used for evaluation. QuantGAN is implemented as close to
the original paper as possible, using a temporal convolutional
network trained with RMSProp for 50 epochs. Other experi-
mental settings are the same as that of S&P500 experiment in
15",2021-06-08T05:26:58Z, case august may  ruary eximent  unr formance ht cay   t for msoft choosi   usi icombini  it arisoseor aug ours com servicconsumer disc consumer staplenergy nancial alth care industrial informattematerials real estate utit  formance share rat quant  with quant  quant quant  quant protr
paper_qf_15.pdf,16,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","the manuscript. See the right Table. Also, compare with Table 1 in the manuscript. We see that directly
applying QuantGAN to the portfolio construction problem in our setting does not signicantly improve over
the baseline without any augmentation and achieves a much lower Sharpe ratio than our suggested method.
This underperformance is possibly because the QuantGAN is not designed for Sharpe ratio maximization.
A.5.1 Case Study
In this section, we qualitatively study the behavior of the learned portfolio of the proposed method. The
model is trained as in the other S&P500 experiments. See Figure 4. We see that the model learns to invest
less and less as the stock price rises to an excessive level, thus avoiding the high risk of the market crash in
February 2020. This avoidance demonstrates the eectiveness of the proposed method qualitatively.
A.5.2 List of Symbols for S&P500
The following are the symbols we used for the S&P500 experiments, separated by quotation marks.
['A' 'AAPL' 'ABC' 'ABT' 'ADBE' 'ADI' 'ADM' 'ADP' 'ADSK' 'AEE' 'AEP' 'AES' 'AFL' 'AIG' 'AIV'
'AJG' 'AKAM' 'ALB' 'ALL' 'ALXN' 'AMAT' 'AMD' 'AME' 'AMG' 'AMGN' 'AMT' 'AMZN' 'ANSS' 'AON'
'AOS' 'APA' 'APD' 'APH' 'ARE' 'ATVI' 'AVB' 'AVY' 'AXP' 'AZO' 'BA' 'BAC' 'BAX' 'BBT' 'BBY' 'BDX'
'BEN' 'BIIB' 'BK' 'BKNG' 'BLK' 'BLL' 'BMY' 'BRK.B' 'BSX' 'BWA' 'BXP' 'C' 'CAG' 'CAH' 'CAT' 'CB'
'CCI' 'CCL' 'CDNS' 'CERN' 'CHD' 'CHRW' 'CI' 'CINF' 'CL' 'CLX' 'CMA' 'CMCSA' 'CMI' 'CMS' 'CNP'
'COF' 'COG' 'COO' 'COP' 'COST' 'CPB' 'CSCO' 'CSX' 'CTAS' 'CTL' 'CTSH' 'CTXS' 'CVS' 'CVX' 'D'
'DE' 'DGX' 'DHI' 'DHR' 'DIS' 'DLTR' 'DOV' 'DRE' 'DRI' 'DTE' 'DUK' 'DVA' 'DVN' 'EA' 'EBAY' 'ECL'
'ED' 'EFX' 'EIX' 'EL' 'EMN' 'EMR' 'EOG' 'EQR' 'EQT' 'ES' 'ESS' 'ETFC' 'ETN' 'ETR' 'EW' 'EXC'
'EXPD' 'F' 'FAST' 'FCX' 'FDX' 'FE' 'FFIV' 'FISV' 'FITB' 'FL' 'FLIR' 'FLS' 'FMC' 'FRT' 'GD' 'GE'
'GILD' 'GIS' 'GLW' 'GPC' 'GPS' 'GS' 'GT' 'GWW' 'HAL' 'HAS' 'HBAN' 'HCP' 'HD' 'HES' 'HIG' 'HOG'
'HOLX' 'HON' 'HP' 'HPQ' 'HRB' 'HRL' 'HRS' 'HSIC' 'HST' 'HSY' 'HUM' 'IBM' 'IDXX' 'IFF' 'INCY'
'INTC' 'INTU' 'IP' 'IPG' 'IRM' 'IT' 'ITW' 'IVZ' 'JBHT' 'JCI' 'JEC' 'JNJ' 'JNPR' 'JPM' 'JWN' 'K' 'KEY'
'KLAC' 'KMB' 'KMX' 'KO' 'KR' 'KSS' 'KSU' 'L' 'LB' 'LEG' 'LEN' 'LH' 'LLY' 'LMT' 'LNC' 'LNT' 'LOW'
'LRCX' 'LUV' 'M' 'MAA' 'MAC' 'MAR' 'MAS' 'MAT' 'MCD' 'MCHP' 'MCK' 'MCO' 'MDT' 'MET' 'MGM'
'MHK' 'MKC' 'MLM' 'MMC' 'MMM' 'MNST' 'MO' 'MOS' 'MRK' 'MRO' 'MS' 'MSFT' 'MSI' 'MTB' 'MTD'
'MU' 'MYL' 'NBL' 'NEE' 'NEM' 'NI' 'NKE' 'NKTR' 'NOC' 'NOV' 'NSC' 'NTAP' 'NTRS' 'NUE' 'NVDA'
'NWL' 'O' 'OKE' 'OMC' 'ORCL' 'ORLY' 'OXY' 'PAYX' 'PBCT' 'PCAR' 'PCG' 'PEG' 'PEP' 'PFE' 'PG'
'PGR' 'PH' 'PHM' 'PKG' 'PKI' 'PLD' 'PNC' 'PNR' 'PNW' 'PPG' 'PPL' 'PRGO' 'PSA' 'PVH' 'PWR'
'PXD' 'QCOM' 'RCL' 'RE' 'REG' 'REGN' 'RF' 'RJF' 'RL' 'RMD' 'ROK' 'ROP' 'ROST' 'RRC' 'RSG'
'SBAC' 'SBUX' 'SCHW' 'SEE' 'SHW' 'SIVB' 'SJM' 'SLB' 'SLG' 'SNA' 'SNPS' 'SO' 'SPG' 'SRCL' 'SRE'
'STT' 'STZ' 'SWK' 'SWKS' 'SYK' 'SYMC' 'SYY' 'T' 'TAP' 'TGT' 'TIF' 'TJX' 'TMK' 'TMO' 'TROW'
'TRV' 'TSCO' 'TSN' 'TTWO' 'TXN' 'TXT' 'UDR' 'UHS' 'UNH' 'UNM' 'UNP' 'UPS' 'URI' 'USB' 'UTX'
'VAR' 'VFC' 'VLO' 'VMC' 'VNO' 'VRSN' 'VRTX' 'VTR' 'VZ' 'WAT' 'WBA' 'WDC' 'WEC' 'WFC' 'WHR'
'WM' 'WMB' 'WMT' 'WY' 'XEL' 'XLNX' 'XOM' 'XRAY' 'XRX' 'YUM' 'ZION']
16",2021-06-08T05:26:58Z, table also table  quant share  quant share case study it    ruary  t symbols t
paper_qf_15.pdf,17,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","B Additional Discussion of the Proposed Algorithm
B.1 Derivation
We rst derive Equation 18. The original training loss is
1
TT
/summation.disp
t=1Et[Gt()]−Vart[Gt()]: (26)
The last term can be written as
Vart[Gt()]=Ez1;:::;zt[z2
t2
t]−Ez1;:::;zt[ztt]2(27)
=Ezt[z2
t]Ez1;:::;zt−1[2
t]−Ezt[zt]2Ez1;:::;zt−1[t]2(28)
=r2
tEz1;:::;zt−1[2
t]+c2^2
t/divides.alt0rt/divides.alt0Ez1;:::;zt−1[2
t]−r2
tEz1;:::;zt−1[t]2(29)
=r2
tVarz1;:::;zt−1[t]+c2^2
t/divides.alt0rt/divides.alt0Ez1;:::;zt−1[2
t] (30)
Plug in, this leads to the following maximization problem, which is the desired equation.
arg max
bt/uni23A7/uni23AA/uni23AA/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23AA/uni23AA/uni23A91
TT
/summation.disp
t=1Ex[Gt()]
/dcurlyleft/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/dcurlymid/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/dcurlyright
A∶wealth gain−r2
tVarz1;:::;zt−1[t]
/dcurlyleft/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/dcurlymid/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/dcurlyright
B∶Risk due to uncertainty in past price−c2^2
t/divides.alt0rt/divides.alt0Ez1;:::;zt−1[2
t]
/dcurlyleft/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/dcurlymid/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/udcurlymod/dcurlyright
C∶Risk due to Future Price/uni23AB/uni23AA/uni23AA/uni23AA/uni23AA/uni23AC/uni23AA/uni23AA/uni23AA/uni23AA/uni23AD; (31)
where we have given each term a name for reference; the expectation is taken with respect to the augmented
data points zi:
ri→zi=ri+c/radical.alt2
^2
i/divides.alt0ri/divides.alt0iforri∈x: (32)
Under the GBM model (or when the optimal portfolio only weakly depends on z1;:::;zt−1), the optimal
tdoes not depend on z1;:::;zt−1, and so the objective can be further simplied to be
arg max
bt/braceleft.alt41
TT
/summation.disp
t=1Ez[Gt()]−c2^2
t/divides.alt0rt/divides.alt02
t/braceright.alt4; (33)
the rst term is the data augmentation for wealth gain, and the second term is a regularization for risk
control. Most of the experiments in this paper use this equation for training. When it does not work well,
the readers are encouraged to try the full training objective in Equation 31.
B.2 Extension to Multi-Asset Setting
It is possible and interesting to derive the data augmentation for a multi-asset setting. However, this
is hindered by the lack of a standard model to describe the co-motion of multiple stocks. For example,
it is unsure what the geometric Brownian motion should be for a multi-stock setting. In this case, we
tentatively suggest the following form of the formula for the injected noise, whose eectiveness and theoretical
justication are left for future work. Let St=(S1;t;:::;SN;t)be the prices of the stocks viewed as an N-
dimensional vector. The return is assumed to have covariance , then, by analogy with the discovery of this
work:
Si;t→Si;t+c/radical.alt4
/summation.disp
jij/divides.alt0rj/divides.alt0S2
jt (34)
for some white gaussian noise and some tunable parameter c. The matrix  has to be estimated by the
data using standard methods of estimating multi-stock volatility.
B.3 Non-Gaussian Noise
While the proposed algorithm proposes to use Gaussian noise for injection; the theory developed in this work
only requires the noise to have a nite second moment and, therefore, any other distribution works as well
for the particular kind of utility function we specied in Eq. (5). This is because this utility function only
17",2021-06-08T05:26:58Z,additnal discussproposed algorithm rivat equatt et gt art gt t art gt ez ez ez ez ez ez ez ez ez var ez plug ex gt var risk ez risk future price unr ez gt most quatextensmulti asset sei it for brownish i st t si si t nonoise w eq 
paper_qf_15.pdf,18,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","contains the second moments of the wealth return and is indierent to higher moments. Therefore, there is
one caveat to the present theory: when the utility function involves higher moments of the wealth return,
the utility of a certain type of noise injection is not indierent to the choice of the injection distribution.
The practitioners are recommended to analyze the specic utility function they use in our framework and
decide on the strength and distribution of the injected noise.
18",2021-06-08T05:26:58Z,trefore t
paper_qf_15.pdf,19,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","C Additional Theory and Proofs
This section contains all the additional theoretical discussions and the proofs.
C.1 Background: Classical Solution to the Portfolio Construction Problem
Consider a market with Nstocks, we denote the price of all these stocks as St∈RN, and one can likewise
dene the stock price return as
rt=St+1−St
St: (35)
which is a random variable with the covariance matrix Ct∶=Cov[rt]and the expected return gt∶=E[rt].
Our wealth at time step tis dened as Wt=Mt+∑N
int;iSt;i∶=nT
tStwhereMtis the amount of cash we
hold, andnithe shares of stock we hold for the i-th stock; we also dened vectors nt∶=(nt;i;::;nt;N;1)
andSt∶=(St;1;:::;St;N;Mt)where the cash is included in the denition of nt. As in the standard nance
literature, we assume that the shares are innitely divisible; usually, a positive nidenotes holding (long)
and a negative ndenotes borrowing (short). The wealth we hold initially is W0>0, and we would like
to invest our money on Nstocks; we denote the relative value of each stock we hold also as a vector
t;i=nt;iSt;i
Wt∈RN+1;is called a portfolio ; the central challenge in portfolio theory is to nd the best .
At timet, our relative wealth is Wt; after one time step, our wealth changes due to a change in the price of
the stocks:  Wt∶=Wt+1−Wt=Wtt⋅rt. The standard goal is to maximize the wealth return Gt∶=t⋅rt
at every time step while minimizing risk8. The risk is dened as the variance of the wealth change9:
Rt∶=R(t)∶=Varrt[Gt]=T
t/parenleft.alt1E[rtrT
t]−gtgT
t/parenright.alt1t=T
tCtt: (36)
The standard way to control risk is to introduce a \\\\risk regularizer"" that punishes the portfolios with a large
risk (Markowitz, 1959; Rubinstein, 2002). Introducing a parameter for the strength of regularization (the
factor of 1 /slash.left2 appears for convention), we can now write down our objective:
∗
t=arg max
U()∶=arg max
/bracketleft.alt3TGt−
2R()/bracketright.alt3: (37)
Here,Ustands for the utility function; can be set to be the desired level of risk-aversion. When gtandCt
is known, this problem can be explicitly solved (see Section C.1). However, one main problem in nance is
that its data is very limited and we only observe one particular realized data trajectory, and, therefore, gt
andCtcannot be accurately estimated.
Eq. (37) can be solved directly by taking derivative and set to 0; we can obtain
/uni23A7/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23A9∗
t=1
C−1
tgt;
Rt(∗
t)=1
2gT
tC−1
tgt:(38)
This formula is the standard formula to use when both gtandCtare known or can be accurately estimated
(Bouchaud and Potters, 2009). Meanwhile, when one nds diculty estimating gtor, more importantly, Ct,
then the above formula can go arbitrarily wrong. Let ^Cdenote our estimated covariance and ^ gthe estimated
mean10, then the in-sample risk and the true is respectively given by
/uni23A7/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23A9^Rt(∗
t)=1
2^gT
t^C−1
t^gt;
Rt(∗
t)=1
2^gT
t^C−1
tCt^C−1
t^gt:(39)
The readers are encouraged to examine the dierences between the two equations carefully.
8It is important to not to confuse the price return rtwith the wealth return Gt.
9In principle, any concave function in Gtcan be a risk regularizer from classical economic theory (Von Neumann and
Morgenstern, 1947), and our framework can be easily extended to such cases; one common alternative would be R(G)=log(G).
10For example, using some Bayesian machine learning model.
19",2021-06-08T05:26:58Z,additnal tory proofs  background assical soluttfconstruproblem consir stocks st st st st  cov our  mt st st wre mt is st st st mt as t stocks st  at      t gt t rt var rt gt  t marwitz rubinsteiintroduci gt re stands w se cannot eq rt   are  chaters meanw   note rt rt  t it gt t caewmamorgensterfor bayesian
paper_qf_15.pdf,20,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","Financial Terms Statistical Terms
UtilityU Loss function L
Expected Utility V RiskR
Data Augmentation Parameter  Estimator ^
True Parameter 
 True Parameter 
Priorp(
) Priorp()
Table 2: Correspondence table between the concepts in our general theory and the classical statistical
decision theory.
C.2 Analogy to Statistical Decision Theory and the Minimax Formulation
In Section 4.6, we mentioned that the procedure we used is analogous to the process of nding a Bayesian
estimator in a statistical decision theory. Here, we explain this analogy a little more (but keep in mind that
this is only an analogy, not a rigorous equivalence relation). Equation 14 of empirical utility can be seen as
an equivalent of the statistical risk function R; nding the optimal data augmentation strength is similar to
nding the best Bayesian estimator. To make an exact agreement with the statistical decision theory, we
also need to dene a prior over the risk in Equation 14:
r∶=Ep(
)[V(^())]=Ep(
)Ex∼p(x;
)E{xi}N∼pN(x)[U(x;^())] (40)
where we have written the distributions p(x; 
)as a function of the true parameters in our underlying
model.11In the main text, we have eectively assumed that p(
)is a Dirac delta distribution, but, in the
more general case, it is possible that the true parameter is not known or cannot be accurately estimated,
and it makes sense to assign a prior distribution to them. One can then nd the optimal data augmentation
with respect to r:∗=arg maxr.
See table 2 for the list of correspondences. This analogy breaks down at the following point: the Bayesian
estimator tries to nd ^that is as close to as possible, while, in our formulation, the goal is not to make
data augmentation as close as possible to 
.
One might also hope to establish an analogous \\\\minimax"" theory for the portfolio construction problem.
This can be done simply by replacing the expectation over p(
)with a minimization over 
:
rminimax∶=min

V(^()) (41)
and the best augmentation parameter can be found as the maximizer of this risk: ∗=maxmin
V.
C.3 Proof for no data augmentation
when there is no data augmentation, Et[Gt()]=btrtand Vart[Gt()]=0. One immediately see that the
utility is then maximized at
∗
t=/uni23A7/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23A91;ifrt≥0
−1;ifrt<0:(42)
We restate the theorem here.
Proposition 4. (Utility of no-data augmentation strategy.) Let the strategy be as specied in Eq. (42), and
let the price trajectory be generated with GBM in Eq. (6)with initial price S0, then the true utility is
U=[1−2(−r/slash.left)]r−
22(43)
whereU()is the utility function dened in Eq. (3).
11For example, in the GBM model, the true parameters are the growth rate rand the volatility , and so 
 =(r;), and
p(
)=p(r;).
20",2021-06-08T05:26:58Z,nancial terms statistical terms utity loss eeed utity risk data augmentatmeter estimator true meter true meter prr prr table corresponnce analogy statistical cistory mini max tisebayesiare equatbayesiato equatep ep ex idfrac one   bayesiaone  proof et gt art gt one  propositutity  eq eq eq for
paper_qf_15.pdf,21,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","Proof. For a time-dependent strategy ∗
t, the true utility is dened as12
U(∗)=ES′
0;S′
1;:::;S′
T;S′
T+1/bracketleft.alt41
TT+1
/summation.disp
t=1∗
tr′
t−/parenleft.alt4
2TT
/summation.disp
t=1(∗
tr′
t)2−ES′
0;S′
1;:::;S′
T;S′
T+1[∗
tr′
t]2/parenright.alt4/bracketright.alt4 (44)
whereS′
1;:::;S′
T;S′
T+1is an independently sampled distribution for testing, and r′
t∶=S′
t+1−S′
t
S′
tare their respec-
tive returns. Now, we note that we can write the price update equation (the GBM model) in terms of the
returns:
St+1=(1+r)St+tStt→rt=r+t (45)
which means that rt∼N(r;2)obeys a Gaussian distribution. Therefore,
U(∗)=r
TT
/summation.disp
t=1∗
t−2
2TT
/summation.disp
t=1(∗
t)2: (46)
Now we would like to average over ∗
t, because we also want to average over the realizations of the
training set to make the true utility independent of the sampling of the training set (see Section 4.6 for an
explanation).
Recall that the strategy is dened as
∗
t=/uni23A7/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23A91;ifrt≥0
−1;ifrt<0:=(rt≥1)−(rt<1) (47)
for a training set {S0;:::;ST}, and  is the Heaviside step function. We thus have that
/uni23A7/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23A9(∗
t)2=1;
ES1;:::ST+1[∗
t]=ES1;:::ST+1[(rt≥0)−(rt<0)]=1−2(−r/slash.left)(48)
where  is the Gauss c.d.f. We can use this to average the utility over the training set; noticing that the
training set and the test set are independent, we can obtain
U=ES1;:::ST+1[U(∗)] (49)
=1
TT
/summation.disp
t=1[1−2(−r/slash.left)]r−
21
TT
/summation.disp
t=12(50)
=[1−2(−r/slash.left)]r−
22: (51)
This nishes the proof. /uni25FB
C.4 Proof for Additive Gaussian noise
Before we prove the proposition, we rst prove that the strategy is indeed the one given in Eq. (52):
Lemma 1. The maximizer of the utility function in Eq. 4 with additive gaussian noise is
∗
t()=/uni23A7/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23A9rtS2
t
22; if−1<rtS2
t
22<1;
sgn(rt);otherwise.(52)
Proof. With additive Gaussian noise, we have
/uni23A7/uni23AA/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23AA/uni23A9Et[Gt()]=tEt[~rt]=tEt/bracketleft.alt2St+1+t+1t+1−St−tt
St/bracketright.alt2=tSt+1−St
St=trt;
Vart[Gt()]=2
tVart[~rt]=2
tVart/bracketleft.alt2t+1t+1−tt
St/bracketright.alt2=222
t
S2
t;(53)
12While we mainly use  (x)as the Heaviside step function, we overload this notation a little. When we write  (x>0), 
is dened as the indicator function. We think that this is harmless because the dierence is clearly shown by the argument to
the function.
21",2021-06-08T05:26:58Z,proof for  st st st trefore  serecall isi  gam  proof additive before eq lea t eq proof with et gt et et st st st st st st art gt art art st w isi w
paper_qf_15.pdf,22,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","where the last line follows from the denition for additive Gaussian noise that 1=:::T=. The training
objective becomes
∗
t=arg max
t/braceleft.alt41
TT
/summation.disp
t=1Et[Gt()]−
2Vart[Gt()]/braceright.alt4 (54)
=arg max
t/braceleft.alt41
TT
/summation.disp
t=1trt−22
t
S2
t/braceright.alt4: (55)
This maximization problem can be maximized for every trespectively. Taking derivative and set to 0, we
nd the condition that ∗
tsatises
@
@t/parenleft.alt4trt−22
t
S2
t/parenright.alt4=0 (56)
/leftrightline→∗
t()=rtS2
t
22: (57)
By denition, we also have /divides.alt0t/divides.alt0≤1, and so
∗
t()=/uni23A7/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23A9rtS2
t
22; if−1<rtS2
t
22<1;
sgn(rt);otherwise,(58)
which is the desired result. /uni25FB
We would like to comment that, although we paid special attention to enforcing the constraint /divides.alt0t/divides.alt0≤it
is often not needed in practice because investors tend to be quite risk-averse, and it is hard to imagine that
any investor would invest all his or her money in the nancial market such that t=1. Mathematically, this
means that it is often the case that ≥/divides.alt0rt/divides.alt0S2
t
22. Therefore, for what comes, we assume that ≥/divides.alt0rt/divides.alt0S2
t
22for all
tfor notational simplicity; note that, even without assumption, the conclusion that a multiplicative noise is
the better kind of data augmentation will not change. Now we are ready to prove the proposition.
Proposition 5. (Utility of additive Gaussian noise strategy.) Let the strategy be as specied in Eq. (52),
and other conditions the same as in Proposition 1, then the true utility is
UAdd=r2
22TESt/bracketleft.alt4(∑trtSt)2
∑t(rtSt)2(/summation.disp
trtS2
t)/bracketright.alt4: (59)
Proof. The beginning of the proof is similar to the case with no data augmentation. Following the same
procedure, we obtain an equation that is the same as Eq. (46):
U(∗)=r
TT
/summation.disp
t=1∗
t−2
2TT
/summation.disp
t=1(∗
t)2: (60)
Plug in the preceding lemma, we have
U(∗)=r
TT
/summation.disp
t=1rtS2
t
22−2
2TT
/summation.disp
t=1/parenleft.alt4rtS2
t
22/parenright.alt42
: (61)
This utility is a function of the data augmentation strength . For a xed training set, we would like to
nd the best that maximizes the above utility. Note that the maximizer of the utility is dierent depending
on the sign of ∑rtS2
t. Taking derivative and set to 0, we obtain that
(∗)2=/uni23A7/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23A92
2r∑T
t(rtS2
t)2
∑T
trtS2
t;if∑T
trtS2
t>0
∞; otherwise.(62)
Plug in to the previous lemma, we have
∗
t(∗)=rtS2
t
2(∗)2=rrtS2
t
2∑trtS2
t
∑t(rtS2
t)2/parenleft.alt4/summation.disp
trtS2
t/parenright.alt4: (63)
22",2021-06-08T05:26:58Z,t et gt art gt  taki by  matmatically trefore  propositutity  eq propositadd st st st proof t followi eq plug  for note taki plug
paper_qf_15.pdf,23,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","One thing to notice is that the optimal strength is independent of , which is an arbitrary value and dependent
only on the investor's psychology. Plug into the utility function and take expectation with respect to the
training set, we obtain
Uadd=ES1;:::;ST+1[U(∗(∗))] (64)
=U(∗)=r
TT
/summation.disp
t=1∗
t(∗)−2
2TT
/summation.disp
t=1[∗
t(2)]2(65)
=r2
22TES1;:::;ST+1/bracketleft.alt4(∑trtSt)2
∑t(rtSt)2/parenleft.alt4/summation.disp
trtS2
t/parenright.alt4/bracketright.alt4: (66)
This nishes the proof. /uni25FB
Remark. Notice that the term in the expectation generally depends on Tin a non-trivial way and cannot be
obtained explicitly. However, it does not cause a problem since the nal goal is to compare it with the result
in the next section.
C.5 Proof for General Multiplicative Gaussian noise
Before we prove the proposition, we rst nd the strategy for this case. Note that the term 2
t+2
t+1appears
repetitively in this section, and so we dene a shorthand notation for it:1
2(2
t+2
t+1)∶=2
t.
Lemma 2. The maximizer of the utility function in Eq. 4 with multiplicative gaussian noise is
∗
t()=/uni23A7/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23A9rtS2
t
22
t=rtS2
t
(2
t+2
t+1);if−1<rtS2
t
22
t<1;
sgn(rt); otherwise.(67)
Proof. With additive Gaussian noise, we have
/uni23A7/uni23AA/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23AA/uni23A9Et[Gt()]=tEt[~rt]=tEt/bracketleft.alt2St+1+t+1t+1−St−tt
St/bracketright.alt2=tSt+1−St
St=trt;
Vart[Gt()]=2
tVart[~rt]=2
tVart/bracketleft.alt2t+1t+1−tt
St/bracketright.alt2=(2
t+t+1)22
t
S2
t:(68)
We see that1
2(2
t+2
t+1)∶=2
treplaces the role of 2 2for additive Gaussian noise. The training objective
becomes
∗
t=arg max
t/braceleft.alt41
TT
/summation.disp
t=1Et[Gt()]−
2Vart[Gt()]/braceright.alt4 (69)
=arg max
t/braceleft.alt41
TT
/summation.disp
t=1trt−2
t2
t
S2
t/braceright.alt4: (70)
This maximization problem can be maximized for every trespectively. Taking derivative and set to 0, we
nd the condition that ∗
tsatises
@
@t/parenleft.alt4trt−2
t2
t
S2
t/parenright.alt4=0 (71)
/leftrightline→∗
t(t)=rtS2
t
22
t: (72)
By denition, we also have /divides.alt0t/divides.alt0≤1, and so
∗
t()=/uni23A7/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23A9rtS2
t
22
t; if−1<rtS2
t
22
t<1;
sgn(rt);otherwise,(73)
which is the desired result. /uni25FB
23",2021-06-08T05:26:58Z,one plug add st st  remark notice tiproof genl multiplicative before note lea t eq proof with et gt et et st st st st st st art gt art art st  t et gt art gt  taki by
paper_qf_15.pdf,24,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","Remark. For fair comparison with the previous section, we also assume that ≥/divides.alt0rt/divides.alt0S2
2
t. Again, this is the
same as assume that the investors are reasonably risk-averse and is the correct assumption for all practical
circumstances.
Proposition 6. (Utility of general multiplicative Gaussian noise strategy.) Let the strategy be as specied
in Eq. (52), and other conditions the same as in Proposition 1, then the true utility is
Umult=r2
22[1−(−r/slash.left)]: (74)
Proof. Most of the proof is similar to the Gaussian case by replaing 2with2
t. Following the same
procedure, We have:
U(∗)=r
TT
/summation.disp
t=1∗
t−2
2TT
/summation.disp
t=1(∗
t)2: (75)
Plug in the preceding lemma, we have
U(∗)=r
TT
/summation.disp
t=1rtS2
t
22
t−2
2TT
/summation.disp
t=1/parenleft.alt4rtS2
t
22
t/parenright.alt42
: (76)
This utility is a function of of the data augmentation strength t, and, unlikely the additive Gaussian
case, can be maximized term by term for dierent t. For a xed training set, we would like to nd the best
tthat maximizes the above utility. Note that the maximizer of the utility is dierent depending on the sign
ofrtS2
t. Taking derivative and set to 0, we obtain that
(∗
t)2=/uni23A7/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23A92
2rrtS2
t;ifrtS2
t>0
∞; otherwise.(77)
Plug in to the previous lemma, we have
∗
t(∗
t)=rtS2
t
2(∗
t)2=r
2(rt): (78)
One thing to notice that the optimal strength is independent of , which is an arbitrary value and dependent
only on the psychology of the investor. Plug into the utility function and take expectation with respect to
the training set, we obtain
Uadd=ES1;:::;ST+1[U(∗(∗))] (79)
=U(∗)=r
TT
/summation.disp
t=1∗
t(∗
t)−2
2TT
/summation.disp
t=1[∗
t(∗
t)]2(80)
=r2
2[E]t[(rt)]−r2
22(rt)[E]t[(rt)] (81)
=r2
22[1−(−r/slash.left)] (82)
=r2
22(r/slash.left) (83)
This nishes the proof. /uni25FB
This result can be directly compared to the results in the previous section, and the following remark
shows that the multiplicative noise injection is the best kind of noise.
Remark. (Innite augmentation strength.) For all of the theoretical results, there is a corner case when
the optimal injection strength is equal to innity, which leads to a non-investing portfolio =0. This case
requires special interpretation. This corner case is due to the fact the underlying model we use has a constant,
positive expected price return equal to r, and so it leads to the bizarre data augmentation which eectively
amounts to throwing away all the training points with <0return. This is unnatural for a real market. It is
possible for the real market to have short-term negative return when conditioned on the previous prices, and
so one should not simply discard the negative points. Therefore, in our algorithm section, we recommend
treating the training points with positive and negative return equally by taking the absolute value of the data
augmentation strength and ignoring ∞case.
24",2021-06-08T05:26:58Z,remark for ag articial intellence propositutity  eq propositmult proof most followi  plug  for note taki plug one plug add   remark ifor    it trefore
paper_qf_15.pdf,25,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","C.6 Proof of Theorem 1
Remark. Combining the above propositions, one can quickly obtain that, if ≠0, thenUmult>Uaddand
Umult>Uno−augwith probability 1(Proof in Appendix).
Proof . We rst show that Umult>Uadd. Recall that
UAdd=r2
22TESt/bracketleft.alt4(∑trtSt)2
∑t(rtSt)2/parenleft.alt4/summation.disp
trtS2
t/parenright.alt4/bracketright.alt4 (84)
≤r2
22TESt/bracketleft.alt4(∑trtSt(rt>0))2
∑t(rtSt)2/parenleft.alt4/summation.disp
trtS2
t/parenright.alt4/bracketright.alt4 (85)
≤r2
22TESt/bracketleft.alt4(∑trtSt(rt>0))2
∑t(rtSt)2/bracketright.alt4 (86)
≤(Cauchy Inequality)r2
22TESt/bracketleft.alt4/summation.disp
t(rtSt(rt>0))2
(rtSt)2/bracketright.alt4 (87)
=r2
22TESt/bracketleft.alt4/summation.disp
t(rt>0)/bracketright.alt4 (88)
=r2
22T/bracketleft.alt4/summation.disp
tP(rt>0)/bracketright.alt4 (89)
=r2
22(r/slash.leftt)=Umult: (90)
The Cauchy equality holds if and only if S1=:::=ST+1; this event has probability measure 0, and so, with
probability 1, Uadd<Umult.
Now we prove the second inequality. Recall that
Uno−aug=[1−2(−r/slash.left)]r−
22: (91)
We divide into 2 subcases. Case 1: >r
2. We have
Uno−aug<−2r(−r/slash.left)<0<Umult: (92)
Case 2: 0 <≤r
2. We have
Uno−aug<[1−2(−r/slash.left)]r (93)
<(r/slash.left)r (94)
≤r2
22(r/slash.leftt)=Umult: (95)
This nishes the proof. /uni25FB
C.7 Augmentation for a naive multiplicative noise
In the discussion and experiment sections in the main text, we also mentioned a \\\\naive"" version of the
multiplicative noise. The motivation for this kind of noise is simple, since the underlying noise in the
theoretical models are all of the form 2S2
t, and so it is tempting to also inject noise that mimicks the
underlying noise. It turns out that this is not a good idea.
In this section, we let t=0Stfor some positive, time-independent 0. Our goal is to nd the optimal
0. With the same calculation, one nds that the learned portfolio is given by the same formula in Lemma 2:
∗
t()=rtS2
t
22
t: (96)
With this strategy, one can nd the optimal noise injection strength to be given by the following proposition.
25",2021-06-08T05:26:58Z,proof torem remark combini mult add and mult uno proof  proof  mult add recall add st st st st st st st st st catc inequality st st st st mult t catc add mult  recall uno  case  uno mult case  uno mult  augmentatit it ist for our with lea with
paper_qf_15.pdf,26,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","Proposition 7. Let the portfolio be given by Eq. (96) and let the price be generated by the GBM, then the
optimal noise strength is
(∗
0)2=/uni23A7/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23A9∑T
tr2
t
∑T
trt;if∑T
trt>0
∞; otherwise.(97)
Proof. As before, We have:
U(∗)=r
TT
/summation.disp
t=1∗
t−2
2TT
/summation.disp
t=1(∗
t)2: (98)
Plug in the portfolio, we have
U(∗)=r
TT
/summation.disp
t=1rtS2
t
22
t−2
2TT
/summation.disp
t=1/parenleft.alt4rtS2
t
22
t/parenright.alt42
: (99)
Plug in2
t=2
0S2
tand take derivative, we obtain that
(∗
0)2=/uni23A7/uni23AA/uni23AA/uni23A8/uni23AA/uni23AA/uni23A9∑T
tr2
t
∑T
trt;if∑T
trt>0
∞; otherwise.(100)
This nishes the proof. /uni25FB
C.8 Data Augmentation for a Stationary Portfolio
While the main theory focused on the case with a dynamic portfolio that is updated through time, we
also present a study of the stationary portfolio in this section. While this kind of portfolio is less relevant
for deep-learning-based nance, we study this case to show that, even in this setting, there is still strong
motivation to inject noise of strength rtS2
t. Now we state the formal denition of a stationary porfolio.
Denition 1. A portfolio {t}T
t=1is said to be stationary if t=for some constant for allt.
In the language of machine learning, this corresponds to choosing our model as having only a single
parameter, whose output is input independent:
f(x)=: (101)
In traditional nance theory, stationary portfolios have been very important. In practice, most portfolios
are \\\\approximately"" stationary, since most portfolio managers tend to not to change their portfolio weight
at a very short time-scale unless the market is very unstable due to market failure or external information
injection. For a stationary portfolio, one still would like to maximize the utility function given in Eq. 4.
For conciseness, we only examine the case when we inject a general time-dependent noise. The curious
readers are encouraged to examine the cases with no data augmentation and with constant data augmenta-
tion. As before, the following lemma gives the portfolio of the empirical utility. Again, we use the shorthand:
1
2(2
t+2
t+1)∶=2
t. For illustrative purpose, we have ignore the corner cases of being greater than 1 or
smaller than −1.
Lemma 3. The stationary portfolio that maximizes the utility function in Eq. 4 with multiplicative gaussian
noise is
∗
t()=∑T
trt
2∑T
t(2
t/slash.leftS2
t)(102)
Proof sketch . The proof follows almost the same as the previous sections. With a slight dierence that
is no more time-dependent and can be taken out of the sum.
Proposition 8. Let the portfolio be given in Eq. (102) , then an augmentation strength satisfying the relation
(∗
t)2=crtS2
t (103)
is an optimal data augmentation for constant c=22
r.
26",2021-06-08T05:26:58Z,proposit eq proof as  plug plug  data augmentatstatnary tfw w   iiifor eq for t as ag articial intellence for lea t eq proof t with proposit eq
paper_qf_15.pdf,27,"Theoretically Motivated Data Augmentation and Regularization for
  Portfolio Construction","  The task we consider is portfolio construction in a speculative market, a
fundamental problem in modern finance. While various empirical works now exist
to explore deep learning in finance, the theory side is almost non-existent. In
this work, we focus on developing a theoretical framework for understanding the
use of data augmentation for deep-learning-based approaches to quantitative
finance. The proposed theory clarifies the role and necessity of data
augmentation for finance; moreover, our theory implies that a simple algorithm
of injecting a random noise of strength $\\\\sqrt{|r_{t-1}|}$ to the observed
return $r_{t}$ is better than not injecting any noise and a few other
financially irrelevant data augmentation techniques.
","Proof Sketch . The proof is also simple and very similar to the proofs for the dynamic portfolio case. One
rst solve for the optimal augmentation strength ∗
tand nd that it satises the following relation
/summation.disp
t(∗
t)2
S2
t=cT
/summation.disp
trt (104)
withc=22
r, and then it suces to check that the following is one solution
(∗
t)2=crtS2
t: (105)
This nishes the proof sketch. /uni25FB
The curious readers are encouraged to check the intermediate steps. We see that, even for a stationary
portfolio case, there is still strong motivation for using a augmentation with strength proportional to rtS2
t.
We would like to compare this with the best achievable stationary portfolio, which is solved by the
following proposition.
Proposition 9. (Optimal Stationary Portfolio). The optimal stationary portfolio for GBM is ∗
stat=r
2,
i.e., for any other portfolio ,U(∗
stat)≥U().
Proof. It suces to nd the maximizer portfolio of the true utility:
∗
stat=arg max
/braceleft.alt3r−
222/braceright.alt3: (106)
The solution is simple and given by
∗
stat=r
2: (107)
This completes the proof. /uni25FB
Note that the above optimality result holds for both in-sample counterfactual utility and out-of-sample
utility. This proposition can be seen as the discrete-time version of the famous Merton's portfolio solution
(Merton, 1969), where the optimal stationary portfolio is also found to ber
2. In fact, it is well-known that,
for a static market, the stationary portfolios are optimal, but this is beyond the scope of this work (Cover
and Thomas, 2006).
Combining the above two propositions, one obtain the following theorem.
Theorem 3. The stationary portfolio obtained by training with data augmentation strength in given in
Proposition 8 is optimal, i.e., it is no worse than any other stationary portfolio.
Proof. Plug in=crtS2
t, we have that the trained portfolio is ∗=r
2, which is equivalent to the
optimal stationary portfolio, and we are done. /uni25FB
This shows that, even for a stationary portfolio, it is useful to use the proposed data augmentation
technique.
27",2021-06-08T05:26:58Z,proof skett one  t   propositoptimal statnary tft proof it t  note  mortomortoicothomas combini torem t propositproof plug 
paper_qf_16.pdf,1,Comment on recent claims by Sornette and Zhou,"  Comment on recent claims by Sornette and Zhou: D. Sornette and W. Zhou,
Quantitative Finance 2 (6), 468-481 (2002); Evidence of a Worldwide Stock
Market Log-Periodic Anti-Bubble Since Mid-2000, cond-mat/0212010;
Renormalization Group Analysis of the 2000-2002 anti-bubble in the US SP 500
index, physics/0301023
","arXiv:cond-mat/0302141v1  7 Feb 2003CommentonrecentclaimsbySornetteandZhou
AndersJohansen
(1)RisøNationalLaboratory,Department ofWindEnergy
Frederiksborgvej399,P.O. 49, DK-4000Roskilde,Denmark
e-mail: anders.johansen@risoe.dk,URL: http://www.riso e.dk/vea/staff/andj/
November6,2018
Owingtoalargenumberofpressreleasesinwhichmyworkhasb eenheavilycitedinsupport oftherecentSP500
prediction by Sornette and Zhou (SZ), I feel it necessary to c omment on this work and their follow-up preprint [1].
Thepredictions bySZregarding thefuturebehaviour ofinpa rticular theSP500hasreceivedquitesomeattention and
asubstantial part of theevidence presented supporting the predictions of SZisbased on mynumerical analysis of the
Nikkeiintheperiod1990-2000[9]. Hence,Ifeelurgedtopre sent myownviewonthelog-periodic powerlaw(LPPL)
analysis of the ﬁnancial markets (FM) made by SZ and in partic ular on the claims of LPPL behaviour in the FM in
general and the SP500 in particular as well as the prediction s that SZ derive from their analysis. In 1996 and 1997
twogroups independently proposed that power lawswithcomp lex exponents, e.g.,
p(t)≈A+B(tc−t)z+C(tc−t)zcos(ωlog(tc−t)+φ) (1)
were relevant modeling tools for the description of price p(t)increases a few years prior to very large crashes [3, 5].
The background for the original suggestion of LPPL signatur es in the ﬁnancial markets was an analogy between
second order phase transitions and rupture, inthis context a“rupture inmarket belief”. Furthermore, it wasproposed
thatthedomainofthepowerlawexponent shouldnotberestri cted toreal valuesonly. Consequently, theanalogywas
notconﬁnedtoapurepowerlawbehaviour butallowedforapow erlawbehaviour decoratedbyso-calledlog-periodic
oscillations, retrospectively to be seen as a quite provoca tive claim [8]. Disregarding the rupture analogy (for which
the empirical evidence is scarce), one may also consider the proposed frame work simply asan Ansatz
dF(x)
dlogx=αF(t)+higher order terms (2)
for thedynamical rescaling of aprice(or somerelated quant ity)F(x)asafunction of “timetothecrash” x=tc−t.
Such anAnsatzapproach is not uncommon in the ﬁeld of critical phenomena. B efore commenting further on the
recent claims by SZ, I should stress that the present author w ith D. Sornette in [7] has presented a synthesis of two
independent research directions, namely that of LPPLanaly sis on the one hand and the “outlier” classiﬁcation of the
largest negative market events on the other. In essence, tha t paper propose an objective criterion for the selection
of events which couldhave LPPL precursors. The conclusion of that analysis is tha t a large negative market event
whichclassiﬁesandanoutlier iseither preceded byanLPPLs peculative bubble oranunsuspected (tojudgefrom the
market response) historical event. (This does not exclude t he possibility of “other precursory events.”) Thestatisti cal
evidence for this proposition is quite convincing. Further more, a statistical analysis of what has been referred to as
the two “physical variables” zandω[4, 7] has been presented. (The background for the term “phys ical variables” is
that the variables A,B,C,φ are nothing but units and tcis event speciﬁc.) In this context, it is worth noting that th e
“double cosine” equation proposed by SZ, i.e.,
p(t)≈A+B(tc−t)z+C(tc−t)zcos(ωlog(tc−t)+φ1)+D(tc−t)zcos(2ωlog(tc−t)+φ2)(3)
has different phases. Since the phases in eq.’s (1) and (3) si mply are time units (changing the time units of the data
from for example days to months only changes radically the va lue ofφ, as it should, and not the other variables)
needed because of the log(tc−t)−φ= log((tc−t)/φ′), a sound theoretical justiﬁcation for such a “phase-shift”
",2003-02-07T09:55:32Z,  coent orecent  articial intellence ms by corvee and hou anrs sois natnal laboratory partment wind energy freriksborg ve rosk nmark november owi to large number of  releasiwhimy work has corvee hou t predins regardi is based  kei it d nce feel urged to pre it furtr consequently disregardi aat suaat approacorvee laall it ls  t status furtr t isince
paper_qf_16.pdf,2,Comment on recent claims by Sornette and Zhou,"  Comment on recent claims by Sornette and Zhou: D. Sornette and W. Zhou,
Quantitative Finance 2 (6), 468-481 (2002); Evidence of a Worldwide Stock
Market Log-Periodic Anti-Bubble Since Mid-2000, cond-mat/0212010;
Renormalization Group Analysis of the 2000-2002 anti-bubble in the US SP 500
index, physics/0301023
","stress that the conclusion of the analysis of [4] of the value s obtained for the physical variables zandω(based on a
Gaussian null-hypothesis for the pdf) including over 30 cas e studies is that
ω≈6.36±1.56z≈0.33±0.18. (4)
Unfortunately, a comparison between this statistical esti mate and the more recent analysis presented by SZ is com-
pletely absent. In fact, making a similar statistical analy sis of the results presented in [1] on anti-bubbles (since SZ
advocates the existence of bubbles and anti-bubbles from a s ymmetry perspective, a compassion between the es-
timates 4 for bubbles and their results for anti-bubbles is n ecessary and easy) yields a uniform distribution of the
physical variable ωwith high probability. What I ﬁnd quite peculiar is that I wit h Sornette proposed in [2] a set of
very basic assumptions which a LPPL analysis of ﬁnancial dat a should full-ﬁll: 1) Landau expansions, i.e.,, eq. (2);
2) Bounded rationality (or “conservation laws”), e.g., prices should not go to inﬁnity as they do in the the so-calle d
bullish anti-bubble of SZ, where they accept B >0; 3) Symmetry considerations, e.q., Statistical long-term asym-
metry where market drops are fast and market increases are sl ow; 4) Probabilistic framework, due to the fact that the
ﬁnancial markets are a non-closed system, which however may behave as a semi-closed system over time; 5) Most
importantly, any validation of a model must come from the data, e.g., a statistical analysis of the empirical results
obtained from the numerous case studies presented in the lit erature. My main objection to the work of SZ(as well as
those of others others) is that the fundamental concept of cr iticality has apparently been abandoned, e.g., many case
studies have been presented by DZ (among others) where z≈1seems to have become so natural that nobody seems
to question it anymore. Another violation of the framework p roposed above is that SZ now have changed the control
parameter x=tc−t(or−xfor anti-bubbles) in eq. (1) to x=|tc−t|. This means that another restriction coming
form the data has disappeared. A comment on the so-called “fr actal” concept, (LPPL within LPPL) where authors
have claim such a signature on a single case study in which the analysis by eye has identiﬁed an single example.
As I previously performed an extensive analysis of such “fra ctal structures” mainly in collaboration with Matt Lee,
another former post doc of Sornette. We analyzed over 10 diff erent statistical indexes of stock, currencies and bonds
withoutanyconclusiveresults. Eachdatasethadalengthof 2-4years. Wedidgetaslightly(1-5%)betterbinary(“up
or down”) prediction rate for the US market, the DAX and the FT SE on a two to four week prediction horizon. As
describedindetailin[6]therealsuccesshoweverwaswitha LPPLanalysisontimescalesof1-2yearsusingthesame
time period for the data. It should be stressed that one of the crucial criteria for this success rate of crash and LPPL
bubble identiﬁcation was the restriction B <0as well as the bound on the physical variables zandωcorresponding
(4). Most importantly, I wish to stress that the postulated s imilarity between the behaviour of the Nikkei index in the
period 1990-2000 years withthat of the SP500inthepast coup le of years iscompletely unsubstantiated inthepapers
bySZ.Iﬁnditinappropriate thatmynumerical analysis pres ented in[9]canbeusedtosupport thepresent prediction
of SZ. Just to mention three serious discrepancies between t he two countries (Japan and the U.S.A.),the value of the
log-periodic frequency differs by a factor of 2 despite the “ double cosine” eq.(3. Furthermore, the Nikkei did not go
througha”classical” LPPLbubblepriortotheonsetofthean ti-bubble astheU.S.market(Nasdaq) did. (Areal-estate
bubbleseemstobethefavorite explanation forthis. Thesta tistical evidence sofaronanti-bubbles seemsthatexterna l
shocks such as, e.g.,theeffect of the theburst of theAsian bubble of ’97on thema jor western stock markets, are“the
cause” and not internally generated.) Last, but not least, t he Nikkei analysis was based on 9 years of data, with the
ﬁrst data point objectively being chosen as the peak of the ma rket price. The present SP500 prediction of SZ is not
consistent withthese facts.
References
[1] D. Sornette and W. Zhou, Quantitative Finance 2 (6), 468- 481 (2002); Evidence of a Worldwide Stock Market
Log-Periodic Anti-Bubble Since Mid-2000, cond-mat/02120 10; Renormalization Group Analysis of the 2000-
2002 anti-bubble in the USSP500 index, physics/0301023
[2] A.Johansen and D.Sornette, Eur. Phys.J. B9, pp. 167-174 (1999).
[3] D.Sornette, A.Johansen and J.P.Bouchaud, J. Phys. I.Fr ance 6 pp. 167-175 (1996)
",2003-02-07T09:55:32Z,unfortunately iwhat corvee land nd syetry statistical probabistic most my anotr  as ma  corvee  eadata set had h of  did get slhtly as analysis otimescale of it most  kei just  furtr  kei bubble prr to t onset of t anasdaq real t st  last  kei t referenccorvee hou quantitative nance evince worldwi stock market log dic anti bubble since mid re normalizatgroup analysis socorvee eur ph ys corvee so chaph ys fr
paper_qf_16.pdf,3,Comment on recent claims by Sornette and Zhou,"  Comment on recent claims by Sornette and Zhou: D. Sornette and W. Zhou,
Quantitative Finance 2 (6), 468-481 (2002); Evidence of a Worldwide Stock
Market Log-Periodic Anti-Bubble Since Mid-2000, cond-mat/0212010;
Renormalization Group Analysis of the 2000-2002 anti-bubble in the US SP 500
index, physics/0301023
","[5] J.A. Feigenbaum and P.G.O. Freund, (1998), Modern Physi cs Letters B 12: 57. J. A. Feigenbaum and P.G.O.
Freund, (1996), Int. J. Moder Phys. B10: 3737
[6] D.Sornette and A.Johansen, Quantitative Finance vol.1 pp. 452-471 (2001)
[7] A.Johansen and D.Sornette, Endogenous versus Exogenou s Crashes in Financial Markets. Submitted toJournal
of Economic Dynamics and Control.
[8] A.Johansen, Europhys. Lett.60 (5), pp.809-810 (2002) a nd references therein.
[9] A.Johansen andD.Sornette,Int.J.Mod.Phys.10,pp.563 -575(1999), A.JohansenandD.Sornette,Int.J.Mod.
Phys. C11 no. 2pp. 359-364 (2000)
",2003-02-07T09:55:32Z,genbaum found morph ys ters genbaum found imo ph ys corvee soquantitative nance socorvee endogenous exo geno ass nancial markets submied journal economic dynamics contrsoeur  socorvee imod ph ys soand corvee imod ph ys
paper_qf_17.pdf,1,"Generative Models for Stochastic Processes Using Convolutional Neural
  Networks","  The present paper aims to demonstrate the usage of Convolutional Neural
Networks as a generative model for stochastic processes, enabling researchers
from a wide range of fields (such as quantitative finance and physics) to
develop a general tool for forecasts and simulations without the need to
identify/assume a specific system structure or estimate its parameters.
","Abstract  
The present paper aims to demonstrate the usage of 
Convolutional Neural Networks as a generative 
model for stochastic process es, enabling research-
ers from a wide range of fields – such as quantita-
tive finance and physics – to develop a general to ol 
for forecasts and simulations without the need to 
identify/assume a specific system structure or esti-
mate its parameters.  
1 Introduction  
It is widely known that procedures of identification, estima-
tion and simulation of stochastic processes are somewhat  
already well established in mathematics, computing and 
related fields and sub -fields such as statistics, physics and 
econometrics. On the other hand, most of them rely on the 
fact that, to work properly, the observer must impose a sys-
tem structure to esti mate its respective parameters – see 
Hamilton [1994] and Hayashi [2000 ], and, very often, as-
sume a probability distribution function.  
On top of this specified and estimated system, the re-
searcher can accomplish tasks such as forecasting and simu-
lating the system’s states.  
In parallel to these strategies, it is possible to verify the 
revival of artificial neural networks, which have  mostly 
been forgotten during the 1990s and mid -2000s because 
they were considered black -boxes without any intelligibility 
of its inner computations, as can be seen in Benitez et. al  
[1997] and in Kolman and Margaliot [2007 ].  
One of the most important reasons for this revival is due 
to the success of Deep Neural Networks (neural networks 
with several layers), which ha ve been succe ssfully em-
ployed on tasks such as pattern recognition, classification 
and prediction, performing better than humans do. These 
tasks encompass a wide range of different problems such as 
computer vision problems – character recognition, object 
recognition an d others; audio processing; and defeating 
world -class human players in complex computer games, Go 
and Chess. [see Silver et. al , 2017; Oshri and Khandwala, 
2015 ]. 
A great part of these successes has been attributed to a 
new hybrid architecture called Convo lutional Neural Net-
works, where convolutional filters are placed and stacked composing deep networks – enabling the filtering of desired 
multiscale/multidimensional features that enhance classifi-
cation/forecasting capabilities – that can be interpreted and  
understood – plus a Softmax classifier/regressor, which ba-
sically works as a normalized multinomial logistic regressor 
– see Bishop [2006 ]. 
Given this huge success, the idea of the present paper is 
to adapt this specific kind of deep neural network, which  has 
been successfully applied on the generation of raw audio 
waveforms as in Van den Oord et. al  [2016 -a], images as in 
Van den Oord et. al  [2016 -b], text [see Józefowicz et. al , 
2016 ] and multivariate systems [see Borovykh et. al , 2017 ]; 
and show that th is kind of neural network can also be used 
to work as a generative model on top of data retrieved from 
a wide set of known deterministic/stochastic data generation 
processes – from the simplest to the most complex process-
es, from damped oscillators to auto regressive conditional 
heteroskedastic (ARCH) and jump -diffusion models.  
Avoiding the traditional identification and estimation 
procedures, a new approach is proposed here: estimate only 
the hyperparameters of a convolutional neural network, i.e. 
number of  convolutional layers, discretization scheme (en-
coding) and dilations. Hence, we hope to demonstrate that 
data generation processes can be understood and simulated 
using a new statistical approach, without the need of assum-
ing any hard -structural form or i mposing any kind of re-
strictions. Moreover, as the data is encoded/decoded outside 
the neural network, by means of transforming a regression 
task into a classification task, no assumption about the dis-
tribution of the data generating process must be made. In 
addition to that, we demonstrate that the original data distri-
bution can be recovered.   
Potential applications are huge. Being able to simulate 
and predict stochastic processes properly is desired in a 
wide range of sub -fields within the scope of finan ce and 
economics, such as asset pricing, time series analysis and 
risk analysis.  
To accomplish this goal, we have modified an existing 
Python/Tensorflow implementation of WaveNet as in Van 
den Oord et. al  [2016 -a], in order to read synthetic time se-
ries in stead of raw audio files, avoiding any discussion 
about implementation strategies, focusing solely on the 
mathematical/statistical aspects of its usage. After that, we 
simulate each data generating process using a properly Generative Models for Stochastic Processes Using Convolutional Neural Networks  
 
Fernando Fernandes Neto  
University of São Paulo - Brazil   
fernando_fernandes_neto@usp.br  
 
 
Rodrigo de Losso da Silveira Bueno  
University of São Paulo - Brazil   
delosso @usp.br  
 
 ",2018-01-09T03:35:20Z,abstra t convolutnal neural networks introduit ohamtohayashi oibe nitz kmaarial ot one ep neural networks tse go css sshri kh and wal covo neural net soft max bishgivevaor vaor borvy kh oidi nce oiential bei to pythotensor flow  net vaor after gentive mols stochastic processusi convolutnal neural networks   net  paulo braz rodro los so sbueno  paulo braz
paper_qf_17.pdf,2,"Generative Models for Stochastic Processes Using Convolutional Neural
  Networks","  The present paper aims to demonstrate the usage of Convolutional Neural
Networks as a generative model for stochastic processes, enabling researchers
from a wide range of fields (such as quantitative finance and physics) to
develop a general tool for forecasts and simulations without the need to
identify/assume a specific system structure or estimate its parameters.
","trained WaveNet, which their resp ective true parameters are 
known. Finally, using the R statistical package, the parame-
ters of the simulated processes are estimated and analyzed.  
That said, the paper is divided into these topics as fol-
lows:  
• a brief description of the WaveNet architec-
ture; 
• description of the synthetic time series gener-
ated according to their respective generation process-
es; 
• discussion of the hyperparameters chosen for 
each time series and a brief methodology of how to 
set up them;  
• discussion of research findings ; 
• conclusion s and propositions for future 
works.  
2 A Brief Description of the WaveNet Archi-
tecture  
The main idea of the original WaveNet paper [Van den 
Oord et. al , 2016 -a] is to model the joint probability of a 
stochastic process  
  as a product of 
condit ional probabilities  
 
 
 
In other words, the probability of 
  is conditioned to 
all previous observations.  
To model the time series following this approach in terms 
of a Convolutional Neural Network, the WaveNet architec-
ture consists of stacking  what is called dilated causal convo-
lutional layers, which consists of stacking structures as in 
Figure 1 and, as pointed before, a Softmax layer, which con-
sists of a multinomial logistic classifier given by:  
 
 
 
where 
  denotes the number of d ifferent classes; 
  denotes 
the features (independent variables) extracted in the previ-
ous layers;  
  denotes the weights of the features used 
to classify the output, which are filtered by means of convo-
lution operations specified i n Figure 1; and 
  denotes the 
hypothesis of the output pertaining to a specific class – here, 
it is worth mentioning that, given this classification struc-
ture, the observed variables in the stochastic process must be encoded into a discrete var iable with 
  different classes, 
where:  
 
 
 
Figure 1: Dilation Causal Convolutional Layer. Source: Van den Oord et. 
al, 2016 -a 
 
In terms of the dilation causal convolutional layers, it is 
worth pointing out some importa nt observations about the 
main features of these structures:  
• stacked layers structures act as a generaliza-
tion of Discrete Wavelet filters [see Borovykh et. al , 
2017 ], given the fact that, basically, Discrete Wavelet 
transforms can be thought as a cascade of linear oper-
ations;  
• stacking such features with non -linear opera-
tors can provide a general approximator due to the 
shift-invariance, as discussed in Bruna  and Mallat 
[2013 ], Cheng et. al  [2016] and Fernandes [2017 ], en-
abling the researcher to capture imp ortant non -
linearities;  
• this structure helps obtaining a very wide re-
ceptive field, which facilitates extracting long -range 
dependencies in conjunction with short memory (as 
can be seen in Figure 2).  
 
 
Figure 2: Multiscale feature s in Dilation Causal Convolutional Layers. 
Source: Van den Oord et. al , 2016 -a 
 
Keeping that in mind, these dilation convolutional layers 
are stacked and organized in a way that each feature is add-
ed to the previous layer features following a residual 
scheme, as follows in Figure 3, enabling deeper models and 
faster convergence, according to He et. al  [2015 ].  ",2018-01-09T03:35:20Z, net nally that  net brief sipt net art  net vaor ito convolutnal neural network  net  soft max   datcausal convolutnal layer source vaor idisete wal borvy kh disete wal bruno mall at c    multi scale datcausal convolutnal layers source vaor keepi  
paper_qf_17.pdf,3,"Generative Models for Stochastic Processes Using Convolutional Neural
  Networks","  The present paper aims to demonstrate the usage of Convolutional Neural
Networks as a generative model for stochastic processes, enabling researchers
from a wide range of fields (such as quantitative finance and physics) to
develop a general tool for forecasts and simulations without the need to
identify/assume a specific system structure or estimate its parameters.
"," 
Figure 3: Overview of the Architecture. Source: Van den Oord  et.al, 2016 -
a 
It is also important to notice that there is a Gated Activa-
tion Function instead of Rectified Linear Activation Func-
tions (ReLU), which are the most popular activation funci-
ton in this kind of neural network, due to the fact that, as 
shown in Van den Oord et. al  [2016 -b], it outperforms the 
traditional  approach. In this case, this Gated Activation 
Function is described by:  
 
 
 
where 
  denotes the element -wise multiplication operator; 
 denotes the convolution operator; 
  denotes the input; 
 denotes a lear nable convolutional filter at the 
 -th 
layer; and 
  denotes a learnable convolutional gate.  
Aiming to focus on the main subject of this paper, for fur-
ther architecture details, one should read the original 
WaveNet implementation pap er. 
3 Description of the Synthetic Time Series   
In the present paper, four deterministic data generation pro-
cesses and five stochastic process were tested, in order to 
verify the generative capabilities of the WaveNet architec-
ture. The chosen deterministic  processes were:  
• Harmonic Oscillator  
 
 
• Damped Harmonic Oscillator (without any external 
forces)  
 
 
• Logistic Map with a chaotic choice of the parameter 
[see May, 1976 ] 
 • Lorenz System, as  simulated  in Borovykh et. al  
[2017 ] 
 
 
In these four deterministic c ases, the Convolutional Neu-
ral Network should behave similar to a standard ordinary 
differential equation solver / difference equation solver.  
In the case of the stochastic processes, we have chosen the 
following processes:  
• Standard diffusion process with mean -reversion  
 
 
 
• Jump -Diffusion Process (see Matsuda, 2004)  
 
 
 
• Autoregressive Process of Order 1 (AR)  
 
 
• Autoregressive -Moving Average Process of Order 1 
(ARMA)  
 
 
• Autoregressive Conditional Heteroskedastic Process 
of Order 1 (ARCH)  
 
 
In these cases, t he Convolutional Neural Network should 
simulate a stochastic process compatible with the original 
one, as in a standard stochastic differential/difference equa-
tion simulator.  
 
4 Description of the Methodology for the 
Choice of Hyperparameters  
 
In order to  setup the hyperparameters of the Convolutional 
Neural Network according to each synthetic time series, it 
was adopted a forward method, where we start with two 
layers and dilations up to the second order (i.e. dilations 1,2) ",2018-01-09T03:35:20Z, overview architeure source vaor it gated a iv funreied linear aivatfure vaor ated aivatfunaimi  net siptsynic time serii net t harmonic osclator dumped harmonic osclator logistic map may lorenzo tem borvy kh iconvolutnal neu network istandard jump dfusprocess matsuda auto reive process orr auto reive movi ge process orr auto reive conditnal meters ke da stic process orr iconvolutnal neural network siptmethodology choice  meters iconvolutnal neural network
paper_qf_17.pdf,4,"Generative Models for Stochastic Processes Using Convolutional Neural
  Networks","  The present paper aims to demonstrate the usage of Convolutional Neural
Networks as a generative model for stochastic processes, enabling researchers
from a wide range of fields (such as quantitative finance and physics) to
develop a general tool for forecasts and simulations without the need to
identify/assume a specific system structure or estimate its parameters.
","and increased the dilations up  to 256th order , which were the 
dilations in the original paper chosen to deal with eventual 
long-range dependencies found in text -to-speech applica-
tions.  
 In addition to that, it was generated  a single  time series  
(for each data generating process)  with 1 2000 samples – 
10000 samples were used to train the neural network and 
2000 samples were used for back -testing purposes.  
 Whenever poor results were obtained , an additional con-
volutional layer was added, starting again with only dila-
tions up to the second order.  
 Moreover, in this specific application, an 8 -bit encoding 
was used to discretize the data into 256 classes; 256 skip -
channels were used; with a filter width set equal to 2 – cov-
ering all the hyperparameters established in Van den Oord  
et. al  [2016 -a]. 
 
5 Simulations and Discussion of Research 
Findings  
 
To test the capabilities of this architecture, we show first the 
results of the deterministic processes simulations and, af-
terwards, the results of the stochastic processes simulations. 
For visibility  purposes, in the case of the Logistic Map 
(Figure 7), we show only the first 90 observations out -of-
sample.  It is also w orth mentioning that, as we are model-
ling deterministic processes, in this case we use the 
numpy.argsort() method  (present in the Num Py package for 
Python), to generate deterministic choices of the Softmax 
layer output, obeying the magnitude of the probabilities.  
 
 
Figure 4: Simulation of a Deterministic Harmonic Oscillator (WaveNet 
with two layers with dilatio ns up to 8)  
 
Figure 5: Simulation of a Deterministic Harmonic Oscillator (WaveNet 
with three  layers with dilations up to 256) 
 
Figure 6: Simulation of a Deterministic Damped Harmonic Oscillator 
(WaveNet w ith nine layers with dilations up to 4) ",2018-01-09T03:35:20Z,iwneovaor simulatns discussresearndis to for logistic map  it num py pythosoft max  simulatterministic harmonic osclator  net  simulatterministic harmonic osclator  net  simulatterministic dumped harmonic osclator  net
paper_qf_17.pdf,5,"Generative Models for Stochastic Processes Using Convolutional Neural
  Networks","  The present paper aims to demonstrate the usage of Convolutional Neural
Networks as a generative model for stochastic processes, enabling researchers
from a wide range of fields (such as quantitative finance and physics) to
develop a general tool for forecasts and simulations without the need to
identify/assume a specific system structure or estimate its parameters.
"," 
Figure 7: Simulation of a Deterministic Chaotic Logistic Map  (WaveNet 
with five layers with dilations up to 2) 
In the Figures 4 to 7,  in all four graphics, dashed green 
lines represent obse rvations used for back -testing purposes; 
the red lines represent predicted outputs by the model; and 
dashed black lines represent the original signal used to train 
the Neural Network.  
Given that, it is possible to see that the WaveNet architec-
ture is able to capture and fairly reproduce the main dynam-
ics of the processes in Figures 4 and 5.  
It is also interesting to notice that, in Figure 7, until the 
10th observation, the model is able to obtain precise out -of-
sample forecasts of the data, while in the Fig ure 6, given the 
non-linear multivariable nature of the system, the best out -
of-sample guess is an average of the past occurrences.  
That said, we repeat the same experiment with the five 
proposed stochastic processes. However, instead of  only 
plotting the time series (except for the jump diffusion pro-
cess – plotted in Figure 8, which would require a lot of ef-
fort to be estimated, extrapolating the original scope of this 
present work), we also plot  the distribution of the structural 
parameters of the simulat ed series.  
Our hypothesis is supported by the fact that, if the struc-
tural parameters are compatible with the original ones – 
given the  fact that we know the true data generating process 
parameters – the simulated process is compatible with the 
original on e. 
Hence, in Figure 8, we show first a Jump -Diffusion pro-
cess with a negative drift, with a low probability of occur-
ring a high intensity positive jump.  
It is also worth noticing that in Figures 9 to 12, the medi-
an values are plotted in red and the true pa rameter values are 
plotted in blue.  So, a direct comparison between the true 
known structural parameter values and the estimated one 
from the simulated series can be done.  To generate different realizations of the processes  (repre-
sented by different color s), 100 simulations of each process 
were carried out, as in a Monte -Carlo approach, using the 
numpy.random.choice() method  (present in the NumPy 
package  for Python ), to generate random choices that obey a 
given distribution, which is returned by the Softm ax layer  of 
WaveNet . 
 
Figure 8: Simulation of a Jump -Diffusion Process  (WaveNet with five 
layers with dilations up to 4) 
 
Figure 9 – Simulation and Inference of a Mean -Reverting Diffusion Pro-
cess (True Me an Reversion Speed Parameter = 0.1  – blue line not shown ) ",2018-01-09T03:35:20Z, simulatterministic chaotic logistic map  net is neural network give net s it   that  our nce  jump dfusit s so to   num py pythosoft  net  simulatjump dfusprocess  net  simulatinference meareverti dfuspro true me reversspeed meter
paper_qf_17.pdf,6,"Generative Models for Stochastic Processes Using Convolutional Neural
  Networks","  The present paper aims to demonstrate the usage of Convolutional Neural
Networks as a generative model for stochastic processes, enabling researchers
from a wide range of fields (such as quantitative finance and physics) to
develop a general tool for forecasts and simulations without the need to
identify/assume a specific system structure or estimate its parameters.
"," 
Figure 10: Simulation and Inference of an AR(1) Process  (WaveNet with 
five layers with dilations up to 4) 
 
Figure 11: Simulation and Inference of  an ARMA(1,1) Process  (WaveNet 
with five layers with dilations up to 4) 
In these three first  linear models (Figures 9 to 11), it is 
possible to verify that the results are very reasonable, given 
the fact that the Networks have learnt from only one realiza-
tion of each stochastic process (despite a large sample), and 
the parameters deviation from the true values are not large.   Also, it is important to state that, as all experiments used 
training sets where the first samples were equal to zero, it 
explains so me of the valleys found in the simulations.  
 
Figure 12: Simulation and Inference of an ARCH(1) Process  (WaveNet 
with five layers with dilations up to 4) 
In Figure 12, we can also verify that the structural charac-
teristics of the d ata generation process are compatible with 
the true parameters.  
6 Conclusions and Propositions for Future 
Works  
In the present work, we aimed to establish a new simulation 
of data generation processes that avoid the traditional identi-
fication and estimatio n procedures, proposing here a new 
technique based on a Convolutional Neural Network – 
namely WaveNet architecture – where we estimate only its 
hyperparameters, in this case : number of convolutional lay-
ers, discretization scheme (encoding) and dilations.  
To accomplish that, we have simulated different deter-
ministic and stochastic processes, using an existing imple-
mentation of the WaveNet architecture code, adapted for 
this specific purpose, in conjunction with the R Statistical 
Package.  On top of these dif ferent simulations, we show that 
the generated data is compatible with original data genera-
tion process , in a fair wide extent, being a potential attrac-
tive tool that can be employed in several different research 
areas.  
As perspective for future works and research, we suggest 
following these experiments with more complex stochastic 
processes and, also, given the high computational cost of the 
training procedures (all of them were done using only one 
GPU in TensorFlow), it is important to develop an infor-",2018-01-09T03:35:20Z, simulatinference process  net  simulatinference process  net is networks also  simulatinference process  net i conusns propositns future works iconvolutnal neural network  net to  net statistical package oas tensor flow
paper_qf_17.pdf,7,"Generative Models for Stochastic Processes Using Convolutional Neural
  Networks","  The present paper aims to demonstrate the usage of Convolutional Neural
Networks as a generative model for stochastic processes, enabling researchers
from a wide range of fields (such as quantitative finance and physics) to
develop a general tool for forecasts and simulations without the need to
identify/assume a specific system structure or estimate its parameters.
","mation criterion for such kind of models, in order to guide 
and speed -up the hyperparameters choice.   
Moreover, extending this architecture for multivariate 
processes is a desirable path towards new interesting results.  
  
References  
[Benitez  et al ., 19 97] J.M Benitez , J.L. Castro , and I. Re-
quena . Are Artificial Neural Networks Black Boxes?  
IEEE Transactions on Neural Networks , 8(5):1156 -
1164, 1997.  
[Bishop, 2006] Christopher M. Bishop. Pattern Recognition 
and Machine Learning.  Spring er, Cambridge , United 
Kingd om, 2006.  
[Borovykh et. al , 2017 ] Anastasia Borovykh, Sander Bohte, 
Cornelis W. Oosterlee . Conditional Time Series Fore-
casting with Convolutional Neural Networs . 
arXiv:1703.04691v3 [stat.ML] , 201 7. 
[Bruna  and Mall at, 2013] Joán Bruna and S téphane Mallat . 
Deep Haar  scattering networks  IEEE Transactions on 
Pattern Analysis and Machine Intelligence , 35(8):1872 -
1886, 2013 . 
[Cheng et. al , 2016] Xiuyuan Cheng, Xu Chen, St éphane 
Mallat. Deep Haar Scattering Networks. Information and 
Inference: A Journal of IMA , 5:105–133, June 2016.  
[Fernandes, 2017] Fernando Fernandes Neto. Building 
Function Approximators on top of Haar Scattering Net-
works. Working Paper  on Research Net , 2017.  
[Hamilton , 1994 ] James D. Hamilton.  Time Series Analysis . 
Princeton University Press, Pr inceton, New Jersey, 1994 . 
[Hayashi, 2000] Fumi Hayashi Econometrics . Princeton 
University  Press, Princeton , New Jersey , 2000 . 
[He et. al , 2013 ] Kaiming He, Xiangyu Zhang , Ren 
Shaoqing  and Jian Sun , Koray Kavukcuoglu. Deep re-
sidual learning for image recog nition. 
arXiv:1512.03385v1 [cs.CV], 2015.  
[Józefowicz et. al , 2016 -a] Rafal Józefowicz, Oriol Vinyals, 
Mike Schuster, Noam Shazeer and Yonghui Wu. Explor-
ing the limits of language modeling. 
arXiv:1602.02410v2 [cs.CL], 2016.  
[Kolman and Margaliot, 2007] E. Kolman and M. Margali-
ot. Knowledge extraction from neural networks using 
all-permutation fuzzy rule base. IEEE Transactions on 
Neural Networks , 18: 925 -931, 2007.  
[Matsuda , 200 4] Kazuhisa Matsuda . Introduction to Merton 
Jump Diffusion Model. Working Paper , 2004.  
[Oshri  and Khandwala , 2015] Barak Oshri and Nishith 
Khandwala . Predicting Moves in Chess using Convolu-
tional Neural Networks . Working Paper , 201 5. 
[Silver et. al , 2016 -a] David Silver, Julian Schrittwieser, 
Karen Simonyan, Ioannis Antonoglou, Aja Hu ang, Ar-
thur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, 
Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore 
Graepel & Demis Hassabis. Mastering the game of Go 
without human knowledge. Nature, 550(19) : 354-372, 
2017.  
 [Van den Oord  et. al , 2016 -a] Aäron van den Oord, Sander 
Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, 
Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray 
Kavukcuoglu . WaveNet: A Generative Model for Raw 
Audio.  arXiv:1609.03499v2 [cs.S D], 2016 . 
[Van den Oord et. al , 2016 -b] Aäron van den Oord, Sander 
Dieleman, Lasse Espeholt , Oriol Vinyals, Alex Graves, 
Nal Kalchbrenner, Koray Kavukcuoglu.  Conditional 
Image Generation with PixelCNN Decoders. 
arXiv:1606.05328v2 [cs.CV], 2016.  
 
 ",2018-01-09T03:35:20Z,oreferencbe nitz be nitz castro re are articial neural networks  boxtransans neural networks bish bishpaerrenitmachine learni spri cambridge united ki borvy kh anastasia borvy kh sanrs boh te corners most er  conditnal time serifore convolutnal neural ne two rs  bruno mall jo bruno mall at ep  transans paeranalysis machine intellence c xi yuac xu cst mall at ep  scaeri networks informatinference journal june    net budi funap proximal tons  scaeri net worki pa researnet hamtojamhamtotime serianalysis princeto  pr new jersey hayashi fu mi hayashi econometrics princeto  princetonew jersey  articial intellence mi  lia yu  resha ki jasu ray ka uk co lu ep  raf al r vi ny als mike uster nom haze er yo hui wu e lor  kmaarial ot kmaarial kledge transans neural networks matsuda kazuhisa matsuda introdumortojump dfusmworki pa shri kh and wal ark shri nht th kh and wal predii movcss covneural networks worki pa sdid sjuliascar it wise karesimoaloais antoog lou aja hu ar guez thomas hurt lucas baker maw articial intellence adriaboltoyu thaimot l li ap fahui laurent  re george drisc tre gra ep el m is has sab is masteri go nature vaor or sanrs die le ma a zekaresimoar vi ny als alex grnal kal runner andrew senr  ray ka uk co lu  net gentive mraw aud  vaor or sanrs die le malapse esp holt r vi ny als alex grnal kal runner  ray ka uk co lu conditnal image gentpixel cor 
paper_qf_18.pdf,1,Change of Measure in Midcurve Pricing,"  We derive measure change formulae required to price midcurve swaptions in the
forward swap annuity measure with stochastic annuities' ratios. We construct
the corresponding linear and exponential terminal swap rate pricing models and
show how they capture the midcurve swaption correlation skew.
","Change of Measure in Midcurve Pricing
K.E. Feldman
Abstract
We derive measure change formulae required to price midcurve swaptions in the
forward swap annuity measure with stochastic annuities' ratios. We construct the
corresponding linear and exponential terminal swap rate pricing models and show how
they capture the midcurve swaption correlation skew.
Introduction
An interest rate swap is a nancial instrument with a triangle property. The value of two
swapsSt1t2,St2t3between times t1andt2and between times t2andt3is equal to the value
of the swap St1t3between times t1andt3(we assume that all three swaps have the same
xed leg strike). Equivalently, we may say that the swap St2t3is the dierence between
a long swap St1t3and a short swap St1t2. To express views on swap rates in the future,
the interest rate market actively trades options on swaps, i.e. swaptions. Swaptions are
non-linear products. The triangle property of the swap generalises into the property of the
swaptions by including the convexity. A portfolio of a vanilla swaption on the short swap
St1t2and an option (midcurve swaption) on the swap St2t3is more expensive than the price
of the long swaption St1t3(when the strikes are the same, and the exercise time of all of the
swaptions is the same, t1- the start of the short and long swaps). If, in the Black-Scholes
world, we assume also that the long and short annuities ratios to the annuity of the midcurve
swap are deterministic, then from the swap triangle one can derive a useful relationship for
the volatilities of all of the three swap rates. The challenge comes when we look into the
relations between the volatility smiles (skews) of those rates. In this paper, we discuss a
modelling approach for pricing midcurve swaptions that allows one to take into account the
stochasticity of the swaps' annuities and to generate pronounced correlation skews which are
typically observed in the midcurve swaption market.
1arXiv:1812.07415v2  [q-fin.PR]  22 Jun 2019",2018-12-10T09:36:39Z,e measure mid curve prici goldmaabstra   introduat st st st equivalently st st st to swap ns t st st st   schools t i jun
paper_qf_18.pdf,2,Change of Measure in Midcurve Pricing,"  We derive measure change formulae required to price midcurve swaptions in the
forward swap annuity measure with stochastic annuities' ratios. We construct
the corresponding linear and exponential terminal swap rate pricing models and
show how they capture the midcurve swaption correlation skew.
","A midcurve swaption is an ecient way to trade correlations between the short and long
swap rates. Others also used this product to trade on the dierence between levels in the
short and long term implied volatilities [1]. Being the simplest product on forward volatility,
midcurve swaptions can be used for the calibration of the mean reversion parameters in the
one factor short rate models [2].
The rich structure of the interest rate market oers two approaches to modelling the price
of a midcurve swaption. The product can be viewed dynamically and be priced by modelling
the time evolution of the underlying swap rate, or it can be viewed statically and its price
can be derived from prices of closely related products - the long and the short swaptions
traded in the market.
We shall be looking at the static way of pricing the midcurve swaption using a gener-
alisation of the triangle property of the swaps to the case of the swaptions. A midcurve
swaption can be priced as an option on a weighted basket of the short and long swap rates
with the same xing date. The weights coecients are functions of the swap annuities ra-
tios. The industry standard is to freeze these ratios to be constants. Taking correlation as
an input parameter, the weighted basket can be priced by pairing the short and long swap
rate distributions via a copula.
Some use more advanced models to account for stochasticity of the annuity ratios. The
approach that has been adopted by the larger banks is rst to move both the short and the
long swaps rates distributions to the same terminal (discount bond) measure, and then to
approximate each of the short and long annuities by deterministic functions of the corre-
sponding (short or long) swap rates. This is an extension of the idea [3] where the authors
developed a model that directly links constant maturity swap to volatilities of swaptions of
all relevant tenors. Note that midcurve swaptions are not in the scope of [3]. This product
is liquidly traded in the US Dollar market where the settlement style is physical. Thus, the
natural pricing measure for this product is the annuity measure.
While allowing a better risk management of the midcurve correlation skew, the terminal
measure approach suers from an inconsistency. In this paper, we show that once you x
the stochastic form of the annuity ratio, the measure change is no longer free. We derive
the explicit formulae for the measure change in terms of the functional forms of the annuity
ratios. One other deciency of the terminal measure approach are negative ratios of annuities.
The exponential terminal swap rate model developed in this paper is free of this problem by
2",2018-12-10T09:36:39Z,otrs bei t t  t t taki some t  note  dollar  w i one t
paper_qf_18.pdf,3,Change of Measure in Midcurve Pricing,"  We derive measure change formulae required to price midcurve swaptions in the
forward swap annuity measure with stochastic annuities' ratios. We construct
the corresponding linear and exponential terminal swap rate pricing models and
show how they capture the midcurve swaption correlation skew.
","construction.
We analyse in detail the measure change formulae in the case where the annuity ratio
is a linear or an exponential function of the short and the long swap rates. The price of
a midcurve swaption is often parameterised by its implied correlation as a function of the
strike. Even if we use a model that captures the implied volatility smiles of the long and short
swap rates well, the implied correlation is still not a constant function of the strike. The
termianl swap rate models with stochastic annuities developed in this paper give a handle
to match the implied correlation skew.
The eect studied in this paper is applicable in conjunction with any smile model. In
particular, it is present in the at volatility world. We provide numerical results on how our
methodology captures the midcurve correlation skew in the case when the underlying swap
rates are modelled as standard normal variables with a at smile (i.e. constant across all
strikes' volatilities).
1 Product valuation
A midcurve receiver swaption Wrec=Wrec(Srec;Te(x)piry) on a swap Srec(T(s)tart;T(e)nd;K)
with a xed leg rate Kgives the holder an option to enter into a receiver swap Srecat
expiry time Tx, where the swap starts on Ts, ends onTeand the holder receives the xed
rateKaccrued on a notional Nover all periods in the schedule formed by a sequence
of dates:Tfix
1; :::; Tfix
n=Te, withnpayment dates in the xed leg schedule. In
return the holder pays oating rate payments on the sequence of dates from the oating
rate schedule: Tfl
1; :::; Tfl
m=Te. We will use short notations for the time intervals
between two consecutive payments on each of the swap legs: fix
i=Tfix
i",2018-12-10T09:36:39Z, t evet t i produ rec rec rec te rec givrec at tx ts te and acued ox x te itfl tfl te  x
paper_qf_18.pdf,4,Change of Measure in Midcurve Pricing,"  We derive measure change formulae required to price midcurve swaptions in the
forward swap annuity measure with stochastic annuities' ratios. We construct
the corresponding linear and exponential terminal swap rate pricing models and
show how they capture the midcurve swaption correlation skew.
","order to price the swaption Wrec, we can model the distribution for the R(Tx;Ts;Te) in the
annuity measure and calculate the value of the swaption as:
Wrec(t) =A(t)EA[W(Tx)=A(Tx)]
=A(t;Ts;Te)NEA[[K",2018-12-10T09:36:39Z,rec tx ts te rec tx tx ts te
paper_qf_18.pdf,5,Change of Measure in Midcurve Pricing,"  We derive measure change formulae required to price midcurve swaptions in the
forward swap annuity measure with stochastic annuities' ratios. We construct
the corresponding linear and exponential terminal swap rate pricing models and
show how they capture the midcurve swaption correlation skew.
","are related via
A(Tx;Ts;Te) =A(Tx;Tx;Te)",2018-12-10T09:36:39Z,tx ts te tx tx te
paper_qf_18.pdf,6,Change of Measure in Midcurve Pricing,"  We derive measure change formulae required to price midcurve swaptions in the
forward swap annuity measure with stochastic annuities' ratios. We construct
the corresponding linear and exponential terminal swap rate pricing models and
show how they capture the midcurve swaption correlation skew.
","e(y) := PDFu
Re(y) =Z+1
",2018-12-10T09:36:39Z,fu re
paper_qf_18.pdf,7,Change of Measure in Midcurve Pricing,"  We derive measure change formulae required to price midcurve swaptions in the
forward swap annuity measure with stochastic annuities' ratios. We construct
the corresponding linear and exponential terminal swap rate pricing models and
show how they capture the midcurve swaption correlation skew.
","where  is the CDF of a univariate normal variable and cdfs(x),cdfe(y) are the CDFs
corresponding to pdfs s(x) from (15) and e(y) from (16).
2 First order approximations
The Radon-Nikodym derivative for measure change in (12) and the payo in (17) depend
only on the ratio of the annuities A(t;Tx;Ts) andA(t;Tx;Te). Therefore, to use the copula
valuation by means of (17) it is sucient to model dynamics of the ratio of annuities. A
convenient way for modelling dynamics of the ratio of annuities is provided by the Terminal
Swap Rate Model methodology. It covers the zero-th and the rst order approximations for
the ratio. We discuss the corresponding approximations below.
Deterministic Annuity Ratio: Assume that the conditional expectations in (15-16)
are independent from the respective variables xandy(we may think of GJ;Tx,J=e;s
from (13), for example, as being deterministic). Then in (15) s(x)PDFs
Rs(x) and in (16)
e(y)PDFe
Re(y), i.e. no change of the measure is needed and
w1(y;x) =G",2018-12-10T09:36:39Z,fs rst t radody tx ts tx te trefore terminal swap rate mit  terministic annuity rat  tx tfs rs fe re
paper_qf_18.pdf,8,Change of Measure in Midcurve Pricing,"  We derive measure change formulae required to price midcurve swaptions in the
forward swap annuity measure with stochastic annuities' ratios. We construct
the corresponding linear and exponential terminal swap rate pricing models and
show how they capture the midcurve swaption correlation skew.
","and bothw1(y;x),w2(y;x) areA(Ts;Te)-martingales.
Equating coecients under xandyinw1(y;x)",2018-12-10T09:36:39Z,ts te equati
paper_qf_18.pdf,9,Change of Measure in Midcurve Pricing,"  We derive measure change formulae required to price midcurve swaptions in the
forward swap annuity measure with stochastic annuities' ratios. We construct
the corresponding linear and exponential terminal swap rate pricing models and
show how they capture the midcurve swaption correlation skew.
","Proof: Under an assumption that the long and the short swap rates are approximately
Gaussian we can project yon toxas:
EA(Tx;Ts)[yjx] = EA(Tx;Ts)[R(Tx;Tx;Te)jR(Tx;Tx;Ts) =x]
=EA(Tx;Ts)[R(Tx;Tx;Te)] +e
s(x",2018-12-10T09:36:39Z,proof unr tx ts tx ts tx tx te tx tx ts tx ts tx tx te
paper_qf_18.pdf,10,Change of Measure in Midcurve Pricing,"  We derive measure change formulae required to price midcurve swaptions in the
forward swap annuity measure with stochastic annuities' ratios. We construct
the corresponding linear and exponential terminal swap rate pricing models and
show how they capture the midcurve swaption correlation skew.
","Log Linear Approximation: The rst order approximation does not immediately prevent
weight coecients w1(y;x) andw2(y;x) from going negative. This can be addressed by an
exponential approximation for w2(y;x):
w2(y;x)",2018-12-10T09:36:39Z,log linear approximatt 
paper_qf_18.pdf,11,Change of Measure in Midcurve Pricing,"  We derive measure change formulae required to price midcurve swaptions in the
forward swap annuity measure with stochastic annuities' ratios. We construct
the corresponding linear and exponential terminal swap rate pricing models and
show how they capture the midcurve swaption correlation skew.
","Lemma 5. If the long and the short rates are approximately Gaussian then the log linear
approximation for the weights w2(y;x)
s(x)PDFs
Rs(x)e",2018-12-10T09:36:39Z,lea  fs rs
paper_qf_18.pdf,12,Change of Measure in Midcurve Pricing,"  We derive measure change formulae required to price midcurve swaptions in the
forward swap annuity measure with stochastic annuities' ratios. We construct
the corresponding linear and exponential terminal swap rate pricing models and
show how they capture the midcurve swaption correlation skew.
","In the log linear approximation for the weights we can explicitly evaluate ~R(t;Tx;Ts)
from (40) and ~R(t;Tx;Te) from (34) by observing that w1(y;x) andw2(y;x) areA(t;Ts;Te)-
martingales. We nd, similarly to the linear case:
~R(t;Tx;Ts) =R(t0;Tx;Ts)",2018-12-10T09:36:39Z,itx ts tx te ts te  tx ts tx ts
paper_qf_18.pdf,13,Change of Measure in Midcurve Pricing,"  We derive measure change formulae required to price midcurve swaptions in the
forward swap annuity measure with stochastic annuities' ratios. We construct
the corresponding linear and exponential terminal swap rate pricing models and
show how they capture the midcurve swaption correlation skew.
","3 Estimating parameters eandsand some numerical
results.
Parameters eandsintroduced in (24) for the linear approximation and in (47) for the log
linear approximation, are related to the covariances between swap annuities' ratios and the
swap rates via
CovA(Tx;Te)hA(Tx;Ts;Te)
A(Tx;Tx;Te);R(Tx;Tx;Te)i=",2018-12-10T09:36:39Z,estimati meters cov tx te tx ts te tx tx te tx tx te
paper_qf_18.pdf,14,Change of Measure in Midcurve Pricing,"  We derive measure change formulae required to price midcurve swaptions in the
forward swap annuity measure with stochastic annuities' ratios. We construct
the corresponding linear and exponential terminal swap rate pricing models and
show how they capture the midcurve swaption correlation skew.
",Linearising the ratio ( A(Tx;Tx;Te),2018-12-10T09:36:39Z,line arisi tx tx te
paper_qf_18.pdf,15,Change of Measure in Midcurve Pricing,"  We derive measure change formulae required to price midcurve swaptions in the
forward swap annuity measure with stochastic annuities' ratios. We construct
the corresponding linear and exponential terminal swap rate pricing models and
show how they capture the midcurve swaption correlation skew.
","that the stochastic annuity ratio assumption introduces a skew into the midcurve implied
correlation even in the case of at implied volatilities of the long and the short swap rates.
Conclusion
We developed a consistent model for midcurve swaption pricing which explicitly accounts for
stochasticity of the ratios of the annuities. It gives a handle on the correlation skew which
is typically risk managed via the correlation-by-strike. The latter approach is not arbitrage
free.
Our paper shares a common idea with [3], and, thus, depending on the size of the book
the model presented here can be used for trading a small number of midcurve products and
understanding their correlation risk in terms of linear regression coecients eands, or
the model can be used for risk managing large books of swaptions and CMS products via full
projection of the correlation risk on all of the swap rates' volatilities and all of their pairwise
correlations.
Email address: kostyafeldman@gmail.com.
References
[1] Caron, J.A., McGraw, W.J. and Stipanov, J.D. 2009. System and
method for calculating a volatility carry metric. Morgan Stanley Research,
http://patents.justia.com/patent/7958036.
[2] Andersen, B.G. and Piterbarg, V.V. 2012. Interest Rate Modeling. Volume 2: Term
Structure Models. Atlantic Financial Press.
[3] Cedervall, S. and Piterbarg, V. 2012. CMS: covering all bases. Risk March, 64-69.
[4] Breeden, D. and Litzenberger, R. 1978. Prices of State Contingent Claims Implicit in
Options Prices. Journal of Business 51, 621-651.
[5] Andersen, B.G. and Piterbarg, V.V. 2012. Interest Rate Modeling. Volume 3: Products
and Risk Management. Atlantic Financial Press.
15",2018-12-10T09:36:39Z,conus it t our em articial intellence referenccarbomc raw sti pano tem morgastanley researanrsosite rb arg interest rate moli volume term struure mols atlantic nancial  ce rv all site rb arg risk marbreed elitzenberger pricstate contient  articial intellence ms implicit optns pricjournal business anrsosite rb arg interest rate moli volume produs risk management atlantic nancial 
paper_qf_19.pdf,1,Radical Complexity,"  This is an informal and sketchy review of six topical, somewhat unrelated
subjects in quantitative finance: rough volatility models; random covariance
matrix theory; copulas; crowded trades; high-frequency trading & market
stability; and ""radical complexity"" & scenario based (macro)economics. Some
open questions and research directions are briefly discussed.
","arXiv:2103.09692v1  [q-fin.GN]  17 Mar 2021Radical Complexity
Jean-Philippe Bouchaud, CFM & Académie des Sciences
March 18, 2021
Abstract
This is an informal and sketchy review of six topical, somewh at unrelated subjects in quantitative
ﬁnance: rough volatility models; random covariance matrix theory; copulas; crowded trades; high-
frequency trading & market stability; and “radical complex ity” & scenario based (macro)economics. Some
open questions and research directions are brieﬂy discusse d.
1 From Random Walks to Rough Volatility
Since we will never really know whythe prices of ﬁnancial assets move, we should at least make a m odel
ofhowthey move. This was the motivation of Bachelier in 1900, when he wrote in the introduction of his
thesis that contradictory opinions in regard to [price]ﬂuctuations are so diverse that at the same instant buyers
believe the market is rising and sellers that it is falling . He went on to propose the ﬁrst mathematical model
of prices: the Brownian motion. He then built an option prici ng theory that he compared to empirical data
available to him — which already revealed, quite remarkably , what is now called the volatility smile!
After 120 years of improvements and reﬁnements, we are closi ng in on a remarkably realistic model,
which reproduces almost all known stylized facts of ﬁnancia l price series. But are we there yet? As Benoît
Mandelbrot once remarked: In economics, there can never be a “theory of everything”. Bu t I believe each
attempt comes closer to a proper understanding of how market s behave. In order to close the gap, and justify
the modern mathematical apparatus that has slowly matured, we will need to understand the interactions
between the behaviour of zillions of traders — each with his o r her own investment style, trading frequency ,
risk limits, etc. and the price process itself. Interesting ly , recent research strongly suggests that markets
self organise in subtle way , as to be poised at the border betw een stability and instability . This could be the
missing link — or the holy grail – that researchers have been l ooking for.
For many years, the only modiﬁcation to Bachelier’s proposa l was to consider that log-prices, not prices
themselves, are described by a Brownian motion. Apart from t he fact that this modiﬁcation prevents prices
from becoming negative, none of the ﬂaws of the Bachelier mod el were seriously tackled. Notwithstanding,
the heyday of Brownian ﬁnance came when Black & Scholes publi shed their famous 1973 paper, with the
striking result that perfect delta-hedging is possible. Bu t this is because, in the Black-Scholes world, price
jumps are absent and crashes impossible. This is of course a v ery problematic assumption, specially because
the fat-tailed distribution of returns had been highlighte d as soon as 1963 by Mandelbrot — who noted, in
the same paper, that large changes tend to be followed by large changes, of either sign, and small changes tend
to be followed by small changes , an effect now commonly referred to as “volatility clusteri ng”, and captured
by the extended family of GARCH models.
It took the violent crash of October 1987, exacerbated by the massive impact of Black-Scholes’ delta-
hedging, for new models to emerge. The Heston model, publish ed in 1993, is among the most famous
post-Black-Scholes models, encapsulating volatility clu stering within a continuous time, Brownian motion
formalism. However, like GARCH, the Heston model predicts t hat volatility ﬂuctuations decay over a single
time scale — in other words that periods of high or low volatil ity have a rather well deﬁned duration. This
is not compatible with market data: volatility ﬂuctuations have no clear characteristic time scale; volatility
bursts can last anything between a few hours and a few years.
1",2021-03-17T14:43:30Z, mar radical lexity  phippe  chaacad sciencmarabstra  some from random walks rough volatity since  cac li brownish  after but as be no manlbrot ibu iinteresti  for cac librownish apart cac linotwithstandi brownish  schools bu  schools  manlbrot it ober  schools t sto schools brownish sto
paper_qf_19.pdf,2,Radical Complexity,"  This is an informal and sketchy review of six topical, somewhat unrelated
subjects in quantitative finance: rough volatility models; random covariance
matrix theory; copulas; crowded trades; high-frequency trading & market
stability; and ""radical complexity"" & scenario based (macro)economics. Some
open questions and research directions are briefly discussed.
","Mandelbrot had been mulling about this for a long while, and a ctually proposed in 1974 a model to
describe a very similar phenomenon in turbulent ﬂows, calle d “multifractality”. He adapted his theory in
1997 to describe currency exchange rates, before Emmanuel B acry , Jean-Francois Muzy & Jean Delour for-
mulated in 2000 a more convincing version of the model, which they called the Multifractal Random Walk
(MRW)[1]. With a single extra parameter (interpreted as a kind of vola tility of volatility), the MRW cap-
tures satisfactorily many important empirical observatio ns: fat-tailed distribution of returns, long-memory
of volatility ﬂuctuations. In 2014, Jim Gatheral, Thibault Jaisson and Mathieu Rosenbaum introduced their
now famous “Rough Volatility” model [2], which can be seen as an extension of the MRW with an extra
parameter allowing one to tune at will the roughness of volat ility , while it is ﬁxed in stone in the MRW
model. And indeed, empirical data suggests that volatility is slightly less rough than what the MRW posits.
Technically , the Holder regularity of the volatility is H=0 in the MRW and found to be H≈0.1 when
calibrated within the Rough Volatility speciﬁcation.
The next episode of the long saga came in 2009 when Gilles Zumb ach noticed a subtle, yet crucial aspect
of empirical ﬁnancial time series: they are not statistical ly invariant upon time reversal [3]. Past and future
are not equivalent, whereas almost all models to that date, i ncluding the MRW , did not distinguish past
from future. More precisely , past price trends, whether up o r down, lead to higher future volatility , but
not the other way round. In 2019, following some work by Pierr e Blanc, Jonathan Donier and myself [4],
Aditi Dandapani, Paul Jusselin, Mathieu Rosenbaum propose d to describe ﬁnancial time series with what
they called a “Quadratic Rough Heston Model” [5], which is a synthesis of all the ideas reviewed above.
It is probably the most realistic model of ﬁnancial price ser ies to date. In particular, it provides a natural
solution to a long standing puzzle, namely the joint calibra tion of the volatility smile of the S&P 500 and
VIX options, which had eluded quants for many years [6]. The missing ingredient was indeed the Zumbach
effect[7].
Is this the end of the saga? From a purely engineering point of view, the latest version of the Rough
Volatility model is probably hard to beat. But the remaining challenge is to justify how this particular
model emerges from the underlying ﬂow of buy and sell trades t hat interacts with market makers and
high frequency traders. Parts of the story are already clear ; in particular, as argued by Jaisson, Jusselin
& Rosenbaum in a remarkable series of papers, the Rough Volat ility model is intimately related to the
proximity of an instability [8](see also[9]), that justiﬁes the rough, multi-timescale nature of volat ility .
But what is the self-organising mechanism through which all markets appear to settle close to such a critical
point? Could this scenario allow one to understand why ﬁnanc ial time series all look so much alike —
stocks, futures, commodities, exchange rates, etc. share v ery similar statistical features, in particular in the
tails. Beyond being the denouement of a 120 years odyssey , we would be allowed to believe that the ﬁnal
model is not only a ﬁgment of our mathematical imagination, b ut a robust, trustworthy framework for risk
management and derivative pricing.
2 Random Matrix Theory to the Rescue
Harry Markowitz famously quipped that diversiﬁcation is th e only free lunch in ﬁnance. This is nevertheless
only true if correlations are known and stable over time. Mar kowitz’ optimal portfolio offers the best risk-
reward tradeoff, for a given set of predictors, but requires the covariance matrix – of a potentially large pool
of assets – to be known and representative of the future reali zed correlations. The empirical determination
of large covariance matrices is however fraught with difﬁcu lties and biases. Interestingly , the vibrant ﬁeld
of “Random Matrix Theory” has provided original solutions t o this big data problem, and suggests droves
of possible applications in econometrics, machine learnin g or other large dimensional models.
But even for the simplest two-asset bond /equity allocation problem, the knowledge of the forward loo k-
ing correlation has momentous consequences for most asset a llocators in the planet. Will this correlation
remain negative in the years to come, as it has been since late 1997, or will it jump back to positive ter-
ritories? But compared to volatility , our understanding of correlation dynamics is remarkably poor and,
surprisingly , the hedging instruments allowing one to miti gate the risk of bond /equity correlation swings
are nowhere as liquid as the VIX itself.
2",2021-03-17T14:43:30Z,manlbrot  eanuel  francois mu   l our multi fraal random walk with ijim gatr al thibault is somathit roseaum rough volatity and technically horough volatity t gls zum past  ipier blanc jonathadoi er edit panda paipaul ju ssel imathit roseaum quadratic rough stomit it zum bais from rough volatity but parts is soju ssel iroseaum rough vat but could beyond random matrix tory rescue harry marwitz  mar t interesti random matrix tory but wl but
paper_qf_19.pdf,3,Radical Complexity,"  This is an informal and sketchy review of six topical, somewhat unrelated
subjects in quantitative finance: rough volatility models; random covariance
matrix theory; copulas; crowded trades; high-frequency trading & market
stability; and ""radical complexity"" & scenario based (macro)economics. Some
open questions and research directions are briefly discussed.
","So there are two distinct problems in estimating correlatio n matrices. One is lack of data, the other one
is time non-stationarity . Consider a pool of Nassets, with Nlarge. We have at our disposal Tobservations
(say daily returns) for each of the Ntime series. The paradoxical situation is this: even though each
individual off-diagonal covariance is accurately determi ned when Tis large, the covariance matrix as a
whole is strongly biased unless Tis much larger than Nitself. For large portfolios, with Na few thousands,
the number of days in the sample should be in the tens of thousa nds – say 50 years of data. This is simply
absurd: Amazon and Tesla did not even exist 25 years ago. Mayb e use 5 minutes returns then, increasing
the number of data points by a factor 100? Yes, except that 5 mi nute correlations are not necessarily
representative of the risk of much lower frequency strategi es, with other possible biases creeping in the
resulting portfolios.
So in what sense are covariance matrices biased when Tis not very large compared to N? The best
way to describe such biases is in terms of eigenvalues. One ﬁn ds that the smallest eigenvalues are way too
small and the largest eigenvalues are too large. This result s, in the Markowitz optimization program, in
a substantial over-allocation on combination of assets tha t happened to have a small volatility in the past,
with no guarantee that this will persist looking forward. Th e Markowitz construction can therefore lead to
a considerable under-estimation of the realized risk in the next period.
Out-of-sample results are of course always worse than expec ted, but Random Matrix Theory (RMT)
offers a guide to (partially) correct these biases when Nis large. In fact, RMT gives an optimal, mathe-
matically rigorous, recipe to tweak the value of the eigenva lues so that the resulting “cleaned” covariance
matrix is as close as possible to the “true” (but unknown) one , in the absence of any prior information [10].
Such a result, ﬁrst derived by Ledoit and Péché in 2011 [11], is already a classic and has been extended in
many directions. The underlying mathematics, initially ba sed on abstract “free probabilities”, are now in
a ready-to-use format, much like Fourier transforms or Ito c alculus (see[12]for an introductory account).
One of the exciting, and relatively unexplored direction, i s to add some ﬁnancially motivated prior, like
industrial sectors or groups, to improve upon the default “a gnostic” recipe.
Now the data problem is solved as best as possible, the statio narity problem pops up. Correlations, like
volatility , are not ﬁxed in stone but evolve with time. Even t he sign of correlations can suddenly ﬂip, as was
the case for the S&P500 /Treasuries during the 1997 Asian crisis. After 30 years of co rrelations staunchly
in positive territory (1965 – 1997), bonds and equities have been in a “ﬂight-to-quality” mode (i.e. equities
down and bonds up) ever since. More subtle, but signiﬁcant, c hanges of correlations can also be observed
between single stocks and /or between sectors in the stock market. For example, a downwa rd move of the
S&P500 leads to an increased average correlation between st ocks. Here again, RMT provides powerful
tools to describe the time evolution of the full covariance m atrix[13,14].
As I discussed in the previous section, stochastic volatili ty models have made signiﬁcant progress re-
cently , and now encode feedback loops that originate at the m icrostructural level. Unfortunately , we are
very far from having a similar theoretical handle to underst and correlation ﬂuctuations, although Matthieu
Wyart and I had proposed in 2007 a self-reﬂexive mechanism to account for correlation jumps like the
one that took place in 1997 [15]. Parallel to the development of descriptive and predictive models, the
introduction of standardized instruments that hedge again st such correlation jumps would clearly serve a
purpose. This is especially true in the current environment [16]where inﬂation fears could trigger another
inversion of the equity /bond correlation structure, possibly devastating for many strategies that – implicitly
or explicitly – rely on persistent negative correlations. M arkowitz diversiﬁcation free lunch can sometimes
be poisonous!
3 My Kingdom for a Copula
As I just discussed, assessing linear correlations between ﬁnancial assets is already hard enough. What
about non-linear correlations then? If ﬁnancial markets were kind enough to a bide to Gaussian statistics,
non-linear correlations would be entirely subsumed by line ar ones. But this is not the case: genuine non-
linear correlations pervade the ﬁnancial world and are quit e relevant, both for the buy side and the sell side.
For example, tail correlations in equity markets (i.e. stoc ks plummeting simultaneously) are notoriously
3",2021-03-17T14:43:30Z,so one consir assets large  observatns time t tis tis itself for na  amazotesla may yso tis t one  marwitz th marwitz out random matrix tory is isule do it t courier to one  correlatns evetreasur after  for re as unfortunately maw wy art llel  my kidom popula as what  but for
paper_qf_19.pdf,4,Radical Complexity,"  This is an informal and sketchy review of six topical, somewhat unrelated
subjects in quantitative finance: rough volatility models; random covariance
matrix theory; copulas; crowded trades; high-frequency trading & market
stability; and ""radical complexity"" & scenario based (macro)economics. Some
open questions and research directions are briefly discussed.
","higher than bulk correlations. Another apposite context is the Gamma-risk of large option portfolios, the
management of which requires an adequate description of qua dratic return correlations of the underlying
assets.
In order to deal with non-linear correlations, mathematics has afforded us with a seemingly powerful
tool – “copulas”. Copulas are supposed to encapsulate all po ssible forms of multivariate dependence. But
in the zoo of all conceivable copulas, which one should one ch oose to faithfully represent ﬁnancial data?
Following an unfortunate, but typical pattern of mathemati cal ﬁnance, the introduction of copulas
twenty years ago has been followed by a calibration spree, wi th academics and ﬁnancial engineers alike
frantically looking for copulas to best represent their pet multivariate problem. But instead of ﬁrst de-
veloping an intuitive understanding of the economic or ﬁnan cial mechanisms that suggest some particu-
lar dependence between assets and construct adequate copul as accordingly , the methodology has been to
brute-force calibrate copulas straight out from statistic s handbooks. The “best” copula is then decided from
some quality-of-ﬁt criterion, irrespective of whether the copula makes any intuitive sense at all.
This is reminiscent of local volatility models for option ma rkets: although the model makes no intuitive
sense and cannot describe the actual dynamics of the underly ing asset, it is versatile enough to allow the
calibration of almost any option smile. Unfortunately , a bl ind calibration of some unwarranted model
(even when the ﬁt is perfect) is a recipe for disaster. If the u nderlying reality is not captured by the model,
it will most likely derail in rough times — a particularly bad feature for risk management (recall the use
of Gaussian copulas to price CDOs before the 2008 crisis). An other way to express this point is to use a
Bayesian language: there are families of models for which th e ‘prior’ likelihood is clearly extremely small,
because no plausible scenarios for such models to emerge fro m market mechanisms. Statistical tests are
not enough — the art of modelling is precisely to recognize th at not all models are equally likely .
The best way to foster intuition is to look at data before cobb ling up a model, and come up with a
few robust “stylized facts” that you deem relevant and that y our model should capture. In the case of
copulas, one interesting stylised fact is the way the probab ility that two assets have returns simultaneously
smaller than their respective medians depends on the linear correlation between the said two assets. Such
a dependence exists clearly and persistently in stocks and, strikingly , it cannot be reproduced by most “out-
of-a-book” copula families.
In particular, the popular class of so-called “elliptical” copulas is ruled out by such an observation. El-
liptical copulas assume, in a nutshell, that there is a commo n volatility factor for all stocks: when the index
becomes more or less volatile, all stocks follow suit. A mome nt of reﬂection reveals that this assumption
is absurd, since one expects that volatility patterns are at least industry speciﬁc. But this consideration
also suggests a way to build copulas specially adapted to ﬁna ncial markets. In Ref. [17], Rémy Chichepor-
tiche and I showed how to weld the standard factor model for re turns with a factor model for volatilities.
Perhaps surprisingly , the common volatility factor is not t he market volatility , although it contributes to it.
With a relatively parcimonious parameterisation, most mul tivariate “stylized facts” of stock returns can be
reproduced, including the non-trivial joint-probability pattern alluded to above.
I have often ranted against the over-mathematisation of qua nt models, favouring theorems over intu-
ition and convenient models over empirical data. Reliance o n rigorous but misguided statistical tests is also
plaguing the ﬁeld. As an illustration related to the topic of copulas, let me consider the following question:
is the univariate distribution of standardized stock retur nsuniversal , i.e. independent of the considered
stock? In particular, is the famous “inverse-cubic law” [18]for the tail of the distribution indeed common
to all stocks?
A standard procedure for rejecting such an hypothesis is the Kolmogorov-Smirnov (or Anderson-Darling)
statistics. And lo and behold, the hypothesis is strongly re jected. But, wait – the test is only valid if returns
can be considered as independent, identically distributed random variables. Whereas returns are close to
being uncorrelated, non-linear dependencies along the tim e axis are strong and long-ranged. Adapting the
Kolmogorov-Smirnov test in the presence of long-ranged “se lf-copulas” is possible [19]and now leads to
the conclusion that the universality hypothesis cannot be rejected. Here again, thinking about the problem
before blindly applying standard recipes is of paramount im portance to get it right.
The ﬁner we want to hone in on the subtleties of ﬁnancial marke ts, the more we need to rely on making
sense of empirical data, and to remember what the great Richa rd Feynman used to say: It doesn’t matter
4",2021-03-17T14:43:30Z,anotr gaa ipopular but followi but t  unfortunately  os abayesiastatistical t isuiel but iref whiep or haps with reliance as ilmogorov mir nov anrsodarli and but wreas adapti lmogorov mir nov re t rileymait
paper_qf_19.pdf,5,Radical Complexity,"  This is an informal and sketchy review of six topical, somewhat unrelated
subjects in quantitative finance: rough volatility models; random covariance
matrix theory; copulas; crowded trades; high-frequency trading & market
stability; and ""radical complexity"" & scenario based (macro)economics. Some
open questions and research directions are briefly discussed.
","how beautiful your theory is, it doesn’t matter how smart you are. If it doesn’t agree with experiment, it’s
wrong.
4 Crowded Trades: Whales or Minnows?
It is funny how, sometimes, seemingly obvious concepts do be come paradoxical when one starts thinking
harder about what they really mean. One topical example is th e idea of “crowded trades” that has recently
become a major talking point in the face of the disheartening performance of many Alternative Beta /Risk
Premia funds. It seems self-evident to many that when invest ors pile into a given trade, future returns are
necessarily degraded. But on the other hand, for each buyer t here must be a seller – so isn’t the opposite
trade crowded too? In what sense, then, is a crowded trade tox ic? Can one come up with useful measures
of crowding, that would allow one to construct portfolios as immune as possible to crowding risk?
In the mind of investors, the word “crowding” summons two dis tinct fears. One is that any mispricing
that motivates a strategy is arbitraged away by the crowd, re ndering that strategy unproﬁtable in the future.
The second is crash risk: harmful deleveraging spirals may o ccur as the crowd suddenly decides to run for
the exit. Here we see how the symmetry between the two sides of the trade can be broken: a trade is
crowded when investors on – say – the buy side are more prone to act in sync than those of the sell side.
In fact, the most crowded trade of all is, and always has been, long the stock market. Crashes indeed
happened many times in the past and will happen again in the fu ture. Be that as it may , is investing in the
stock market a bad decision? Certainly not, in fact all Risk P remia are proﬁtable on the long run precisely
because of such negatively skewed events. The equity risk pr emium is abnormally high – but in fact it
compensates for the deleveraging risk associated with the m adness of crowds. In fact, we have argued in
Ref.[20]that the return of Risk Premia strategies are strongly corre lated with their negative skewness, i.e.
their propensity to crash. So in many cases, crowding is simp ly unavoidable – the only question is whether
the associated downside risk is adequately compensated or n ot.
This brings us back to our ﬁrst point: that of withering retur ns. The mechanism usually put forth is
that the spread between the fundamental price and the market price is closed by those who trade based
on that mispricing. If the cake is shared between a larger num ber of participants, each of them must get
a smaller piece. Although this makes intuitive sense, the st ory cannot be that simple. When one looks at
different measures of mispricing on which classical factor s (momentum, proﬁtability , low volatility , etc.)
are supposed to thrive, there is no sign of a recent narrowing of the valuation spread between the short
and long legs of the corresponding portfolios. The situatio n is even the exact opposite for price-to-book
spreads, which have become wider since 2016 – tantamount to s aying that Value strategies are currently in
the doldrums.
But to argue that the plight of Value is due to crowding is at be st misleading. Value as a strategy has
been used extensively by market participants for decades. W hat we are seeing is more like a Value crash in
slow motion, with investors getting out more aggressively o f this strategy in the years 2019-2020, after a
period of disappointing (but certainly not unprecedented! ) performance. A rough estimation shows that if
250 B$ are progressively redeemed from value strategies ove r a year, typical market neutral value portfolios
should suffer a≈20% loss from price impact alone – all very much in line with re cent industry ﬁgures.
While there is no smoking gun that this is what happened recen tly , the argument that crowding is
detrimental to convergent strategies (i.e. trades that red uce valuation spreads) is not without merit. But
then how do we understand the effect of crowding on divergent strategies, such as momentum or trend
following? Here, price impact arguments would suggest that more trend followers should bolster trends,
not make them weaker. Crowding could even be beneﬁcial for su ch strategies, at least up to a point.
The problem with this optimistic surmise is that it neglects yet another facet of price impact, namely
transaction costs. The point is that, according to our deﬁni tion, crowded strategies are precisely those
leading to correlated trades, i.e. many managers entering o r leaving the market simultaneously . As an
extreme outcome, this can lead to crashes, as discussed abov e. But even in perfectly normal regimes,
trading in the same direction as others can signiﬁcantly inc rease impact costs, a.k.a. “slippage”. It is not my
own traded quantity that matters, it is the aggregate quanti ty traded by all managers following the same
5",2021-03-17T14:43:30Z, owd tras whalmin it one alternative beta risk premi it but icaione t re iass be cert articial intellence only risk t iref risk premi so  t  although  value but value value value w but re owdi t t as but it
paper_qf_19.pdf,6,Radical Complexity,"  This is an informal and sketchy review of six topical, somewhat unrelated
subjects in quantitative finance: rough volatility models; random covariance
matrix theory; copulas; crowded trades; high-frequency trading & market
stability; and ""radical complexity"" & scenario based (macro)economics. Some
open questions and research directions are briefly discussed.
","trading idea. Although the strength of the trading signal is not necessarily impaired, crowded trades may
suffer from so much “co-impact” [21]that the proﬁtability of the strategy quickly shrivels to ze ro, or even
below zero.
This suggests an interesting metric to detect crowded strat egies and estimate such co-impact costs. The
ﬁrst step is to reconstruct the putative trades that a manage r following a given strategy – say Fama-French
Momentum – would send to the market on a given day . One then use s order-book tick data to determine
the actual buy/sell order imbalances for each trading day . This allows one t o compute the correlation of
the overall market imbalance with the imbalance expected fr om the strategy under scrutiny . A statistically
signiﬁcant correlation means that the strategy does leave a detectable trace in the markets. One can also
measure the correlation between these reconstructed trade s and the price return of each stock; this provides
a direct estimate of the co-impact costs.
This is precisely what we did in Ref. [22]. The conclusion is that the classical Fama-French momentum
has indeed become more and more crowded in the last ten years, and, as of today , the estimated co-impact
costs make the strategy all but unproﬁtable. The good news, o n the other hand, is that there are many
different ways to implement a given trading idea – some more, some less correlated with the “crowd”. This
paves the way to portfolio constructions that attempt to min imize the correlation of trades with identiﬁed
trading strategies, with the hope of eschewing the curse of c rowded trades – deleveraging spirals and all.
5 High-Frequency Trading & Market Stability
In the midst of the ﬁrst COVID lockdown, the 10th anniversary of the infamous May 6th, 2010 “Flash
Crash” went unnoticed. At the time, ﬁngers were pointed at Hi gh Frequency Trading (HFT), accused of
both rigging the markets and destabilizing them. Research h as since then conﬁrmed that HFT in fact
results in signiﬁcantly lower bid-ask spread costs and, aft er correcting for technological glitches and bugs,
does notincrease the frequency of large price jumps. In fact, recent models explain why market liquidity is
intrinsically unstable: managing the risk associated to ma rket-making, whether by humans or by computers,
unavoidably creates destabilising feedback loops. In orde r to make markets more resilient, research should
focus on better market design and /or smart regulation that nip nascent instabilities in the bu d.
Since orders to buy or to sell arrive at random times, ﬁnancia l markets are necessarily most of the
time unbalanced. In such conditions, market-makers play a c rucial role in allowing smooth trading and
continuous prices. They act as liquidity buffers, that abso rb any temporary surplus of buy orders or sell
orders. Their reward for providing such a service is the bid- ask spread – systematically buying a wee lower
and selling a wee higher, and pocketing the difference.
What is the fair value of the bid-ask spread? Well, it must at l east compensate the cost of providing
liquidity , which is adverse selection . Indeed, market-makers must post prices that can be picked u p if deemed
advantageous by traders with superior information. The cla ssic Glosten-Milgrom model provides an elegant
conceptual framework to rationalize the trade-off between adverse selection and bid-ask spread, but fails
to give a quantitative, operational answer (see e.g. [23]for a recent discussion). In a 2008 study [24]
we came up with a remarkably simple answer: the fair value of t he bid-ask spread is equal to the ratio
of the volatility to the square-root of the trade frequency . This simple rule of thumb has many interesting
consequences.
First, it tells us that for a ﬁxed level of volatility increas ing the trade frequency allows market-makers to
reduce the spread, and hence the trading costs for ﬁnal inves tors. The logic is that trading smaller chunks
more often reduces the risk of adverse selection. This expla ins in part the rise of HFT as modern market
making, and the corresponding reduction of the spreads. Thr oughout the period 1900-1980, the spread on
US stocks hovered around a whopping 60 basis points, whereas it is now only a few basis points. In the
meantime, volatility has always wandered around 40% per yea r – with of course troughs and occasional
spikes, as we discuss below. In other words, investors were p aying a much higher price for liquidity before
HFT , in spite of wild claims that nowadays electronic market s are “rigged”. In fact, after a few prosperous
years before 2010, high frequency market-making has become extremely competitive and average spreads
are now compressed to minimum values.
6",2021-03-17T14:43:30Z,although  t fam frenmomentum one  one  ref t fam frent  hh frequency tradi market stabity imay flash ash at hi frequency tradi researiisince ity tir what ll ined t gl oftem gro i rst t  thr iiin
paper_qf_19.pdf,7,Radical Complexity,"  This is an informal and sketchy review of six topical, somewhat unrelated
subjects in quantitative finance: rough volatility models; random covariance
matrix theory; copulas; crowded trades; high-frequency trading & market
stability; and ""radical complexity"" & scenario based (macro)economics. Some
open questions and research directions are briefly discussed.
","From this point of view, the economic rents available to liqu idity providers have greatly decreased since
the advent of HFT . But has this made markets more stable, or ha s the decrease in the proﬁtability of market-
making also made them more fragile? The second consequence o f our simple relation between spread and
volatility relates to this important question. The point is that this relation can be understood in a two-way
fashion: clearly , when volatility increases, the impact of adverse selection can be dire for market makers
who mechanically increase their spreads. Periods of high vo latility can however be quite proﬁtable for HFT
since competition for liquidity providing is then less ﬁerc e.
But in fact higher spreads by themselves lead to higher volat ility , since transactions generate a larger
price jump – or even a crash when liquidity is low and the order book is sparse. So we diagnose a funda-
mental destabilising feedback loop, intrinsic to any marke t-making activity:
volatility−→ higher spreads & lower liquidity −→ more volatility.
Such a feedback loop can actually be included in stochastic o rder book models (such as the now commonly
used family of “Hawkes processes” [25]). As the strength of the feedback increases, one ﬁnds a phase
transition between a stable market and a market prone to spontaneous liquidity crises , even in the absence
of exogenous shocks or news [26].
This theoretical result suggests that when market-makers ( humans or machines) react too strongly to
unexpected events, liquidity can enter a death spiral. But w ho will blame them? As an old saying goes,
liquidity is a coward, it is never there when it is needed .
Such a paradigm allows one to understand why a large fraction of price jumps in fact occur without
any signiﬁcant news – rather, they result from endogenous, u nstable feedback loops [27]. Empirically , the
frequency of 10-sigma daily moves of US stock prices has been fairly constant in the last 30 years, with no
signiﬁcant change between the pre-HFT epoch and more recent years[23]. Even the 6th May 2010 Flash
Crash has a pre-HFT counterpart: on the May 28th 1962, the sto ck market plunged 9% within a matter of
minutes, for no particular cause, before recovering – much t he same weird price trajectory as in 2010. Our
conjecture: markets are intrinsically unstable, and have a lways been so. As noted in section 1 above, this
chronic instability may lie at the heart of the turbulent, mu ltiscale nature of ﬁnancial ﬂuctuations.
Can one engineer a smart solution that make markets less pron e to such dislocations? From our argu-
ments above, we know that the task would be to crush the volati lity/liquidity feedback loop, by promoting
liquidity provision when it is on the verge of disappearing. One idea would be to introduce dynamical
make/take fees, which would make cancellations more costly and li mit order posting more proﬁtable de-
pending on the current state of the order book. These fees wou ld then funnel into HFT’s optimisation
algorithms, and (hopefully) yank the system away from the re gime of recurrent endogenous liquidity cri-
sis.
6 Radical Complexity & Scenario Based Macro-economics
Good science is often associated with accurate, testable pr edictions. Classical economics has tried to con-
form to this standard, and developed an arsenal of methods to come up with precise numbers for next year’s
GDP , inﬂation and exchange rates, among (many) other things . Few, however, will disagree with the fact
that the economy is a complex system, with a large number of he terogeneous interacting units, of different
categories (ﬁrms, banks, households, public institutions ) and very different sizes. In such complex systems,
even qualitative predictions are hard. So maybe we should ab andon our pretense of exactitude and turn to
another way to do science, based on scenario identiﬁcation. Aided by qualitative (agent based) simulations,
swans that appear black to the myopic eye may in fact be perfec tly white.
The main issue in economics is precisely about the emergent o rganization, cooperation and coordination
of a motley crowd of micro-units. Treating them as a unique re presentative ﬁrm or household clearly
throws the baby with the bathwater. Understanding and chara cterizing such emergent properties is however
difﬁcult: genuine surprises can appear from micro- to macro -. One well-known example is the Schelling
segregation model: even when all agents prefer to live is mix ed neighborhoods, myopic dynamics can lead
to completely segregated ghettos [28]. In this case, Adam Smith’s invisible hand badly fails.
7",2021-03-17T14:43:30Z,from but t t ds but so suhawks as  but as suemically evemay flash ash may our as cafrom one tse radical lexity scenar based mao good assical few iso aid t treati unrstandi one slli iadam smith
paper_qf_19.pdf,8,Radical Complexity,"  This is an informal and sketchy review of six topical, somewhat unrelated
subjects in quantitative finance: rough volatility models; random covariance
matrix theory; copulas; crowded trades; high-frequency trading & market
stability; and ""radical complexity"" & scenario based (macro)economics. Some
open questions and research directions are briefly discussed.
","More generally , slightly different micro-rules /micro parameters can lead to very different macro-states:
this is the idea of “phase transitions”; sudden discontinui ties (aka crises) can appear when a parameter
is only slightly changed. Because of feedback loops of diffe rent signs, heterogeneities and non-linearities,
these surprises are hard, if not impossible to imagine or ant icipate, even aided with the best mathematical
apparatus.
This is what I would like to call “Radical Complexity”. Simpl e models can lead to unknowable behaviour,
where “Black Swans” or “Unknown Unknowns” can be present, ev en if all the rules of the model are known
in detail. In these models, even probabilities are hard to pi n down, and rationality is de facto limited. For
example, the probability of rare events can be exponentiall y sensitive to the model parameters, and hence
unknowable in practice [29]. In these circumstances, precise quantitative prediction s are unreasonable.
But this does not imply the demise of the scientiﬁc method. Fo r such situations, one should opt for a more
qualitative, scenario based approach, with emphasis on mec hanisms, feedback loops, etc. rather than on
precise, but misleading numbers. This is actually the path t aken by modern climate change science.
Establishing the list of possible (or plausible) scenarios is itself difﬁcult. We need numerical simulations
of Agent Based Models. While it is still cumbersome to experi ment on large scale human systems (although
more and more possible using web-based protocols), experim enting with Agent Based Models is easy and
fun and indeed full of unexpected phenomena. These experime ntsin silico allow one to elicit scenarios
that would be nearly impossible to imagine, because of said f eedback loops and non-linearities. Think for
example of the spontaneous synchronization of ﬁreﬂies (or o f neuron activity in our brains). It took nearly
70 years to come up with an explanation. Complex endogenous d ynamics is pervasive, but hard to guess
without appropriate tools.
Experimenting with Agent Based Models is interesting on man y counts. One hugely important aspect
is, in my opinion, that it allows to teach students in a playfu l, engaging way how complex social and
economic systems work. Such simulations would foster their intuition and their imagination, much like lab
experiments train the intuition of physicists about the rea l world, beyond abstract mathematical formalism.
Creating one’s own world and seeing how it unfolds clearly ha s tremendous pedagogical merits. It is
also an intellectual exercise of genuine value: if we are not able to make sense of an emergent phenomenon
within a world in which we set all the rules, how can we expect t o be successful in the real world? We have
to train our minds to grasp these collective phenomena and to understand how and why some scenarios
can materialize and others not. The versatility of ABM allow s one to include ingredients that are almost
impossible to accommodate in classical economic models, an d explore their impact on the dynamics of the
systems[30,31].
ABM are often spurned because they are in general hard to cali brate, and therefore the numbers they spit
out cannot and should not be taken at face value. They should r ather be regarded as all-purpose scenario
generators , allowing one to shape one’s intuition about phenomena, to u ncover different possibilities and
reduce the realm of Black Swans. The latter are often the resu lt of our lack of imagination or of the simplicity
of our models, rather than being inherently impossible to fo resee.
Expanding the study of toy-models of economic complexity wi ll create a useful corpus of scenario-
based, qualitative macroeconomics [32,33]. Instead of aiming for precise numerical predictions based
on unrealistic assumptions, one should make sure that model s rely on plausible causal mechanisms and
encompass all plausible scenarios, even when these scenari os cannot be fully characterized mathematically .
A qualitative approach to complexity economics should be hi gh on the research agenda. As Keynes said, it
is better to be roughly right than exactly wrong.
Acknowledgments
I want to warmly thank all my collaborators on these topics, e specially: R. Allez, R. Benichou, M. Benza-
quen, P . Blanc, J. Bonart, F . Bucci, J. Bun, R. Chicheportich e, J. Donier, Z. Eisler, A. Fosset, M. Gould, S.
Gualdi, S. Hardiman, A. Karami, Y. Lempérière, F . Lillo, R. M arcaccioli, I. Mastromatteo, F . Morelli, M. Pot-
ters, P . A. Reigneron, P . Seager, D. Sharma, M. Tarzia, B. Tot h, V . Volpati, M. Wyart, F . Zamponi. I also want
to pay tribute to various people with whom I had exciting and e nlightening discussions on these matters,
8",2021-03-17T14:43:30Z, because  radical lexity sim pl  swans unkunkns ifor ibut fo  estabhi  agent based mols w agent based mols tse think it lex eximenti agent based mols one sueati it  t ty  swans t eandi instead as eyackledgment all ez be ni hou  blanc boart bunbuchic  t idoi er eitr posse gould gu ali hard mam le mp lle astro maer  lli  reer oeager sharma tar zia to vpath wy art zam pond
paper_qf_19.pdf,9,Radical Complexity,"  This is an informal and sketchy review of six topical, somewhat unrelated
subjects in quantitative finance: rough volatility models; random covariance
matrix theory; copulas; crowded trades; high-frequency trading & market
stability; and ""radical complexity"" & scenario based (macro)economics. Some
open questions and research directions are briefly discussed.
","in particular R. Bookstaber, D. Farmer, X. Gabaix, J. Gather al, J. Guyon, A. Kirman, C. Lehalle, J. Moran,
M. Rosenbaum, N. Taleb. Finally , I am deeply indebted to Maur o Cesa who offered me the possibility of
putting my thoughts together and publishing them as six mont hly columns in Risk.net, from September
2020 to February 2021.
References
[1]Muzy , J. F ., Delour, J., & Bacry , E. (2000). Modelling ﬂuctua tions of ﬁnancial time series: from cascade
process to stochastic volatility model. The European Physi cal Journal B-Condensed Matter and Complex
Systems, 17(3), 537-548.
[2]Gatheral, J., Jaisson, T ., & Rosenbaum, M. (2018). Volatili ty is rough. Quantitative Finance, 18(6),
933-949.
[3]Zumbach, G. (2009). Time reversal invariance in ﬁnance. Qua ntitative Finance, 9(5), 505-515.
[4]Blanc, P ., Donier, J., & Bouchaud, J. P . (2017). Quadratic Ha wkes processes for ﬁnancial prices. Quan-
titative Finance, 17(2), 171-188.
[5]Dandapani, A., Jusselin, P ., & Rosenbaum, M. (2019). From qu adratic Hawkes processes to super-
Heston rough volatility models with Zumbach effect. arXiv p reprint arXiv:1907.06151 .
[6]Guyon, J. (2020). The joint S&P 500 /VIX smile calibration puzzle solved. Risk, April.
[7]Gatheral, J., Jusselin, P ., & Rosenbaum, M. (2020). The quad ratic rough Heston model and the joint
S&P 500/VIX smile calibration problem. arXiv preprint arXiv:2001.01789 .
[8]Jaisson, T ., & Rosenbaum, M. (2016). Rough fractional diffu sions as scaling limits of nearly unstable
heavy tailed Hawkes processes. Annals of Applied Probabili ty , 26(5), 2860-2882.
[9]Hardiman, S. J., Bercot, N., & Bouchaud, J. P . (2013). Critic al reﬂexivity in ﬁnancial markets: a Hawkes
process analysis. The European Physical Journal B, 86(10), 1-9.
[10]Bun, J., Bouchaud, J.P ., & Potters, M. (2016). Cleaning Corr elation Matrices,
https://www.risk.net/risk-magazine/technical-paper/2452666
[11]Ledoit, O., & Péché, S. (2011). Eigenvectors of some large sa mple covariance matrix ensembles. Prob-
ability Theory and Related Fields, 151(1-2), 233-264.
[12]Potters, M., & Bouchaud, J.P . (2020). A First Course in Rando m Matrix Theory: For Physicists, Engi-
neers and Data Scientists. Cambridge: Cambridge Universit y Press. doi:10.1017 /9781108768900
[13]Reigneron, P . A., Allez, R., & Bouchaud, J. P . (2011). Princi pal regression analysis and the index
leverage effect. Physica A: Statistical Mechanics and its A pplications, 390(17), 3026-3035.
[14]Karami, A., Benichou, R., Benzaquen, M., & Bouchaud, J. P . (2 021). Conditional Correlations and
Principal Regression Analysis for Futures. Wilmott, 2021( 111), 63-73.
[15]Wyart, M., & Bouchaud, J. P . (2007). Self-referential behav iour, overreaction and conventions in ﬁ-
nancial markets. Journal of Economic Behavior & Organizati on, 63(1), 1-24.
[16]Breedt, A. & Seager, P . (2020). https: //www.cfm.fr/insights/bond-equity-correlations-are-the-times-
a-changin/
[17]Chicheportiche, R., & Bouchaud, J. P . (2015). A nested facto r model for non-linear dependencies in
stock returns. Quantitative Finance, 15(11), 1789-1804.
9",2021-03-17T14:43:30Z,books haber farmer ga articial intellence gatr guy okir male halle moraroseaum tale nally mar crisk september ruary referencmu  l our ba y molli t aph ys journal connsed maer lex tems gatr al is soroseaum vat i quantitative nance zum batime qua nance blanc doi er  chaquadratic ha quanance panda paiju ssel iroseaum from hawks stozum ba  guy ot risk apr gatr al ju ssel iroseaum t sto  is soroseaum rough hawks annals applied pro babel hard maber cot  chaitic hawks t apsical journal bu chaters eani corr matricle do it ei geveors pro tory related elds ters  charst course rado matrix tory for psicists e data scientists cambridge cambridge   reer oall ez  chapri nci psics statistical meics m be ni hou  aqu e chaconditnal correlatns principal regressanalysis futurpot wy art  chaself journal economic behr orgaiz at breed eager chic  t ice  chaquantitative nance
paper_qf_19.pdf,10,Radical Complexity,"  This is an informal and sketchy review of six topical, somewhat unrelated
subjects in quantitative finance: rough volatility models; random covariance
matrix theory; copulas; crowded trades; high-frequency trading & market
stability; and ""radical complexity"" & scenario based (macro)economics. Some
open questions and research directions are briefly discussed.
","[18]Gabaix, X., Gopikrishnan, P ., Plerou, V ., & Stanley , H. E. (2 006). Institutional investors and stock
market volatility . The Quarterly Journal of Economics, 121 (2), 461-504.
[19]Chicheportiche, R., & Bouchaud, J. P . (2011). Goodness-of- ﬁt tests with dependent observations. Jour-
nal of Statistical Mechanics: Theory and Experiment, 2011( 09), P09003.
[20]Lempérière, Y., Deremble, C., Nguyen, T . T ., Seager, P ., Pot ters, M., & Bouchaud, J. P . (2017). Risk
premia: Asymmetric tail risks and excess returns. Quantita tive Finance, 17(1), 1-14.
[21]Bucci, F ., Mastromatteo, I., Eisler, Z., Lillo, F ., Bouchau d, J. P ., & Lehalle, C. A. (2020). Co-impact:
Crowding effects in institutional trading activity . Quant itative Finance, 20(2), 193-205.
[22]Volpati, V ., Benzaquen, M., Eisler, Z., Mastromatteo, I., T óth, B., & Bouchaud, J. P . (2020). Zoom-
ing in on equity factor crowding. https: //www.risk.net/cutting-edge/investments/7711291/zooming-
in-on-equity-factor-crowding
[23]Bouchaud, J. P ., Bonart, J., Donier, J., & Gould, M. (2018). T rades, quotes and prices: ﬁnancial markets
under the microscope. Cambridge University Press.
[24]Wyart, M. Bouchaud, J. P ., Kockelkoren, J., Potters, M., & Ve ttorazzo, M. (2008). Relation between
bid–ask spread, impact and volatility in order-driven mark ets. Quantitative ﬁnance, 8(1), 41-57.
[25]Bacry , E., Mastromatteo, I., & Muzy , J. F . (2015). Hawkes pro cesses in ﬁnance. Market Microstructure
and Liquidity , 1(01), 1550005.
[26]Fosset, A., Bouchaud, J. P ., & Benzaquen, M. (2020). Endogen ous liquidity crises. Journal of Statistical
Mechanics: Theory and Experiment, 2020(6), 063401.
[27]Joulin, A., Lefevre, A., Grunberg, D., & Bouchaud, J. P . (200 8). Stock price jumps: news and volume
play a minor role. Wilmott Mag. Sep /Oct 1-7.
[28]Grauwin, S., Bertin, E., Lemoy , R., & Jensen, P . (2009). Comp etition between collective and individual
dynamics. Proceedings of the National Academy of Sciences, 106(49), 20622-20626.
[29]Morelli, F . G., Benzaquen, M., Tarzia, M., & Bouchaud, J. P . ( 2020). Conﬁdence collapse in a mul-
tihousehold, self-reﬂexive DSGE model. Proceedings of the National Academy of Sciences, 117(17),
9244-9249.
[30]Gualdi, S., Tarzia, M., Zamponi, F ., & Bouchaud, J. P . (2015) . Tipping points in macroeconomic agent-
based models. Journal of Economic Dynamics and Control, 50, 29-61.
[31]Sharma, D., Bouchaud, J. P ., Gualdi, S., Tarzia, M., & Zampon i, F . (2021). V–, U–, L–or W–shaped
economic recovery after Covid-19: Insights from an Agent Ba sed Model. PloS one, 16(3), e0247823.
[32]Bookstaber, R. (2017). The end of theory . Princeton Univers ity Press.
[33]Mounﬁeld, C. C. (2021). The Handbook of Agent Based Modellin g. Cambridge University Press.
10",2021-03-17T14:43:30Z,ga articial intellence go pi krishna le rou stanley institutnal t quarterly journal economics chic  t ice  chagoodness jour statistical meics tory eximent le mp  rem ble uyeeager   charisk asyetric quant it nance bunastro maer eitr lle  chale halle co owdi quant nance vpath  aqu eeitr astro maer  chazoom  chaboart doi er gould cambridge   wy art  cha ckel  ters ve relatquantitative ba y astro maer mu  hawks market m struure liquidity posse  cha aqu eendo gejournal statistical meics tory eximent jou lilefevre ru nberg  chastock pot mag sep  gra uw ibert ile may jense proceedis natnal acamy scienc lli  aqu etar zia  chacoproceedis natnal acamy sciencgu ali tar zia zam pond  chatippi journal economic dynamics contrsharma  chagu ali tar zia zam pooid inshts agent ba mplo books haber t princetouniverse  mou t handbook agent based mlicambridge  
paper_qf_20.pdf,1,A generalization of the rational rough Heston approximation,"  Previously, in [GR19], we derived a rational approximation of the solution of
the rough Heston fractional ODE in the special case \\\\lambda = 0, which
corresponds to a pure power-law kernel. In this paper we extend this solution
to the general case of the Mittag-Leffler kernel with \\\\lambda \\\\geq 0. We
provide numerical evidence of the convergence of the solution.
","arXiv:2310.09181v2  [q-fin.CP]  14 Feb 2024A generalization of the rational rough Heston approximatio n
Jim Gatheral, Baruch College, CUNY,
jim.gatheral@baruch.cuny.edu
Radoˇ s Radoiˇ ci´ c, Baruch College, CUNY,
rados.radoicic@baruch.cuny.edu
February 15, 2024
Abstract
Previously, in [ GR19], we derived a rational approximationof the solution of the rough
Heston fractional ODE in the special case λ= 0, which corresponds to a pure power-law
kernel. In this paper we extend this solution to the general case of the Mittag-Leﬄer
kernel with λ≥0. We provide numerical evidence of the convergence of the solutio n.
1 Introduction
In the case λ≥0, the rough Heston model of [ JR16] may be written in forward variance form
(see [GKR19]) as
dSt
St=/radicalbig
Vt/braceleftbig
ρdWt+/radicalbig
1−ρ2dW⊥
t/bracerightbig
dξt(u) =/radicalbig
Vtκ(u−t)dWt, u≥t (1.1)
whereξt(u) =Et[Vu],u > tis the forward variance curve,1
2< α=H+1
2≤1, and the kernel
κis given by
κ(x) =νxα−1Eα,α(−λxα),
whereEα,α(·) denotes the generalized Mittag-Leﬄer function.
LetX= logSandXt,T:=XT−Xt. According to [ GKR19], the forward variance model
(1.1) has a cumulant generating function (CGF) of the form
ϕT
t(a) := logE/bracketleftbig
eiaXt,T/vextendsingle/vextendsingleFt/bracketrightbig
=/integraldisplayT
tξt(s)g(T−s;a)ds (1.2)
if and only if g(t;u) satisﬁes the convolution integral equation
g=−1
2a(a+i)+ρai(κ⋆g)+1
2(κ⋆g)2, (1.3)
where (κ⋆g)(t;u) :=/integraltextt
0κ(t−s)g(s;u)ds.
Previously, in [ GR19], we derived various rational approximations to the soluti on of (1.3)
in the special case λ= 0 where the kernel simpliﬁes to
κ0(x) =νxα−1
Γ(α). (1.4)
1",2023-10-13T15:19:31Z,  stojim gatr al brancollege ra do ra doi brancollege ruary abstra usly stoimit tag le  introduistost st vt  vt  et vu mit tag le  and xt xt accordi xt ft usly
paper_qf_20.pdf,2,A generalization of the rational rough Heston approximation,"  Previously, in [GR19], we derived a rational approximation of the solution of
the rough Heston fractional ODE in the special case \\\\lambda = 0, which
corresponds to a pure power-law kernel. In this paper we extend this solution
to the general case of the Mittag-Leffler kernel with \\\\lambda \\\\geq 0. We
provide numerical evidence of the convergence of the solution.
","As pointed out in [ BBRR22 ] for example, such rational approximations are extremely f ast to
compute relative to the alternatives, enabling eﬃcient cal ibration of the rough Heston model
in this special case.
In the present note, we extend these rational approximation s to the case λ >0. This
enables fast calibration of the rough Heston model in the gen eral case, with the extra param-
eterλproviding additional ﬂexibility to ﬁt market implied volat ility smiles. Moreover, when
H=1
2, we retrieve the classical Heston model for which there is a w ell-known closed-form
solution.
To proceed, let DαandI1−αrepresent respectively fractional diﬀerential and integra l
operators (see Appendix A of [ GR19] for a very brief introduction to fractional calculus).
The following result was originally proved in [ EER19].
Lemma 1.1. Letκ(τ) =ντα−1Eα,α(−λτα)andh(t;a) =1
ν(κ ⋆ g)(t;a). Thenhsatisﬁes
the fractional ODE
Dαh(t;a) =−1
2a(a+i)+(iρνa−λ)h(t;a)+1
2ν2h2(t;a);I1−αh(t;a) = 0.(1.5)
Proof.For ease of notation, we drop the explicit dependence of handgontanda. Letκ0
be the power-law kernel given by ( 1.4). The Laplace transforms of κ0andκare given (for
suitablep) by
ˆκ0(p) =ν
pα; ˆκ(p) =ν
pα+λ.
Letλ′=λ/ν. Then ˆκ0−ˆκ=λ′ˆκ0ˆκ, andκ0−κ=λ′κ0⋆ κ. Also, by deﬁnition of the
fractional integral operator,
Iαf=1
ν/integraldisplayt
0κ0(t−s)f(s)ds.
Using that ( κ0⋆κ)⋆g=κ0⋆(κ⋆g), it follows that
h=1
ν(κ⋆g) =1
ν(κ0⋆g−λκ0⋆κ⋆g) =Iαg−λIαh.
Operating with DαgivesDαh(t;a) =g(t;a)−λh(t;a). Substituting into ( 1.3) gives the
result.
Given an approximate solution to the Riccati Equation ( 1.5), an accurate approximation
to the CGF ( 1.2) may be easily computed. European option prices may then be o btained
using the Lewis formula ([ Lew00,Gat06]):
C(S,K,T) =S−√
SK1
π/integraldisplay∞
0du
u2+1
4Re/bracketleftBig
e−iukϕT
t(u−i/2)/bracketrightBig
(1.6)
whereSis the current stock price, Kthe strike price and Texpiration. Finally, implied
volatilities may be computed by numerical inversion of the B lack-Scholes formula.
A key observation is that for option pricing with equation ( 1.6), we need only ﬁnd a good
approximation to the solution h(a,x) of the rough Heston Riccati equation for a∈ Awith
A={z∈C:ℜ(z)≥0,−1≤ ℑ(z)≤0} (1.7)
whereℜandℑdenote real and imaginary parts respectively.
2",2023-10-13T15:19:31Z,as stoi stoostoto  t lea  t nhs at is proof for  t place  talso usi oati substituti giveric cat equatalewis new gat re b b is t eiratnally schools storic cat with
paper_qf_20.pdf,3,A generalization of the rational rough Heston approximation,"  Previously, in [GR19], we derived a rational approximation of the solution of
the rough Heston fractional ODE in the special case \\\\lambda = 0, which
corresponds to a pure power-law kernel. In this paper we extend this solution
to the general case of the Mittag-Leffler kernel with \\\\lambda \\\\geq 0. We
provide numerical evidence of the convergence of the solution.
","1.1 Main results and organization of the paper
Our paper is organized as follows. In Section 2, we derive a short-time expansion of the
solution hof the rough Heston Riccati equation ( 1.5). Then in Section 3, we derive an
asymptotic solution to ( 1.5) in the long-time limit τ=T−t→ ∞. In Section 4, we
explain how to construct global rational approximations to hand present numerical results.
In particular, we exhibit (near) exponential convergence o f the rational approximations with
respect to their order. Finally, in Section 5, we summarize and conclude. Some technical
details are relegated to the appendix.
2 Solving the rough Heston Riccati equation for short times
First, wederiveashort-timeexpansion ofthesolution h(t;a) of thefractional Riccati equation
(1.5). Inspired by the λ= 0 case, consider the small tansatz
h(t;a) =∞/summationdisplay
j=1bjtjα. (2.1)
Then,
Dαh=∞/summationdisplay
j=1bjΓ(1+jα)
Γ(1+(j−1)α)t(j−1)α=∞/summationdisplay
j=0bj+1Γ(1+(j+1)α)
Γ(1+jα)tjα.
Substituting into ( 1.5) and matching coeﬃcients of t0gives
b1=−1
Γ(1+α)1
2a(a+i).
Doing the same with the coeﬃcient of tαgives
b2=Γ(1+α)
Γ(1+2α)(iρa−λ′)νb1,
where as before, λ′=λ/ν. This generalizes to the recursion
b1=−1
Γ(1+α)1
2a(a+i)
bk=Γ(1+(k−1)α)
Γ(1+kα)

−˜λνbk−1+1
2ν2k−1/summationdisplay
i,j=1
/BDi+j=k−1bibj

,
where˜λ=λ′−iρa.
3 Solving the rough Heston Riccati equation for long times
The fractional Riccati equation ( 1.5) can be re-expressed as
Dαh(t;a) =1
2(νh(t;a)−r−) (νh(t;a)−r+), (3.1)
3",2023-10-13T15:19:31Z,articial intellence our isestoric cat tseiseinally sesome solvi storic cat rst ric cat insed tsubstituti doi  di solvi storic cat t ric cat
paper_qf_20.pdf,4,A generalization of the rational rough Heston approximation,"  Previously, in [GR19], we derived a rational approximation of the solution of
the rough Heston fractional ODE in the special case \\\\lambda = 0, which
corresponds to a pure power-law kernel. In this paper we extend this solution
to the general case of the Mittag-Leffler kernel with \\\\lambda \\\\geq 0. We
provide numerical evidence of the convergence of the solution.
","withA=/radicalbig
a(a+i)+(λ′−iρa)2;r±={λ′−iρa±A};λ′=λ/ν. Letνh∞(t;a) =
r−[1−Eα(−Aνtα)] where Eαis the Mittag-Leﬄer function. Then, for t∈R≥0anda∈ A
(given in ( 1.7)), as in Proposition 3.1 of [ GR19],h∞(t;a) satisﬁes
νh∞(t;a)−r−=−r−
Aνt−α
Γ(1−α)+O/parenleftbig
|Aνtα|−2/parenrightbig
. (3.2)
and thus solves the rough Heston Riccati equation ( 3.1) up to an error term of O/parenleftbig
|Aνtα|−2/parenrightbig
,
ast→ ∞.
The form of the asymptotic expansion of Eα(−Aνtα) in Corollary A.1motivates the
following ansatz for h(t;a) ast→ ∞:
h(t;a) =∞/summationdisplay
k=0gkt−kα. (3.3)
Then
Dαh(t;a) =∞/summationdisplay
k=1gk−1Γ(1−(k−1)α)
Γ(1−kα)t−kα. (3.4)
Note that ( 3.2) gives
g0=r−
ν;g1=−r−
Aν21
Γ(1−α).
Also, from the fractional Riccati equation ( 3.1), using that g0=r−/ν,
Dαh(a,x) =1
2(νh(t;a)−r−)(νh(t;a)−r+)
=ν∞/summationdisplay
k=1gkt−kα/parenleftBigg
−A+1
2ν∞/summationdisplay
k=1gkt−kα/parenrightBigg
. (3.5)
Equating ( 3.4) and (3.5) gives
∞/summationdisplay
k=1gk−1Γ(1−(k−1)α)
Γ(1−kα)t−kα=ν∞/summationdisplay
k=1gkt−kα/parenleftBigg
−A+1
2ν∞/summationdisplay
k=1gkt−kα/parenrightBigg
.
Matching coeﬃcients of t−αgives
g1=−1
Aν1
Γ(1−α)g0.
Similarly, matching coeﬃcients of t−2αgives
g2=−1
Aν/braceleftbiggΓ(1−α)
Γ(1−2α)g1−1
2ν2g2
1/bracerightbigg
.
The general recursion for k >2 is given by
gk=−1
Aν

Γ(1−(k−1)α)
Γ(1−kα)gk−1−1
2ν2∞/summationdisplay
i,j=1
/BDi+j=kgigj

.
4",2023-10-13T15:19:31Z, mit tag le tpropositstoric cat t corollary tnote also ric cat b b equati b b matchi simarly t di
paper_qf_20.pdf,5,A generalization of the rational rough Heston approximation,"  Previously, in [GR19], we derived a rational approximation of the solution of
the rough Heston fractional ODE in the special case \\\\lambda = 0, which
corresponds to a pure power-law kernel. In this paper we extend this solution
to the general case of the Mittag-Leffler kernel with \\\\lambda \\\\geq 0. We
provide numerical evidence of the convergence of the solution.
","4 Rational approximations of hand numerical results
In previous sections, we derived the short-time and long-ti me asymptotics of h. As in [GR19],
the only admissible global rational approximations of hthat match both ( 2.1) and (3.3) are
of the diagonal form
h(n,n)(t;a) =/summationtextn
i=1pn,iyn
/summationtextn
j=0qn,jyn(4.1)
withy=νtα.
Explicit expressions for the coeﬃcients pn,iandqn,jin (4.1) are provided in the R-ﬁle
oughHestonPadeLambda.R }, made openly accessible at \\\\u lhttps://github.com/jgatheral/RationalRoughHeston,
together with Jupyter notebooks illustrating the usage of t heh(n,n).
4.1 Numerical results
Thanks to Giacomo Bormetti and his collaborators, we now hav e much more eﬃcient code
for the Adams scheme that seems to converge (for our purposes ) with only 200 steps. To be
safe, our benchmark run of the Adams scheme will use 1,000 tim e steps.
In the following, we assume the following realistic rough He ston parameters:
H= 0.05;ν= 0.4;ρ=−0.65;λ= 1. (4.2)
The rational approximations h(n,n)(t;a) toh(t;a) forn∈ {2,3,4,5}with the particular
choicea= 3−i/2 and rough Heston parameters ( 4.2) are plotted in Figure 4.1.h(3,3),h(4,4)
andh(5,5)are almost indistinguishable from the 1,000 step Adams esti mate and signiﬁcantly
better than h(2,2). Moreover, all of these rational approximations are at leas t as fast to
compute as the classical Heston solution.
0 2 4 6 8−4 −3 −2 −1
tRe Dαh(t;a)
h(2, 2)
h(3, 3)
h(4, 4)
h(5, 5)
0 2 4 6 80.0 0.2 0.4 0.6 0.8
tIm Dαh(t;a)h(2, 2)
h(3, 3)
h(4, 4)
h(5, 5)
Figure 4.1: In the left panel ℜDαh(t;3−i/2) and in the right panel ℑDαh(t;3−i/2).
Naturally the coeﬃcients of the higher order diagonal appro ximants become successively
harder to compute in closed form. And since the formulae are m ore complex, the functions
5",2023-10-13T15:19:31Z,ratnal ias elicit stopage lambda ratnal rough stoup ter numerical thanks giacomo bor met ti adams to adams i t sto adams ostore im  inaturally and
paper_qf_20.pdf,6,A generalization of the rational rough Heston approximation,"  Previously, in [GR19], we derived a rational approximation of the solution of
the rough Heston fractional ODE in the special case \\\\lambda = 0, which
corresponds to a pure power-law kernel. In this paper we extend this solution
to the general case of the Mittag-Leffler kernel with \\\\lambda \\\\geq 0. We
provide numerical evidence of the convergence of the solution.
","when implemented take longer to compute. Thus, even if, for e xample, h(7,7)were to be a
better approximation than h(4,4), it would be much slower to compute and h(4,4)would likely
be the approximation of choice in practice.
4.2 Dependence of approximation quality on H
So far, we have assessed the quality of our rational approxim ations with the realistic but ﬁxed
set of parameters ( 4.2). It turns out that the quality of the rational approximatio ns decreases
asHincreases from 0 to1
2, which corresponds to the classical Heston model. Indeed, i t
is evident from Figure 4.2that the h(n,n)approximate halmost perfectly when H= 0;1
the approximation quality deteriorates as Hincreases. Nevertheless, we observe that the
approximation h(5,5)is very accurate, even in the classical case H=1
2.
0 2 4 6 80.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7H= 0
tRe Dαh(t;a)
0 2 4 6 80.0 0.2 0.4 0.6H= 0.1
tRe Dαh(t;a)
0 2 4 6 80.0 0.2 0.4 0.6 0.8H= 0.2
tRe Dαh(t;a)
0 2 4 6 80.0 0.2 0.4 0.6 0.8H= 0.3
tRe Dαh(t;a)
0 2 4 6 80.0 0.2 0.4 0.6 0.8H= 0.4
tRe Dαh(t;a)
0 2 4 6 80.0 0.2 0.4 0.6 0.8H= 0.5
tRe Dαh(t;a)
Figure 4.2: ℑDαh(t;3−i/2) computed for various values of H; solid pink lines are Adams
scheme estimates with 1,000 steps; dashed lines are the rati onal approximations h(3,3),h(4,4),
andh(5,5)respectively. The rational approximations and the numeric al solution are almost
indistinguishable when H= 0.
1More precisely in the limit H↓0 in the sense of [ FGS21].
6",2023-10-13T15:19:31Z, pennce so it ineasstoined  ineasnevertless re re re re re re  adams t 
paper_qf_20.pdf,7,A generalization of the rational rough Heston approximation,"  Previously, in [GR19], we derived a rational approximation of the solution of
the rough Heston fractional ODE in the special case \\\\lambda = 0, which
corresponds to a pure power-law kernel. In this paper we extend this solution
to the general case of the Mittag-Leffler kernel with \\\\lambda \\\\geq 0. We
provide numerical evidence of the convergence of the solution.
","4.3 Convergence of the h(n,n)in the classical Heston case
The limit of the Mittag-Leﬄer kernel when H=1
2is the exponential, corresponding to the
classical Heston model. Since we know the classical Heston c haracteristic function in closed
form, wemay study theconvergence of the various rational ap proximations withoutnumerical
error from the Adams scheme.
In Figure 4.3, we plot approximation errors in the classical Heston case f or the Pad´ e
approximations h(2,2),h(3,3),h(4,4),h(5,5). To the naked eye, it looks as if convergence of
the rational approximations h(n,n)may be exponential in the approximation order n. This
conjecture is conﬁrmed numerically in Figure 4.4.
0 2 4 6 8−0.4 −0.3 −0.2 −0.1 0.0
tError in Re Dαh(t;a)
h(2, 2)
h(3, 3)
h(4, 4)
h(5, 5)
0 2 4 6 8−0.10 −0.05 0.00 0.05 0.10 0.15
tError in Im Dαh(t;a)
h(2, 2)
h(3, 3)
h(4, 4)
h(5, 5)
Figure 4.3: Approximation errors for ℜDαh(t;3−i/2) (left) and ℑDαh(t;3−i/2) (right).0.05 0.10 0.20
Approximation order (n)Maximum approximation error (Re)
2 3 4 5
0.02 0.05 0.10
Approximation order (n)Maximum approximation error (Im)
2 3 4 5
Figure 4.4: Maximum absolute approximation errors for ℜDαh(t;3−i/2) (left) and
ℑDαh(t;3−i/2) (right) as a function of the order nof the Pad´ e approximation h(n,n). The
y-axis scale is logarithmic.
7",2023-10-13T15:19:31Z,convergence stot mit tag le stosince stoadams i stopad to   error re error im  approximatapproximatmaximum re approximatmaximum im  maximum pad t
paper_qf_20.pdf,8,A generalization of the rational rough Heston approximation,"  Previously, in [GR19], we derived a rational approximation of the solution of
the rough Heston fractional ODE in the special case \\\\lambda = 0, which
corresponds to a pure power-law kernel. In this paper we extend this solution
to the general case of the Mittag-Leffler kernel with \\\\lambda \\\\geq 0. We
provide numerical evidence of the convergence of the solution.
","4.4 Convergence in the case H <1/2
In the general case 0 < H < 1/2, there is no closed-form solution for the characteristic
function, so we must measure errors relative to the Adams sch eme, which is itself an approxi-
mation. We choose an intermediate value H= 0.2 for our experiments, which is high relative
to the 0< H <0.1 typically estimated from time series or calibrated to impl ied volatilities. It
is worth noting that the rational approximations are so good , that to get accurate estimates
of approximation errors, the benchmark Adams scheme needs t o be run with at least 1,000
steps.
0 2 4 6 8−0.10 −0.08 −0.06 −0.04 −0.02 0.00
tError in Re Dαh(t;a)
h(2, 2)
h(3, 3)
h(4, 4)
h(5, 5)
0 2 4 6 8−0.01 0.00 0.01 0.02 0.03 0.04 0.05
tError in Im Dαh(t;a)h(2, 2)
h(3, 3)
h(4, 4)
h(5, 5)
Figure 4.5: Approximation errors for ℜDαh(t;3−i/2) (left) and ℑDαh(t;3−i/2) (right) in
the case H= 0.2.5e−04 2e−03 5e−03 2e−02 5e−02
Approximation order (n)Maximum approximation error (Re)
2 3 4 5
0.001 0.002 0.005 0.010 0.020 0.050
Approximation order (n)Maximum approximation error (Im)
2 3 4 5
Figure 4.6: Maximum absolute approximation errors for ℜDαh(t;3−i/2) (left) and
ℑDαh(t;3−i/2) (right) as a function of the order nof the Pad´ e approximation h(n,n). The
y-axis scale is logarithmic.
8",2023-10-13T15:19:31Z,convergence iadams  it adams error re error im  approximatapproximatmaximum re approximatmaximum im  maximum pad t
paper_qf_20.pdf,9,A generalization of the rational rough Heston approximation,"  Previously, in [GR19], we derived a rational approximation of the solution of
the rough Heston fractional ODE in the special case \\\\lambda = 0, which
corresponds to a pure power-law kernel. In this paper we extend this solution
to the general case of the Mittag-Leffler kernel with \\\\lambda \\\\geq 0. We
provide numerical evidence of the convergence of the solution.
","In Figure 4.5, withH= 0.2, we plot h(n,n)errors for n∈ {2,3,4,5}. To the naked eye,
it looks as if convergence of the h(n,n)may once again be exponential in the approximation
ordern. This conjecture is roughly conﬁrmed in Figure 4.6. Note also that relative to the
classical Heston case, the sizes of the errors are much small er, consistent with Figure 4.2.
5 Summary and conclusions
The rough Heston cumulant generating function, as with all a ﬃne forward variance models,
is a convolution of the forward variance curve with a functio ngthat satisﬁes the convolution
Riccati integral equation ( 1.3). In [GR19] we constructed rational approximations to the
solution of this equation in the special case where the kerne l is power-law. In this paper, we
haveextendedthatapproximationtothemoregeneralcasewh erethekernelisaMittag-Leﬄer
function.
We focused on the diagonal approximants h(n,n),n∈ {2,3,4,5}, thelast three of which are
eﬃcient to compute, rendering them of great interest for pra ctical application. Moreover, we
have provided numerical evidence of exponential convergen ce of the h(n,n)with respect to the
approximationorder n. Codeismadefreelyavailableonlineat https://github.com/jgatheral/RationalRoughH 
6 Acknowledgements
We are grateful to Giacomo Bormetti and his collaborators fo r sharing their eﬃcient Adams
scheme code and to Stefano Marmi for enlightening conversat ions.
References
[BBRR22] Fabio Baschetti, Giacomo Bormetti, Silvia Romagn oli, and Pietro Rossi. The
SINCway: A fastandaccurate approach to Fourier pricing. Quantitative Finance ,
22(3):427–446, 2022.
[EER19] Omar El Euch and Mathieu Rosenbaum. The characteris tic function of rough
Heston models. Mathematical Finance , 29(1):3–38, 2019.
[FGS21] Martin Forde, Stefan Gerhold, and Benjamin Smith. S mall-time, large-time, and
asymptotics for the Rough Heston model. Mathematical Finance , 31(1):203–241,
2021.
[Gat06] Jim Gatheral. The volatility surface: A practitioner’s guide . John Wiley & Sons,
2006.
[GKR19] Jim Gatheral and Martin Keller-Ressel. Aﬃne forwar d variance models. Finance
and Stochastics , 23(3):501–533, 2019.
[GR19] Jim Gatheral and Radoˇ s Radoiˇ ci´ c. Rational approx imation of the rough Heston
solution. International Journal of Theoretical and Applied Finance , 22(3):1950010,
2019.
9",2023-10-13T15:19:31Z,i to   note sto suary t storic cat iimit tag le  oco is ma freely  articial intellence  online at ratnal rough ackledgement  giacomo bor met ti adams stefamark referencb bate giacomo bor met ti sva roma gmetro rossi t way courier quantitative nance omar el eu mathit roseaum t stomatmatical nance martifor  stefager hold benjamismith rough stomatmatical nance gat jim gatr al t  rey sons jim gatr al martikler re ssel nance stochastic jim gatr al ra do ra doi ratnal stointernatnal journal toical applied nance
paper_qf_20.pdf,10,A generalization of the rational rough Heston approximation,"  Previously, in [GR19], we derived a rational approximation of the solution of
the rough Heston fractional ODE in the special case \\\\lambda = 0, which
corresponds to a pure power-law kernel. In this paper we extend this solution
to the general case of the Mittag-Leffler kernel with \\\\lambda \\\\geq 0. We
provide numerical evidence of the convergence of the solution.
","[JR16] Thibault Jaisson and Mathieu Rosenbaum. Rough fract ional diﬀusions as scaling
limits of nearly unstable heavy tailed Hawkes processes. The Annals of Applied
Probability , 26(5):2860–2882, 2016.
[Lew00] AL Lewis. Option Valuation under Stochastic Volatility . Finance Press: Newport
Beach, CA, 2000.
[Pod98] Igor Podlubny. Fractional diﬀerential equations: an introduction to frac tional
derivatives, fractional diﬀerential equations, to method s of their solution and some
of their applications , volume 198. Elsevier, 1998.
A Asymptotic expansion of the Mittag-Leﬄer function
The following lemma is a straightforward corollary of Theor em 1.4 of [ Pod98].
Lemma A.1. Let0< α≤1andµ∈Rbe such that
πα
2< µ < πα.
Then, for any integer p >0, the following expansion holds:
Eα(z) =−p/summationdisplay
k=1z−k
Γ(1−kα)+O/parenleftbig
|z|−1−p/parenrightbig
,|z| → ∞, µ≤ |arg(z)| ≤π.
Lemma A.2. Let0< α≤1. Further let a=u+ iywithu∈R≥0,y∈[−1,0]and let
A=/radicalbig
a(a+i)+(λ′−iρa)2, ν >0,t >0. Then for any x∈R>0,
|arg(−Axα)| ∈/bracketleftbigg3π
4,π/bracketrightbigg
.
Proof.Let ¯ρ=/radicalbig
1−ρ2. Then
ℜ(A2) = ¯ρ2u2−y(y+1)+(λ′+ρy)2,
which is positive, so arg A2∈[−π
2,π
2], and so arg A∈[−π
4,π
4]. It follows that
|arg(−Axα)| ∈/bracketleftbigg3π
4,π/bracketrightbigg
.
Corollary A.1. Let0< α≤1. Further let a=u+iywithu∈R>0andy∈[−1,0]. Further
letA=/radicalbig
a(a+i)+(λ′−iρa)2, ν >0,t >0. For any positive integer pandx∈R>0,
Eα(−Axα) =p/summationdisplay
k=1(−1)k−1
Akx−kα
Γ(1−kα)+O/parenleftBig
|Axα|−1−p/parenrightBig
, x→ ∞.
Proof.Apply Lemma A.1and Lemma A.2withµ=3
4πα.
10",2023-10-13T15:19:31Z,thibault is somathit roseaum rough hawks t annals applied probabity new lewis optvaluatstochastic volatity nance  ne bea or  sub ny franal else vier asymptotic mit tag le t t or  lea  be tlea  furtr tproof  tit corollary  furtr furtr for ak b b proof apply lea lea
paper_qf_21.pdf,1,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","arXiv:0912.1617v1  [q-fin.ST]  8 Dec 2009Article No. ectj??????Econometrics Journal (2010), volume 10, pp. 1–25.
Homogeneous Volatility Bridge Estimators
A. Saichev1,3, D. Sornette1,2, V. Filimonov1,3and F. Corsi4
1ETH Zurich – Department of Management, Technology and Econom ics, Switzerland.
2Swiss Finance Institute, 40, Boulevard du Pont-d’ Arve, Cas e Postale 3, 1211 Geneva 4, Switzerland.
3Nizhni Novgorod State University – Department of Mathemati cs, Russia.
4University of Lugano and Swiss Finance Institute, Via G. Buﬃ 13, CH-6904 Lugano, Switzerland.
E-mail:dsornette@ethz.ch
Received: December 2009
Summary We present a theory of homogeneous volatility bridge estima tors for log-
price stochastic processes. The main tool of our theory is th e parsimonious encoding
of the information contained in the open, high and low prices of incomplete bridge,
corresponding to given log-price stochastic process, and i n its close value, for a given
time interval. The eﬃciency of the new proposed estimators i s favorably compared with
that of the Garman-Klass and Parkinson estimators.
Keywords :volatility, variance, estimators, eﬃciency, Wiener proce sses, homoge-
neous functions
1. INTRODUCTION
Volatility, deﬁned as the standard deviation of the increments of th e log-price over a
speciﬁc time interval, is a universally used risk indicator. With the grow ing availabil-
ity of high-frequency tick-by-tick price time series, a number of ne w eﬃcient volatility
estimators have been developed (see, for instance, Yang and Zha ng (2000), Corsi et al.
(2001), Andersen et al. (2003), A¨ ıt-Sahalia (2005), Zhang et al. (2005)). However, for
most applications involving risk assessment and management of inves tment portfolios, it
is the common practice to use time series of prices recorded at ﬁxed time intervals, such
as 1 minute, 5 minutes, 30 minutes, 1 hour, 1 day, 1 week, 1 month an d so on. For such
time series, four prices are actually recorded, called the open-high -low-close (OHLC) of
the price for each time interval.
Our purpose is to provide new tools to exploit systematically the OHLC to improve
volatility estimators, compared with techniques using only the close p rice time series.
It is intuitively appealing that close-minus-open, high-minus-open an d low-minus-open
should provide signiﬁcant information on the variability of the price, t hat should help
improve the volatility estimators. We present here a comprehensive theory of homoge-
neous volatility bridge estimators for arbitrary stochastic proces ses, that fully exploit the
OHLC prices. For this, we have started to develop the theory of th e most eﬃcient point-
wise homogeneous OHLC volatility estimators, valid for any price proc esses (Saichev et
al., 2009). The main tool of our theory is the parsimonious encoding o f all the informa-
tion contained in the mentioned OHLC prices for a given time interval in the form of
general “diagrams” associated with the joint distributions of the h igh-minus-open, low-
minus-open and close-minus-open values. The diagrams can be tailor ed to yield the most
eﬃcient estimators associated to any statistical properties of th e underlying log-price
stochastic process.
The present work extends and generalizes (Saichev et al., 2009) by developing most
c/circlecopyrtRoyal Economic Society 2010. Published by Blackwell Publis hers Ltd, 108 Cowley Road, Oxford OX4
1JF, UK and 350 Main Street, Malden, MA, 02148, USA.",2009-12-08T21:47:01Z, c artie no econometrics journal homogeneous volatity bridge estimator ice corvee  limo nov corse ipartment management technology eco nom switzerland swiss nance institute levard pont ar ve cas postal geneva switzerland niz hnovgorod state  partment ma t math   land swiss nance institute via bu land switzerland received cember suary  t t germaass parsokeywords winner volatity with ya sha corse anrsosha lia  for our it  for ice t t t ice royal economic society pubd ll pub  ltd cole road oxford articial intellence street main
paper_qf_21.pdf,2,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","2 A. Saichev, D. Sornette, V. Filimonov, F. Corsi
eﬃcient OHLC bridge estimators. We ﬁnd that the new OHLC bridge es timators are
signiﬁcantly more eﬃcient than the OHLC estimators obtained from t he untransformed
(or “unbridged”) process. For Wiener and similar processes, this c an be intuitively un-
derstand as follows. It is well-known that the high and low values of a W iener process
are most probably found in the neighborhood of the edges of the ob servation interval.
In contrast, by construction of the bridge, its high and low values a re in general distant
from the edges. As a result, the high and low of a bridge incorporate signiﬁcantly more
information about the variability of the original stochastic process than its own high and
low values. This is the motivation for us to extend the theory of (Saic hev et al., 2009)
for bridges and to provide explicit analytical expressions for the mo st eﬃcient point-wise
volatility bridge estimators, based on the analytical expression of t he joint distribution
of its high-minus-open, low-minus-open and close-minus-open value s.
Our work also improves on the following papers as follows. Garman and Klass (G&K)
(1980) introduced a quadratic estimator for the variance of the W iener process for the
log-price, which has rather low variance. Parkinson (PARK) (1980) proposed a simple
quadratic variance estimator proportional to ( H−L)2, which is using only a part of
the information available from OHLC prices. Rogers and Satchell (R& S) (1991,1994) in-
troduced another quadratic estimator for the variance of the Wie ner process with drift,
which is unbiased for all drifts. Both G&K and R&S estimators are foc used on the
variance, and do not present estimators for the volatility, which is o f obvious interest
for ﬁnancial applications. Yang and Zhang (2000) produced an unb iased and eﬃcient
quadratic variance estimator, taking into account the OHLC of log- prices forn>1 con-
secutive days. Their main novelty is to take into account the possible existence of jumps
(or gaps) of prices from yesterday’s close till today’s open prices. Their minimization of
the variance of their estimators requires the estimation of expect ations of a quadratic
form of the OHLC which they only partly achieve due to the lack of kno wledge of the
full joint distribution, which we oﬀer in this paper. Chan and Lien (200 3) compared the
empirical eﬀectiveness of four estimators, the PARK, the G&K and R&S ones, and the
naive excursion range H−Lestimator. From the perspective oﬀered by these previous
works, the present paper can be viewed as providing their full unde rpinning theory, since
we are able to express eﬃcient estimators in the presence of arbitr ary constraints from
the explicit knowledge of the joint distribution of the OHLC log-prices .
The paper is organized as follows. Section 2 describes the propertie s of the stochastic
processes for which our theory of most eﬃcient homogeneous vola tility bridge estimators
is developed. Section 3 derives the generalexpressions for the mo st eﬃcient homogeneous
volatility OHLC bridge estimators. Section 4 provides a detailed analyt ical description of
the statistical properties of incomplete bridges of Wiener process with drift, describing
log-price dynamics. Section 5 compares the eﬃciency of our derived most eﬃcient ho-
mogeneous bridge estimators and the eﬃciency of the generalized G &K bridge estimator
and of the normalized PARK estimator. Section 6 tests our results u sing synthetic time
series generated by numerical simulations, which mimic the tick-by-t ick nature of real
log-price processes. Section 7 concludes.
2. HOMOGENEOUS VOLATILITY BRIDGE ESTIMATORS
The main goal of this paper is to construct eﬃcient bridge estimator s using the open,
high, low, close (OHLC) prices for the variance and the volatility of so me asset log-price
processA(t).
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,ice corvee  limo nov corse  for winner it ias  articial intellence our germaass parsoro matcs wie both ya  tir tir chalieestimator from t sesesewinner seseset royal economic society
paper_qf_21.pdf,3,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","Bridge volatility estimators 3
2.1. Volatility of order λ
The conventional deﬁnition of the volatility V(t0,T0) of a stochastic process A(t) at time
t0and time scale T0is the standard deviation of its increment
∆(t0,T0) =A(t0+T0)−A(t0)
within the time interval t∈(t0,t0+T0):
V(t0,T0) =/radicalbig
Var[∆(t0,T0)].
The time scale T0can be for instance 5 minutes, 1 day or 1 year, corresponding resp ec-
tively to intraday, daily or yearly volatility.
Since diﬀerent measures of the variability of log-price processes ar e used in the litera-
ture, it is convenient to deﬁne a generalized volatility of order λas follows.
Deﬁnition 2.1 The volatility of order λof the stochastic process A(t) is the power λof
the conventional volatility
Vλ(t0,T0) :=Vλ(t0,T0) = (Var[∆( t0,T0)])λ/2.
Remark 2.1 Forλ= 1, the volatility of order λcoincides with the conventional volatil-
ity, while, for λ= 2,V2(t0,T0) is the variance of the increment ∆( t0,T0). Most known
estimators, for instance the R&S, G&K and PARK ones, are varianc e estimators. Intro-
ducing the volatility of order λgives us the possibility later on to compare the diﬀerences
and relations between the volatility and variance estimators.
2.2. Wiener process model of log-price increments
We will analyze the properties of the estimatorsof the volatility of or derλfor the Wiener
process with drift, posing without loss of generality t0= 0 andA(0) = 0. This implies
that
A(t) :=µt+σW(t), (2.1)
whereµis the drift of the log-price process A(t) andσis its standard deviation at t= 1,
whileW(t) is the standard Wiener process with zero drift and variance E[ W2(t)] =t.
The self-similar properties of the Wiener process allow us to choose t he time scale by
T0= 1 without loss of generality, so that the volatility of order λis simply equal to the
standard deviation σraised to the power λ:
Vλ:=Vλ(t0= 0,T0= 1) =σλ.
We analyze below the volatility estimators based on the high, low and clo se values of the
incomplete bridge of the Wiener process with drift A(t) deﬁned by (2.1).
Deﬁnition 2.2 The stochastic process
B(t,κ,T) :=A(t)−κt
TA(T) =µ(1−κ)t+σ/bracketleftbigg
W(t)−κt
TW(T)/bracketrightbigg
,(2.2)
whereκis arbitrary constant, is called the incomplete bridge of the original s tochastic
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,bridge volatity t var t since  t var remark for most intro winner  winner  winner t winner  winner  t royal economic society
paper_qf_21.pdf,4,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","4 A. Saichev, D. Sornette, V. Filimonov, F. Corsi
processA(t). Forκ= 1, the incomplete bridge is nothing but the standard (complete)
bridge
B(t,T) :=σ/bracketleftbigg
W(t)−t
TW(T)/bracketrightbigg
.
Using the self-similar properties of the Wiener process, one can rew rite (2.1), (2.2) in
the form
A(t) =σ√
TX/parenleftbiggt
T,γ/parenrightbigg
, B(t,κ,T) =σ√
TY/parenleftbiggt
T,κ,γ/parenrightbigg
, (2.3)
whereY(t,κ,γ) is the incomplete bridge
Y(t,κ,γ) :=X(t,γ)−κtX(1,γ) (2.4)
of the Wiener process with drift
X(t,γ) :=γt+W(t), t∈(0,1), (2.5)
and the auxiliary parameter
γ=µ
σ√
T (2.6)
plays the role of a 1ststandardized moment (or inverse coeﬃcient of variation) of the
distribution of increments of the process A(t) over the time interval T. Figure 1 shows a
realization of the Wiener process W(t) and its complete bridge. The high and low values
of the Wiener process and of its bridge are in general drastically diﬀe rent.
Remark 2.2 In ﬁnancial markets applications, both the drift µand the standard devia-
tionσare unknown. Thus, the value of the parameter γis unknown as well. Nevertheless,
for the convenience of our analysis, we will suppose in the following de rivations that the
value of parameter γis given. One can take into account the indeterminateness of the
parameterγby exploring in detail the dependence as a function of γof the bias and of
the eﬃciency of the OHLC volatility bridge estimators, following the an alysis performed
by Saichev et al. (2009) for κ= 0.
2.3. Homogeneous volatility bridge estimators
Deﬁnition 2.3 A volatility estimator Vλis called an homogeneous OHLC volatility
bridge estimator of order λif it has the form
ˆσλ=1
Tλ/2hλ(¯H,¯L,¯C), (2.7)
wherehλis a homogeneous function of order λ, the random values ¯Hand¯L, are the
high and low of the incomplete bridge B(t,κ,T) deﬁned by (2.2) within the observation
interval (0,T),
¯H:= sup
t∈(0,T)B(t,κ,T),¯L:= inf
t∈(0,T)B(t,κ,T),
and
¯C:=A(T) =µT+σW(T)
is the close value of the original stochastic process for the log-pric eA(t).
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,ice corvee  limo nov corse for usi winner winner  winner t winner remark i nevertless one ice homogeneous  hand royal economic society
paper_qf_21.pdf,5,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","Bridge volatility estimators 5
0 0.2 0.4 0.6 0.8 1−1−0.500.511.5
tW(t);Y(t,1,0)W(t)
bridge
Figure 1.A realization of the Wiener process W(t) and complete bridge W(t)−tW(1)
A remarkable property of homogeneous estimators deﬁned by (2.7 ) is that, for a given
γ, their statistical propertiesdo not depend on the duration Tofthe observationinterval.
Mathematically, this fact is expressed by the following theorem.
Theorem 2.1 The estimator deﬁned by (2.7)is equal to
ˆσλ=σλhλ(H,L,C), (2.8)
whereHandLare the high and low values of the incomplete bridge Y(t,κ,γ)deﬁned by
expression (2.4)
H:= sup
t∈(0,1)Y(t,κ,γ), L:= inf
t∈(0,1)Y(t,κ,γ), (2.9)
whileC:=X(1,γ)is the close value of the Wiener process X(t,γ)with drift, deﬁned by
(2.5).
Proof.Substituting the right-hand-side of the equalities of (2.3) into the r ight-hand-
side of expression (2.7) and using the homogeneity of the function hλ, we obtain equality
(2.8). /squaresolid
Deﬁnition 2.4 We refer to the function
ˆeλ=hλ(H,L,C) (2.10)
as the canonical OHLC volatility bridge estimator of order λ. Using this deﬁnition, one
can rewrite expression (2.8) in the form
ˆσλ=σλˆeλ. (2.11)
Remark 2.3 The statistical properties of the canonical estimators (2.10) dep end on the
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,bridge  winner of t matmatically torem t hand are winner proof substituti   usi remark t royal economic society
paper_qf_21.pdf,6,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","6 A. Saichev, D. Sornette, V. Filimonov, F. Corsi
standard deviation σonly through the parameter γdeﬁned in (2.6), which we assume in
the following derivations to be known.
3. MOST EFFICIENT HOMOGENEOUS BRIDGE ESTIMATORS
3.1. Diagrams of homogeneous bridge estimators
It results immediately from expressions (2.11), (2.10) that the hom ogeneous estimator
given by (2.7) is unbiased, if the expected value of the correspondin g canonical estimator
given by (2.10) is equal to unity:
E[ˆeλ] = E[hλ(H,L,C)] = 1. (3.1)
Deﬁnition 3.1 The homogeneous volatility bridge estimator of order λgiven by (2.7)
is called the most eﬃcient one, for a given value γ0of the parameter γand for a ﬁxed
value of the parameter κ, if, forγ=γ0and ﬁxed values of κandλ, the equality (3.1)
holds while the variance of the corresponding canonical estimator a chieves the minimal
value among the variances of all canonical estimators of given orde rλand for the same
parameters γ=γ0andκ.
In this section, we provide the explicit expressions of the most eﬃcie nt homogeneous
volatility estimators. For this, it is convenient to use a change of var iables from the ran-
dom variables{H,L,C}to their corresponding spherical (geographic) coordinate vari-
ables{R,Θ,Φ}:
H=RcosΘcosΦ, L=RcosΘsinΦ, C=RsinΘ. (3.2)
Inversely, we have
R=/radicalbig
H2+L2+C2,Θ = arctan/parenleftBigg
C/radicalbig
H2+L2/parenrightBigg
,Φ = arctan/parenleftbiggL
H/parenrightbigg
.(3.3)
Substituting (3.2) into (2.10) and taking into account the homogene ity of the function
hλ, we obtain
ˆeλ=Rλψλ(Θ,Φ), (3.4)
where
ψλ(θ,φ) =hλ(cosθcosφ,cosθsinφ,sinθ). (3.5)
Deﬁnition 3.2 The function ψλ(θ,φ) deﬁned by expression (3.5) is called the diagram
of the canonical estimator of order λ.
Remark 3.1 The spherical coordinate system is intrinsic to homogeneous estima tors,
allowing us to split them into a known power function Rand an arbitrary function
of the variables Θ and Φ (see Eq. (3.4)). The spherical coordinate s ystem reduces the
search of eﬃcient OHLC estimators from three-dimensional funct ions to the appropriate
two-dimensional function ψλ(θ,φ).
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,ice corvee  limo nov corse diagrams it  t ifor co co rs iinversely b b substituti  t remark t rand eq t royal economic society
paper_qf_21.pdf,7,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","Bridge volatility estimators 7
3.2. Domain of possible {Θ,Φ}values
Below, we will need the domain of existence for the values of the rand om variables
{R,Θ,Φ}deﬁned by (3.3). First, it is obvious that R∈(0,∞). The domainSof the pos-
sible values of the two other random variables {Θ,Φ}depends on the interplay between
the random high Hand lowLof the incomplete bridge given by (2.4), and the close
valueCof the Wiener process with drift deﬁned by (2.5). It will be clear below thatS
depends on the parameter κ. Thus, we denote it by Sκ. We use the same notation Sκfor
the domain of the arguments {θ,φ}of the diagram ψλ(θ,φ) deﬁned by (3.5): {θ,φ}∈Sκ.
SinceH/greaterorequalslant0 andL/lessorequalslant0, in view of (3.2), we have
tanΦ =L
H∈(−∞,0]⇒ −π
2/lessorequalslantΦ<0.
In turn, as it seen from (2.4) and (2.5), the values {H,L,C}satisfy to the inequalities
L/lessorequalslant(1−κ)C/lessorequalslantHor, using (3.2),
sinΦ/lessorequalslant(1−κ)tanΘ/lessorequalslantcosΦ⇒arctan/parenleftbiggsinΦ
1−κ/parenrightbigg
/lessorequalslantΘ/lessorequalslantarctan/parenleftbiggcosΦ
1−κ/parenrightbigg
.
Thus
Sκ=/braceleftbigg
arctan/parenleftbiggsinΦ
1−κ/parenrightbigg
/lessorequalslantΘ/lessorequalslantarctan/parenleftbiggcosΦ
1−κ/parenrightbigg
,−π
2/lessorequalslantΦ<0/bracerightbigg
.(3.6)
3.3. Most eﬃcient OHLC homogeneous bridge estimators
Letusdenotethejointprobabilitydensityfunction(pdf) ofthera ndomvariables{H,L,C}
byQ(h,ℓ,c;κ,γ). Then, the expected value of the canonical estimator deﬁned by (3.4)
is equal to
E[ˆeλ|κ,γ] =Mλ(κ,γ) :=/integraldisplay/integraldisplay
Sκψλ(θ,φ)gλ(θ,φ;κ,γ)cosθdθdφ, (3.7)
where
gλ(θ,φ;κ,γ) =/integraldisplay∞
0ρλ+2Q(ρcosθcosφ,ρcosθsinφ,ρsinθ;κ,γ)dρ. (3.8)
Accordingly, at γ=γ0and givenκ, one can represent the diagram of any unbiased,
homogeneous estimator in the form
ψλ(θ,φ;κ,γ0) =G(θ,φ)/integraltext/integraltext
SκG(θ,φ)gλ(θ,φ;κ,γ0)cosθdθdφ, (3.9)
whereG(θ,φ) is an arbitrary function. The following theorem gives the expressio n for
the diagram (3.9) corresponding to the most eﬃcient (for any given κandγ=γ0)
homogeneous estimator of order λ.
Theorem 3.1 The diagram of the most eﬃcient (for a given κandγ=γ0) homogeneous
bridge estimator of order λis equal to
ψme,λ(θ,φ;κ,γ0) =Gλ(θ,φ;κ,γ0)
Eλ(κ,γ0),{θ,φ}∈Sκ, (3.10)
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,bridge dom articial intellence below rst t of hand of of winner it   since ihor  most  us note t joint probabity nsity funtaccordy t torem t royal economic society
paper_qf_21.pdf,8,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","8 A. Saichev, D. Sornette, V. Filimonov, F. Corsi
where
Gλ(θ,φ;κ,γ) =gλ(θ,φ;κ,γ)
g2λ(θ,φ;κ,γ),Eλ(κ,γ) =/integraldisplay/integraldisplay
Sκg2
λ(θ,φ;κ,γ)
g2λ(θ,φ;κ,γ)cosθdθdφ. (3.11)
The proof is given in Appendix A.1.
4. STATISTICAL DESCRIPTION OF INCOMPLETE BRIDGES
4.1. Identical in law Wiener process
In order to get the most eﬃcient homogeneous OHLC bridge estimat or, we need the pdf
Q(h,ℓ,c;κ,γ) of the high and low of the incomplete bridge Y(t,κ,γ) deﬁned by (2.4)
and the close value Cof the underlying process X(t,γ) deﬁned by (2.5). Before giving
the explicit solution, it is useful to discuss their general statistical properties.
Theorem 4.1 The incomplete bridge Y(t,κ,γ)given by (2.4)is identical in law to the
diﬀusion process
Y(t,κ,γ) :=γ(1−κ)t+W(t,κ), (4.1)
where
W(t,κ) := (1−t+(1−κ)2t)W/parenleftbiggt
1−t+(1−κ)2t/parenrightbigg
. (4.2)
Proof.After substitution (2.5) into (2.4), we obtain
Y(t,κ,γ) =γ(1−κ)t+Ω(t,κ), (4.3)
where
Ω(t,κ) :=W(t)−κtW(1).
One can easily verify that Ω( t,κ) is a Gaussian process with zero mean and covariance
given by
E[Ω(t1,κ)Ω(t2,κ)] = (t1∧t2)−[1−(1−κ)2]t1t2,0/lessorequalslantt1,t2/lessorequalslant1.(4.4)
Direct calculations show that the Gaussian process W(t,κ) deﬁned by (4.2) is also char-
acterized by a zero mean and the same covariance (4.4). This implies t hat the incomplete
bridgeY(t,κ,γ)givenby (4.3)isidenticalin lawtothe diﬀusionprocess Y(t,κ,γ) deﬁned
in (4.1). /squaresolid
4.2. Change of time
Henceforth,fortheanalysisofthestatisticalpropertiesofthe incomplete bridge Y(t,κ,γ)
deﬁned by (2.4), we will use the equivalence in law stated in theorem 4.1 , which allows
us to to replace the incomplete bridge by the diﬀusion process Y(t,κ,γ) deﬁned by (4.1).
As will be clear below, it is convenient to explore the extremal proper ties of the diﬀusion
processY(t,κ,γ) using the change of time
τ=τ(t,κ) :=(1−κ)2t
1−t+(1−κ)2t.
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,ice corvee  limo nov corse t  intical winner iof before torem t proof after one dire  e nceforth as royal economic society
paper_qf_21.pdf,9,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","Bridge volatility estimators 9
Inversely,
t=t(τ,κ) :=τ
τ+(1−κ)2(1−τ).
The function τ(t,κ) maps the interval t∈(0,1) onto the same interval τ∈(0,1).
Let us introduce the auxiliary stochastic process
Z(τ,κ,γ) :=Y(t(τ,κ),κ,γ). (4.5)
Usingrelations(4.1),(4.2),andself-similarpropertiesofWienerpro cess,rewriteZ(τ,κ,γ)
in the form
Z(τ,κ,γ) =1−κ
τ+(1−κ)2(1−τ)[γτ+W(τ)]. (4.6)
Below we assume, for deﬁniteness, κ<1.
It follows from the construction (4.5) of the stochastic process Z(τ,κ,γ) and from the
equality (4.6) that the following inequalities are equivalent
L/lessorequalslantY(t,κ,γ)/lessorequalslantH,⇔a+ατ/lessorequalslantW(τ)/lessorequalslantb+βτ, t,τ∈(0,1),(4.7)
where
a= (1−κ)L, b= (1−κ)H, α=1−(1−κ)2
1−κL−γ, β=1−(1−κ)2
1−κH−γ.(4.8)
Additionally, the close value C=X(1,γ) of the stochastic process X(t,γ) (2.5) is
tied to the close value of the incomplete bridge Y(t,κ,γ) given by (2.4) by the equality
Y(1,κ,γ) = (1−κ)C. In turn, it follows from the identity in law of the stochastic
processesY(t,κ,γ) andY(t,κ,γ) and from relations (4.5), (4.6) that one may replace
Y(1,κ,γ) by
Z(1,κ,γ) = (1−κ)[γ+W(1)].
Thus, one obtains
W(1) =C−γ. (4.9)
4.3. Diﬀusion equation
Let us deﬁne the probability
f(h,ℓ,c;κ,γ)dc:= Pr{C∈(c,c+dc)∩ℓ/lessorequalslantY(t,κ,γ)/lessorequalslanth;t∈(0,1)}.
Then, the joint pdf of the high and low values {H,L}(2.9) of the incomplete bridge
Y(t,κ,γ), and of the close value Cof the original process X(t,γ) (2.5), is equal to
Q(h,ℓ,c;κ,γ) =−∂2f(h,ℓ,c;κ,γ)
∂h∂ℓ, (4.10)
h>h−, ℓ<ℓ +,ℓ
1−κ/lessorequalslantc/lessorequalslanth
1−κ, h−= 0∨(1−κ)c, ℓ+= 0∧(1−κ)c.
Fromtherelationsoftheprevioussubsection4.2,onecanexpress thefunction f(h,ℓ,c;κ,γ)
via the auxiliary function ϕ(ω;τ)
ϕ(ω;τ)dω:= Pr{W(τ)∈(ω,ω+dω)∩a+ατ′/lessorequalslantW(τ′)/lessorequalslantb+βτ′;τ′∈(0,τ)},
according to
f(h,ℓ,c;κ,γ) =ϕ(c−γ;1,a,b,α,β ). (4.11)
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,bridge inversely t  usi relatns winner pro below it additnally i di  pr tof from t relatns of t us subsepr royal economic society
paper_qf_21.pdf,10,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","10 A. Saichev, D. Sornette, V. Filimonov, F. Corsi
The theory of Wiener processes implies that the auxiliary function ϕ(ω;τ) is the solu-
tion of the diﬀusion equation
∂ϕ
∂τ=1
2∂2ϕ
∂ω2, (4.12)
with initial condition
ϕ(ω;τ= 0) =δ(ω) (4.13)
and absorbing boundary conditions
ϕ(ω=a+ατ;τ) = 0, ϕ(ω=b+βτ;τ) = 0, τ >0, (4.14)
which account for the inequalities (4.7).
Below, we solve this initial-boundary problem (4.12), (4.13), (4.14) an d determine the
joint pdf of the high and low values of the incomplete bridge Y(t,κ,γ) and of the close
value of the Wiener process X(t,γ) with drift, using the following relation that derives
from (4.10) and (4.11):
Q(h,ℓ,c;κ,γ) =−∂2ϕ(c−γ;1,a,b,α,β )
∂h∂ℓ. (4.15)
4.4. Useful properties of the solutions of diﬀusion equatio ns
Before solving explicitly the initial-boundary problem (4.12), (4.13), ( 4.14), it is useful
to present some general properties of its solutions. Firstly, if ϕ(ω;τ) is some solution
of diﬀusion equation (4.12), then Aϕ(ω+a;τ), whereaandAare arbitrary constants,
is also a solution. Such relation tying together diﬀerent solutions of t he same diﬀusion
equation can be written as
ϕ(ω;τ)←→Aϕ(ω+a;τ). (4.16)
In order to solve the initial-boundary problem (4.12), (4.13), (4.14) , we will need two
lemmas.
Lemma 4.1 Ifϕ(ω,τ)of the form
ϕ(ω;τ) =1√
2πτ/integraldisplay∞
−∞ϕ(y)exp/parenleftbigg
−(ω−y)2
2τ/parenrightbigg
dy (4.17)
is a solution of the diﬀusion equation (4.12), satisfying the initial condition
ϕ(ω;t) =ϕ(ω),
whereϕ(ω)is such that ϕ(ω,τ)is a continuous function of ωfor anyτ >0, then it
generates a family of continuous solutions via the transfor mation
ϕ(ω;τ)←→Aϕ(2ατ−ω;τ)e2α(ατ−ω), (4.18)
whereAandαare arbitrary constants.
Proof.Let us write the function on the right of the relation (4.18) in explicit f orm:
ϕ(2ατ−ω;τ)e2α(ατ−ω)=1√
2πτ/integraldisplay∞
−∞ϕ(y)exp/parenleftbigg
−(2ατ−ω−y)2
2τ/parenrightbigg
dy e2α(ατ−ω).
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,ice corvee  limo nov corse t winner below winner useful before rstly are suilea  and proof  royal economic society
paper_qf_21.pdf,11,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","Bridge volatility estimators 11
Since
−(2ατ−ω−y)2
2τ+2α(ατ−ω) =−(ω+y)2
2τ+2αy,
the right-hand side of relation (4.18) is a continuous solution of the d iﬀusion equation
(4.12), satisfying the initial condition
˜ϕ(ω) =ϕ(−ω)e2αω,
analogously to (4.17). /squaresolid
The second lemma needed to ﬁnd the solution of the initial-boundary p roblem (4.12),
(4.13), (4.14) can be stated as follows.
Lemma 4.2 Consider the function ϕ(ω)which veriﬁes to symmetry relation
ϕ(ω) =−ϕ(2a−ω)e2α(a−ω). (4.19)
Then, the solution ϕ(ω;τ)of the diﬀusion equation (4.17), which is continuous with
respect toωand with initial condition equal to ϕ(ω), is vanishing on the line ω=a+ατ:
ϕ(a+ατ;τ) = 0, τ > 0.
Proof.Consider the function
˜ϕ(ω;τ) =ϕ(2ατ+2a−ω;τ)e2α(ατ+a−ω), (4.20)
whereϕ(ω;τ) is given by expression (4.17) and ϕ(ω) obeys to symmetry relation (4.19).
It follows from (4.16), (4.18) and from the conditions of lemma 4.1, th at ˜ϕ(ω;τ) satisﬁes
the diﬀusion equation (4.12) and is, for τ >0, a continuous function of the argument
ω. Expressions (4.20) and (4.19) ensure that the solution ˜ ϕ(ω;τ) satisﬁes the initial
condition
˜ϕ(ω;τ= 0) =ϕ(2a−ω)e2α(a−ω)=−ϕ(ω).
This means in turn that
˜ϕ(ω;τ) =−ϕ(ω;τ),
or in explicit form
ϕ(ω;τ) =−ϕ(2ατ+2a−ω;τ)e2α(ατ+a−ω).
In particular
ϕ(a+ατ;τ) =−ϕ(a+ατ;τ)⇒ϕ(a+ατ;τ) = 0, τ >0. /squaresolid
4.5. Solution of the initial-boundary problem
The solution of the initial-boundary problem (4.12), (4.13), (4.14) is o btained below by
using the reﬂection method and the ﬁnal result is stated in the follow ing theorem.
Theorem 4.2 The solution of the diﬀusion equation (4.12),satisfying the initial-boundary
conditions (4.13),(4.14), is given by
ϕ(ω;τ) =∞/summationdisplay
m=−∞e2(α−β)(b−a)m2+2(αb−βa)m×(4.21)
/bracketleftBig
g(ω+2m(b−a);τ)−e2a(2(β−α)m−α)g(ω+2m(b−a)−2a;τ)/bracketrightBig
,
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,bridge since t lea consir tproof consir it essns  isolutt torem t b b royal economic society
paper_qf_21.pdf,12,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","12 A. Saichev, D. Sornette, V. Filimonov, F. Corsi
where
g(ω;τ) =1√
2πτexp/parenleftbigg
−ω2
2τ/parenrightbigg
.
The proof is given in Appendix A.2.
Substituting (4.21) with (4.8) into (4.11), we obtain
f(h,ℓ,c;κ,γ) =g(c−γ)∞/summationdisplay
m=−∞e−2(h−ℓ)2m2−2m(h−ℓ)(1−κ)c/bracketleftBig
1−e4(h−ℓ)ℓm−2ℓ(ℓ−(1−κ)c)/bracketrightBig
,
(4.22)
where
g(c) =1√
2πexp/parenleftbigg
−c2
2/parenrightbigg
.
4.6. Joint pdf of high, low and close values
Using relations (4.11), (4.15) and (4.22), we obtain the sought joint pdfQ(h,ℓ,c;κ,γ)
of the high and low values {H,L}deﬁned by (2.9) of the incomplete bridge Y(t,κ,γ)
deﬁned by (2.4), together with the close value C=X(1,γ) of the Wiener process X(t,γ)
with drift given by (2.5). Namely,
Q(h,ℓ,c;κ,γ) =g(c−γ)R(h,ℓ;κ|c), (4.23)
where
R(h,ℓ;κ|c) =∞/summationdisplay
m=−∞m/bracketleftbig
mD(m(h−ℓ),(1−κ)c)+(1−m)D(m(h−ℓ)+ℓ,(1−κ)c)/bracketrightbig
(4.24)
and
D(h,c) = 4[(c−2h)2−1]e2h(c−h). (4.25)
Obviously,R(h,ℓ;κ|c) is the conditional pdf of the high and low values {H,L}, under
the condition that the close value Cis equal to a given c. For anyκ, the conditional
pdfR(h,ℓ;κ|c) does not depend on the normalized drift parameter γ. Furthermore, it
satisﬁes the normalizing condition
/integraldisplay∞
h−dh/integraldisplayℓ+
−∞dℓR(h,ℓ;κ|c) = 1.
Taking the limit κ→1 corresponds to the complete bridge, which is an important case
for our analysis below. Let us thus deﬁne the joint pdf limit
Q(h,ℓ,c;γ) := lim
κ→1Q(h,ℓ,c;κ,γ).
Expressions (2.5), (4.24) and (4.25) show that it is equal to
Q(h,ℓ,c;γ) =g(c−γ)R(h,ℓ),−∞<c<∞, h>0, ℓ< 0,(4.26)
where
R(h,ℓ) =∞/summationdisplay
m=−∞m[mD(m(h−ℓ))+(1−m)D(m(h−ℓ)+ℓ)] (4.27)
and
D(h) = 4(4h2−1)e−2h2.
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,ice corvee  limo nov corse t  substituti b b joint usi winner namely obvusly is for furtr taki  essns royal economic society
paper_qf_21.pdf,13,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","Bridge volatility estimators 13
Expression (4.26) has a clear probabilistic interpretation. It means that the high and
low values{H,L}of the complete bridge Y(t,1,γ) are statistically independent from the
close value C=X(1,γ) of the original Wiener process with drift. Accordingly, R(h,ℓ)
given by (4.27) reduces to the unconditional joint pdf of the high an d low values of the
complete bridge.
4.7. Diagrams of the most eﬃcient homogeneous OHLC bridge es timators
Knowing the joint pdf of the random variables {H,L,C}, one can calculate the auxiliary
functionsgλ(θ,φ;κ,γ) (3.8) needed in the deﬁnition of the diagrams (3.10), (3.11) of the
most eﬃcient homogeneous OHLC volatility bridge estimators. This allo ws us to derive
a number of properties of the functions gλ(θ,φ;κ,γ). It follows from (3.8) and (4.23),
(4.24), that
gλ(θ,φ;κ,γ) = (4.28)
1√
2πe−γ2/2∞/summationdisplay
m=−∞m/bracketleftbig
mIλ(m(˜h−˜l),˜c;κ,γ)+(1−m)Iλ(m(˜h−˜l)+˜l,˜c;κ,γ)/bracketrightbig
,
where
Iλ(h,c;κ,γ) =/integraldisplay∞
0ρ2+λexp/parenleftbigg
γcρ−c2
2ρ2/parenrightbigg
D(hρ,(1−κ)cρ)dρ,
and
˜h= cosθcosφ,˜l= cosθsinφ,˜c= sinθ.
All calculations done, the explicit expression of Iλ(h,c;κ,γ) reads
Iλ(h,c;κ,γ)=/parenleftbigg2
a/parenrightbigg3+λ
2
×
/bracketleftbigg
b√
2aΓ/parenleftbigg5+λ
2/parenrightbigg
M/parenleftbigg5+λ
2,1
2,d2
2a/parenrightbigg
−a/radicalbigga
2Γ/parenleftbigg3+λ
2/parenrightbigg
M/parenleftbigg3+λ
2,1
2,d2
2a/parenrightbigg
+2dbΓ/parenleftbigg
3+λ
2/parenrightbigg
M/parenleftbigg
3+λ
2,3
2,d2
2a/parenrightbigg
−daΓ/parenleftbigg
2+λ
2/parenrightbigg
M/parenleftbigg
2+λ
2,3
2,d2
2a/parenrightbigg/bracketrightbigg
.
Here,
M(a,b,z) :=Γ(b)
Γ(a)Γ(b−a)/integraldisplay1
0du ezuua−1(1−u)b−a−1,Re{b}>Re{a}>0
is the Kummer function (see Abramowitz M., and A. Stegun. (1964)) . We have used the
following notations
a= 4h(h−(1−κ)c)+c2, b= (2h−(1−κ)c)2, d=γc.
In the particular case γ= 0, we obtain
Iλ(h,c;κ) :=Iλ(h,c;κ,γ= 0) = (4.29)
25+λ
2Γ/parenleftbigg3+λ
2/parenrightbigg(3+λ)[2h−(1−κ)c]2−(2h−c)2−4κch
[(2h−c)2+4hκc]5+λ
2.
Figure 2 shows a 3D plot of the diagram obtained from (3.10), (3.11), (4.28), (4.29) of
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,bridge essit winner accordy diagrams ki  it all re re re suer  with ste gu i royal economic society
paper_qf_21.pdf,14,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","14 A. Saichev, D. Sornette, V. Filimonov, F. Corsi
the most eﬃcient variance bridge estimator, for κ= 0.95 andγ= 0. On the plane ( θ,φ)
depicted boundary of domain Sκ(3.6).
−1.5−1−0.500.511.5
−1.5−1−0.5000.511.5
θ φ
Figure 2.Diagram of the most eﬃcient variance estimator, for κ= 0.95 andγ= 0
5. COMPARISON OF THE MOST EFFICIENT BRIDGE ESTIMATORS WITH
THE G&K AND PARK ESTIMATORS
5.1. Expectation and variance of arbitrary canonical bridg e estimators
In this section, we compare the eﬃciency of the most eﬃcient homog eneous bridge es-
timators derived in previous sections with that of the G&K and PARK e stimators. We
thus give the formulas for the expected value and the variance of a rbitrary canonical
homogeneous OHLC bridge estimators deﬁned by (3.4). First, their expected values are
Mλ(κ,γ) given by (3.7).
In general, homogeneous bridge estimators are biased. Thus, one needs a normaliza-
tion procedure for a practical comparison. We will normalize the hom ogeneous bridge
estimators by the one obtained for a zero drift ( γ= 0). Thus, for each estimator (3.4),
we consider its normalized version
˜eλ=Rλψλ(Θ,Φ)
Mλ(κ),
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,ice corvee  limo nov corse o diagram eeati rst i   royal economic society
paper_qf_21.pdf,15,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","Bridge volatility estimators 15
whereMλ(κ) =M(κ,γ= 0). The expected value of the normalized estimator is
E[˜eλ|κ,γ] =Mλ(κ,γ)
Mλ(κ).
The variance of the normalized (at γ= 0) estimator is
Var[˜eλ|κ,γ] =Nλ(κ,γ)−M2
λ(κ,γ)
M2
λ(κ),
where
Nλ(κ,γ) =/integraldisplay/integraldisplay
Sκψ2
λ(θ,φ)g2λ(θ,φ;κ,γ)cosθdθdφ.
5.2. Generalized G&K bridge estimator
We recall that the G&K canonical variance estimator is given by
ˆeGK=k1(H−L)2−k2(C(H+L)−2HL)−k3C2, (5.1)
k1=0.511, k 2= 0.019, k 3= 0.383.
The random variables {H,L,C}are the high, low and close values of the Wiener process
X(t,γ) with drift deﬁned by (2.5). In order to compare the eﬃciencies of t he G&K
estimator and of most eﬃcient bridge estimators, we modify the G&K estimator (5.1)
by replacing the high, low and close values of the Wiener process X(t,γ) with drift by
the high, low and close values of the incomplete bridge Y(t,κ,γ) deﬁned by (2.4). This
yields
ˆeGK(κ) =k1(H−L)2−k2((1−κ)C(H+L)−2HL)−k3(1−κ)2C2.(5.2)
The estimator (5.2) can be expressed in a form analogous to (3.4),
ˆeGK=R2ψGK(Θ,Φ,κ), (5.3)
with
ψGK(θ,φ,κ) =k1cos2θ(cosφ−sinφ)2
+k2/bracketleftbigg
cos2θsin2φ−1−κ
2sin2θ(cosφ+sinφ)/bracketrightbigg
−k3(1−κ)2sin2θ.
To compare the eﬃciencies of the G&K estimator and of the most eﬃc ient bridge
estimators of arbitrary order λ, let us introduce the G&K estimator of order λ:
˜eGK,λ=Rλ
MGK,λ(κ)ψλ/2
GK(Θ,Φ,κ), (5.4)
whereMGK,λ(κ) is given by the following expression
MGK,λ(κ) =/integraldisplay/integraldisplay
Sκψλ/2
GK(θ,φ,κ)gλ(θ,φ;κ)cosθdθdφ.
Forκ= 0 andλ= 2, the estimator (5.4) reduces to the original G&K estimator deﬁn ed
in (5.1).
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,bridge t t var genlized  t winner iwinner  t to for royal economic society
paper_qf_21.pdf,16,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","16 A. Saichev, D. Sornette, V. Filimonov, F. Corsi
5.3. PARK normalized estimator
The canonical PARK variance estimator is given by
˜sP=(H−L)2
4ln2. (5.5)
We generalize it by the corresponding normalized PARK estimator of o rderλ,
˜eP,λ=Rλ
MP,λ(κ)ψλ/2
P(Θ,Φ), (5.6)
where
ψP(θ,φ) =cos2θ(1−sin2φ)
4ln2,MP,λ(κ) =/integraldisplay/integraldisplay
Sκψλ/2
P(θ,φ)gλ(θ,φ;κ)cosθdθdφ.
Remark 5.1 Below, we compare the eﬃciencies of the G&K, of the PARK and of the
mosteﬃcientestimators,anddonotdiscusstheeﬃciencyoftheRo ger-Satchellestimator.
Indeed, it follows from our preliminary calculations for κ≃1 that the Rogers-Satchell
bridge estimator is signiﬁcantly less eﬃcient than even the PARK estim ator.
5.4. Comparison of variance estimators
Figure 3 shows the dependence as a function of the bridge paramet erκof the expected
values of the G&K (5.2) and PARK (5.5) variance estimators, in the ca se of zero drift
(γ= 0). One can observe that, for κ/ne}ationslash= 0, the G&K and PARK variance estimators are
biased, so it is convenient to compare their normalized versions (5.4) and (5.6).
0 0.2 0.4 0.6 0.8 10.550.60.650.70.750.80.850.90.951
κExpectationsG&K
PARK
Figure 3.Dependence of the expected values of the G&K (5.2) and PARK (5.5) canonical
bridge estimators as a function of the parameter κ, in the zero drift ( γ= 0) case
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,ice corvee  limo nov corse t  remark below ro matcs estimator ined ro matcs ariso one eeatns  pennce royal economic society
paper_qf_21.pdf,17,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","Bridge volatility estimators 17
0  0.2 0.40,20,250,30,35
κVariancesPARK
G&K
most eﬃcient
Figure 4.Variancesof the most eﬃcient, normalized G&K (5.4) and PARK (5.6) v ariance
bridge estimators, as functions of the parameter κ, in the case of zero drift ( γ= 0)
Figure 4 plots the numerically calculated dependencies as a function o fκof the vari-
ances of the most eﬃcient canonical variance bridge estimator, wit h diagram (3.10),
(3.11) (λ= 2), and of the variances of the G&K and PARK canonical variance e stima-
tors (5.4), (5.6). For κ= 0, i.e. in the case of “standard” OHLC estimators, the variances
of the most eﬃcient and of the G&K estimators are rather close to e ach other, while the
variance of the PARK estimator is much larger than the former ones :
Var[ˆeme,2|κ= 0] = 0.2584,Var[˜eGK,2|κ= 0] = 0.2693,Var[˜eP,2|κ= 0] = 0.4073.
In contrast, in the case of an almost complete bridge κ∈(0.9,1), the variance of the
most eﬃcient variance estimator is signiﬁcantly smaller than the varia nces of the G&K
and PARK estimators:
Var[ˆeme,2|κ= 1] = 0.1794,Var[˜eGK,2|κ= 1]≃Var[˜eP,2|κ= 1]≃0.2.
Notice that the eﬃciencies of the G&K and PARK estimators almost co incide forκ≃1.
This is due to the fact that, for κ= 1, the G&K variance bridge estimator (5.2) becomes
close to the PARK estimator:
˜eP,2(κ= 1)≃˜eGK,2(κ= 1)∼k1(H−L)2+2k2HL, k 1= 0.511, k2= 0.0019.
Simulating 200 realizations of a Wiener process and recording the ass ociated OHLC,
ﬁgure5showsthe 200correspondingG&Kestimator(5.1), PARKes timator(5.6)( λ= 2)
and most eﬃcient canonical variance bridge estimator, for κ= 0.99 andγ= 0. It
is visually apparent that the most eﬃcient variance bridge estimator exhibits smaller
ﬂuctuations and is more eﬃcient than the PARK and G&K bridge varian ce estimators.
Remark 5.2 The fact that the OHLC bridge estimators, with κ≃1, are signiﬁcantly
more eﬃcient than the “standard” OHLC estimators, correspond ing toκ= 0, can be
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,bridge variance  variance of  for var var var ivar var var notice  simulati winner estimator kit remark t royal economic society
paper_qf_21.pdf,18,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","18 A. Saichev, D. Sornette, V. Filimonov, F. Corsi
0 50 100 150 2001234
0 50 100 150 2001234
0 50 100 150 20001234
Figure 5.Top to bottom: 200 samples of the G&K estimator (5.1), PARK estima tor (5.6)
(λ= 2) and most eﬃcient canonical variance bridge estimator, for 200 realizations of a
Wiener process with zero drift ( γ= 0) and for κ= 0.99
intuitively explained as follows. It is well-known that the high and low valu es of a Wiener
process are most probably found in the neighborhood of the edges of the observation
interval. In contrast, by construction of the bridge, its high and lo w values are in general
distant from the edges, as illustrated in ﬁgure 1. As a result, the hig h and low of a bridge
incorporate signiﬁcantly more information about the behavior of th e original stochastic
process than its own high and low values.
Remark 5.3 It is noteworthy that the most eﬃcient estimator at γ= 0 remains more
eﬃcient than the G&K and PARK estimators as long as γremains less than 0 .8 (for
κ= 0.95) and similar values for other κ’s. These condition are not restrictive since
relevant values of γare quite small. Indeed, consider a typical stock with yearly volatility
σ= 0.2 and mean return µ= 0.1. Then, the value of γfor an estimator calculated at
the daily scale T≃0.004 year is γ= (µ/σ)√
T≃0.032. For estimators at intra-day
high-frequencies, for instance for T= 5 minutes = 0 .00004 year, we have γ≃0.0032.
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,ice corvee  limo nov corse  twinner it winner ias remark it tse ined tfor royal economic society
paper_qf_21.pdf,19,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","Bridge volatility estimators 19
0  0.2 0.40.040.0450.050.0550.060.0650.070.0750.08
PARK
G&K
most eﬃcient
Figure 6.Variances of the most eﬃcient canonical volatility bridge estimator, G&K (5.4)
and PARK (5.6) volatility estimators ( λ= 1), as functions of κ(forγ= 0)
5.5. Comparison of volatility estimators
Figure 6 shows the dependencies as a function of κof the variances of the most eﬃcient
canonical bridge volatility ( λ= 1) estimator, and the variances of the corresponding
G&K and PARK volatility estimators (5.4), (5.6). In the case of almost complete bridges
κ∈(0.9,1), the variance of the most eﬃcient volatility estimator is signiﬁcant ly smaller
than the variances of the analogous G&K and PARK estimators:
Var[ˆeme,1|κ= 1] = 0.0428,Var[˜eGK,1|κ= 1] = 0.0473,Var[˜eP,1|κ= 1] = 0.0472.
Simulating 200 realizations of a Wiener process and recording the ass ociated OHLC,
ﬁgure 7 shows the 200 corresponding G&K volatility ( λ= 1) estimator (5.4) for κ= 0,
the PARK estimator (5.6) ( λ= 1) and the most eﬃcient canonical bridge volatility
estimators, for κ= 0.99 andγ= 0. It is visually apparent that the most eﬃcient
volatility bridge estimator exhibits smaller ﬂuctuations and is more eﬃc ient than the
PARK and G&K bridge variance estimators.
6. SIMULATED MOST EFFICIENT ESTIMATORS
The previous sections have derived the most eﬃcient homogeneous bridge estimators,
whose diagrams (3.10), (3.11), are deﬁned in terms of the function gλ(θ,φ;κ,γ) (3.8),
which depends in turn on the pdf Q(h,ℓ,c;κ,γ) (4.23). Notice that relation (3.8) allows
one to determine the function gλ(θ,φ;κ,γ) even when the pdf is unknown. The function
gλ(θ,φ;κ,γ) can indeed be determined by simulating M≫1 times the stochastic pro-
cessX(t) which describes the statistical properties of the log-price dynam ics, and then
estimate the function gλ(θ,φ;κ,γ) by its statistical average. This is particularly conve-
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,bridge  variance ariso ivar var var simulati winner it t notice t  royal economic society
paper_qf_21.pdf,20,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","20 A. Saichev, D. Sornette, V. Filimonov, F. Corsi
0 50 100 150 2000.511.52
0 50 100 150 2000.511.52
0 50 100 150 2000.511.52
Figure 7.Top to bottom: 200 samples of the G&K volatility estimator, for κ= 0, PARK
volatility estimator and most eﬃcient canonical bridge volatility estima tor, forκ= 0.99
nient when theoretical formulas are not available, as occurs when c onsidering stochastic
processes more complex than the Wiener process with drift.
To illustrate this possibility of simulating the diagrams associated with t he most eﬃ-
cient estimators, consider the discrete normalized random walk
X(k) =1√
Kk/summationdisplay
i=1ǫi, k= 1,...,K, X (0) = 0, (6.1)
where{ǫi}is a sequence of iid random variables with zero expectation and unit va riance.
The randomwalk(6.1)mimicsthe discrete,tick-by-tick,natureoft he log-pricestochastic
process.
In the limit K→∞, the random walk X(k) (6.1) converges (even if {ǫi}are non-
Gaussian as long as the tail of their pdf is not too heavy) to the Wiene r processW(t), so
that the joint pdf of the random variables {H,L,C}is known theoretically. In contrast,
in the case of “ﬁnite number of ticks” ( K <∞), the joint pdf is unknown. Nevertheless,
one can obtain an approximate expression for the diagram of the mo st eﬃcient estimator
by numerical simulation.
In order to construct the simulated diagram for K= 10;102and 103, we divided the
domainSκdeﬁned in (3.6) in 50 ×50 rectangle bins and, for each K, we generated
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,ice corvee  limo nov corse  twinner to kk t iwieinevertless iroyal economic society
paper_qf_21.pdf,21,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","Bridge volatility estimators 21
−1.5−1−0.500.511.5
−1.5−1−0.5000.511.5
θφ
Figure 8.Synthetic diagram of the most eﬃcient variance canonical bridge es timator for
K= 10 andκ= 1,γ= 0
Table 1.Variances of G&K and simulated most eﬃcient variance estimators. T he vari-
ances of the G&K estimators and of the simulated most eﬃcient varia nce estimators,
both forκ= 0 andκ= 1, are obtained by averaging over N= 106simulations of the
discrete random walk X(k) (6.1).
K= 10 100 1000 ∞
Var[ˆeGK,2](κ= 0) 0.5103 0.3272 0.2858 0.2693
Var[ˆeme,2](κ= 0) 0.4759 0.3130 0.2755 0.2584
Var[ˆeGK,2](κ= 1) 0.4062 0.2434 0.2125 0.1996
Var[ˆeme,2](κ= 1) 0.3373 0.2151 0.1896 0.1794
M= 108simulations of the random walk X(k) (6.1) with Gaussian summands {ǫi}.
We then calculated the function gλ(θ,φ;κ) (forγ= 0) using the approximate statistical
relation
gλ(θ,φ;κ)cosθdθdφ≃1
MM/summationdisplay
m=1Rλ
mIδ(Θm,Φm).
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,bridge  synic table variance var var var var  royal economic society
paper_qf_21.pdf,22,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","22 A. Saichev, D. Sornette, V. Filimonov, F. Corsi
0 50 100 150 2000123
0 50 100 150 2000123
0 50 100 150 2000123
Figure 9 .Top to bottom: numerical samples of the G&K estimator, for κ= 0 and
κ= 1,andthenumericalsamplesofthesimulatedmosteﬃcientcanonic albridgevariance
estimator, for κ= 1, obtained for discrete random walk X(k) (6.1) with K= 10
The set{Θm,Φm,Rm}are samples of the random variables (3.3) obtained for the m-th
simulation, I δis the indicator of the set
δ= (θ,θ+dθ)×(φ,φ+dφ),
and the summation is performed over Msimulations of the random walk (6.1). The
histograms of the function gλ(θ,φ;κ) thus obtained is then substituted into the diagram
function (3.10), (3.11) to produce its 2D linear interpolation.
Figure 8 presents the 3D plot of the synthetic diagram of the most e ﬃcient variance
estimator, obtained by statistical averaging for K= 10 andκ= 1,γ= 0. Notwithstand-
ing the visible ﬂuctuations, table 1 shows that this level of numerical approximation is
suﬃcient to obtain signiﬁcantly better eﬃcient estimators than for instance, G&K esti-
mator. Table 1 gives the variancesofthe canonical variancebridge estimators.The values
shown in table 1 have been obtained by statistical averaging over N= 106simulations
of the random walk (6.1).
Simulating 200 realizations of a Wiener process (6.1) for K= 10, ﬁgure 9 shows the
simulated most eﬃcient canonical variance bridge estimator, for κ= 1, and the samples
of the G&K estimators, for κ= 0 andκ= 1. It is clear that the simulated most eﬃcient
bridge variance estimator is signiﬁcantly more eﬃcient than the G&K e stimator.
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,ice corvee  limo nov corse  tt rm simulatns t  not withstand table t simulati winner it royal economic society
paper_qf_21.pdf,23,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","Bridge volatility estimators 23
7. CONCLUSIONS
In this paper, we have pursued the development of a comprehensiv e theory of homoge-
neous volatility estimators of arbitrary stochastic processes. Ou r focus has been to derive
OHLC (open-high-low-close) log-prices bridge volatility estimators, which can span time
intervalsextendingfromsecondstoyears.Themaintoolofourth eoryistheparsimonious
encoding of all the information contained in the OHLC in the form of ge neral “diagrams”
associated with the joint distributions ofthe high-minus-open, low- minus-openand close-
minus-open values of the original log-price process and its bridge. T he diagrams can be
tailored to yield the most eﬃcient estimators associated to any stat istical properties of
the underlying log-price stochastic process.
Previousworkshavedevelopedvarianceestimatorswhicharequad raticfunctionsofthe
OHLC. Our main contribution is to stress the remarkable fact that q uadratic estimators
are only particular cases of general homogeneous estimators. Ou r theory constructs the
tools to ﬁnd most eﬃcient homogenous estimators which, by constr uction, are always
more eﬃcient than the most eﬃcient quadratic estimators. Perhap s paradoxically, it
turns out that the search for the most eﬃcient quadratic estimat ors is more tedious than
that of the more eﬃcient homogeneous estimators. Another adva ntage of homogeneous
estimators is that they give the possibility to develop eﬃcient volatility in addition to
variance estimators, while quadratic estimators are specialized to v ariance estimators.
Our theory opens several interesting developments. First, the d etermination of the key
functionsgλ(θ,φ;γ), deﬁning the above diagrams, provides the tools to develop eﬃcien t
bridge volatility estimators for arbitrary non-Gaussian log-price pr ocesses, including the
presence of micro-structure as in tick-by-tick price series. Our m ethods should lead to the
development of eﬀective algorithms for low- and high-frequency OH LC volatility bridge
estimators, that can be applied in practice to any kind of ﬁnancial ma rkets.
ACKNOWLEDGMENTS
One of us (FC) was inspired on the subject of this paper via an early c ollaboration with
Prof. Curci.
A. APPENDIX
A.1. Proof of Theorem 3.1
For given values of the parameters κandγ=γ0, the variance of the unbiased canonical
estimator, with diagram (3.9), is equal to
Var[ˆeλ;κ,γ0] =/integraltext/integraltext
SκG2(θ,φ)g2λ(θ,φ;κ,γ0)cosθdθdφ
/parenleftBigg
/integraltext/integraltext
SκG(θ,φ)gλ(θ,φ;κ,γ0)cosθdθdφ/parenrightBigg2−1. (A.1)
Using the Schwarz inequality

/integraldisplay/integraldisplay
SκA(θ,φ)B(θ,φ)dθdφ
2
/lessorequalslant/integraldisplay/integraldisplay
SκA2(θ,φ)dθdφ/integraldisplay/integraldisplay
SκB2(θ,φ)dθdφ,
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,bridge iou t articial intellence tofourth us works he veloped variance estimator whiare quad our ou  hap anotr our rst our one prof cur ci proof torem for var b b usi schwartz royal economic society
paper_qf_21.pdf,24,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","24 A. Saichev, D. Sornette, V. Filimonov, F. Corsi
for arbitrary locally integrable real-valued functions A(θ,φ) andB(θ,φ), we take
A(θ,φ) =G(θ,φ)/radicalbig
g2λ(θ,φ;κ,γ0)cosθ,
B(θ,φ) =gλ(θ,φ;κ,γ0)/radicalBigg
cosθ
g2λ(θ,φ;κ,γ0),
and obtain

/integraldisplay/integraldisplay
SκG(θ,φ)gλ(θ,φ;κ,γ)cosθdθdφ
2
/lessorequalslant
/integraldisplay/integraldisplay
SκG2(θ,φ)g2λ(θ,φ;κ,γ0)cosθdθdφ/integraldisplay/integraldisplay
Sκg2
λ(θ,φ;κ,γ0)
g2λ(θ,φ;κ,γ0)cosθdθdφ.
It follows from (A.1) and from the above inequality that the variance of any canonical
homogeneous volatility estimator of order λsatisﬁes the inequality
Var[ˆeλ;κ,γ0]/greaterorequalslantVλ(κ,γ0), V λ(κ,γ) =1
Eλ(κ,γ)−1, (A.2)
whereEλ(κ,γ) is deﬁned by expression(3.11). It follows from (A.1), (A.2) and (3.1 1) that
the variance of the canonical volatility estimator of order λreaches its minimal value for
a givenγ=γ0andκ, ifG(θ,φ) is given by the left equality of (3.11). /squaresolid
A.2. Proof of Theorem 4.2
It is convenient to replace the initial condition (4.13) by the more gen eral one
ϕ(ω;τ= 0) =ϕ(ω), ω∈(a,b). (A.3)
At the end of proof, we obtain formula (4.21) by taking ϕ(ω) =δ(ω).
The idea of the proof consists in redeﬁning the function ϕ(ω) in (A.3) outside the
intervalω∈(a,b) in such a way that the solution of equation (4.12), supplemented by
the initial condition
ϕ(ω;τ= 0) =ϕ(ω), ω∈(−∞,∞), (A.4)
satisﬁes the absorbing boundary conditions (4.14). In other word s, it should be equal to
zero on the lines ω=a+ατ,ω=b+βτ,τ >0. Let us deﬁne the auxiliary function
ϕ0(ω) =ϕ(ω)I(a,b)(ω), ω∈(−∞,∞), (A.5)
where I E(x) is the indicator of the set E.
It follows from lemma 4.2 that the solution of the diﬀusion equation, su pplemented
by the initial condition (A.4), satisﬁes the boundary conditions (4.14 ) ifϕ(ω) obeys to
symmetry relations
ϕ(ω) =−ϕ(2a−ω)e2α(a−ω), ϕ(ω) =−ϕ(2b−ω)e2β(b−ω).(A.6)
Using the ﬁrst of these two equalities and deﬁnition (A.5) of the func tionϕ0(ω), let us
redeﬁneϕ(ω) onto the interval ω∈(2a−b,b) as follows:
ϕ(ω) =ϕ0(ω), ω∈(2a−b,b), ϕ0(ω) =ϕ0(ω)−ϕ0(2a−ω)e2α(a−ω).
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,ice corvee  limo nov corse b it var it proof torem it at t i it usi royal economic society
paper_qf_21.pdf,25,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","Bridge volatility estimators 25
Then, the equalities (A.6) provide the “quasiperiodic” relation
ϕ(ω) =ϕ(ω+2(b−a))e2(β−α)(ω+b−a)+2(αb−βa),
which yields
ϕ(ω) =∞/summationdisplay
m=−∞ϕm(ω), (A.7)
where
ϕm(ω) =ϕ0(ω+2(b−a)m)e2(β−α)(ω+m∆)m+2(αb−βa)m. (A.8)
Substituting ϕ(ω) given by (A.7) with (A.8) into (4.17), we obtain the sought solution
of the initial-boundary problem (4.12), (4.13), (4.14). In particular , usingϕ0(ω) =δ(ω),
that is
ϕ0(ω)⇒ϕ0(ω) =δ(ω)−e−2αaδ(ω−2a),
we obtain the solution (4.21). /squaresolid
REFERENCES
Abramowitz M., and A. Stegun. (1964). Handbook of Mathematical Functions , National
Bureau of Standards Applied Mathematics Series - 55.
A¨ ıt-Sahalia, Y., P.A. Mykland, and L. Zhang (2005). How often to sa mple a continuous-
time process in the presence of market microstructure noise. Rev. Fin. Stud. 18 ,
351-416.
Andersen, T. G., T. Bollershev, F. X. Diebolt and P. Labys (2003). M odeling and
Forecasting Realized Volatility. Econometrica, 71 , 529-626.
Chan, L. and D. Lien (2003). Using high, low, open, and closing prices to estimate the
eﬀects ofcashsettlement on futuresprices. International Review of Financial Analysis,
12, 35-47.
Corsi, F., G. Zumbach, U. M¨ uller, and M. Dacorogna (2001). Consis tent high-precision
volatility from high-frequency data. Economic Notes, 30 , 183-204.
Garman, M. and M. J. Klass (1980). On the Estimation of Security Pr ice Volatilities
From Historical Data. Journal of Business, 53 , 67-78.
McKenzie, D. (2006). An engine, not a camera (how ﬁnancial models shape markets) ,
MIT Press, Cambridge, MA.
Parkinson, M. (1980). The Extreme Value Method for Estimating th e Variance of the
Rate of Return. Journal of Business, 53 , 61-65.
Rogers, L. C. G., and S. E. Satchell (1991). Estimating Variance Fr om High, Low and
Closing Prices. The Annals of Applied Probability, 4 , 504-512.
Rogers, L. C. G., S. E. Satchell, and Y. Yoon (1994). Estimating the Volatility of Stock
Prices: A Comparison of Methods that use High and Low Prices. Applied Financial
Economics, 4 , 241-247.
Saichev, A., D. Sornette, V. Filimonov (2009). Most Eﬃcient Homoge neous Volatility
Estimators. ETH Zurich working paper ,http://ssrn.com/abstract=1470004 .
Yang, D., and Q. Zhang (2000). Drift-independent Volatility Estimat ion Based on High,
Low, Open and Close Prices. Journal of Business, 73 , 477-491.
Zhang, L., Mykland, P.A. and At-Sahalia, Y. (2005). A tale of two time scales: deter-
mining integrated volatility with noisy high-frequency data. J. Amer. Statist. Assoc.
100, 1394-1411.
c/circlecopyrtRoyal Economic Society 2010",2009-12-08T21:47:01Z,bridge tsubstituti i with ste guhandbook matmatical funns natnal bureau standards applied matmatics serisha lia my land  how rev stud anrsoroller s die bolt la by forecasti realized volatity econometric chalieusi internatnal review nancial analysis corse zum bada core gcons is economic notgermaass oestimatsecurity pr vat i tifrom historical data journal business mc keie a cambridge parsot extreme value method estimati variance rate urjournal business ro matcs estimati variance fr hh low osi pri annals applied probabity ro matcs yooestimati volatity stock pricarisomethods hh low pricapplied nancial economics ice corvee  limo nov most homo ge volatity estimator iya  drt volatity tim at based hh low opeose pricjournal business  my land at sha lia amer status as soc royal economic society
paper_qf_21.pdf,26,Homogeneous Volatility Bridge Estimators,"  We present a theory of homogeneous volatility bridge estimators for log-price
stochastic processes. The main tool of our theory is the parsimonious encoding
of the information contained in the open, high and low prices of incomplete
bridge, corresponding to given log-price stochastic process, and in its close
value, for a given time interval. The efficiency of the new proposed estimators
is favorably compared with that of the Garman-Klass and Parkinson estimators.
","0 0.2 0.4 0.6 0.8 10.180.20.220.240.260.280.30.32
γVariancesG&K (κ=0)
ME (κ=0.95)G&K (κ=0.95)",2009-12-08T21:47:01Z,variance
paper_qf_22.pdf,1,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","arXiv:1405.1948v2  [q-fin.MF]  4 Mar 2015Phynance
Zura Kakushadze§†1
§Quantigic/circleRSolutions LLC
1127 High Ridge Road #135, Stamford, CT 069052
†Department of Physics, University of Connecticut
1 University Place, Stamford, CT 06901
(May 6, 2014)
Dedicated to the memory of my father
Jemal Kakushadze, Ph.D. (1940-2005)
Abstract
These are the lecture notes for an advanced Ph.D. level cours e I taught in
Spring’02 at the C.N. Yang Institute for Theoretical Physic s at Stony Brook.
The course primarily focused on an introduction to stochast ic calculus and
derivative pricing with various stochastic computations r ecast in the language
of path integral, which is usedin theoretical physics, henc e“Phynance”. Ialso
includedseveral “quiz”problems(withsolutions) compris edof(pre-)interview
questions quantitative ﬁnance job candidates were sometim es asked back in
thosedays. Thecoursetoacertain extent follows anexcelle nt book“Financial
Calculus: AnIntroductiontoDerivative Pricing”byM.Baxt erandA.Rennie.
1Email: zura@quantigic.com . Emails pointing out any typos or other inadvertent errors
that slipped through the cracks are more than welcome and will be gr eatly appreciated.
2DISCLAIMER: This address is used by the corresponding author fo r no purpose other than
to indicate his professional aﬃliation as is customary in publications. I n particular, the contents
of this paper are not intended as an investment, legal, tax or any ot her such advice, and in no way
represent views of Quantigic Solutions LLC, the website www.quantig ic.comor any of their other
aﬃliates.",2014-05-07T01:13:23Z, mar ph dance  u sha quant ic solutns hh ridge road stanford partment psics  conneicut  place stanford may dicated jem al u sha ph abstra tse ph spri ya institute toical psics stony brook t ph dance also t course to cert articial intellence nancial calculus aintroduto rivative prici box ronnie em articial intellence em articial intellence ls  quant ic solutns
paper_qf_22.pdf,2,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Contents
1 Introduction: How Does “Bookie the Crookie” Make Money? 4
2 Bid, Ask and Spread 5
3 Stocks, Bonds and Free Markets 6
4 Arbitrage Pricing 11
5 Binomial Tree Model 12
5.1 Risk-neutral Measure . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
5.2 An Example: Baseball World Series . . . . . . . . . . . . . . . . . . . 14
6 Martingales 16
6.1 The Tower Law . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
6.2 Martingale Measure . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
6.3 Binomial Representation Theorem . . . . . . . . . . . . . . . . . . . . 18
6.4 Self-ﬁnancing Hedging Strategies . . . . . . . . . . . . . . . . . . . . 19
6.5 The Self-ﬁnancing Property . . . . . . . . . . . . . . . . . . . . . . . 20
7 Discrete vs.Continuous Models 21
7.1 Brownian Motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
8 Stochastic Calculus 23
8.1 Itˆ o Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
8.2 Radon-Nikodym Process . . . . . . . . . . . . . . . . . . . . . . . . . 25
8.3 Path Integral . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
8.4 Continuous Radon-Nikodym Process . . . . . . . . . . . . . . . . . . 28
8.5 Cameron-Martin-Girsanov Theorem . . . . . . . . . . . . . . . . . . . 28
9 Continuous Martingales 31
9.1 Driftlessness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
9.2 Martingale Representation Theorem . . . . . . . . . . . . . . . . . . . 34
10 Continuous Hedging 35
10.1 Change of Measure in the General One-Stock Model . . . . . . . . . . 35
10.2 Terminal Value Pricing . . . . . . . . . . . . . . . . . . . . . . . . . . 36
10.3 A Diﬀerent Formulation . . . . . . . . . . . . . . . . . . . . . . . . . 38
10.4 An Instructive Example . . . . . . . . . . . . . . . . . . . . . . . . . 39
10.5 The Heat Kernel Method . . . . . . . . . . . . . . . . . . . . . . . . . 41
11 European Options: Call, Put and Binary 42
1",2014-05-07T01:13:23Z,contents introduhow dorookie rookie make money bid ask spread stocks bonds free markets arb it rage prici binomial tree mrisk measure a baseball world serimartalt tor law martale measure binomial representattorem self edgi strategit self proty disete continuous mols brownish motstochastic calculus it calculus radody process path integral continuous radody process cameromartir nov torem continuous martaldrt less ness martale representattorem continuous edgi e measure genl one stock mterminal value prici di tainstruive  t at kernel method aoptns call put binary
paper_qf_22.pdf,3,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","12 The Black-Scholes Model 43
12.1 Call Option . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
12.2 Put Option . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
12.3 Binary Option . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
13 Hedging in the Black-Scholes Model 46
13.1 Call Option . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
13.2 Put Option . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
13.3 Binary Option . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
14 Price, Time and Volatility Dependence 49
14.1 Call Option . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
14.2 Put Option . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
14.3 Binary Option . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
14.4 American Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
15 Upper and Lower Bounds on Option Prices 53
15.1 Early Exercise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
16 Equities and Dividends 54
16.1 An Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
16.2 Periodic Dividends . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
17 Multiple Stock Models 57
17.1 The Degenerate Case . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
17.2 Arbitrage-free Complete Models . . . . . . . . . . . . . . . . . . . . . 61
18 Numeraires 63
18.1 Change of Numeraire . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
19 Foreign Exchange 66
20 The Interest Rate Market 67
20.1 The Heath-Jarrow-Morton (HJM) Model . . . . . . . . . . . . . . . . 67
20.2 Multi-factor HJM Models . . . . . . . . . . . . . . . . . . . . . . . . 70
21 Short-rate Models 71
21.1 The Ho and Lee Model . . . . . . . . . . . . . . . . . . . . . . . . . . 73
21.2 The Vasicek/Hull-White Model . . . . . . . . . . . . . . . . . . . . . 74
21.3 The Cox-Ingersoll-Ross Model . . . . . . . . . . . . . . . . . . . . . . 75
21.4 The Black-Karasinski Model . . . . . . . . . . . . . . . . . . . . . . . 76
2",2014-05-07T01:13:23Z,t  schools mcall optput optbinary optedgi  schools mcall optput optbinary optprice time volatity pennce call optput optbinary optoptns up lor nds optpric exercise entitidivinds a dic divinds multiple stock mols t gente case arb it rage e mols umer articial intellence re num  ire foreexe t interest rate market t ath narrow mortommulti mols short mols t ho  mt vas ice hull white mt cox inoll ross mt  kara i mol
paper_qf_22.pdf,4,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","22 Interest Rate Products 77
22.1 Forward Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
22.2 Multiple Payment Contracts . . . . . . . . . . . . . . . . . . . . . . . 78
22.3 Bonds with Coupons . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
22.4 Floating Rate Bonds . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
22.5 Swaps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
22.6 Bond Options . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
22.7 Bond Options in the Vasicek Model . . . . . . . . . . . . . . . . . . . 83
22.8 Options on Coupon Bonds . . . . . . . . . . . . . . . . . . . . . . . . 85
22.9 Caps and Floors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
22.10Swaptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
23 The General Multi-factor Log-Normal Model 87
23.1 The Brace-Gatarek-Musiela (BGM) Model . . . . . . . . . . . . . . . 8 8
24 Foreign Currency Interest-rate Models 90
25 Quantos 90
25.1 A Forward Quanto Contract . . . . . . . . . . . . . . . . . . . . . . . 91
26 Optimal Hedge Ratio 91
Acknowledgments 92
A Some Fun Questions 92
B Quiz 1 92
C Quiz 2 97
Bibliography 110
List of Figures
1 Figure for Problem 3 in Quiz 1 . . . . . . . . . . . . . . . . . . . . . 94
3",2014-05-07T01:13:23Z,interest rate produs forward measurmultiple payment contras bonds coupons floati rate bonds swaps bond optns bond optns vas ice moptns coupobonds caps floors swap ns t genl multi log normal mt brace gat are mus ie la mforecurrency interest mols quato forward quato contra optimal dge rat ackledgment some futns quiz quiz biblgrap t s  problem quiz
paper_qf_22.pdf,5,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","1 Introduction: How Does “Bookie the Crookie”
Make Money?
When odds are quoted in the form “ n−magainst”, it means that the event has
probability m/(n+m), and a successful bet of $ mis rewarded with $ n(plus the
stake returned).
Similarly, when the odds are quoted in the form “ n−mon”, it is the same as
“m−nagainst”.
Suppose we have two horses, with the true odds n−magainst the ﬁrst horse.
Suppose the gamblers bet total of B1on the ﬁrst horse, and B2on the other horse.
Then if the ﬁrst horse wins, the bookmaker makes a net proﬁt (this could be a gain
or a loss) of
P1=B2−n
mB1, (1)
while if the second horse wins, the bookmaker makes a net proﬁt of
P2=B1−m
nB2. (2)
The average long-term proﬁt is
/an}bracketle{tP/an}bracketri}ht=m
n+mP1+n
n+mP2= 0, (3)
so the bookmaker breaks even by quoting the true odds.
To make a long-term proﬁt, the bookmaker sells more than 100% of t he race by
quoting somewhat diﬀerent odds than the true odds. Thus, let the odds quoted for
the ﬁrst and the second horses be n1−m1against and n2−m2on, respectively.
Now the average long-term proﬁt is
/an}bracketle{tP/an}bracketri}ht=m
n+m/bracketleftbigg
B2−n1
m1B1/bracketrightbigg
+n
n+m/bracketleftbigg
B1−m2
n2B2/bracketrightbigg
=
nB1
n+m/bracketleftbigg
1−mn1
nm1/bracketrightbigg
+mB2
n+m/bracketleftbigg
1−nm2
mn2/bracketrightbigg
. (4)
Thus, the bookmaker can guarantee positive /an}bracketle{tP/an}bracketri}htby settingn1,m1andn2,m2such
that
n1
m1<n
m, (5)
m2
n2<m
n. (6)
Note that the implied probabilities then are larger than the true prob abilities:
m1
n1+m1>m
n+m, (7)
n2
n2+m2>n
n+m, (8)
so that the bookmaker is, in fact, selling more than 100% of the race . As the saying
goes, lottery is a tax on people who don’t know math.
4",2014-05-07T01:13:23Z,introduhow dorookie rookie make money wsimarly suppose suppose tt to    note as
paper_qf_22.pdf,6,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","2 Bid, Ask and Spread
Something similar to the bookmaker example discussed above occurs in ﬁnancial
markets. Let’s consider a stock XYZ. There are the buyers, and t here are the
sellers. The buyers quote their bids, the sellers quote their asks (o r oﬀers), together
with how many shares of the stock they want to buy/sell. Let Bbe the highest bid
price, and let Abe the lowest ask price. The diﬀerence S≡A−Bis called the
bid-ask spread. Typically, S >0.
IfS= 0 (this is called locked market ), then the lowest ask Ais the same as the
highest bid B, and a transaction will occur at that price P=A=B, where a seller
(or sellers) will transfer to a buyer (or buyers) their shares. The number of shares
Vsold at that price equals V= min(VBid,VAsk), whereVBidis the total number of
shares quoted by the buyers at the price PandVAskis the total number of shares
quoted by the sellers at the price P.
IfS <0 (this is called crossed market ), then the lowest ask is below the highest
bid, and a transaction will also occur, but the price Pat which it occurs will be in
the rangeA≤P≤Band it can depend on a variety of factors, e.g., the precise
algorithmemployed byagivenexchange fordetermining Pcandependonthetiming
of when various bids and asks where placed into the queue by the buy ers and sellers.
In fact, there might be more than one prices Piat which the transactions can occur
with varying numbers of shares Visold at those prices. Some buyers/sellers may
receive what is known as price improvement ,e.g., a buyer bids 100 shares of XYZ
at the price Band his order is ﬁlled(this is market lingo) at a better price P <B.
So, one way to make money in the stock market is to be a market-mak er, con-
stantly selling at the ask and buying at the bid. Assuming the spread S >0, if you
buyVshares of XYZ at the bid Band then turn around and sell them at the ask A,
your proﬁt will be V·(A−B) =V·S. You have traded 2 Vshares (bought Vshares
and soldVshares), so your proﬁt-per-share is S/2. (Typically, the spread is quoted
in cents, and the proﬁt-per-share is quoted in cents-per-share .) This is known as
makinghalf-spread . Similarly, if you go into the market and buy at the ask and sell
at the bid – this is called buying and selling at market (because you’re paying the
market prices) – then you’re incurring half-spread transaction cost on your trades,
and the market-makers are making their half-spread on your tran sactions.
Nonetheless, plenty of people incur half-spread transaction cost on their trades
because the way they make money is not by market-making but by ca pitalizing on
stock price movements that are larger than the bid-ask spread. T here istechnical
analysis, which is based on statistical analysis of market activity based on pa tterns
anddoesnotconcernitselfwiththefundamentalsofeachcompany , whichincontrast
is whatfundamental analysis does – it makes investment decisions based on the
fundamentals of the company, such as growth potential, earnings ,etc.By its very
nature, typically fundamental analysis operates on the time scales which are longer
than those of technical analysis. Whatever the method, the mone y making motto
is “Buy low, sell high!” In practice, it’s much harder to do than it sound s.
5",2014-05-07T01:13:23Z,bid ask spread somethi  tre t  be be t is typically  is t sold bid ask bid is and ask is  pat band capend ot timi ipi at vi sold some band so assumi sharband you sharsharshartypically  simarly nonetless by whatebuy in
paper_qf_22.pdf,7,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","3 Stocks, Bonds and Free Markets
Stocks and bonds as well as other ﬁnancial instruments are import ant ingredients
of free market economy. Financial markets and the economy itself are products of
humancivilization, and, therefore, arenotdirectlygovernedbyth efundamentallaws
of nature ( i.e., laws of physics). Nonetheless, it is fascinating that they are base d on
certain universal principles, and there are reasons why the ﬁnanc ial markets have
been eﬃciently integrated into the free market economy notwithst anding the fact
that the system is by no means perfect, which sometimes results in f ailures such as
stock market bubbles and crashes.
One of the most fundamental principles of the free market econom y is the in-
terplay between supply and demand. Thus, regardless of what spe ciﬁcally is being
traded, whether it is goods, commodities, stocks or other valuable instruments, buy-
ers, who create demand, drive its price up, while sellers, who are sup pliers, drive
the price down. The supply and demand then determine the price. Fo r instance, if
sellers are asking an unreasonably high price not reﬂecting current demand levels,
trades at this price are unlikely to occur in large quantities as the buy ers will not be
willing to pay more than they have to. Similarly, if the current supply lev el is low,
then a buyer bidding at an unreasonably low price cannot expect to s uccessfully
complete a trade at that price – most likely there will be other buyers bidding at
higher price levels more acceptable to the suppliers.
Stock and bond markets as any other free market generally are ex pected to op-
erate in this way – buyers drive stock prices up, while sellers drive the m down. This
simple principle does indeed work in the ﬁnancial markets, but what de termines the
supply and demand for a given ﬁnancial instrument is quite nontrivial and is often
times dictated by certain important details of how these markets ar e structured,
which set the rules of the game. The purpose of this section3is to elucidate some
aspects of ﬁnancial markets, in particular, why there exists dema nd for stocks and
bonds, that is, why investors are willing to allocate their funds in thes e ﬁnancial
instruments. Nontrivial, and perhaps even controversial, issues a rise in this regard
as there is no fundamental law of nature that would dictate that an y of these in-
struments should exist in the ﬁrst place.
Letusbeginwithbonds. Therearevarioustypesofbondswithdiﬀer entfeatures,
and we will not attempt to describe them all in detail; rather, we will fo cus on those
that most bonds have in common. A bond is an obligation where the issu er of the
bond promises to the purchaser to pay back the so-called face or p ar value of the
bondorsomeotheramountatsomelatertimecalledmaturityoftheb ond. Typically
bonds also make periodic (mostly annual or semi-annual) coupon pay ments to the
purchaser. Basically, the issuer of the bond borrows money from t he purchaser and
makes a promise that at maturity this money will bereturned to the p urchaser along
with some additional amount, some of which might be paid before matu rity, which
3This section (with minor modiﬁcations) appeared some number of yea rs ago as a standalone
article in the online magazine Kvali.com.
6",2014-05-07T01:13:23Z,stocks bonds free markets stocks nancial nonetless one  t fo simarly stock  t nontrivial  us begiwith bonds tre are varus s of bonds with di typically basically  kv ali
paper_qf_22.pdf,8,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","is essentially the interest the purchaser earns. Thus, consider a s imple example
where a bond, which matures in exactly one year, has a face value of $1,000. The
purchaser pays this amount now to acquire the bond, and the issue r promises to
pay back $1,000 at maturity (that is, in one year from the purchase date) plus $50
as a one-time coupon payment, which is also paid at maturity. The pur chaser’s
investment of $1,000, therefore, has 5% annual return or yield. N ote that if for
some reason the price to purchase such a bond went up to, say, $1 ,250, then the
corresponding yield would go down to 4% (assuming that the coupon p ayment is
ﬁxed), while if it dropped down to, say, $500, then the correspond ing yield would
go up to 10%. Thus, the higher the price the lower the yield, and vice-versa .
Bonds, being obligations, are typically relatively low risk investments. However,
they do bear some risk, in particular, credit risk – after all, the bond issuer can
sometimes default, that is, declare bankruptcy, in which case it migh t not always
be possible to receive the originally invested amount as well as some or all of the
promised coupon payments. Bonds issued by governments of stab le countries such
as U.S. Treasury bonds are virtually risk free – government debt is a very low risk
investment because it is backed by thetaxation power of thegover nment.4Indeed, if
thegovernmentdebtisnotunreasonablyhigh, thegovernment ca nexerciseitsability
toincrease taxestopaydownitsdebt. Municipal bondsareissuedb yStateandlocal
governments, typically to raise money for developing local infrastr ucture (building
roads, hospitals, etc.). State issued bonds can also be backed by the taxation power
ofaState. IntheUnitedStatesinterestearnedfromsuchbonds isexempt fromState
taxes, albeit Federal taxes must still be paid on such interest incom e. State issued
bonds, therefore, typically have lower yields than other comparab le bonds (with
the same credit risk) – this is because otherwise it would be more adva ntageous
to invest into State issued bonds than in the comparable bonds as th e former earn
interest taxed at a lower rate, so increased demand on such bonds would drive their
prices up, and, consequently, yields down, until it is no longer more a dvantageous
to invest in the State issued bonds over the comparable bonds. Oth er Municipal
bonds, suchasthoseissued bylocalgovernments, usuallybearhig herriskas(atleast
partially) they are typically backed by future returns of the invest ment for which
the money is raised by issuing the bonds. For instance, if a town need s to build a
new hospital, to raise required funds it could issue bonds backed by f uture returns
from the hospital. However, not all such undertakings are always s uccessful, hence
higher risk associated with such bonds. Higher risk bonds typically ha ve higher
yields. This is an example of a more general principle – higher risk invest ments
should have higher expected returns. Indeed, if one could enjoy t he same return
from a lower risk investment as from a riskier one, one would clearly te nd to choose
the former. Since the demand for lower risk investments would then be higher than
for their higher risk counterparts, the price one would end up payin g for a lower
risk investment would also be higher, while the corresponding yield wou ld be lower.
4Nonetheless, S&P’s downgrade of the U.S. credit rating from AAA (o utstanding) to AA+
(excellent) on August 5, 2011 is a fact!
7",2014-05-07T01:13:23Z, t t  bonds bonds treasury ined municipal state and local state state it united statinterest earned from subonds state fl state state state th municipal for hr  ined since nonetless august
paper_qf_22.pdf,9,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Put another way, the ratio of the return over the associated risk should generally be
approximately the same for all investments or else the supply and de mand paradigm
will eventually make sure that it is.
Notonlyvariousgovernmentsbutalsoprivatesectorcorporation scanissuebonds
to raise money to develop a new product, open a new factory plant, etc.Corporate
bonds have higher yields as they are riskier than government issued bonds – their
credit risk is higher. Corporate bonds are backed by the ability of a c orporation
to generate earnings from sales of products and/or services, so if the business is
not doing too well, the credit rating or the corporation goes down, t he prices of
its bonds also go down, and the yields go up. Since corporations can d efault, the
ability of their bond holders to collect at least portions of their origina l investments
in the case of bankruptcy is important. In fact, corporate bond h olders are the ﬁrst
ones in line to partially if not completely get their money back from the p roceeds
of liquidation of the corporation after its default. The stock holder s, on the other
hand, have lower priority in the liquidation process and may receive no thing even if
the bondholders are completely or partially compensated.
Stocks and corporate bonds are diﬀerent in many more ways than t he one just
mentioned. When a corporation issues bonds, it borrows money fro m bond holders,
that is, its outstanding bonds count toward its debt. There is an alt ernative and
somewhat easier way for a corporation to raise money – it can issue s tock. There
are two main types of stock, preferred stock and common stock. The preferred stock
can roughly be thought of as a hybrid between a corporate bond an d the common
stock. Inthe following we will mostly focusonthecommon stock, and forthesake of
brevity we will omit the adjective “common”. Let us, however, ment ion that, once
the corporation defaults, in the liquidation process bond holders, p referred stock
holders and common stock holders are compensated with the decre asing priority.
Outstanding stock is not a form of corporate debt, in particular, s tocks are not
obligations, theyhavenomaturity, andthecorporationdoesnotp romisetopayback
the stockholders their originally invested amount any time later. Ins tead, stockhold-
ers or shareholders are owners of the corporation in the proport ion to the total stock
issued by the corporation. Some of this stock, which is called treasu ry stock, can be
owned by the corporation itself. In fact, the total value of the co rporation, which is
referred to as its market capitalization, is determined by the numbe r of issued shares
multiplied by the current market price of one share. The latter, in tu rn, depends
on the free market supply and demand levels for the shares of the c orporation. It is
important to note that stocks can and do become undervalued or o verpriced in the
free market, and the reasons for this are manifold. We will return t o this point once
we discuss some of the factors that are expected to determine wh at the “fair” price
of a given stock should be.
So, what compels investors to allocate their funds in stocks? Thus, unlike bonds,
stocks do not pay coupons, that is, shareholders do not earn inte rest. Some stocks
do pay dividends, however. Typically the annualized stock dividend is a low single
digit percentage of the current stock price. Whether the stock p ays a dividend is
8",2014-05-07T01:13:23Z,put not only varus governments but also private seor coratcorate corate since it stocks re tre t it  outstandi isome it it  so  some typically r
paper_qf_22.pdf,10,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","decided by the corporate governing body (the board of directors ), and the amount
of dividend can be changed (including to paying no dividend at all) withou t the
shareholders’ approval. For instance, if the business is not doing t oo well, the
corporation may decide to no longer pay out dividends. This usually will result in
a decline in the stock price as the demand for the stock most likely will d ecrease.
Even though shareholders do not earn interest, do not essentially have any guar-
antees as to recovering their investment in the future, and may no t even be paid
any dividends, they are (partial) owners of the corporation. This o wnership enti-
tles them to certain rights such as a right to vote for various corpo rate decisions
including electing the board of directors. Also, if another entity (su ch as another
corporation) intends to acquire the corporation, the current st ock holders can vote
for or against such a takeover depending on whether it is in their inte rests or not.
This isoneof thekey reasons why some investors arewilling to become shareholders.
Thus, imagine that a corporation is doing well, and has good revenues as well as
earnings. If, for some reason, the market price for its shares is u nreasonably low,
another entity could buy enough shares in the open market and att empt a hostile
takeover of the corporation – each share gives this hostile entity o ne vote, and all
it needs is 51% of the votes for a successful takeover. This might n ot be in the
interests of the corporation, which includes its board of directors , who are typically
shareholders themselves, its oﬃcers as well as all other sharehold ers. The board
of directors, which is expected to act in the interests of at least mo st shareholders
(after all, it was elected by the majority of shareholders’ votes), in this case is likely
to decide that the corporation should buy back some of the outsta nding shares in
the open market, which will ultimately result in an increase in the stock price. This
buy-back mechanism then is expected to ensure that the stock pr ice grows as the
revenues and more importantly earnings of the corporation grow – the corporation
must pay cash to buy back some of its outstanding shares, and the ability to do so
is directly linked to its earnings.
Thus, it is the earnings of the corporation that are expected to de termine the
price for its shares. Therefore, if an investor believes that the co rporationhas strong
fundamentals, i.e., the ability to generate earnings in the future, he or she might
decidetobecomeashareholder. Generally, suchaninvestment bea rshigherriskthan
a comparable bond investment. Thus, a typical stock price has ann ual volatility,
which is a measure of how much it ﬂuctuates, of 30-35%, while bonds u sually have
volatility in the 5-7% range. (These ﬁgures can vary depending on th e economic
cycle.) Since stocks are higher risk investments, they should have a dequately higher
returns, and historically on average this indeed appears to be the c ase.
As we already mentioned, even though the stock market system ha s worked over
many decades, it is by no means perfect. Thus, corporations are e xpected to buy
back their stock if its price falls too low, but there is no actual law or r ule that they
must do so. If such a rule were in place, corporations would be much le ss inclined
to exaggerate their earnings. Thus, imagine that a corporation ha d to buy back
some of its outstanding stock according to its reported earnings le vels (say, in some
9",2014-05-07T01:13:23Z,for  eve also     t   trefore genlly  tse since as   
paper_qf_22.pdf,11,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","proportion to earnings per share). If it exaggerated its earnings , the corporation
would then have to buy back more outstanding stock (and at a highe r price as the
market demand on its stock would be artiﬁcially inﬂated), that is, the corporation
would have to pay more cash than if it reported its earnings correct ly. This would
clearly be diﬃcult to do if the corporation did not actually have the cas h. The lack
of such a rule (or an analogous regulation) might be (at least partially or indirectly)
contributing into stock market bubbles.
Thus, many of the new internet companies during the .com boom neve r intended
to buy back their stock, and the stock prices soared to more than unreasonably high
levels as many investors were betting their money on the future pot ential of these
companiestogenerateearnings, whichwasoftentimesexaggerat edbythecompanies
themselves without any evident strong fundamentals present at t he time. In fact,
in many cases stocks trade at prices that factor in a potential for growth, and not
just the current earnings levels. Sometimes such optimistic bets do not pay oﬀ,
and the investors bear losses. On the other hand, many companies do meet or
even outperform investors’ expectations (typically these are co mpanies with strong
fundamentals), in which case such investments pay oﬀ well. Another important
point is that the stock market does not like uncertainty. If, for ins tance, there is
a possibility that the economy might not do well in the nearest future , or, say,
there could be a war and its outcome is somewhat uncertain, many inv estors tend
to get out of their stock positions, which can sometimes lead to panic selling, and
stock market crashes. Thus, the stock market sentiment goes a long way, and stock
prices are substantially aﬀected by what various investors think at any given time.
This is partly responsible for the fact that stocks are more volatile t han some other
ﬁnancial instruments such as bonds. This volatility makes the stock market game
rather exciting, at least for some investors.
In some sense stock market is analogous to foreign currency exch ange – corpo-
rations are like countries, and stocks are like their currencies. Tra ding stocks is
then like reallocating funds between diﬀerent currencies. However , this resemblance
does not go all the way – there are important diﬀerences as well. Thu s, convertible
currencies are backed by reserves of the countries as well as by la ws ensuring that
they can be used to purchase goods, services, etc.For instance, all U.S. Federal
Reserve Notes (that is, cash) regardless of denomination have th e following crucial
statement on their faces: “This is legal tender for all debts, public and private”.
This statement is backed by the U.S. Federal law. Stockholders do n ot enjoy such a
privilege – you cannot exchange stocks for a bowl of soup at a local deli, you must
ﬁrst sell them on a national stock exchange for cash!
There are many rules and regulations that stock markets must follo w. These
rules have been evolving by learning from the past experiences as we ll as to ensure
that investors’ interests are most adequately protected from p otential fraud, market
manipulation, misinformation (such as exaggerated corporate ear nings),etc. The
stock market is an important ingredient of the free market econom y. And there is a
ﬁne line between regulation and overregulation; it’s a balancing act.
10",2014-05-07T01:13:23Z,  t  isometimoanotr     itra thu for fl reserve not  fl stoclrs tre tse t and
paper_qf_22.pdf,12,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","4 Arbitrage Pricing
Supposewehaveastock Sandacashbondwithcontinuouslycompounded5constant
interest rate r. Let the stock price at time t= 0 beS0.
Consider a forwardcontract, where one of the two parties agrees to sell the other
the stock at some future time T(which is known as expiry/delivery date/maturity
of the contract) for the strike price kon which they agree now, that is, at t= 0.
The forward price is actually independent of the stock movements b etweent= 0
andt=T, and is given by:
k=S0exp(rT). (9)
The reason for this is arbitrage . Generally, arbitrage is a mechanism for making
“correct” market prices, known as arbitrage pricing . In itsidealized form arbitrage
means that, if the price of something is not “correct”, i.e., it is not priced according
to arbitrage pricing, there is a risk-free way of making proﬁt.6
Thus, suppose a bank was oﬀering a forward with a strike price k>S0exp(rT).
Then att= 0 we could borrow S0dollars by selling cash bonds, and purchase one
unit of stock. At time Twe could sell our stock to that bank for kdollars, repay our
debt, which is now S0exp(rT), and make a risk-free proﬁt of k−S0exp(rT) dollars.
Next, suppose a bank was oﬀering a forward with a strike price k<S0exp(rT).
Then att= 0 we could sell one unit of stock, and buy S0worth of cash bonds. At
timeTour bonds are worth S0exp(rT), and we could buy one unit of stock from
that bank for kdollars, hence making a risk-free proﬁt of S0exp(rT)−kdollars.
So, now that we have ﬁgured out the arbitrage pricing for our forw ard, we come
to the simplest example of what is known as hedging, which is investing to reduce
the risk of adverse price movements in a given asset. Typically, a hed ge consists
of taking an oﬀsetting position in another asset. So, suppose a ban k enters into
the above forward contract to deliver the stock at maturity Tat the strike price
k=S0exp(rT). To hedge its exposure to adverse price movements of the stock ,
5As mentioned in the previous section, usually bonds pay coupons ann ually or semi-annually.
Continuous compounding with constant interest rate rmeans that, if we have $1 at time t, at time
t+∆t, where ∆ tis small, it earns additional r∆tdollars in interest, and this occurs continuously.
The net result is that $1 at t= 0 turns into exp( rt) dollars at time t. The reason why interest
exists in the ﬁrst instance is because of the time value of money: typ ically, barring deﬂation, $1
today is worth more than $1 a year from now. The “fundamental” re ason for this is related to
economic growth and the fact that investing, e.g., in businesses is expected to generate returns –
which is one reason why interest rates are low when the economy is ba d. More prosaically, the
time value of money can be traced to human mortality and the fact th at time is the most valuable
commodity as it is in ﬁnite and rather short supply for each individual h uman being – all the
eternity notwithstanding.
6The real life usually is much trickier than the idealized form of arbitrag e. There are many
things that can go wrong in reaping this “risk-free” proﬁt, making it not so risk-free. Furthermore,
in real life there are transaction costs, which are ignoredin the argument below. Even if there
was risk-free proﬁt to be made on paper, in real life such proﬁt cou ld be reduced to breaking
even or even loss by transaction costs. In fact, some people make money by essentially exclusively
becoming a transaction cost to others’ trading, an example being n otorious high frequency traders.
11",2014-05-07T01:13:23Z,arb it rage prici suppose  he stock and cash bond with continuously ound  consir t t genlly i tat  next tat tour so typically so tat to as continuous t t t  t tre furtr evein
paper_qf_22.pdf,13,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","which could increase in price by the time T, the bank would borrow S0dollars worth
of cash bonds at t= 0 and buy the stock at price S0with that cash. At time Tthe
bank delivers the stock to the other party of the forward contra ct, collects kdollars
from said party, and pays oﬀ its debt, which is worth exactly kdollars at time T
because of the accrued interest. The bank breaks even.
But banks are for-proﬁt organizations, they are not in the busine ss of breaking
even. So, how does a bank make money in this particular example? Jus t as the
bookmaker, the bankmust charge apremium to make money. So, th e eﬀective strike
price in the forward contract must be k′>k, and the bank makes proﬁt equal the
diﬀerencek′−k(in reality, less any other transaction costs, such as those assoc iated
with purchasing the stock, and any costs of carry and/or other e xpenses – the bank
has to pay its employees salary, rent, etc.– which we will not delve into here). The
diﬀerence between k′andkmay be structured as a commission or some other way
in the actual forward contract. To the other party to the contr act, the diﬀerence
betweenk′andkis then basically a transaction cost. As mentioned above, in many
cases proﬁt is made in the form of transaction cost, one way or ano ther.
Forwards are the simplest forward-looking contracts. Complexity is added once
derivatives such ascall andput optionsareconsidered. Wewill discu ss these inmore
detail insubsequent sections. Herewe simply deﬁne thesimplest ofs uch contracts to
motivate further developing the mathematical machinery in the sub sequent sections.
A European call option is a right (but not obligation) to buy a stock at the maturity
timeTfor the strike price kagreed on at time t= 0. The claim for the call option
fc(ST,k) = (ST−k)+. Here (x)+=xifx >0, and (x)+= 0 ifx≤0. By the
“claim” we mean how much the option is worth at maturity T. If the stock price at
maturityST>k, then theoptionholder gains ST−k(excluding the cost paidforthe
option att= 0). If the price at maturity ST≤k, then there is no proﬁt to be made
from the option as it makes no sense to exercise it if ST<k(as it is cheaper to buy
the stock on the market) and it makes no diﬀerence if ST=k– all this is assuming
no transaction costs. Similarly, a Europeanput option is a right (but not obligation)
to sell a stock at the maturity time Tfor the strike price kagreed on at time t= 0.
The claim for the put option is given by fp(ST,k) = (k−ST)+. To understand how
to price these and other derivatives, we need some more mathemat ical tools.
5 Binomial Tree Model
One such tool is the binomial tree model. At time t= 0 the stock price is S0. At
timet=δtthe stock price can take two values: S+andS−. Att= 0 the bond is
worthB0, and at time t=δtit is worthB0exp(rδt).
Suppose we have a clam f, which at time t=δttakes two values f+andf−
according to the stock price.7We can synthesize this derivative as follows. Let
7Here the claim fis completely arbitrary and can correspond to the most exotic deriv atives
imaginable. The discussion below is completely general.
12",2014-05-07T01:13:23Z,at t t but so jus so t to as forwards lexity  wl re  afor t re by   simarly aput for t to binomial tree mone at at at suppose   re t
paper_qf_22.pdf,14,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","(φ,ψ) be a general portfolio of φunits of stock Sandψunits of the cash bond B.
Further, let
φS++ψB0exp(rδt) =f+, (10)
φS−+ψB0exp(rδt) =f−, (11)
so that we have
φ=f+−f−
S+−S−, (12)
ψ=B−1
0exp(−rδt)S+f−−S−f+
S+−S−. (13)
Thus, if we buy this portfolio at t= 0, we will guarantee the correct outcome for
the derivative.
The price of this portfolio at time t= 0 is given by
V=S0φ+B0ψ=S0f+−f−
S+−S−+exp(−rδt)S+f−−S−f+
S+−S−. (14)
In the case of a forward we have f=S−k, so that
V=S0−exp(−rδt)k , (15)
which vanishes for k=S0exp(rδt) as it should according to the arbitrage pricing.
In fact, the above price for a general derivative fis precisely the arbitrage price.
This can be seen as follows. Suppose a bank was oﬀering to buy or sell the derivative
for a price Pless thanV. We can buy the derivative from that bank, and sell the
(φ,ψ) portfolio to exactly match it with a net proﬁt V−P. At the maturity time
the derivative would exactly cancel the value of the portfolio (which replicates the
claimf) regardless of the stock price. So we are making a risk-free proﬁt V−P.
Similarly, if a bank was oﬀering the above derivative at a price P >V, we could
sell this derivative to that bank, and buy the ( φ,ψ) portfolio. At the end of the day
we have a risk-free proﬁt P−V.
The hedge in replicating the claim fat timet=δtis in that one purchases the
(φ,ψ) portfolio at t= 0, which reproduces the claim fno matter whether the price
goes fromS0at timet= 0 toS+orS−at timet=δt. Put diﬀerently, arbitrage
and hedging are two sides of the same coin.
5.1 Risk-neutral Measure
We can rewrite the price Vas
V= exp(−rδt)[qf++(1−q)f−], (16)
where
q≡S0exp(rδt)−S−
S+−S−. (17)
13",2014-05-07T01:13:23Z,and furtr  t ii suppose less  at so simarly at t put risk measure  vas
paper_qf_22.pdf,15,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","The setQ≡ {q,1−q}is called the risk-neutral measure . The fact that q >0 follows
fromthe fact that otherwise we have S0exp(rδt)≤S−<S+, which would guarantee
unlimited risk-free proﬁt by selling the cash bond and buying stock at t= 0. On
the other hand, we also have q <1 as otherwise we have S−< S+≤S0exp(rδt),
which would guarantee unlimited risk-free proﬁt by selling the stock a nd buying the
cash bond at t= 0. That is, arbitrage pricing requires that S−<S0exp(rδt)<S+.
Thus, as we see, the price of the derivative is given by the expectat ion of the
discounted8claim exp( −rδt)fwith respect to the risk-neutral measure Q:
V=V0=B0/an}bracketle{tB−1
Tf/an}bracketri}htQ, (18)
where the maturity time T=δt.
The above results are straightforwardly generalized to the case o f a binomial
tree with multiple time-ticks. Starting from the last time-tick we can r econstruct
the claimfat earlier times via
fnow= exp(−rδt)[qfup+(1−q)fdown], (19)
where
q=exp(rδt)Snow−Sdown
Sup−Sdown. (20)
The trading strategy is given by:
φ=fup−fdown
Sup−Sdown, (21)
ψ=B−1
now[fnow−φSnow]. (22)
The price of the derivative is given by
Vt=Bt/an}bracketle{tB−1
Tf/an}bracketri}htQ, (23)
whereQis the corresponding risk-neutral measure.
5.2 An Example: Baseball World Series
Suppose 2 teams play a series of up to (2 n+1) games – think Baseball World Series
with 7 games – in which the ﬁrst team to win ( n+ 1) games wins the series and
then no other games are played. Suppose that you want to bet on e ach individual
game in such a way that when the series ends you will be ahead $100 if y our team
wins the series, or behind by exactly $100 if your team loses the serie s, no matter
how many games it takes. How much would you bet on the ﬁrst game?
8Intuitively, we can understand why the expectation is of the discou nted claim and not the
claim itself from the time value of money argument: the claim fis at a future time t=T, whereas
V0is computed at the present time t= 0, so we must discount the claim to arrive at its current
worth.
14",2014-05-07T01:13:23Z,t t othat  t starti s dowsup do sup dows t vt  is a baseball world serisuppose baseball world serisuppose how intuitively
paper_qf_22.pdf,16,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","This can be thought of as a derivative pricing question. Indeed, we c an view the
series as a binomial process with the known claim at the end of the ser ies. Thus,
to solve this problem we can draw a binary tree and work backwards. Let us put
“+” if our team wins, and put “ −” if our team loses. We will put “0” at the root of
the binary tree (the beginning of the series). The longest branche s of the tree have
2(n+ 1) nodes (corresponding to all (2 n+ 1) games played), the node before the
last one having n+’s andn−’s, no matter in what order. If, however, ( n+1) +’s
or−’s occur before (2 n+1) games, the corresponding branch is shorter as the series
ends. For us it will be convenient to have all branches of the same len gth (that is,
containing 2( n+1) nodes). To achieve this, we will continue a terminated branch
so that it has 2( n+1) nodes, and at the last nodes put the claim of +$100 or −$100
depending upon whether ( n+1) +’s or ( n+1)−’s occurred ﬁrst in this branch.
Next, note that we can conﬁne our attention to only a half of the bin ary tree,
say, the half that corresponds to our team winning the ﬁrst game – indeed, the other
half is the same as this half up to exchanging +’s and −’s. So, our truncated tree
will now have branches of uniform length containing (2 n+ 1) nodes, and the ﬁrst
node has + in it, which corresponds to our team winning. Let Xbe the bet we
made on the ﬁrst game. Then this is exactly how much money we have in the ﬁrst
node of the truncated binary tree (where our team won the ﬁrst g ame). Thus, we
can viewXas the value of the claim f(F1), wherei= 1,2,...,(2n+ 1) numbers
the game, while Fiis a particular ﬁltration (orhistory) up to the ith game. Thus,
F1={+},F2={++},{+−}, and so on (we are focusing on the truncated tree).
In particular, X=f(F1). On the other hand, we also know that f(F2n+1) = +$100
if inF2n+1(n+1) +’s occur ﬁrst, and f(F2n+1) =−$100 ifF2n+1(n+1)−’s occur
ﬁrst. Thus, we would like to deduce the initial value of the claim from th e known
ﬁnal values of it – so this is indeed a pricing question.
To determine X, we do not actually need the details of the underlying market
instruments we are trading to replicate the ﬁnal claim. All we need is t he risk-
neutral measure Q. The elements of this measure are all 1 /2, in particular, they
are independent of the actual probabilities for our team to win or los e at any given
time (assuming that they are neither 0 nor 1). Indeed, suppose at any given time
we purchase a bet for Ydollars (by holding the zero interest rate cash bond short
Ydollars). If our team wins, we get 2 Ydollars back (the reward of Ydollars plus
the stake returned). If our team loses, we get nothing back. This implies that the
risk-neutral probability for this bet is indeed q= 1/2. Now we can immediately
write down the value of the claim at time i= 1:
X=f(F1) =/an}bracketle{tf(F2n+1)/an}bracketri}htQ=/parenleftbigg1
2/parenrightbigg2n2n/summationdisplay
k=0(2n)!
k!(2n−k)!($100ǫk).(24)
Here
(2n)!
k!(2n−k)!(25)
15",2014-05-07T01:13:23Z, ined    t  for to next so  be t as  is  io to all t ined dollars dollars  dollars dollars    re
paper_qf_22.pdf,17,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","is the number of k+’s we can place in 2 nslots in an arbitrary order (here we are
taking into account that to specify F2n+1we only need to specify the last 2 nentries
as the ﬁrst entry is always + for the truncated tree), and ǫk= +1 ifk≥n, while
ǫk=−1 ifk<n. We then have:
X=(2n)!
22n(n!)2$100. (26)
Thus, forn= 0 we have X= $100, for n= 1X= $50, forn= 2X= $75/2, for
n= 3X= $125/4, and so on.
6 Martingales
Aﬁltration Fiis the history of a stock (or some other process) up until the tick- time
ion the tree.
AclaimXon the tree is a function of the ﬁltration FTfor some horizon time T.
The conditional expectation operator /an}bracketle{t·/an}bracketri}htQ,Fiis deﬁned along the latter portion
of paths that have initial segments Fi.
Aprevisible processφiis a process on the tree whose values at any tick-time i
depend only on the history up to one tick-time earlier, Fi−1.
A processMiis amartingale with respect to a measure Pand a ﬁltration Fiif
/an}bracketle{tMj/an}bracketri}htP,Fi=Mi,∀i≤j . (27)
Note that for a martingale its expectation is independent of time:
/an}bracketle{tMj/an}bracketri}htP=/an}bracketle{tMj/an}bracketri}htP,F0=M0, (28)
that is, it has no drift.
6.1 The Tower Law
LetX=XTbe a claim. Then the process
Nj≡ /an}bracketle{tX/an}bracketri}htP,Fj (29)
is aP-martingale.
This follows from the tower law :
/angbracketleftbig
/an}bracketle{tX/an}bracketri}htP,Fj/angbracketrightbig
P,Fi=/an}bracketle{tX/an}bracketri}htP,Fi, i≤j . (30)
To prove the tower law, let us represent a ﬁltration Fias follows:
Fi={ǫ1,...,ǫ i}, (31)
16",2014-05-07T01:13:23Z,  martal is  articial intellence ofor t  is  pre visible  miss and   mj  mi note mj mj t tor law  be tnj fj  fj   to  as 
paper_qf_22.pdf,18,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","whereǫk=±. Let the probability of the path starting from the event correspo nding
toFiand ending with the event corresponding to Fj,i≤j, bePǫ1,...,ǫi(ǫi+1,...,ǫ j).
Then we have:
/an}bracketle{tNj/an}bracketri}htP,Fi=/summationdisplay
ǫi+1,...,ǫjPǫ1,...,ǫi(ǫi+1,...,ǫ j)Nj=
/summationdisplay
ǫi+1,...,ǫjPǫ1,...,ǫi(ǫi+1,...,ǫ j)/summationdisplay
ǫj+1,...,ǫTPǫ1,...,ǫj(ǫj+1,...,ǫ T)XT=
/summationdisplay
ǫi+1,...,ǫTPǫ1,...,ǫi(ǫi+1,...,ǫ j)Pǫ1,...,ǫj(ǫj+1,...,ǫ T)XT=
/summationdisplay
ǫi+1,...,ǫTPǫ1,...,ǫi(ǫi+1,...,ǫ T)XT=
/an}bracketle{tX/an}bracketri}htP,Fi=Ni. (32)
Here we have used Pǫ1,...,ǫi(ǫi+1,...,ǫ j)Pǫ1,...,ǫj(ǫj+1,...,ǫ T) =Pǫ1,...,ǫi(ǫi+1,...,ǫ T).
6.2 Martingale Measure
LetSbe the stock process, and Bbe the cash bond process. Deﬁne the discounted
stock process Zi≡B−1
iSi. Let us determine the martingale measure QforZ.
Under the martingale measure Qwe have
/an}bracketle{tZj/an}bracketri}htQ,Fi=Zi, i≤j . (33)
Note that by Zion the r.h.s. we mean the value of Zicorresponding to the ﬁltration
Fi. Let this value be denoted by Z∗(Fi). In particular,
/an}bracketle{tZi+1/an}bracketri}htQ,Fi=Z∗(Fi). (34)
LetFi={ǫ1,...,ǫ i}, andF±
i+1={ǫ1,...,ǫ i,±}. Then we have
/an}bracketle{tZi+1/an}bracketri}htQ,Fi=QFi(+)Z∗(F+
i+1)+QFi(−)Z∗(F−
i+1) =
QFi(+)Z∗(F+
i+1)+[1−QFi(+)]Z∗(F−
i+1). (35)
On the other hand, we have (34). Thus, we have
QFi(±) =Z∗(Fi)−Z∗(F∓
i+1)
Z∗(F±
i+1)−Z∗(F∓
i+1). (36)
This determines QFi(ǫi+1). We can now determine all the other Q-probabilities.
Thus,
QFi(ǫi+1,ǫi+2) =QFi(ǫi+1)QFi+1(ǫi+2), (37)
and so on.
17",2014-05-07T01:13:23Z,  and fj tnj  nj  ni re martale measure  be be  zi si  for unr   zi note lzi correspondi    izi     tzi      o         
paper_qf_22.pdf,19,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Finally, let us rewrite the Q-probabilities in terms of values of S. Let us assume
thatBi=B0exp(rti),ti+1−ti≡δt. Then we have
QFi(±) =exp(rδt)S∗(Fi)−S∗(F∓
i+1)
S∗(F±
i+1)−S∗(F∓
i+1), (38)
which is a formula we have derived earlier for the risk-neutral measu re. That is, the
risk-neutral measure is the martingale measure.
6.3 Binomial Representation Theorem
Suppose we have a binomial tree with two processes SandN. Then we have
∆Ni=φi∆Si+ki, (39)
where ∆Ni≡Ni−Ni−1, ∆Si≡Si−Si−1, and bothφandkareprevisible processes.
To show this, consider a particular ﬁltration Fi−1. LetS∗(Fi−1)≡S∗,S∗(F±
i)≡
S±, and similarly for N. Then ∆Sitakes two values S±−S∗, and ∆Nitakes two
valuesN±−N∗. Let
φi=N+−N−
S+−S−. (40)
Note thatφiis previsible by deﬁnition. We must now show that kiis also previsible.
To do this, let us show that k∗(F+
i) =k∗(F−
i):
k∗(F+
i)−k∗(F−
i) = [(N+−N∗)−φi(S+−S∗)]−[(N−−N∗)−φi(S−−S∗)] =
(N+−N−)−φi(S+−S−) = 0. (41)
This implies that kiis indeed previsible. Indeed, at the node i,kiis independent of
±, therefore it depends only on Fi−1.
Now suppose that both SandNareQ-martingales. Then kiis identically zero.
Indeed, since both φiandkiare previsible, we have
/an}bracketle{t∆Ni/an}bracketri}htQ,Fi−1=φi/an}bracketle{t∆Si/an}bracketri}htQ,Fi−1+ki. (42)
However,
/an}bracketle{t∆Ni/an}bracketri}htQ,Fi−1=/an}bracketle{tNi/an}bracketri}htQ,Fi−1−/an}bracketle{tNi−1/an}bracketri}htQ,Fi−1=/an}bracketle{tNi/an}bracketri}htQ,Fi−1−N∗= 0,(43)
and similarly for ∆ Si. This then implies that ki≡0.
Thus, for any two Q-martingales SandNwe have
∆Ni=φi∆Si, (44)
whereφiis previsible and plays the role of a “discrete derivative”. This leads to the
binomial representation theorem forQ-martingales:
Ni=N0+i/summationdisplay
k=1φk∆Sk. (45)
18",2014-05-07T01:13:23Z,nally  bi t  that binomial representattorem suppose and tni si ni ni ni si si si to    tsi takni tak note  to  ined   and are tined ni  si  ni  ni  ni  ni  si   and  ni si  ni sk
paper_qf_22.pdf,20,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","6.4 Self-ﬁnancing Hedging Strategies
Before going into the details of self-ﬁnancing hedging strategies, le t us mention that
in ﬁnance one can take a long position, e.g., by purchasing a stock, and a short
position, which means that one “owns” a negative number of shares of the stock
to be covered at some later time. With a long position, if the stock pric e goes up,
the position has a gain, and if the stock price goes down, the position bears a loss.
With the short position it is the opposite, if the stock price goes up, t he position
bears a loss, and if the stock price goes down, the position has a gain . To take
a long position, one needs to borrow money to buy stock. When takin g a short
position, one receives the cash value equivalent to the price of the s horted stock at
the time or shorting. In real life there are transaction costs asso ciated with this,
e.g., the interest rate at which the received cash accrues interest wh en the stock is
shorted is typically lower than the interest accrued on the borrowe d cash when a
long position is taken. Below we ignore any such discrepancies and tra nsaction costs
and consider the idealized situation where long and short positions ar e treated on
an equal footing.
Let us construct a hedge for the claim X=XTon the stock Sin the presence
of the cash bond Bi. First, let us deﬁne the discounted stock process Zi≡B−1
iSi,
and the discounted claim YT≡B−1
TXT. Let
Ei≡ /an}bracketle{tYT/an}bracketri}htQ,Fi. (46)
Note thatEiis aQ-martingale. Moreover, ET=YT, so at the end of the day Ei
replicates the discounted claim Y. LetQbe the martingale measure for Z. Then
there exists a previsible process φsuch that
Ei=E0+i/summationdisplay
k=1φk∆Zk. (47)
The previsible process φis determined from
φi=E∗(F+
i)−E∗(F−
i)
S∗(F+
i)−S∗(F−
i). (48)
Next, deﬁne the following previsible process:
ψi=Ei−1−φiZi−1. (49)
Finally, deﬁne a portfolio Π i= (φi+1,ψi+1) consisting of holding φi+1units of stock
andψi+1units of the cash bond at time i. This portfolio is worth
Vi=φi+1Si+ψi+1Bi=BiEi. (50)
If we hold this portfolio across the next time-tick, it is worth
/hatwideVi=φi+1Si+1+ψi+1Bi+1. (51)
19",2014-05-07T01:13:23Z,self edgi strategibefore with with to wibelow  toibi rst zi si  ei  note ei is oei  be tei t next ei zi nally  vi si bi bi ei  vi si bi
paper_qf_22.pdf,21,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Now (note that Vi+1is the worth of the portfolio Π i+1, whereas/hatwideViis the worth of
the portfolio Π iby the tick i+1),
Vi+1−/hatwideVi= [φi+2−φi+1]Si+1+[ψi+2−ψi+1]Bi+1=
[φi+2−φi+1]Si+1+[Ei+1−φi+2Zi+1−Ei+φi+1Zi]Bi+1=
[φi+2−φi+1]Si+1+[∆Ei+1−φi+2Zi+1+φi+1Zi]Bi+1=
[φi+2−φi+1]Si+1+[φi+1(Zi+1−Zi)−φi+2Zi+1+φi+1Zi]Bi+1=
[φi+2−φi+1]Si+1+[φi+1−φi+2]Zi+1Bi+1= 0. (52)
That is, the value of the portfolio Π iby the end of the next time-tick, that is, by
timei+ 1 is precisely the same as that of the portfolio Π i+1. So we can sell the
portfolio Π iat the end of this time-tick, and buy the portfolio Π i+1without any loss
or gain. The worth of the ﬁnal portfolio Π Tis
VT=BTET=BTYT=XT. (53)
So this hedging strategy replicates the claim Xat the maturity time T. On the
other hand, note that the price of the portfolio Π 0is given by
V0=B0E0=B0/an}bracketle{tB−1
TX/an}bracketri}htQ. (54)
This is the arbitrage price for the claim Xat timet= 0.
6.5 The Self-ﬁnancing Property
Let us take two arbitrary previsible processes φiandψi, and compute the value of
the corresponding Π iportfolio:
Vi=φi+1Si+ψi+1Bi+1. (55)
In general the change in this value over one time-tick is given by:
∆Vi≡Vi+1−Vi= ∆φi+1Si+∆ψi+1Bi+φi+1∆Si+ψi+1∆Bi,(56)
where ∆φi+1≡φi+2−φi+1, and ∆ψi+1≡ψi+2−ψi+1.
The self-ﬁnancing property means that
∆Vi=φi+1∆Si+ψi+1∆Bi, (57)
that is, the change in the value of the strategy is solely due the chan ges in the stock
and bond values, i.e., there is no cash ﬂowing in or out of the strategy at any time.
The condition for the strategy to be self-ﬁnancing is then
∆φi+1Si+∆ψi+1Bi= 0. (58)
This condition is satisﬁed by the strategy discussed in the previous s ubsection.
20",2014-05-07T01:13:23Z, vi vi is vi vi si bi si ei zi ei zi bi si ei zi zi bi si zi zi zi zi bi si zi bi that so t tis so at o at t self proty  vi si bi ivi vi vi si bi si bi t vi si bi t si bi 
paper_qf_22.pdf,22,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","7 Discrete vs.Continuous Models
Thus far we considered a binomial tree model, which is discrete. While n umerically
one often deals with discrete models, such as binomial/trinomial tree s,etc., there is
certain advantage to considering continuous models. One advanta ge of continuous
models is that certain calculus methods can be applied, analytic compu tations are
more streamlined, and the intuitive understanding is more easily deve loped. This
is analogous to the diﬀerence between the pre-Newtonian physics a nd a much more
streamlined Newtonian description based on continuous methods an d calculus.
Consider the following discrete model:
Bt= exp(rt), (59)
St+δt=Stexp(µδt+σǫt√
δt), (60)
whereǫt=±1 with the equal probabilities: P(ǫt) = 1/2 (r,µ,σare assumed to be
constant). I.e., timettakes values in a semi-inﬁnite discrete set t=kδt,k∈[0,∞).
Let
Xt≡/radicalbigg
δt
tt/δt−1/summationdisplay
k=0ǫkδt. (61)
Note that√
tXtis nothing but a random walk on a discrete binomial tree. The
quantityXttakes values with binomial distribution. As δt→0,Xtbecomes a
normal random variable9with mean zero ( /an}bracketle{tXt/an}bracketri}ht= 0) and variance 1 ( /an}bracketle{tX2
t/an}bracketri}ht−/an}bracketle{tXt/an}bracketri}ht2=
1) – this is the Central Limit Theorem . The stock can then be written as
St=S0exp(µt+σ√
tXt). (62)
So ln(St) is normally distributed with mean ln( S0)+µtand variance σ2t.
Let us compute the martingale measure Q. We have:
q=Stexp(rδt)−S−
t+δt
S+
t+δt−S−
t+δt=exp([r−µ]δt)−exp(−σ√
δt)
exp(σ√
δt)−exp(−σ√
δt)=
1
2/bracketleftbigg
1−√
δtµ+1
2σ2−r
σ+O(δt)/bracketrightbigg
. (63)
Note that this measure is independent of t.
Under this measure we have
/an}bracketle{tXt/an}bracketri}htQ=/radicalbigg
δt
tt/δt−1/summationdisplay
k=0/an}bracketle{tǫkδt/an}bracketri}htQ=
/radicalbigg
t
δt(2q−1) =−√
tµ+1
2σ2−r
σ+O(√
δt), (64)
9Meaning, its distribution is Gaussian.
21",2014-05-07T01:13:23Z,disete continuous mols  w one  neonianeoniaconsir  st st e  xt note xt is t xt takas xt becomxt xt central limit torem t st xt so st   st e note unr xt meani n
paper_qf_22.pdf,23,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","and
/an}bracketle{tX2
t/an}bracketri}htQ−/an}bracketle{tXt/an}bracketri}ht2
Q=δt
tt/δt−1/summationdisplay
k=0/bracketleftbig
/an}bracketle{tǫ2
kδt/an}bracketri}htQ−/an}bracketle{tǫkδt/an}bracketri}ht2
Q/bracketrightbig
= 4q(1−q) = 1+O(δt).(65)
Thus, ln(St) is now normally distributed (w.r.t. the martingale measure Q, that is)
with mean ln( S0)+(r−1
2σ2)tand variance σ2t. This implies that
St=S0exp/parenleftbigg
σ√
tZt+/bracketleftbigg
r−1
2σ2/bracketrightbigg
t/parenrightbigg
, (66)
whereZtisnormallydistributedwithmeanzeroandvariance1under themartin gale
measure Q.
7.1 Brownian Motion
Consider the discrete process
zt≡√
δtt/δt−1/summationdisplay
k=0ǫkδt, (67)
whereǫt=±1, andP(ǫt) = 1/2. In the limit δt→0 this isBrownian motion . The
variableztis normally distributed with mean zero and variance t:
P(z,t) =1√
2πtexp/parenleftbigg
−z2
2t/parenrightbigg
(68)
is the probability distribution for zat timet.
The formal deﬁnition of Brownian motion is as follows:
The process W= (Wt:t≥0) is aP-Brownian motion if and only if:
•Wtis continuous, and W0= 0;
•underPthe value of Wtis distributed as a normal random variable N(0,t) of
mean 0 and variance t;
•the increment Ws+t−Wsis distributed as a normal N(0,t) under P, and is
independent of Fs, that is, of the history of what the process did up to time s.
Let us ask the following question: what is the probability that startin g atz= 0
the Brownian motion ztdeﬁned above hits z∗by timeT? Without loss of generality
we can assume that z∗≥0. Let us ﬁrst consider the case where z∗>0. Then our
probability is
P(z0= 0 &∃t∗≤T:zt∗=z∗) =P(z0= 0 &∃t∗≤T:zt∗=z∗&zT≥z∗)+
P(z0= 0 &∃t∗≤T:zt∗=z∗&zT≤z∗). (69)
Note, however, that
P(z0= 0 &∃t∗≤T:zt∗=z∗&zT≥z∗) =
P(z0= 0 &∃t∗≤T:zt∗=z∗&zT≤z∗). (70)
22",2014-05-07T01:13:23Z,xt  st  st tis normally distributed with meazero and variance brownish motconsir ibrownish t t brownish t  brownish  is t  is ws ws is fs  brownish without  tnote
paper_qf_22.pdf,24,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Indeed, starting from z∗att=t∗it is as probable that we end up with zT≤z∗as
withzT≥z∗. Thus, we have
P(z0= 0&∃t∗≤T:zt∗=z∗) = 2P(z0= 0&∃t∗≤T:zt∗=z∗&zT≥z∗).(71)
Next, note that
P(z0= 0 &∃t∗≤T:zt∗=z∗&zT≥z∗) =
P(z0= 0 &zT≥z∗) =/integraldisplay∞
z∗dz√
2πTexp/parenleftbigg
−z2
2T/parenrightbigg
, (72)
so that
P(z0= 0 &∃t∗≤T:zt∗=z∗) = 2/integraldisplay∞
z∗dz√
2πTexp/parenleftbigg
−z2
2T/parenrightbigg
.(73)
Forz∗→0 this probability goes to 1.
8 Stochastic Calculus
A stochastic process Xis a continuous process ( Xt:t≥0) such that
Xt=X0+/integraldisplayt
0σsdWs+/integraldisplayt
0µsds , (74)
whereσandµare random F-previsible processes10such that
/integraldisplayt
0/bracketleftbig
σ2
s+|µs|/bracketrightbig
ds (75)
is ﬁnite for all t(with probability 1). In the diﬀerential form we have
dXt=σtdWt+µtdt . (76)
Given a process X, there is only one pair of volatility σand driftµthat satis-
ﬁes (74) for all t. (This uniqueness comes from the Doob-Meyer decomposition of
semimartingales.)
Ifσandµdepend on Wonly viaX(that is, ifσt=σ(Xt,t) andµt=µ(Xt,t),
whereσ(x,t) andµ(x,t) are deterministic functions), we have
dXt=σ(Xt,t)dWt+µ(Xt,t)dt , (77)
which is a stochastic diﬀerential equation (SDE).
10A continuous F-previsible process φsis deﬁned as a process which at time sis known given
the ﬁltration Fs,i.e.,φs=φ(Fs), soφsis afunctional of the ﬁltration Fs.
23",2014-05-07T01:13:23Z,ined  next e e for stochastic calculus is xt xt ws ixt  give do ob meyer  only xt xt xt xt  xt fs fs fs
paper_qf_22.pdf,25,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","8.1 Itˆ o Calculus
We can think of Brownian motion Wtas a limitδt→0 of the process
Wt≡√
δtt/δt−1/summationdisplay
k=0ǫkδt. (78)
Consider the increment
δWt≡Wt+δt−Wt=ǫt√
δt . (79)
The continuous version of this is given by:
dWt=ǫt(dt)1/2. (80)
This implies that
(dWt)n= (ǫt)n(dt)n/2. (81)
In particular,
(dWt)2=dt . (82)
This implies that if
dXt=σtdWt+µtdt , (83)
then
(dXt)2=σ2
tdt+O(dt3/2). (84)
This has important consequences.
Thus, consider a function f(x,t). We will denote partial derivatives w.r.t. xvia
a prime:
∂xf(x,t)≡f′(x,t). (85)
Then we have (we keep only terms of order dWtanddt):
df(Xt,t) =f′(Xt,t)dXt+1
2f′′(Xt,t)(dXt)2+∂tf(Xt,t)dt=
σtf′(Xt,t)dWt+/bracketleftbigg
µtf′(Xt,t)+1
2σ2
tf′′(Xt,t)+∂tf(Xt,t)/bracketrightbigg
dt .(86)
As an example consider the function
f(Xt) = exp(Xt). (87)
Then we have
df(Xt) =f(Xt)/bracketleftbigg
σtdWt+/parenleftbigg
µt+1
2σ2
t/parenrightbigg
dt/bracketrightbigg
. (88)
So a solution to the SDE
dSt=St[σtdWt+/tildewideµtdt] (89)
is given by
St=S0exp/bracketleftbigg/integraldisplayt
0σsdWs+/integraldisplayt
0/parenleftbigg
/tildewideµt−1
2σ2
t/parenrightbigg
dt/bracketrightbigg
. (90)
Note the diﬀerence between the drift /tildewideµtin the SDE (89) and in the exponent in
(90), which is shifted by σ2
t/2.
24",2014-05-07T01:13:23Z,it calculus  brownish a  consir    t    i  xt  xt    t and dt xt xt xt xt xt xt xt  xt xt xt as xt xt txt xt  so st st  st ws note
paper_qf_22.pdf,26,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","8.2 Radon-Nikodym Process
Two measures PandQare equivalent if they operate on the same sample space,
and agree on what is possible.
Consider a binomial tree. The Radon-Nikodym process is deﬁned as f ollows:
ζi≡Q(Fi)
P(Fi). (91)
This process is a P-martingale ( i≤j):
/an}bracketle{tζj/an}bracketri}htP,Fi=/summationdisplay
ǫi+1,...,ǫjPǫ1,...,ǫi(ǫi+1,...,ǫ j)Q(ǫ1,...,ǫ j)
P(ǫ1,...,ǫ j)=
/summationdisplay
ǫi+1,...,ǫjPǫ1,...,ǫi(ǫi+1,...,ǫ j)Q(ǫ1,...,ǫ i)Qǫ1,...,ǫi(ǫi+1,...,ǫ j)
P(ǫ1,...,ǫ i)Pǫ1,...,ǫi(ǫi+1,...,ǫ j)=
Q(ǫ1,...,ǫ i)
P(ǫ1,...,ǫ i)/summationdisplay
ǫi+1,...,ǫjQǫ1,...,ǫi(ǫi+1,...,ǫ j) =
Q(ǫ1,...,ǫ i)
P(ǫ1,...,ǫ i)=ζi. (92)
This, in particular, implies, that
ζi=/angbracketleftbiggdQ
dP/angbracketrightbigg
P,Fi, (93)
where
dQ
dP≡ζT=Q(FT)
P(FT)(94)
is the Radon-Nikodym derivative for some horizon time T.
We can use the Radon-Nikodym process to compute expectations w .r.t. the
measure Q. Thus, we have:
/an}bracketle{tXT/an}bracketri}htQ=/angbracketleftbiggdQ
dPXT/angbracketrightbigg
P. (95)
More generally, we have
/an}bracketle{tXj/an}bracketri}htQ,Fi=ζ−1
i/an}bracketle{tζjXj/an}bracketri}htP,Fi, i≤j≤T , (96)
which can be seen from
/an}bracketle{tζjXj/an}bracketri}htP,Fi=/summationdisplay
ǫi+1,...,ǫjPǫ1,...,ǫi(ǫi+1,...,ǫ j)Q(ǫ1,...,ǫ j)
P(ǫ1,...,ǫ j)Xj=
Q(ǫ1,...,ǫ i)
P(ǫ1,...,ǫ i)/summationdisplay
ǫi+1,...,ǫjQǫ1,...,ǫi(ǫi+1,...,ǫ j)Xj=ζi/an}bracketle{tXj/an}bracketri}htQ,Fi,(97)
wherei≤j≤T.
25",2014-05-07T01:13:23Z,radody process two and are consir t radody       radody  radody   xj  xj  xj  xj xj xj 
paper_qf_22.pdf,27,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","8.3 Path Integral
We can generalize the notions of the change of measure and Radon- Nikodym process
to continuous processes using path integral . Consider a P-Brownian motion Wt
betweent= 0 and some horizon time T. Letx(t) be the values of Wt(note that
x(0) = 0). We will divide the time interval [ t0,tf], 0≤t0<tf≤T, intoNintervals
[ti−1,ti],tN=tf,ti−ti−1≡∆ti>0. Let the corresponding values of x(t) be
xi≡x(ti),xN≡xf, ∆xi≡xi−xi−1. LetOt, 0≤t≤T, be a previsible process.
That is,Otdepends only on the path Ft={(x(s),s)|s∈[0,t]}:
Ot=O(Ft). (98)
The conditional expectation (here Ft0={(x∗(s),s)|s∈[0,t0],x∗(0) = 0,x∗(t0) =
x0}, wherex∗(s) is ﬁxed)
/an}bracketle{tOtf/an}bracketri}htP,Ft0(99)
can then be thought of as a ∆ ti→0, that is,N→ ∞, limit of the corresponding
discrete expression:
/an}bracketle{tOtf/an}bracketri}htP,Ft0= limN/productdisplay
i=1/integraldisplay∞
−∞dxi√2π∆tiexp/parenleftbigg
−(∆xi)2
2∆ti/parenrightbigg
Otf,Ft0,(100)
where
Otf,Ft0=O(Ft0∪{(x1,t1),...,(xN,tN)}). (101)
This limit is nothing but a Euclidean path integral
/an}bracketle{tOtf/an}bracketri}htP,Ft0=/integraldisplay
Dxexp(−S[x;t0,tf])Otf,Ft0, (102)
whereDxincludes the properly normalized measure, and
S[x;t0,tf]≡/integraldisplaytf
t0˙x2(t)
2dt (103)
is the Euclidean action functional for a free particle on R(dot in ˙x(t) denotes time
derivative).
To illustrate the above discussion, consider the following simple examp le. Let
Ot= exp/parenleftbigg/integraldisplayt
0ρ(s)dWs/parenrightbigg
, (104)
whereρ(s) is a deterministic function. In the path integral we can rewrite Otas
Ot= exp/parenleftbigg/integraldisplayt
0ρ(s)˙x(s)ds/parenrightbigg
. (105)
26",2014-05-07T01:13:23Z,path integral  radody consir brownish     intervals   ot that ot pends ft ot ft t ft ot ft ot ft ot ft ot ft ft  euiaot ft dx e ot ft dx inus euiato  ot ws iot as ot
paper_qf_22.pdf,28,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","In particular, we have
Otf,Ft0= exp/parenleftbigg/integraldisplayt0
0ρ(s)˙x∗(s)ds/parenrightbigg
exp/parenleftbigg/integraldisplaytf
t0ρ(s)˙x(s)ds/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x(t0)=x0=
O(Ft0) exp/parenleftbigg/integraldisplaytf
t0ρ(s)˙x(s)ds/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x(t0)=x0. (106)
The corresponding expectation is given by:
/an}bracketle{tOtf/an}bracketri}htP,Ft0=O(Ft0)/integraldisplay
Dxexp/parenleftbigg
−S[x;t0,tf]+/integraldisplaytf
t0ρ(s)˙x(s)ds/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x(t0)=x0=
O(Ft0)exp/parenleftbigg/integraldisplaytf
t0ρ2(s)
2ds/parenrightbigg/integraldisplay
Dxexp/parenleftbigg
−/integraldisplaytf
t0(˙x(s)−ρ(s))2
2ds/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x(t0)=x0=
O(Ft0)exp/parenleftbigg/integraldisplaytf
t0ρ2(s)
2ds/parenrightbigg
. (107)
Here we have used the change of variable x(t) =y(t)+/integraltextt
t0ρ(s)ds,t0≤t≤tf, in the
path integral, and took into account that
/integraldisplay
Dyexp(−S[y;t0,tf])|y(t0)=x0=/integraldisplay
Dyexp(−S[y;t0,tf]) = 1,(108)
which follows from our deﬁnition of the path integral. In particular, n ote that the
boundary condition y(t0) =x0at the initial time t0is immaterial – the path integral
(108) is independent of y(t0). The change of the measure Dx/Dyis also trivial –
see the derivation of (121).
Recall from the deﬁnition of the Brownian motion that Zs,s+t≡Ws+t−Wsis a
normalN(0,t) independent of Fs. In the path integral language this can be seen as
follows. Let z(r)≡y(s+r)−y(s), wherey(s) corresponds to Ws,. Thenz(0) = 0,
˙z(r) = ˙y(r+s), and/integraldisplays+t
s˙y2(s′)ds′=/integraldisplayt
0˙z2(r)dr . (109)
Thus, we have
/an}bracketle{tf(Zs,s+t)/an}bracketri}htP,Fs=/integraldisplay
Dyexp(−S[y;s,s+t])f(y(s+t)−y(s))|y(s)=x∗(s)=
/integraldisplay
Dzexp(−S[z;0,t])f(z)|z(0)=0=
/an}bracketle{tf(Wt)/an}bracketri}htP, (110)
soZs,s+tbehaves the same way as the Brownian motion Wtregardless of the history
Fs. This is an example of what we mentioned above, that analytic comput ations
are more streamlined in the continuous langauge, especially once we e mploy path
integral, which makes things much simpler and more intuitive.
27",2014-05-07T01:13:23Z,iot ft ft t ot ft ft dx e ft dx e ft re dye  dye  it dx dy is recall brownish ws ws is fs i ws t nz  fs dye  dz e  brownish  regardless fs 
paper_qf_22.pdf,29,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","8.4 Continuous Radon-Nikodym Process
Suppose we want to change measure from PtoQ. We can deﬁne the continuous
Radon-Nikodym process
ζt≡Q(Ft)
P(Ft). (111)
Then we have
/an}bracketle{tXT/an}bracketri}htQ=/angbracketleftbiggdQ
dPXT/angbracketrightbigg
P, (112)
/an}bracketle{tXt/an}bracketri}htQ,Fs=ζ−1
s/an}bracketle{tζtXt/an}bracketri}htP,Fs, (113)
ζt=/angbracketleftbiggdQ
dP/angbracketrightbigg
P,Ft, (114)
dQ
dP≡ζT, (115)
which are continuous versions of the corresponding discrete stat ements.
8.5 Cameron-Martin-Girsanov Theorem
LetWtbe aP-Brownian motion, and let γtbe anF-previsible process (we will
impose a condition on γtbelow). Deﬁne a measure Qvia
dQ
dP= exp/parenleftbigg
−/integraldisplayT
0γsdWs−1
2/integraldisplayT
0γ2
sds/parenrightbigg
. (116)
This measure is equivalent to P, and
/tildewiderWt≡Wt+/integraldisplayt
0γsds (117)
is aQ-Brownian motion.
To see this, let us ﬁrst compute the Radon-Nikodym process ζt. In fact, it is
given by
ζt= exp/parenleftbigg
−/integraldisplayt
0γsdWs−1
2/integraldisplayt
0γ2
sds/parenrightbigg
. (118)
This process is previsible:
ζt=ζ(Ft). (119)
A quick way to see that ζtis given by (118) is to use (107), according to which, since
γsis previsible, we have
/angbracketleftbigg
exp/parenleftbigg
−/integraldisplayT
0γsdWs/parenrightbigg/angbracketrightbigg
P,Ft= exp/parenleftbigg
−/integraldisplayt
0γsdWs/parenrightbigg
exp/parenleftbigg1
2/integraldisplayT
tγ2
sds/parenrightbigg
.(120)
28",2014-05-07T01:13:23Z,continuous radody process suppose to  radody ft ft txt fs xt fs ft cameromartir nov torem   be brownish  via ws    brownish to radody iws  ft ws ft ws
paper_qf_22.pdf,30,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","However, in deriving (107) we did not deal with the measure, so it is ins tructive to
directly compute the expectation (let γs=γ∗
sandx(s) =x∗(s),s∈[0,t], for the
pathFt) using the path integral:
/angbracketleftbiggdQ
dP/angbracketrightbigg
P,Ft=/integraldisplay
Dxexp(−S[x;t,T])/parenleftbiggdQ
dP/parenrightbigg
Ft=
ζ(Ft)/integraldisplay
Dxexp/parenleftbigg
−S[x;t,T]−/integraldisplayT
tγs˙x(s)ds−1
2/integraldisplayT
tγ2
sds/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x(t)=x∗(t), γt=γ∗
t=
ζ(Ft)/integraldisplay
Dxexp/parenleftbigg
−1
2/integraldisplayT
t(˙x(s)+γs)2ds/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x(t)=x∗(t), γt=γ∗
t=
ζ(Ft)/integraldisplay
Dxexp/parenleftbigg
−1
2/integraldisplayT
t˙y2(s)ds/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
y(t)=x∗(t), (121)
wherey(s)≡x(s)+/integraltexts
tγs′ds′,t≤s≤T. To evaluate this last integral, we need to
convertDxintoDywith the appropriate measure. This measure, in fact, is trivial:
Dx=Dy. To see this, let us discretize our path integral. Then we have ( t0=t,
tN=T)
∆yi≡y(ti)−y(ti−1) = ∆xi+γi−1∆ti. (122)
We, therefore, have
/integraldisplay
Dxexp/parenleftbigg
−1
2/integraldisplayT
t˙y2(s)ds/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
y(t)=x∗(t)=
limN/productdisplay
i=1/integraldisplay∞
−∞dxi√2π∆tiexp/parenleftigg
−1
2/bracketleftbigg∆xi
∆ti+γi−1/bracketrightbigg2
∆ti/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x0=x∗(t), γ0=γ∗
t=
limN/productdisplay
i=1/integraldisplay∞
−∞d∆xi√2π∆tiexp/parenleftigg
−1
2/bracketleftbigg∆xi
∆ti+γi−1/bracketrightbigg2
∆ti/parenrightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x0=x∗(t), γ0=γ∗
t=
limN/productdisplay
i=1/integraldisplay∞
−∞d∆yi√2π∆tiexp/parenleftbigg
−(∆yi)2
2∆ti/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
y0=x∗(t)=
limN/productdisplay
i=1/integraldisplay∞
−∞dyi√2π∆tiexp/parenleftbigg
−(∆yi)2
2∆ti/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
y0=x∗(t)=
/integraldisplay
Dyexp/parenleftbigg
−1
2/integraldisplayT
t˙y2(s)ds/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
y(t)=x∗(t)= 1. (123)
The key points in the above computation are the following. First, we c an change
the integration variables from xito ∆xi=xi−xi−1,i= 1,...,N. Note that
xi=x0+i/summationdisplay
k=1∆xk, (124)
29",2014-05-07T01:13:23Z,ft ft dx e ft ft dx e ft dx e ft dx e to dx into dy with  dx dy to t dx e dye  t rst note
paper_qf_22.pdf,31,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","so that∂xi
∂∆xj=θij, (125)
whereθij= 0 ifi < j, andθij= 1 ifi≥j. This implies that the corresponding
measure is trivial:
det(θij) = 1. (126)
Next, consider the change of variables from ∆ xito ∆yi. We have
Mij≡∂∆yi
∂∆xj=δij+∆ti∂γi−1
∂∆xj=
δij+∆tiN/summationdisplay
k=1∂γi−1
∂xk∂xk
∂∆xj=
δij+∆tiN/summationdisplay
k=j∂γi−1
∂xk. (127)
Note, however, that γi−1is independent of xkwithk≥i. This implies that Mij= 0
ifi < j, andMii= 1 (that is, Mijis a Jordanian matrix with unit diagonal
elements). It then follows that
det(Mij) = 1, (128)
so that the measure corresponding to the change of variables fro m ∆xito ∆yiis also
trivial. Finally, we can change variables from ∆ yitoyialso with a trivial measure.
Notethatoncewechangevariablesfrom xito∆xitheboundarycondition x0=x∗(t)
becomes immaterial, and it remains such upon changing variables from ∆xito ∆yi
toyi. This also completes our proof of (107).
Thus, we see that ζtis indeed given by (118). This implies that ζtis aP-
martingale (see below), and the measures QandPare equivalent. More precisely,
we must impose a non-trivial condition on γt. In particular, note that the SDE for
ζtis given by:
dζt=−γtζtdWt, (129)
so that the volatility of ζtis−γtζt, and the drift is zero. So ζtis a stochastic process
if (this is a technical condition)
/integraldisplayt
0γ2
sζ2
sds (130)
is ﬁnite (with probability 1).
We can now show that /tildewiderWtis aQ-Brownian motion. Clearly, /tildewiderWtis continuous,
and/tildewiderW0= 0. Letf(/tildewiderWt) be a deterministic function of /tildewiderWt. Then we have ( y(s)≡
x(s)+/integraltexts
0γs′ds′,s∈[0,t]):
/an}bracketle{tf(/tildewiderWt)/an}bracketri}htQ=/an}bracketle{tζtf(/tildewiderWt)/an}bracketri}htP=/integraldisplay
Dyexp(−S[y;0,t])f(y) =/an}bracketle{tf(Wt)/an}bracketri}htP,(131)
30",2014-05-07T01:13:23Z, next  mi note  mi mi mi is jordaniait mi nally note that once  e variablfrom    and are  i so   is brownish   is    t  dye  
paper_qf_22.pdf,32,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","which, in particular, implies that /tildewiderWtis a normal N(0,t) underQjust asWtis under
the measure P.
Next, let us deﬁne a process:
/tildewideZs,s+t≡/tildewiderWs+t−/tildewiderWs=Zs,s+t+/integraldisplays+t
sγs′ds′, (132)
where
Zs,s+t≡Ws+t−Ws. (133)
Then we have ( y(τ)≡x(s)+/integraltextτ
sγs′ds′,τ∈[s,s+t]):
/an}bracketle{tf(/tildewideZs,s+t)/an}bracketri}htQ,Fs=ζ−1
s/an}bracketle{tζs+tf(/tildewideZs,s+t)/an}bracketri}htP,Fs=/integraldisplay
Dyexp(−S[y;s,s+t])f(y(s+t)−y(s))|y(s)=x∗(s)=
/an}bracketle{tf(Zs,s+t)/an}bracketri}htP,Fs, (134)
so that the process /tildewideZs,s+tunderQbehaves the same way as the process Zs,s+tunder
P. Thus,/tildewiderWtis indeed a Q-Brownian motion.
9 Continuous Martingales
A stochastic process Mtis amartingale w.r.t. a measure Pif and only if for all
t≥0 the expectation /an}bracketle{t|Mt|/an}bracketri}htPis ﬁnite, and
/an}bracketle{tMt/an}bracketri}htP,Fs=Ms,0≤s≤t . (135)
That is, a martingale is expected to be driftless.
Just as in the discrete case, we have the tower law for conditional expectations:
/an}bracketle{t/an}bracketle{tXT/an}bracketri}htP,Ft/an}bracketri}htP,Fs=/an}bracketle{tXT/an}bracketri}htP,Fs,0≤s≤t≤T . (136)
This implies that the process
Nt≡ /an}bracketle{tXT/an}bracketri}htP,Ft (137)
is aP-martingale provided that /an}bracketle{t|XT|/an}bracketri}htPis ﬁnite. The fact that this last condition
is necessary as well as suﬃcient can be seen as follows. First, note t hatNT=XT.
However, we must have ﬁnite /an}bracketle{t|NT|/an}bracketri}htP=/an}bracketle{t|XT|/an}bracketri}htP. Next, note that
|Nt|=|/an}bracketle{tXT/an}bracketri}htP,Ft| ≤ /an}bracketle{t|XT|/an}bracketri}htP,Ft≤ /an}bracketle{t|XT|/an}bracketri}htP. (138)
This then implies that |Nt|is bounded by /an}bracketle{t|XT|/an}bracketri}htP, and, therefore, so is its expecta-
tion.
Note that a P-Brownian motion Wtis aP-martingale. To check the ﬁrst condi-
tion, recall that for any process Atwe have
/an}bracketle{tAt/an}bracketri}ht2
P≤ /an}bracketle{tA2
t/an}bracketri}htP. (139)
31",2014-05-07T01:13:23Z, is just  is next ws ws ws ws tfs fs dye  fs beh  is brownish continuous martalmt is  mt is mt fs ms that just ft fs fs  nt ft is t rst next nt ft ft  nt note brownish  is to at  at
paper_qf_22.pdf,33,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","This implies that
/an}bracketle{t|Wt|/an}bracketri}htP≤/radicalbig
/an}bracketle{t|Wt|2/an}bracketri}htP=√
t . (140)
Furthermore,
/an}bracketle{tWt/an}bracketri}htP,Fs=/an}bracketle{tWs/an}bracketri}htP,Fs+/an}bracketle{t(Wt−Ws)/an}bracketri}htP,Fs=Ws. (141)
Thus,Wtis indeed a P-martingale.
9.1 Driftlessness
Next, consider a general stochastic process Xt:
dXt=σtdWt+µtdt . (142)
Let us show that Xtcan be a P-martingale only if µt≡0. From the deﬁnition of a
martingale we have
0 =/an}bracketle{tdXt/an}bracketri}htP,Ft=/an}bracketle{tσtdWt/an}bracketri}htP,Ft+/an}bracketle{tµtdt/an}bracketri}htP,Ft=σt/an}bracketle{tdWt/an}bracketri}htP,Ft+µtdt=µtdt .(143)
Note thatWtis previsible, but dWtis not.
Next, suppose that Xtis a driftless stochastic process:
dXt=σtdWt. (144)
Then we have
Xt=X0+/integraldisplayt
0σtdWt. (145)
Note thatXtis a previsible process: Xt=X(Ft). Let us compute the conditional
expectation:
/an}bracketle{tXt/an}bracketri}htP,Fs=X(Fs)+/integraldisplay
Dxexp(−S[x;s,t])/integraldisplayt
sσs′˙x(s′)ds′/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x(s)=x∗(s), σs=σ∗s=
X(Fs)+lim/bracketleftiggN/productdisplay
i=1/integraldisplay∞
−∞dxi√2π∆tiexp/parenleftbigg
−∆xi2
∆ti/parenrightbigg/bracketrightiggN/summationdisplay
k=1σk−1∆xk/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x0=x∗(s), σ0=σ∗s=
X(Fs) =Xs. (146)
Here we have taken into account that we can change integration va riables from xi
to ∆xiwith a trivial measure. Then integration over the ∆ xkvariable makes the
corresponding kth term in the sum vanish as σk−1is independent of ∆ xk.
Thus, a driftless stochastic process is a martingale subject to the condition that
/an}bracketle{t|Xt|/an}bracketri}htPis ﬁnite. We can guarantee this as follows. Let Zt≡Xt−X0, and/an}bracketle{tZt/an}bracketri}htP=
/an}bracketle{tXt/an}bracketri}htP−X0= 0. On the other hand,
/an}bracketle{t|Xt|/an}bracketri}htP≤/radicalig
/an}bracketle{tX2
t/an}bracketri}htP=/radicalig
/an}bracketle{tZ2
t/an}bracketri}htP+X2
0, (147)
32",2014-05-07T01:13:23Z,   furtr  fs ws fs  ws fs ws   is drt less ness next xt xt   utc afrom xt ft  ft ft  ft note  is  is next xt is xt  txt  note xt is xt ft  xt fs fs dx e fs fs xs re t xt is   xt xt oxt
paper_qf_22.pdf,34,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","so that/an}bracketle{t|Xt|/an}bracketri}htPis ﬁnite if /an}bracketle{tZ2
t/an}bracketri}htPis ﬁnite. On the other hand,
Z2
t=/integraldisplayt
0d(Zs)2= 2/integraldisplayt
0ZsσsdWs+/integraldisplayt
0σ2
sds . (148)
SinceZsσsis a previsible process,
/angbracketleftbigg/integraldisplayt
0ZsσsdWs/angbracketrightbigg
P= 0, (149)
so we have
/an}bracketle{tZ2
t/an}bracketri}htP=/angbracketleftbigg/integraldisplayt
0σ2
sds/angbracketrightbigg
, (150)
and if the r.h.s. of this equation is ﬁnite, then so is /an}bracketle{t|Xt|/an}bracketri}htP. Here we note that this
condition is suﬃcient but not necessary.
Thus, consider the following SDE:
dXt=σtXtdWt. (151)
The solution to this SDE is given by
Xt=X0exp/parenleftbigg/integraldisplayt
0σsdWs−1
2/integraldisplayt
0σ2
sds/parenrightbigg
. (152)
Note that |Xt|= sign(X0)Xt, so that the requirement that /an}bracketle{t|Xt|/an}bracketri}htPbe ﬁnite is
satisﬁed. Thus, (152) is an exponential martingale.
Here the following remarks are in order. Consider a driftless proces s
dXt=ρtdWt. (153)
In general the condition on /an}bracketle{t|Xt|/an}bracketri}htPis non-trivial. Let us formally rewrite this SDE
as follows:
dXt=σtXtdWt, (154)
whereρt≡σtXt. But the exponential martingale (152) satisﬁes the condition on
/an}bracketle{t|Xt|/an}bracketri}htPregardless of σt. The reason for this apparent discrepancy is that not all
SDEs of the form (153) can be rewritten in the form (154). For inst ance, ifρt≡ρ
is constant, the solution to (153) is simply Xt=ρWt+X0. This process can take
(with probability 1) both positive as well as negative values, while the e xponential
martingale (152), which is the solution to (154), would take only eithe r positive or
negative values (depending on whether X0in (152) is positive or negative).
On the other hand, suppose in (154) we take
σt=1
Wt+β(155)
33",2014-05-07T01:13:23Z,xt is is ows since ws xt re  xt td  t xt ws note xt xt xt be  re consir xt  ixt is  xt td  xt but xt regardless t for xt   o
paper_qf_22.pdf,35,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","with constant β. The formal solution to this SDE is then given by
Xt=X0exp(Yt) (156)
where
dYt=σtdWt−1
2σ2
tdt=dWt
Wt+β−1
2dt
(Wt+β)2. (157)
The solution to this SDE is given by (we are assuming the boundary con dition
Y0= 0):
Yt= ln|Wt+β|−ln|β|. (158)
Thus, we have
Xt=X0/vextendsingle/vextendsingle/vextendsingle/vextendsingleWt+β
β/vextendsingle/vextendsingle/vextendsingle/vextendsingle. (159)
Itisclearthatthisis notamartingale. Thereasonwhythisoccurredisthefollowing.
Note that the volatility of Xtis given by
ρt=σtXt=X0
|β|sign(Wt+β), (160)
which is discontinuous (albeit the requirement that/integraltextt
0ρ2
sdsbe ﬁnite is formally
satisﬁed) and not previsible. This is why the above formal manipulatio ns did not
yield a martingale.
9.2 Martingale Representation Theorem
From the above discussion we have the continuous version of the martingale repre-
sentation theorem :
SupposeMtis aQ-martingale whose volatility σtis always non-vanishing (with
probability 1). Then if Ntis any other Q-martingale, there exists an F-previsible
processφsuch that/integraltextt
0φ2
sσ2
sdsis ﬁnite (with probability 1), and
Nt=N0+/integraldisplayt
0φsdMs. (161)
Further,φsis (essentially) unique.
This can be seen as follows. Since Mtis aQ-martingale, we have
dMt=σtdWt. (162)
Similarly, since Ntis aQ-martingale, we have
Nt=ρtdWt, (163)
whereρtis the volatility of Nt. Then the process φis given by φt=ρt/σt, which is
well deﬁned as σtnever vanishes. Moreover, since/integraltextt
0ρ2
sdsis ﬁnite, then/integraltextt
0φ2
sσ2
sdsis
also ﬁnite.
34",2014-05-07T01:13:23Z,t xt     t   xt  it is ear that  is t reasow  occurred is t followi note xt is xt   martale representattorem from suppose mt is tnt is nt ms furtr  since mt is mt  simarly nt is nt  nt tover
paper_qf_22.pdf,36,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","10 Continuous Hedging
Suppose we have a stock Stand a cash bond Bt. We will assume that the latter is
deterministic. To replicate a claim Xat the maturity time T, we have the following
hedging strategy.
First, deﬁne a discounted stock process Zt≡B−1
tSt. We need a measure Qthat
makesZtinto a martingale with positivevolatility.
Next, deﬁne the process Et≡ /an}bracketle{tB−1
TX/an}bracketri}htQ,Ft. This process is a Q-martingale. It
thenfollowsfromthemartingalerepresentationtheoremthatthe reexistsaprevisible
processφtsuch that
dEt=φtdZt. (164)
Also, deﬁne the process
ψt≡Et−φtZt. (165)
This process is also previsible.
At timethold a portfolio ( φt,ψt) consisting of φtunits of stock Sandψtunits
of the cash bond B. The value of this portfolio is
Vt=φtSt+ψtBt=BtEt. (166)
Let us show that this portfolio is self-ﬁnancing .
First, note that
VT=BTET=X , (167)
so at timeTit replicates the claim X. Furthermore,
dVt=BtdEt+EtdBt=
φtBtdZt+(ψt+φtZt)dBt=
φtdSt+ψtdBt. (168)
The price of the claim Xat timet, therefore, is given by
Vt=Bt/an}bracketle{tB−1
TX/an}bracketri}htQ,Ft. (169)
In particular, V0=B0/an}bracketle{tB−1
TX/an}bracketri}htQ.
10.1 Change of Measure in the General One-Stock Model
Let us consider a general one-stock model:
dBt=rtBtdt , (170)
dSt=St[σtdWt+µtdt], (171)
whereσt,µt,rtare general previsible processes.
35",2014-05-07T01:13:23Z,continuous edgi suppose stand   to at rst st  that into next et ft  it et also et  at and t vt st   et  rst it furtr vt td et td  td  st  t at vt  ft ie measure genl one stock m   dt st st 
paper_qf_22.pdf,37,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","The above equations have the following solutions:
Bt=B0exp/parenleftbigg/integraldisplayt
0rsds/parenrightbigg
, (172)
St=S0exp/parenleftbigg/integraldisplayt
0σsdWs+/integraldisplayt
0/bracketleftbigg
µs−1
2σ2
s/bracketrightbigg
ds/parenrightbigg
. (173)
The discounted stock process is given by:
Zt=B−1
tSt=S0exp/parenleftbigg/integraldisplayt
0σsdWs+/integraldisplayt
0/bracketleftbigg
µs−rs−1
2σ2
s/bracketrightbigg
ds/parenrightbigg
.(174)
We need to change the measure from PtoQso thatZtis a martingale. Let us
deﬁne
/tildewiderWt≡Wt+/integraldisplayt
0γsds , (175)
where
γt≡µt−rt
σt. (176)
Then we have
Zt=B−1
tSt=S0exp/parenleftbigg/integraldisplayt
0σsd/tildewiderWs−1
2/integraldisplayt
0σ2
sds/parenrightbigg
. (177)
Note thatZtis an exponential Q-martingale, where the measure Qis such that/tildewiderWt
is aQ-Brownian motion. The corresponding Radon-Nikodym process is giv en by:
ζt= exp/parenleftbigg
−/integraldisplayt
0γsdWs−1
2/integraldisplayt
0γ2
sds/parenrightbigg
. (178)
Usingζtwe can obtain QfromP.
10.2 Terminal Value Pricing
Let us assume that Bt(and, therefore, rt) isa deterministic function(independent of
which particular history Ftthe stock follows up to time t). Also, let us assume that
the log-volatility σtis a deterministic function: σt=σ(St,t). Then the following is
true.
Suppose the derivative Xis given by f(ST), wheref(x) is some deterministic
function. Then the value of the derivative at time tis given by V(St,t), where
V(z,t)≡Bt/an}bracketle{tB−1
Tf(ST)/an}bracketri}htQ, St=z. (179)
The process φtis then given by
φt=∂zV(z,t)|z=St. (180)
36",2014-05-07T01:13:23Z,t  st ws t st ws  to so tis    tst ws note tis is  brownish t radody ws usi from terminal value prici   ft t also st tsuppose is tst  st t st
paper_qf_22.pdf,38,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","This can be seen as follows.
First, note that
dSt=St/bracketleftig
σtd/tildewiderWt+rtdt/bracketrightig
. (181)
Next,
dVt=dV(St,t) =∂zV(St,t)dSt+1
2∂2
zV(St,t)(dSt)2+∂tV(St,t)dt=
σtSt∂zV(St,t)d/tildewiderWt+/bracketleftbigg
rtSt∂zV(St,t)+1
2σ2
tS2
t∂2
zV(St,t)+∂tV(St,t)/bracketrightbigg
dt . (182)
On the other hand,
dVt=φtdSt+ψtdBt=
σtStφtd/tildewiderWt+rt[φtSt+ψtBt]dt=
σtStφtd/tildewiderWt+rtVtdt . (183)
Comparing these two expressions we see that φtis indeed given by (180). Moreover,
we have the following partial diﬀerential equation (PDE) for V(z,t):
rtz∂zV(z,t)+1
2σ2
tz2∂2
zV(z,t)+∂tV(z,t)−rtV(z,t) = 0 (184)
with the boundary condition V(z,T) =f(z). This PDE, which is called the Black-
Scholes equation , gives another way of solving the pricing problem. Using the so-
calledGreeks
Θ≡∂V
∂t, (185)
∆≡∂V
∂S, (186)
Γ≡∂2V
∂S2, (187)
ν≡∂V
∂σ, (188)
ρ≡∂V
∂r, (189)
where the last two deﬁnitions are given for the sake of completenes s, we have
Θ+rS∆+1
2σ2S2Γ =rV . (190)
The ﬁve Greeks above (and there are more) are called Theta, Delta , Gamma, Vega
and Rho.
37",2014-05-07T01:13:23Z, rst st st  next vt st st st st st st st st  st st st st ovt st  st  st  st  vt dt ari o  schools usi greek t greek a lta gaa vega rho
paper_qf_22.pdf,39,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","10.3 A Diﬀerent Formulation
Suppose we have a general stock model
dBt=rtBtdt , (191)
dSt=σtdWt+µtdt . (192)
The discounted stock process is given by Zt=B−1
tSt, and we have
dZt=B−1
t[σtdWt+(µt−rtSt)dt]. (193)
The shiftγtthat will make Ztinto a martingale is given by
γt=µt−rtSt
σt. (194)
The corresponding change of measure, however, is not always pos sible (we will dis-
cuss an example of this in the following). The reason why is that the vo latilityσt
might sometimes be vanishing. Suppose, however, that the volatility never vanishes.
Then we can ﬁnd the martingale measure Q:
dZt=B−1
tσtd/tildewiderWt, (195)
dSt=σtd/tildewiderWt+rtStdt . (196)
In the following we will assume that both rtandσtare deterministic.
We have:
V(z,t)≡Bt/an}bracketle{tB−1
Tf(ST)/an}bracketri}htQ, St=z. (197)
We, therefore, have:
dVt=dV(St,t) =∂zV(St,t)dSt+1
2∂2
zV(St,t)(dSt)2+∂tV(St,t)dt=
σt∂zV(St,t)d/tildewiderWt+/bracketleftbigg
rtSt∂zV(St,t)+1
2σ2
t∂2
zV(St,t)+∂tV(St,t)/bracketrightbigg
dt .(198)
On the other hand,
dVt=φtdSt+ψtdBt=
σtφtd/tildewiderWt+rt[φtSt+ψtBt]dt=
σtφtd/tildewiderWt+rtVtdt . (199)
We, therefore, have
φt=∂zV(St,t), (200)
ψt=B−1
t[Vt−φtSt] =B−1
t[V(St,t)−St∂zV(St,t)],(201)
and the following PDE for V(z,t):
rtSt∂zV(St,t)+1
2σ2
t∂2
zV(St,t)+∂tV(St,t)−rtV(St,t) = 0 (202)
with the boundary condition V(z,T) =f(z).
38",2014-05-07T01:13:23Z,di tsuppose   dt st  t st  st t into st t t suppose t st  std i  st  vt st st st st st st st  st st st st ovt st   st   vt dt  st vt st st st st st st st st st
paper_qf_22.pdf,40,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","10.4 An Instructive Example
As an example consider the following stock model (we will assume that the cash
bond is constant, that is, we have zero interest rates):
St=S0+αW2
t−βt (203)
with constant αandβ. The corresponding SDE is
dSt= 2αWtdWt+(α−β)dt , (204)
so that the volatility and the drift are given by:
σt= 2αWt, (205)
µt=α−β . (206)
The shiftγtis then
γt=α−β
2αWt, (207)
which is ill-deﬁned at t= 0. It is then not diﬃcult to see that the change of measure
via the Radon-Nikodym process is not possible – the measures QandPare not
equivalent in this case.
Suppose we are lucky, and β=α, so thatStis a martingale to begin with.
Still, to hedge a generic claim we would need to use the martingale repre sentation
theorem to determine the previsible process φt. However, since σtvanishes at t= 0,
for a generic claim this might not be possible. Nonetheless, we can still try to hedge
claims of the form X=f(ST) using the PDE approach to pricing.
In the above example we have (restricting to times t≤T <S 0/αso thatSt>0):
St=S0+α(W2
t−t), (208)
σ2
t= 4α2W2
t= 4α(St−S0+αt). (209)
The corresponding pricing PDE then reads:
2α[z−S0+αt]∂2
zV(z,t)+∂tV(z,t) = 0 (210)
with the boundary condition V(z,T) =f(z).
This PDE can be simpliﬁed as follows. Let
y≡z−S0+αt , (211)
andV(z,t)≡U(y,t). Then we have:
α/bracketleftbig
2y∂2
yU(y,t)+∂yU(y,t)/bracketrightbig
+∂tU(y,t) = 0 (212)
with the boundary condition U(y,T) =f(y+S0−αT).
39",2014-05-07T01:13:23Z,ainstruive  as st t st td   t  it radody and are suppose st is stl nonetless ist st st t   tn
paper_qf_22.pdf,41,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Note that for the allowed stock values yis non-negative. We can therefore
perform the following change of variables:
y≡αx2. (213)
LetV(y,t)≡C(x,t). Then we have the following PDE:
1
2∂2
xC(x,t)+∂tC(x,t) = 0 (214)
with the boundary condition C(x,T) =f(αx2+S0−αT).
Note that this is nothing but the terminal value pricing in terms of Wt– the
variablexis simply the value of Wt. So in this example we might as well price the
option directly via (197) – indeed, in this case we know St=S(Wt,t) explicitly.
However, in general we might not have an explicit solution of the SDE f orSt, in
which casewe canuse thepricing PDE(202)(and, ifnecessary, solv e itnumerically).
Let us determine V(z,t) in the above example directly via (197). We have (in
this case P=Q):
V(z,t) =/an}bracketle{tf(ST)/an}bracketri}htP, St=z. (215)
Note that
WT=Wt+Zt,t+(T−t), (216)
whereZt,t+(T−t)is a normal N(0,T−t), and is independent of Ft. Let the values of
Zt,t+(T−t)bex. Then we have
ST=S0+α/bracketleftbig
(Wt+x)2−T/bracketrightbig
=
St+α/bracketleftbig
x2+2Wtx−(T−t)/bracketrightbig
=
St+α/bracketleftigg
x2+2ǫx/radicalbigg
St−S0+αt
α−(T−t)/bracketrightigg
, (217)
whereǫ=±1 gives two values of Wtcorresponding to a given St.
The pricing function V(z,t) is given by:
V(z,t) =/integraldisplay∞
−∞dx/radicalbig
2π(T−t)exp/parenleftbigg
−x2
2(T−t)/parenrightbigg
×
×f/parenleftigg
z+α/bracketleftigg
x2+2ǫx/radicalbigg
z−S0+αt
α−(T−t)/bracketrightigg/parenrightigg
=
/integraldisplay∞
−∞dy√
2πexp/parenleftbigg
−y2
2/parenrightbigg
×
×f/parenleftigg
z+α(T−t)/bracketleftigg
y2+2ǫy/radicaligg
z−S0+αt
α(T−t)−1/bracketrightigg/parenrightigg
.(218)
40",2014-05-07T01:13:23Z,note   tnote   so st  st   st note  ft  t st tx st st  correspondi st t
paper_qf_22.pdf,42,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Note thatV(z,t) is the same for both ǫ=±1. Also,V(z,T) =f(z) as it should be.
We can obtain the same result from the PDE (214). The solution to th is PDE
with the appropriate boundary condition is given by:
C(x,t) =/integraldisplay∞
−∞dx′K(x′−x,T−t)f/parenleftbig
α(x′)2+S0−αT/parenrightbig
,(219)
whereK(y,τ) is the solution to the PDE
1
2∂2
yK(y,τ) =∂τK(y,τ) (220)
with the boundary condition K(y,0) =δ(y). This solution is given by:
K(y,τ) =1√
2πτexp/parenleftbigg
−y2
2τ/parenrightbigg
. (221)
We, therefore, have
C(x,t) =/integraldisplay∞
−∞dx′
/radicalbig
2π(T−t)exp/parenleftbigg
−(x′−x)2
2(T−t)/parenrightbigg
f/parenleftbig
α(x′)2+S0−αT/parenrightbig
.(222)
It is not diﬃcult to see that this is the same as V(z,t) we obtained above once we
go back from xtozviaz=αx2+S0−αt.
10.5 The Heat Kernel Method
In the previous subsection we solved a pricing PDE using the heat kernel method .
It can also be used in the general case. Thus, let us go back to the g eneral pricing
PDE (184):
rtz∂zV(z,t)+1
2σ2
tz2∂2
zV(z,t)+∂tV(z,t)−rtV(z,t) = 0 (223)
with the boundary condition V(z,T) =f(z). Let us simplify this equation as
follows. Let
V(z,t)≡exp/parenleftbigg
−/integraldisplayT
trsds/parenrightbigg
U(z,t). (224)
The PDE for U(z,t) is given by:
rtz∂zU(z,t)+1
2σ2
tz2∂2
zU(z,t)+∂tU(z,t) = 0 (225)
with the boundary condition U(z,T) =f(z). Next, let us change variables from
(z,t) to (y,t), where
y≡exp/parenleftbigg/integraldisplayT
trsds/parenrightbigg
z . (226)
41",2014-05-07T01:13:23Z,note also  t   it t at kernel method iit    t next
paper_qf_22.pdf,43,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","LetU(z,t) =Y(y,t). Then we have:
∂tU=∂tY+∂yY∂ty=∂tY−rtexp/parenleftbigg/integraldisplayT
trsds/parenrightbigg
∂yY , (227)
∂zU= exp/parenleftbigg/integraldisplayT
trsds/parenrightbigg
∂yY , (228)
∂2
zU= exp/parenleftbigg
2/integraldisplayT
trsds/parenrightbigg
∂2
yY . (229)
The PDE for Y(y,t) is given by (this is the diﬀusion equation ):
1
2D(y,t)∂2
yY(y,t)+∂tY(y,t) = 0 (230)
with the boundary condition U(y,T) =f(y). Here
D(y,t)≡y2σ2
t=y2σ2(z,t) =y2σ2/parenleftbigg
exp/parenleftbigg
−/integraldisplayT
trsds/parenrightbigg
y,t/parenrightbigg
.(231)
The solution to the PDE for Y(y,t) is given by:
Y(y,t) =/integraldisplay∞
−∞dy′K(y′,y;T−t)f(y′). (232)
Theheat kernel K(x′,x;τ) is the solution to the equation
1
2D(x,T−τ)∂2
xK(x′,x;τ) =∂τK(x′,x;τ) (233)
with the boundary condition K(x′,x;0) =δ(x′−x). In terms of this heat kernel we
can write the pricing function V(z,t) as follows:
V(z,t) = exp/parenleftbigg
−/integraldisplayT
trsds/parenrightbigg/integraldisplay∞
−∞dy′K/parenleftbigg
y′,exp/parenleftbigg/integraldisplayT
trsds/parenrightbigg
z;T−t/parenrightbigg
f(y′).
(234)
We can subsequently use V(z,t) to compute the processes φtandψt, and hedge the
derivativef(ST).
11 European Options: Call, Put and Binary
A call option is a right (but not obligation) to buy a stock at the matur ity timeT
for the strike price kagreed on at time t= 0. So the claim for the call option is
given by:
fc(ST,k) = (ST−k)+. (235)
42",2014-05-07T01:13:23Z, tt re t t at i aoptns call put binary so
paper_qf_22.pdf,44,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","The price of the call option is given by:
Vc
t(k) =Bt/an}bracketle{tB−1
Tfc(ST,k)/an}bracketri}htQ,Ft=Bt/an}bracketle{tB−1
T(ST−k)+/an}bracketri}htQ,Ft.(236)
Here (x)+=xifx>0, and (x)+= 0 ifx≤0.
A put option is a right (but not obligation) to sell a stock at the matur ity time
Tfor the strike price kagreed on at time t= 0. So the claim for the put option is
given by:
fp(ST,k) = (k−ST)+. (237)
The price of the put option is given by:
Vp
t(k) =Bt/an}bracketle{tB−1
Tfp(ST,k)/an}bracketri}htQ,Ft=Bt/an}bracketle{tB−1
T(k−ST)+/an}bracketri}htQ,Ft.(238)
Note thatfc(ST,k)−fp(ST,k) =ST−k. Consequently, we have
Vc
t(k)−Vp
t(k) =Bt/an}bracketle{tB−1
T(ST−k)/an}bracketri}htQ,Ft=
St−BtB−1
Tk=Vf
t(k), (239)
whereVf
t(k) is the price of the forward with a strike price k. This result is called
theput-call parity .
A binary (digital) option is a derivative which pays $1 at the maturity tim eTif
a stock grows over the strike price kagreed on at time t= 0. So the claim for the
binary option is given by:
fb(ST,k) =θ(ST−k), (240)
whereθ(x) is the Heavyside step function. The price of the binary option is give n
by:
Vb
t(k) =Bt/an}bracketle{tB−1
Tfb(ST,k)/an}bracketri}htQ,Ft=Bt/an}bracketle{tB−1
Tθ(ST−k)/an}bracketri}htQ,Ft. (241)
Note thatd
dx(x)+=θ(x). (242)
This implies that
Vb
t(k) =−∂
∂kVc
t(k) =B−1
tBT−∂
∂kVp
t(k). (243)
Thus, we can determine the price of the binary option from the spec trum of the
prices of the call (put) options.
12 The Black-Scholes Model
The Black-Scholes model is given by:
Bt= exp(rt), (244)
St=S0exp(σWt+µt), (245)
43",2014-05-07T01:13:23Z,t vc  fc ft  ft re for so t vp  fp ft  ft note consequently vc vp  ft st  tk    t so y si t vb  fb ft  ft note  vb vc vp  t  schools mt  schools  st 
paper_qf_22.pdf,45,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","wherer,σ,µare constant.
The ﬁrst step is to deﬁne the discounted stock process:
Zt≡B−1
tSt=S0exp(σWt+[µ−r]t). (246)
The SDE for Ztis given by:
dZt=Zt/bracketleftbigg
σdWt+/parenleftbigg
µ−r+1
2σ2/parenrightbigg
dt/bracketrightbigg
. (247)
We can make Ztinto a martingale via the following change of variable:
/tildewiderWt=Wt+γt=Wt+1
σ/parenleftbigg
µ−r+1
2σ2/parenrightbigg
t . (248)
Notethat/tildewiderWtisaQ-Brownianmotion, wherethemeasure Qisrelatedtotheoriginal
measure Pvia the Radon-Nikodym process
ζt= exp/parenleftbigg
−γWt−1
2γ2t/parenrightbigg
. (249)
Also, note that dZt=σZtd/tildewiderWt.
The next step is to take a claim X=XT, and construct the process
Et=/an}bracketle{tB−1
TX/an}bracketri}htQ,Ft= exp(−rT)/an}bracketle{tX/an}bracketri}htQ,Ft. (250)
This process is a Q-martingale. We can therefore deﬁne
φt≡dEt
dZt, (251)
which is a previsible process.
Finally, the self ﬁnancing portfolio ( φt,ψt) consists of holding φtunits of stock
andψtunits of the cash bond at time t, where
ψt=Et−φtZt. (252)
The price of this portfolio is given by
Vt=φtSt+ψtBt=BtEt. (253)
Thus, the price of the claim Xat timetis:
Vt= exp(−r[T−t])/an}bracketle{tX/an}bracketri}htQ,Ft. (254)
We can use this formula to value various derivatives in the Black-Scho les model.
44",2014-05-07T01:13:23Z,t st  t tis   into    note that  is brownish motis related to t orinal via radody  also td  t et ft ft   et nally et t vt st   et  at vt ft   cho
paper_qf_22.pdf,46,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","12.1 Call Option
For the call option we have X=fc(ST,k) = (ST−k)+. To compute the pricing
function
Vc(z,t,k) = exp(−r[T−t])/an}bracketle{tfc(ST,k)/an}bracketri}htQ, St=z, (255)
let us rewrite STas follows:
ST=S0exp/parenleftbigg
σ/tildewiderWT+/bracketleftbigg
r−1
2σ2/bracketrightbigg
T/parenrightbigg
=Stexp/parenleftbigg
σx+/bracketleftbigg
r−1
2σ2/bracketrightbigg
[T−t]/parenrightbigg
,(256)
wherexstands for the values of the process WT−Wt, which is a normal N(0,T−t),
and is independent of Ft. Then we have:
Vc(z,t,k) =
e−r[T−t]/integraldisplay∞
−∞dx/radicalbig
2π(T−t)exp/parenleftbigg
−x2
2(T−t)/parenrightbigg
×
×/parenleftbigg
zexp/parenleftbigg
σx+/bracketleftbigg
r−1
2σ2/bracketrightbigg
[T−t]/parenrightbigg
−k/parenrightbigg+
=
/integraldisplay∞
x∗dx/radicalbig
2π(T−t)exp/parenleftbigg
−x2
2(T−t)/parenrightbigg
×
×/parenleftbigg
zexp/parenleftbigg
σx−1
2σ2[T−t]/parenrightbigg
−ke−r[T−t]/parenrightbigg
, (257)
where
x∗=1
σ/bracketleftbigg
ln/parenleftbiggk
z/parenrightbigg
−/bracketleftbigg
r−1
2σ2/bracketrightbigg
[T−t]/bracketrightbigg
. (258)
We have:
Vc(z,t,k) =z/integraldisplay∞
x∗−σ(T−t)dx/radicalbig
2π(T−t)exp/parenleftbigg
−x2
2(T−t)/parenrightbigg
−
ke−r[T−t]/integraldisplay∞
x∗dx/radicalbig
2π(T−t)exp/parenleftbigg
−x2
2(T−t)/parenrightbigg
.(259)
Let
Φ(y)≡/integraldisplayy
−∞dy′
√
2πexp/parenleftbigg
−(y′)2
2/parenrightbigg
. (260)
Then
Vc(z,t,k) =zΦ/parenleftigg
ln/parenleftbigz
k/parenrightbig
+/bracketleftbig
r+1
2σ2/bracketrightbig
[T−t]
σ√
T−t/parenrightigg
−
ke−r[T−t]Φ/parenleftigg
ln/parenleftbigz
k/parenrightbig
+/bracketleftbig
r−1
2σ2/bracketrightbig
[T−t]
σ√
T−t/parenrightigg
. (261)
This is the Black-Scholes formula for pricing a European call option.
45",2014-05-07T01:13:23Z,call optfor to vc st as st e  ft tvc  vc  tvc   schools an
paper_qf_22.pdf,47,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","12.2 Put Option
We can use the put-call parity to price a put option with a strike k:
Vp(z,t,k) =Vc(z,t,k)−z+ke−r[T−t]. (262)
Let us introduce the function
/tildewideΦ(y)≡1−Φ(y) =/integraldisplay∞
ydy′
√
2πexp/parenleftbigg
−(y′)2
2/parenrightbigg
. (263)
Then we have:
Vp(z,t,k) =ke−r[T−t]/tildewideΦ/parenleftigg
ln/parenleftbigz
k/parenrightbig
+/bracketleftbig
r−1
2σ2/bracketrightbig
[T−t]
σ√
T−t/parenrightigg
−
z/tildewideΦ/parenleftigg
ln/parenleftbigz
k/parenrightbig
+/bracketleftbig
r+1
2σ2/bracketrightbig
[T−t]
σ√
T−t/parenrightigg
. (264)
This is the Black-Scholes formula for pricing a European put option.
12.3 Binary Option
We can price a binary option with a strike keither directly or using the relation
between the binary and call (put) prices. The result is
Vb(z,t,k) =e−r[T−t]Φ/parenleftigg
ln/parenleftbigz
k/parenrightbig
+/bracketleftbig
r−1
2σ2/bracketrightbig
[T−t]
σ√
T−t/parenrightigg
. (265)
This is the Black-Scholes formula for pricing a European binary option .
13 Hedging in the Black-Scholes Model
In this section we discuss explicit hedges for European options in the Black-Scholes
model. Since these options are of the form X=f(ST), we can use the pricing
functionV(z,t) to compute φtandψt. Thus, we have
Vt=V(St,t), (266)
φt=∂zV(St,t), (267)
ψt=B−1
t[Vt−φtSt] = exp(−rt)[V(St,t)−St∂zV(St,t)].(268)
These formulas are all we need to hedge a European option in the Blac k-Scholes
model.
46",2014-05-07T01:13:23Z,put opt vp vc  tvp   schools abinary opt t vb   schools aedgi  schools mia schools since  vt st st vt st st st st tse alac schools
paper_qf_22.pdf,48,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","13.1 Call Option
For the call option we have:
Vc(z,t,k) =zΦ/parenleftigg
ln/parenleftbigz
k/parenrightbig
+/bracketleftbig
r+1
2σ2/bracketrightbig
[T−t]
σ√
T−t/parenrightigg
−
ke−r[T−t]Φ/parenleftigg
ln/parenleftbigz
k/parenrightbig
+/bracketleftbig
r−1
2σ2/bracketrightbig
[T−t]
σ√
T−t/parenrightigg
. (269)
This gives
φt= Φ/parenleftigg
ln/parenleftbigSt
k/parenrightbig
+/bracketleftbig
r+1
2σ2/bracketrightbig
[T−t]
σ√
T−t/parenrightigg
, (270)
ψt=−ke−rTΦ/parenleftigg
ln/parenleftbigSt
k/parenrightbig
+/bracketleftbig
r−1
2σ2/bracketrightbig
[T−t]
σ√
T−t/parenrightigg
. (271)
Note that the cash bond is always in the borrowing (albeit Btψtis bounded by the
exercise price k). Also, note that at t=Twe have
φT=θ(ST−k), (272)
ψT=−ke−rTθ(ST−k). (273)
So ifST> k, we have one unit of stock (which is worth ST), and we are short
kexp(−rT) units of the cash bond (which is worth −k). We deliver the stock to
the call option holder, receive kdollars for the transaction, and break even. On the
other hand, if ST< k, we are holding no stock or cash bond, neither do we have
any obligations, so we also break even (unless the option holder decid es to exercise
the call option and buy the stock for kdollars, in which case we have a surplus of
k−STdollars at time T– this is because the option holder did not exercise the
option optimally). Finally, if ST=k, some care is needed as the step-function is
discontinuous. We will address this point in detail when we discuss the hedge for
the binary option.
13.2 Put Option
For the put option we have:
Vp(z,t,k) =ke−r[T−t]/tildewideΦ/parenleftigg
ln/parenleftbigz
k/parenrightbig
+/bracketleftbig
r−1
2σ2/bracketrightbig
[T−t]
σ√
T−t/parenrightigg
−
z/tildewideΦ/parenleftigg
ln/parenleftbigz
k/parenrightbig
+/bracketleftbig
r+1
2σ2/bracketrightbig
[T−t]
σ√
T−t/parenrightigg
. (274)
47",2014-05-07T01:13:23Z,call optfor vc  st st note  also  so  odollars nally  put optfor vp
paper_qf_22.pdf,49,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","This gives
φt=−/tildewideΦ/parenleftigg
ln/parenleftbigSt
k/parenrightbig
+/bracketleftbig
r+1
2σ2/bracketrightbig
[T−t]
σ√
T−t/parenrightigg
, (275)
ψt=ke−rT/tildewideΦ/parenleftigg
ln/parenleftbigSt
k/parenrightbig
+/bracketleftbig
r−1
2σ2/bracketrightbig
[T−t]
σ√
T−t/parenrightigg
. (276)
Note that the stock holding is always short. Also, note that at t=Twe have
φT=θ(ST−k)−1, (277)
ψT=ke−rT[1−θ(ST−k)]. (278)
So ifST< k, we are short one unit of stock (which is worth −ST), and we are
holdingkexp(−rT) units of the cash bond (which is worth k). If the put option
holder decides to exercise the option and sell us one unit of stock, w e receive that
one unit of stock, pay the option holder kdollars, and break even. Similarly, we
break even if ST>k(provided that the optionholder exercises the option optimally,
or else we end up with a surplus). Once again, for ST=ksome additional care is
needed – see below.
13.3 Binary Option
For the binary option we have:
Vb(z,t,k) =e−r[T−t]Φ/parenleftigg
ln/parenleftbigz
k/parenrightbig
+/bracketleftbig
r−1
2σ2/bracketrightbig
[T−t]
σ√
T−t/parenrightigg
. (279)
This gives
φt=/bracketleftig/radicalbig
2π(T−t)σStexp(r[T−t])/bracketrightig−1
×
×exp/parenleftigg
−/bracketleftbig
ln/parenleftbigSt
k/parenrightbig
+/bracketleftbig
r−1
2σ2/bracketrightbig
(T−t)/bracketrightbig2
2σ2(T−t)/parenrightigg
, (280)
ψt=e−rT/bracketleftigg
Φ/parenleftigg
ln/parenleftbigSt
k/parenrightbig
+/bracketleftbig
r−1
2σ2/bracketrightbig
[T−t]
σ√
T−t/parenrightigg
−
/bracketleftig/radicalbig
2π(T−t)σ/bracketrightig−1
exp/parenleftigg
−/bracketleftbig
ln/parenleftbigSt
k/parenrightbig
+/bracketleftbig
r−1
2σ2/bracketrightbig
(T−t)/bracketrightbig2
2σ2(T−t)/parenrightigg/bracketrightigg
.(281)
This hedge has an interesting behavior as t→T. SupposeST−k/ne}ationslash= 0. Then we
have
φT= 0, (282)
ψT=e−rTθ(ST−k). (283)
48",2014-05-07T01:13:23Z, st st note also  so  simarly once binary optfor vb  st e st st st  suppose tn
paper_qf_22.pdf,50,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Thus, we are holding no stock. If ST> k, then we are holding exp( −rT) units of
the cash bond (which is worth $1), and we break even if the option ho lder decides
to exercise the option. If ST<k, we are holding no cash bond either, but we have
no obligation in this case, so we also break even.
Suppose, however, ST=k. Then some care is needed. Recall that
St=S0exp/parenleftbigg
σ/tildewiderWt+/bracketleftbigg
r−1
2σ2/bracketrightbigg
t/parenrightbigg
. (284)
So in this case
k=S0exp/parenleftbigg
σ/tildewiderWT+/bracketleftbigg
r−1
2σ2/bracketrightbigg
T/parenrightbigg
, (285)
and
ln/parenleftbiggSt
k/parenrightbigg
+/bracketleftbigg
r−1
2σ2/bracketrightbigg
(T−t) =σ[/tildewiderWt−/tildewiderWT]. (286)
Lett=T−δt. Then/tildewiderWT−/tildewiderWtis itself a Brownian motion with variance δt. For
smallδtwe have
/tildewiderWT−/tildewiderWt=ǫt√
δt , (287)
and
ln/parenleftbiggSt
k/parenrightbigg
+/bracketleftbigg
r−1
2σ2/bracketrightbigg
(T−t) =−σǫt√
δt , (288)
whereǫt=±1. This implies that, as t→T, we have
Φ/parenleftigg
ln/parenleftbigSt
k/parenrightbig
+/bracketleftbig
r−1
2σ2/bracketrightbig
[T−t]
σ√
T−t/parenrightigg
→Φ(−ǫt), (289)
/bracketleftig/radicalbig
2π(T−t)σ/bracketrightig−1
exp/parenleftigg
−/bracketleftbig
ln/parenleftbigSt
k/parenrightbig
+/bracketleftbig
r−1
2σ2/bracketrightbig
(T−t)/bracketrightbig2
2σ2(T−t)/parenrightigg
→
→exp/parenleftbig
−1
2/parenrightbig
/radicalbig
2π(T−t). (290)
This implies that the value of θ(ST−k) in the hedges for the call and put options
is either Φ(+1) or Φ( −1) forST=k, that is, it is random. This, however, does
not pose a problem as this value is previsible . In the case of the binary option,
however, for ST=kto hedge we would need to borrow more and more cash bond
and buy more and more stock as t→T. This is, however, an idealized model, and
in practice, where we do have transaction costs, this singular beha vior is smoothed
out – without going into details, let us simply observe that, for one th ing, buying
more and more stock becomes prohibitive in the presence of transa ctions costs.
14 Price, Time and Volatility Dependence
In this section we discuss how various option prices depend on the st rike pricek,
maturity time Tand volatility σ. LetF=S0exp(rT) be the forward price at t= 0.
49",2014-05-07T01:13:23Z,   suppose trecall st  so st   t is brownish for  st  st st   i price time volatity pennce iand 
paper_qf_22.pdf,51,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","14.1 Call Option
The price of the call option is given by:
Vc=e−rT/bracketleftigg
FΦ/parenleftigg
ln/parenleftbigF
k/parenrightbig
σ√
T+1
2σ√
T/parenrightigg
−kΦ/parenleftigg
ln/parenleftbigF
k/parenrightbig
σ√
T−1
2σ√
T/parenrightigg/bracketrightigg
.(291)
Suppose ln( F/k)<0, and|ln(F/k)| ≫σ√
T. Then the option is out of the money
and unlikely to recover by the maturity time T. In this case Vcis small. On
the other hand, if ln( F/k)≫σ√
T, then the option loses most of its optionality,
and essentially becomes a forward struck at price kfor timeT, whose value is
S0−kexp(−rT).
The maturity Tdependence goes as follows. For small Tthe chances of anything
substantial happening get smaller, and the option value gets closer and closer to the
claim value taken at the current price: ( S0−k)+. On the other hand, as Tgrows
the option price also grows. The reason why is that at time Twe must deliver one
unit of stock if the option is in the money, and the uncertainty in STgrows with
T. In fact, for large Tthe option price approaches S0, and the corresponding hedge
involves buying one unit of stock at time t= 0 – indeed, this is the only way to
guarantee that we will be able to deliver the stock at time Tfor largeT, even if
the stock price becomes very large, which is not unlikely as Tis large (as we get
closer to the maturity time T, however, our hedge is previsibly dictated by the stock
movements). It is important to note that this is true even if the inte rest rate is
vanishing. The reason why is that the call option issuer has an obligat ion to deliver
a volatile instrument (that is, a stock) if the option is in the money at t imeT.
All else being equal, the option is worth more the more volatile the stoc k is. If
σis very small, the option resembles a riskless bond, and is worth/parenleftbig
S0−ke−rT/parenrightbig+,
which is the value of the corresponding forward if the option is in the m oney, and
zero otherwise. If σis very large, then the option is worth S0.
It is instructive to study the volatility dependence when σ√
T≪1 (this is rele-
vant in the case of bonds with volatile interest rates). It is clear tha t the value of the
option is almost independent of σif|ln(F/k)| ≫σ√
T, that is, if the strike price is
too diﬀerent from the forward price. On the other hand, suppose |ln(F/k)|<∼σ√
T.
Letκ≡ln(F/k)/σ√
T. Note that |κ|<∼1. We have:
Vc≈e−rT/bracketleftigg
(F−k)Φ(κ)+(F+k)Φ′(κ)σ√
T
2/bracketrightigg
≈
ke−rT[κΦ(κ)+Φ′(κ)]σ√
T . (292)
In particular, for κ= 0, that is, when the strike price kis exactly equal the forward
priceF, we have
Vc≈ke−rT
√
2πσ√
T . (293)
Note that the value of the option grows linearly with σ.
50",2014-05-07T01:13:23Z,call optt vc suppose tivc is ot pennce for t ogrows t  grows it for tis it t all   it it o note  vc ivc note
paper_qf_22.pdf,52,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","14.2 Put Option
The price of the put option is given by:
Vp=e−rT/bracketleftigg
k/tildewideΦ/parenleftigg
ln/parenleftbigF
k/parenrightbig
σ√
T−1
2σ√
T/parenrightigg
−F/tildewideΦ/parenleftigg
ln/parenleftbigF
k/parenrightbig
σ√
T+1
2σ√
T/parenrightigg/bracketrightigg
.(294)
Suppose ln( F/k)≫σ√
T. Then the option is out of the money and unlikely to
recover by the maturity time T. In this case Vpis small. On the other hand, if
ln(F/k)<0 and|ln(F/k)| ≫σ√
T, then the option loses most of its optionality,
and essentially is equivalent to a shortholding of a forward struck at price kfor time
T. The value of this holding is kexp(−rT)−S0. Note that these facts can also be
deduced from the put-call parity.
The maturity Tdependence goes as follows. For small Tthe chances of anything
substantial happening get smaller, and the option value gets closer and closer to the
claim value taken at the current price: ( k−S0)+. The large Tbehavior is obscured
in the case of a non-zero interest rate. Indeed, the cost now of p ricekfor largeT
goes to zero if r >0, so that the price of the option goes to zero at large Tin this
case. Let us, therefore, consider the r= 0 case. Then the put option price grows
withTjust as in the case of the call option (in fact, the put-call parity te lls us that
Vp=Vc−S0+k). In fact, in the large TlimitVpapproaches k. This is because in
the case of the put option the option issuer must guarantee kdollars at the maturity
even if the stock (which we receive at time Tif the option is in the money) goes very
low, which is not unlikely as Tis large. In fact, the corresponding hedge consists of
buyingkunits of the cash bond at t= 0 (the hedge is previsibly determined as we
get closer to time T) – indeed, this is the only way we can guarantee that we will be
able to make a payment of kdollars at time Teven if by then the stock price goes
down to zero. Thus, the important point here is that the option issu er is receiving
a volatile instrument (that is, a stock) if the option is in the money at t imeT.
All else being equal, the option is worth more the more volatile the stoc k is. If
σis very small, the option resembles a riskless bond, and is worth/parenleftbig
ke−rT−S0/parenrightbig+,
which is the value of a shortholding of the corresponding forward if the option is in
the money, and zero otherwise. If σis very large, then the option is worth ke−rT.
Let us study the volatility dependence when σ√
T≪1. Thus, we have:
Vp≈e−rT/bracketleftigg
(k−F)/tildewideΦ(κ)−(k+F)/tildewideΦ′(κ)σ√
T
2/bracketrightigg
≈
−ke−rT/bracketleftig
κ/tildewideΦ(κ)+/tildewideΦ′(κ)/bracketrightig
σ√
T . (295)
In particular, for κ= 0 we have
Vp≈ke−rT
√
2πσ√
T . (296)
As in the call option case, the value of the put option grows linearly wit hσ.
51",2014-05-07T01:13:23Z,put optt vp suppose tivp is ot note t pennce for t t behr ined ti tjust vp vc ilimit vp approacs  t tis ieve all     vp ivp as
paper_qf_22.pdf,53,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","14.3 Binary Option
The price of the binary option is given by:
Vb=e−rTΦ/parenleftigg
ln/parenleftbigF
k/parenrightbig
σ√
T−1
2σ√
T/parenrightigg
. (297)
Suppose ln( F/k)<0 and|ln(F/k)| ≫σ√
T. Then the option is out of the money
and unlikely to recover by the maturity time T. In this case Vbis small. On the
other hand, if ln( F/k)≫σ√
T, then the option loses most of its optionality, and
essentially becomes a riskless zero-coupon bond with face value $1. Thet= 0 value
of this bond is exp( −rT).
The maturity Tdependence goes as follows. For small Tthe chances of anything
substantial happening get smaller, and the option value gets closer and closer to the
claim value taken at the current price: θ(S0−k). Once again, the large Tbehavior
is obscured in the case of a non-zero interest rate. Let us, there fore, consider the
r= 0 case. Then as Tgrows the option price gets smaller if the option is in the
money att= 0 as the chances that the option ends up out of the money grow wit h
T. On the other hand, if the option is out of the money at t= 0, the option price
at ﬁrst grows with Tas the chances that it ends up in the money grow with T.
Eventually, however, that is, as Tgets larger and larger, the price goes back to zero.
The reason for this is that the option issuer in this case only has an ob ligation to
deliver $1 at time Tif the option is in the money, that is, at the maturity time the
transaction only involves a non-volatile instrument – a cash bond of a ﬁxed amount.
Then if att= 0 the option is out of the money, even if at some later time t∗it ends
up in the money, as more and more time lapses, the chances that it en ds up out of
the money grow, so the option price gets smaller. So for large Tthe binary option
price goes to zero regardless of the starting point.
The volatility dependence is the same as the Tdependence in the case of r= 0
– indeed, in this case σandTappear in the combination σ√
T. It is then clear
that, all else being equal, this volatility dependence remains the same even ifr>0.
Thus, ifσis very small, then the option resembles a riskless bond, and is worth
e−rTθ/parenleftbig
S0−ke−rT/parenrightbig
, which ise−rTif the option is in the money, and zero otherwise.
Ifσis very large, then the option price goes to zero.
Let us study the volatility dependence when σ√
T≪1. We have:
Vb≈e−rT/bracketleftigg
Φ(κ)−Φ′(κ)σ√
T
2/bracketrightigg
. (298)
In particular, for κ= 0 we have
Vb≈e−rT
2/bracketleftigg
1−σ√
T√
2π/bracketrightigg
. (299)
Note that the option value decreases linearly with σin this case, which is consistent
with our discussion above.
52",2014-05-07T01:13:23Z,binary optt vb suppose tivb is ot t pennce for t once behr  tgrows oas eventually gets t t tso t t pennce appear it  t    vb ivb note
paper_qf_22.pdf,54,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","14.4 American Options
An example of an American option is a call option which allows the option h older to
purchase the stock for the strike price kat any time τ, which is called the stopping
time, betweent= 0 and the expiration time T. The option issuer does not know in
advance what τthe investor will use, so the price of this option is given by:
V= max τ/parenleftig/angbracketleftbig
e−rτ(Sτ−k)+/angbracketrightbig
Q/parenrightig
. (300)
That is, theoptionissuer must chargethevaluemaximized over all po ssible stopping
strategies.
In general, if the option purchaser has a set of options A, and receives a payoﬀ
Xaat timeτa≤Tafter choosing a∈A, the option issuer should charge
V= max a∈A/parenleftig/angbracketleftbig
e−rτaXa/angbracketrightbig
Q/parenrightig
(301)
for the option. If the purchaser does not exercise the option opt imally, then the
hedge will produce a surplus (for the issuer) by the expiration date T.
15 Upper and Lower Bounds on Option Prices
For a European or an American call option the price Vcshould not be greater than
that of the stock S0. SupposeVc>S0for a European call option with maturity T.
Then at time t= 0 we sell the call option, and take a long position in one unit of
stock plusVc−S0worth of the cash bond. If at maturity t=Tthe stock is above
the strike,ST> k, we deliver the stock, receive the payment of kdollars, plus we
have (Vc−S0)erTdollars from the cash bond. So we end up making a proﬁt. On
the other hand, if ST<k, then we are left with one unit of stock, which is worth ST,
plus (Vc−S0)erTdollars from the cash bond. Thus, either way we make a proﬁt,
hence arbitrage. The above argument also holds for an American ca ll option where
instead ofTwe use the stopping time τ.
For a European or an American put option the price Vpshould not be more
than the strike price k. SupposeVp>k. Then at time t= 0 we sell the call option
and buyVpdollars worth of the cash bond. At the stopping time τthe latter is
worthVperτ≥k, so even if the option is exercised, we end up making a proﬁt. For
a European put option the bound is actually more severe, in particula r, we must
haveVp≤ke−rT.
For a European call option the price Vcshould not be lower than S0−ke−rT.
SupposeVc<S0−ke−rT. Then we sell one unit of stock, buy the call option, and
holdS0−Vcworth of the cash bond. The latter grows to ( S0−Vc)erT>kat time
T. If the option is in the money, we receive one unit of stock for kdollars (so we no
longer have a short position in the stock), so we make a proﬁt. If ST< k, we still
make a proﬁt as our cash bond is worth more than k, hence arbitrage.
53",2014-05-07T01:13:23Z,optns at that iat after  up lor nds optpricfor avc should suppose vc atvc  t vc dollars so ovc dollars  t  for avp should suppose vp tvp dollars at  for avp for avc should suppose vc tvc worth t vc  
paper_qf_22.pdf,55,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","For a European put option the price Vpshould not be lower than ke−rT−S0.
SupposeVp<ke−rT−S0. Then at time t= 0 we buy one unit of stock as well as
the put option by shorting Vp+S0worth of the cash bond. The latter position is
worth−(Vp+S0)erT>−kat timeT. IfST<k, then we exercise the option, and
end up receiving a payment of kdollars, so we make a proﬁt. On the other hand, if
ST≥k, then we still make a proﬁt by selling the stock, hence arbitrage.
15.1 Early Exercise
Suppose an American call option is deep in the money at time t= 0. Then it is
never optimal to exercise the option prior to the expiration time Tif the investor
plans to keep the stock for the rest of the life of the option. The re ason is that the
later the option is exercised, the less the worth of the strike price a t the initial time
t= 0. If the investor believes that the stock is overpriced, then it mig ht be tempting
to exercise the option early, and sell the stock. The proﬁt from th is would be Sτ−k.
However, even a better proﬁt is made by selling the option itself. Ind eed, the price
of the option Vc
τ≥Sτ−ke−r(T−τ)> Sτ−kforτ < T. Alternatively, the investor
can keep the option, short the stock, and take a long position in Sτworth of the
cash bond. By time Tthis portfolio is worth at least Sτer(T−τ)−k. This is the case
ifST≥k. IfST<k, then the investor makes a better proﬁt Sτer(T−τ)−ST.
Since an American call option should not be exercised early, it then fo llows that
it is worth the same as the corresponding European call option.
In the case of an American put option the situation is somewhat diﬀer ent. If
it is deep in the money, then it should be exercised early. Thus, suppo se the stock
price is almost zero. Then it is better to exercise early as the stock p rice cannot go
negative, and it is better to receive kdollars now than later.
Since there are circumstance such that an American put option sho uld be exer-
cised early, it is always worth more than the corresponding Europea n put option.
16 Equities and Dividends
An equity is a stock that makes periodiccash payments (that is, divid end payments)
to the stock holder. The simplest model would be an equity with contin uous div-
idends. Thus, let the stock price Stand the cash bond follow the Black-Scholes
model. The dividend payment in time dtstarting at time tisρStdt, whereρis the
dividend rate.
The stock itself is not tradable in this model as we must also take into a ccount
the dividend payments up to time t. We, therefore, need to ﬁnd a new process
corresponding to a tradable. Let us consider the following simple por tfolio strategy.
Let us instantaneously reinvest all the dividends by buying more sto ck. Starting
with one unit of stock at time t= 0, at time twe would then have exp( ρt) units of
54",2014-05-07T01:13:23Z,for avp should suppose vp tvp t vp  o exercise suppose tt t  t ivc alternatively by    since ai  tsince  entitidivinds at  stand  schools t std t    starti
paper_qf_22.pdf,56,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","stock, which is worth
/tildewideSt= exp(ρt)St=S0exp(σWt+[µ+ρ]t). (302)
Thisauxiliary process corresponds to a tradable quantity. We can treat /tildewideStas an
eﬀective stock process to hedge claims for St.
Thus, consider a portfolio ( φt,ψt) consisting of the φtunits of stock Standψt
units of the cash bond Bt= exp(rt). This portfolio is equivalent to the portfolio
(/tildewideφt,ψt) consisting of /tildewideφtunits of the reinvested stock /tildewideStandψtunits of the cash bond
Bt, where/tildewideφt= exp(−ρt)φt. The self-ﬁnancing equation reads:
dVt=/tildewideφtd/tildewideSt+ψtdBt=
φtdSt+ψtdBt+ρφtStdt . (303)
Note that in terms of the tilded quantities we have a self-ﬁnancing pr operty as
expected, while in terms of the original quantities we do not as Stis not tradable.
Now we proceed in the standard way. The discounted eﬀective stoc k is/tildewideZt=
B−1
t/tildewideSt, whose SDE is
d/tildewideZt=/tildewideZt/bracketleftbigg
σdWt+/parenleftbigg
µ+ρ+1
2σ2−r/parenrightbigg
dt/bracketrightbigg
. (304)
We must ﬁnd a measure Qthat makes/tildewideZtinto a martingale. In particular, /tildewiderWt=
Wt+γtis aQ-Brownian motion. The corresponding shift is given by:
γ=µ+ρ+1
2σ2−r
σ. (305)
Thus, we have d/tildewideZt=σ/tildewideZtd/tildewiderWt.
To construct a hedging strategy, we introduce the process Et=/an}bracketle{tB−1
TX/an}bracketri}htQ,Ft.
Then the process /tildewideφtis determined from the equation dEt=/tildewideφtd/tildewideZt, whileψt=
Et−/tildewideφt/tildewideZt. The self-ﬁnancing portfolio then consists of holding φt= exp(ρt)/tildewideφtunits
of stockStandψtunits of the cash bond Bt.
What about the derivative price? Note that under the measure Qwe have
St=S0exp/parenleftbigg
σ/tildewiderWt+/bracketleftbigg
r−ρ−1
2σ2/bracketrightbigg
t/parenrightbigg
. (306)
Thus, the eﬀect of the dividends is to replace rwithr−ρ. This tells us that all the
options in the above equity model can be obtained from the corresp onding options
in the vanilla Black-Scholes model via the substitution r→r−ρ. In particular, note
that the forward price is now given by F=S0exp([r−ρ]T), which is the forward
price we should use in the corresponding Black-Scholes formulas for various option
prices (including call, put and binary).
55",2014-05-07T01:13:23Z,st st   auxiary  st as st  stand   stand  t vt st  st  std note st is  t st   that into i  brownish t  td  to et ft tet et t stand  what note  st     schools i schools
paper_qf_22.pdf,57,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","16.1 An Example
Consider the following 5-year contract (so T= 5). The UK FTSEstock index St
pays out 90% of the ratio of the terminal and initial values of FTSE, or it pays
130% if otherwise it would be less, or 180% if otherwise it would be more. The
data isµ= 7%,σ= 15%,ρ= 4%,r= 6.5%. The FTSEindex is composed of 100
diﬀerent stocks, so their separate dividend payments approximat e a continuously
paying stream.
AssumingS0= 1, the claim Xis
X= min{max{1.3,.9ST},1.8}. (307)
This claim can be rewritten using the identities
max{a,b}= (a−b)++b , (308)
min{a,b}=a−(a−b)+. (309)
Thus, we have
X= min {max{1.3,.9ST},1.8}=
min/braceleftbig
(.9ST−1.3)++1.3,1.8/bracerightbig
=
(.9ST−1.3)++1.3−/parenleftbig
(.9ST−1.3)++1.3−1.8/parenrightbig+=
1.3+(.9ST−1.3)+−/parenleftbig
(.9ST−1.3)+−.5/parenrightbig+=
1.3+(.9ST−1.3)+−(.9ST−1.8)+=
1.3+.9/bracketleftbig
(ST−1.44)+−(ST−2)+/bracketrightbig
. (310)
That is,Xis actually the diﬀerence of two FTSEcalls plus some cash. The calls
can be evaluated with the Black-Scholes formula, where for the for ward price we use
F= exp([r−ρ]T).
16.2 Periodic Dividends
Suppose at deterministic times Tithe equity pays a dividend of a fraction ρof the
stock price which was current just before the dividend was paid. Th e stock price
process is modeled as
St=S0(1−ρ)n[t]exp(σWt+µt), (311)
wheren[t]≡max{i:Ti≤t}is the number of dividend payments made by time t.
As usual, we also have a cash bond Bt= exp(rt). Note that the stock process Stis
discontinuous.
As in the case of the continuous dividends, we introduce the auxiliary process
/tildewideSt= [1−ρ]−n[t]St=S0exp(σWt+µt), (312)
56",2014-05-07T01:13:23Z,a consir t stock st t t inx assumi is   that is calls t  schools dic divinds suppose ti t th st  ti as  note st is as st st 
paper_qf_22.pdf,58,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","which would correspond to reinvesting the dividends back into the st ock. We can
now hedge as before in terms of /tildewideStandBt. The corresponding portfolio is ( /tildewideφt,ψt),
which corresponds to the portfolio ( φt,ψt) of the actual stock Stand the cash bond
Bt, whereφt= (1−ρ)−n[t]/tildewideφt. Under the martingale measure Qwe have
St=S0(1−ρ)n[t]exp/parenleftbigg
σ/tildewiderWt+/bracketleftbigg
r−1
2σ2/bracketrightbigg
t/parenrightbigg
. (313)
So all the option prices can be computed using the corresponding Bla ck-Scholes
formulas with the forward price given by F=S0(1−ρ)n[T]exp(rT).
17 Multiple Stock Models
In many cases it is important to model movements of multiple securitie s which are
intertwined in a non-trivial way. Let us consider a model containing nstocksSi
t
that depend on nindependent Brownian motions dWi
t,i= 1,...,n:
dBt=rtBtdt , (314)
dSi
t=Si
tdYi
t, (315)
where the stochastic processes Yi
thave the following SDEs:
dYi
t=n/summationdisplay
j=1σij
tdWj
t+µi
tdt . (316)
Here Σ t≡(σij
t) is the volatility matrix, and µi
tare the drifts.
Note that
/an}bracketle{tdWi
tdWj
t/an}bracketri}htP=δijdt . (317)
This implies that
/an}bracketle{tdYi
tdYj
t/an}bracketri}htP=Mijdt , (318)
where
Mij
t=n/summationdisplay
k=1σik
tσjk
t, (319)
or in the matrix form
Mt= ΣtΣT
t, (320)
where superscript Tdenotes transposition. Note that Mis a symmetric matrix
Mij
t=Mji
twith positive semi-deﬁnite determinant:
det(Mt) = det2(Σt)≥0. (321)
Mijis thecovariance matrix .
57",2014-05-07T01:13:23Z, stand  t stand  unr  st  so bla schools multiple stock mols i si brownish wi   dt si si yi yi yi re note wi  yi yj mi dt mi mt notnote mis mi ji mt mi is
paper_qf_22.pdf,59,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Let us deﬁne:
(σi
t)2≡Mii
t, (322)
ρij
t≡Mij
t
σi
tσj
t. (323)
Note thatρii
t≡1. Hereσi
tare the volatilities for the processes Yi
t(so they are the
log-volatilities for the stock processes Si
t), whileρij
t(i/ne}ationslash=j) is the correlationbetween
the process Yi
tand the process Yj
t.I.e.,ρij
tis thecorrelation matrix . Note that if
any ofσi
tare zero, then Mt(and, therefore, Σ t) has vanishing determinant:
σi
t=n/summationdisplay
k=1/parenleftbig
σik
t/parenrightbig2, (324)
so that ifσi
t= 0, thenσik= 0,k= 1,...,n, and det(Σ t) = 0. As we will see in a
moment, we will need to assume that det(Σ t)/ne}ationslash= 0. Then it follows that all σi
t/ne}ationslash= 0,
and the matrix ρij
tis well deﬁned.
The solution to the above SDEs is given by:
Bt= exp/parenleftbigg/integraldisplayt
0rsds/parenrightbigg
, (325)
Si
t=Si
0exp/parenleftiggn/summationdisplay
j=1/integraldisplayt
0σij
sdWj
s+/integraldisplayt
0/bracketleftigg
µi
s−1
2n/summationdisplay
j=1/parenleftbig
σij
s/parenrightbig2/bracketrightigg
ds/parenrightigg
=
Si
0exp/parenleftiggn/summationdisplay
j=1/integraldisplayt
0σij
sdWj
s+/integraldisplayt
0/bracketleftbigg
µi
s−1
2/parenleftbig
σi
s/parenrightbig2/bracketrightbigg
ds/parenrightigg
. (326)
This shows that σi
tare indeed log-volatilities of Si
t.
Next, we need to ﬁnd a new measure Qunder which allthe discounted stock
pricesZi
t=B−1
tSi
tbecomeQ-martingales simultaneously. Let
/tildewiderWi
t≡Wi
t+/integraldisplayt
0γi
sds . (327)
The corresponding Radon-Nikodym process is
ζt=n/productdisplay
i=1ζi
t, (328)
whereζi
tare individual Radon-Nikodym processes. The discounted stock pr ocesses
have the following SDEs:
dZi
t=Zi
t/bracketleftiggn/summationdisplay
j=1σij
td/tildewiderWj
t+/parenleftigg
µi
t−rt−n/summationdisplay
j=1σij
tγj
t/parenrightigg
dt/bracketrightigg
. (329)
58",2014-05-07T01:13:23Z, mi mi note re yi si yi yj note mt as tt  si si si  si next unr zi si  wi wi t radody radody t zi zi
paper_qf_22.pdf,60,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","To make the drift terms vanish simultaneously, we must make sure th at the matrix
equation
n/summationdisplay
j=1σij
tγj
t=µi
t−rt (330)
has a solution for γi
t. This is guaranteed if the matrix σij
tis invertible. Then the
matrixMij
tis also invertible. Let M−1
tbe the inverse of Mt(note that since Mtis
a symmetric matrix, its left and right inverse matrices coincide). The n we have
γi
t=n/summationdisplay
j,k=1/parenleftbig
M−1
t/parenrightbigjkσji
t/bracketleftbig
µk
t−rt/bracketrightbig
. (331)
The shiftγi
tis referred to as the market price of risk for the stock Si
t.
To construct replicating strategies, we proceed as follows. We intr oduce a Q-
martingale Et≡ /an}bracketle{tB−1
TX/an}bracketri}htQ,Ft. Then-factor martingale representation theorem then
implies that there exist previsible processes φi
tsuch that
Et=E0+n/summationdisplay
i=1/integraldisplayt
0φi
tdZi
t (332)
as long as the matrix Σ tis invertible. Indeed, since Etis aQ-martingale, we have
dEt=n/summationdisplay
j=1Zj
tλj
td/tildewiderWj
t (333)
for some previsible processes λi
t. On the other hand,
n/summationdisplay
i=1φi
tdZi
t=n/summationdisplay
i,j=1Zi
tφi
tσij
td/tildewiderWj
t. (334)
And since Σ tis invertible, the matrix equation
n/summationdisplay
i=1Zi
tφi
tσij
t=Zj
tλj
t (335)
has a solution for Zi
tφi
t. SinceZi
tis non-vanishing, then we also have a solution for
φi
t, which is previsible as Zi
tandλi
tare previsible.
The hedging portfolio then is ( φ1
t,...,φn
t,ψt), whereψt=Et−/summationtextn
i=1φi
tZi
t, so the
value of the portfolio is Vt=BtEt. We then have
dVt=n/summationdisplay
i=1φi
tdSi
t+ψtdBt, (336)
that is, the portfolio is self-ﬁnancing.
59",2014-05-07T01:13:23Z,to  tmi  mt mt is t t si to  et ft tet zi ined et is et ozi zi and zi zi since zi zi t et zi vt  et  vt si 
paper_qf_22.pdf,61,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","17.1 The Degenerate Case
Let us consider a situation where we have NstocksSI
t,I= 1,...,N, but fewer
Brownian motions Wi
t,i= 1,...,n,n<N. Here we would like to discuss this case
in detail.
Thus, we have
dSI
t=SI
tdYI
t, (337)
where
dYI
t=n/summationdisplay
i=1σIi
tdWi
t+µI
tdt . (338)
That is,
SI
t=SI
0exp/parenleftiggn/summationdisplay
i=1/integraldisplayt
0σIi
sdWi
s+/integraldisplayt
0/bracketleftigg
µI
s−1
2n/summationdisplay
i=1/parenleftbig
σIi
s/parenrightbig2/bracketrightigg
ds/parenrightigg
.(339)
In the following we will assume that the matrix Σ t≡(σij) is invertible (see below).
The discounted stock processes have the following SDEs:
dZI
t=ZI
t/bracketleftiggn/summationdisplay
i=1σIi
td/tildewiderWi
t+/parenleftigg
µI
t−rt−n/summationdisplay
i=1σIi
tγi
t/parenrightigg
dt/bracketrightigg
, (340)
where
/tildewiderWi
t=Wi
t+/integraldisplayt
0γi
sds . (341)
To make the drift terms vanish simultaneously, we must make sure th at
n/summationdisplay
i=1σIi
tγi
t=µI
t−rt. (342)
Thus, we have more equations than unknowns. That is, this system is overcon-
strained, and this imposes non-trivial conditions on the drifts. In p articular, we
have
n/summationdisplay
j=1σij
tγj
t=µi
t−rt, (343)
n/summationdisplay
i=1σαiγi=µα
t−rt, (344)
whereI= (i,α),α=n+1,...,N. This implies that
γi
t=n/summationdisplay
j,k=1/parenleftbig
M−1
t/parenrightbigjkσji
t/bracketleftbig
µk
t−rt/bracketrightbig
, (345)
60",2014-05-07T01:13:23Z,t gente case  stocks brownish wi re  ii wi that ii wi ii it ii wi ii wi wi to ii  that i
paper_qf_22.pdf,62,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","and we have the following conditions on the drifts:
µα
t=rt+n/summationdisplay
i,j,k=1σαi/parenleftbig
M−1
t/parenrightbigjkσji
t/bracketleftbig
µk
t−rt/bracketrightbig
. (346)
Note that these conditions come from the requirement that there exist a martingale
measure Q, which is the requirement that there be no arbitrage.
This fact has animportant implication. In particular, with the above r estrictions
onµα
tonlynout of the original Nprocesses are independent. Thus, note that
dYi
t=n/summationdisplay
j=1σij
tdWj
t+µi
tdt . (347)
This implies that
dWi
t=n/summationdisplay
j,k=1/parenleftbig
M−1
t/parenrightbigjkσji
t/bracketleftbig
dYk
t−µk
tdt/bracketrightbig
. (348)
On the other hand,
dYα
t=n/summationdisplay
i=1σαi
tdWi
t+µα
tdt=
n/summationdisplay
i,j,k=1σαi
t/parenleftbig
M−1
t/parenrightbigjkσji
t/bracketleftbig
dYk
t−µk
tdt/bracketrightbig
+µα
tdt=
n/summationdisplay
i,j,k=1σαi
t/parenleftbig
M−1
t/parenrightbigjkσji
t/bracketleftbig
dYk
t−rtdt/bracketrightbig
+rtdt . (349)
That is, once we specify the cash bond, the processes Yα
tare determined via the
processesYi
t. In particular, we have only ( n+ 1) independent (including the cash
bond) tradables in this market, and not ( N+1) independent tradables. The impli-
cation of the above discussion is that we can still hedge all the claims in this market
using the independent ( n+1) tradables.
Finally, let us note that if we have fewer stocks than Brownian motion s that
they depend on, we will not be able to hedge. Another way of phrasin g this is
that in this case the market is not complete, in particular, we have more then one
martingale measure, so that we do not have unique prices for claims a s the system
is underconstrained.
17.2 Arbitrage-free Complete Models
The above discussion illustrates the general result due to Harrison and Pliska. Thus,
suppose we have a market of securities and a cash bond. Then:
•the market is arbitragefree if and only if there is at least one equivale nt martingale
61",2014-05-07T01:13:23Z,note  iprocess yi  wi owi that yi it nally brownish anotr arb it rage e mols t harrisopli ska  tn
paper_qf_22.pdf,63,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","measure (EMM) Q;
•if so, the market is complete if and only if there is exactly one such EMM Qand
no other.
Thus, a market is arbitrage free if there is no guaranteed way of making riskless
proﬁts. An arbitrage opportunity would be a (self-ﬁnancing) trad ing strategy which
starts at zero value and terminates with a positive value at some deﬁ nite dateT.
For simplicity let us assume that we have one stock Stand the cash bond Bt.
Thus, suppose there exists a measure Qsuch that it makes the discounted stock
processZt=B−1
tStinto a martingale. Let us consider a self-ﬁnancing portfolio
(φt,ψt), whose value
Vt=φtSt+ψtBt (350)
satisﬁes the self-ﬁnancing equation
dVt=φtdSt+ψtdBt. (351)
The discounted value of this portfolio, that is, Et≡B−1
tVt, then satisﬁes the follow-
ing SDE:
dEt=−VtB−2
tdBt+B−1
tdVt=
−[φtSt+ψtBt]B−2
tdBt+B−1
t[φtdSt+ψtdBt] =
φt/bracketleftbig
−B−2
tStdBt+B−1
tdSt/bracketrightbig
=
φtdZt. (352)
And sinceZtis aQ-martingale, then so is Et.
Now, suppose our strategy starts from zero value ( V0= 0), and ﬁnishes with a
non-negative payoﬀ ( VT≥0). We have
/an}bracketle{tET/an}bracketri}htQ=E0=B−1
0V0= 0. (353)
However, since VT≥0, thenET≥0 (sinceBT>0). But the Q-expectation of ET
is zero, soET= 0, and this implies that VT= 0 as well. That is, a self-ﬁnancing
strategy cannot make something from nothing if there exists a mar tingale measure
Q. No free lunch!
Next, let us see how completeness, that is, being able to hedge any p ossible
derivative claim with a self-ﬁnancing portfolio, implies uniqueness of th e martingale
measure. Thus, suppose that we can hedge any claim, but we have t wo diﬀerent
martingale measures QandQ′. LetIAbe theindicator function , which takes value
1 if the event Ahas happened in the history FT, and zero otherwise. Consider a
claimXT=BTIA. This is a valid claim, so we should be able to hedge it according
to our assumption. Our discounted stock process Ztis both a Q- andQ′-martingale,
and, therefore, so is the discounted value Etof our self-ﬁnancing portfolio. This then
implies that (note that ET=B−1
TXT=IA)
E0=/an}bracketle{tET/an}bracketri}htQ=/an}bracketle{tIA/an}bracketri}htQ=Q(A), (354)
E0=/an}bracketle{tET/an}bracketri}htQ′=/an}bracketle{tIA/an}bracketri}htQ′=Q′(A). (355)
62",2014-05-07T01:13:23Z,and  afor stand   sust into  vt st  vt st  t et vt et vt  vt st   st  std  st and tis et   but that no next  and  be has consir  our tis et of 
paper_qf_22.pdf,64,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","That is, for an arbitrary event Awe haveQ(A) =Q′(A), so that the two measures
QandQ′are actually identical. So hedging indeed implies a unique EMM.
18 Numeraires
Thenumeraire is usually chosen to be the cash bond, but it can be chosen to be
any tradable instrument available. In particular, the numeraire can have volatility.
Thus, let us consider a market with nBrownian motions Wi
t,i= 1,...,n. LetSI
t,
I= 1,...,Nbe the stocks, where N≥n, and letBtbe the numeraire. We have
dBt=Bt/bracketleftiggn/summationdisplay
i=1ρi
tdWi
t+rtdt/bracketrightigg
, (356)
dSI
t=SI
t/bracketleftiggn/summationdisplay
i=1σIi
tdWi
t+µI
tdt/bracketrightigg
. (357)
The discounted stock processes have the following SDEs:
dZI
t=ZI
t/bracketleftiggn/summationdisplay
i=1/parenleftbig
σIi
t−ρi
t/parenrightbig
dWi
t+/parenleftbig
µI
t−rt/parenrightbig
dt/bracketrightigg
. (358)
Let/hatwideσIi
t=σIi
t−ρi
t, and/hatwideµI
t≡µI
t−rt. Then we have
dZI
t=ZI
t/bracketleftiggn/summationdisplay
i=1/hatwideσIi
td/tildewiderWi
t+/parenleftigg
/hatwideµI
t−n/summationdisplay
i=1/hatwideσIi
tγi
t/parenrightigg
dt/bracketrightigg
, (359)
where
/tildewiderWi
t=Wi
t+/integraldisplayt
0γi
sds . (360)
To make the drift terms vanish simultaneously, we must make sure th at
n/summationdisplay
i=1/hatwideσIi
tγi
t=/hatwideµI
t. (361)
That is,
n/summationdisplay
j=1/hatwideσij
tγj
t=/hatwideµi
t, (362)
n/summationdisplay
i=1/hatwideσαiγi=/hatwideµα
t, (363)
whereI= (i,α),α=n+1,...,N.
63",2014-05-07T01:13:23Z,that  and so umer articial intellence rtumer articial intellence re i brownish wi  be  be    wi ii wi t ii wi  ii ii tii wi ii wi wi to ii that
paper_qf_22.pdf,65,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Assuming that the matrix /hatwideΣt≡(/hatwideσij
t) is invertible, we have ( /hatwiderMt≡/hatwideΣt/hatwideΣT
t):
γi
t=n/summationdisplay
j,k=1/parenleftig
/hatwiderM−1
t/parenrightigjk
/hatwideσji
t/hatwideµk
t, (364)
and we have the following conditions on the drifts:
/hatwideµα=n/summationdisplay
i,j,k=1/hatwideσαi/parenleftig
/hatwiderM−1
t/parenrightigjk
/hatwideσji
t/hatwideµk
t. (365)
Note that these conditions, which are non-trivial if N >n, come from the require-
ment that there exist a martingale measure Q.
To construct replicating strategies, we proceed as follows. We intr oduce a Q-
martingale Et≡ /an}bracketle{tB−1
TX/an}bracketri}htQ,Ft. Then-factor martingale representation theorem then
implies that there exist previsible processes φi
tsuch that
Et=E0+n/summationdisplay
i=1/integraldisplayt
0φi
tdZi
t (366)
as long as the matrix /hatwideΣtis invertible. Indeed, since Etis aQ-martingale, we have
dEt=n/summationdisplay
j=1Zj
tλj
td/tildewiderWj
t (367)
for some previsible processes λi
t. On the other hand,
n/summationdisplay
i=1φi
tdZi
t=n/summationdisplay
i,j=1Zi
tφi
t/hatwideσij
td/tildewiderWj
t. (368)
And since/hatwideΣtis invertible, the matrix equation
n/summationdisplay
i=1Zi
tφi
t/hatwideσij
t=Zj
tλj
t (369)
has a solution for φi
t.
The hedging portfolio then is ( φ1
t,...,φn
t,ψt), which corresponds to holding φi
t
units of the stocks Si
t,i= 1,...,n, andψtunits of the numeraire Bt, whereψt=
Et−/summationtextn
i=1φi
tZi
t, so the value of the portfolio is
Vt=BtEt=n/summationdisplay
i=1φi
tSi
t+ψtBt. (370)
64",2014-05-07T01:13:23Z,assumi mt note to  et ft tet zi ined et is et ozi zi and zi t si  et zi vt  et si 
paper_qf_22.pdf,66,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","We then have
dVt=d(BtEt) =BtdEt+EtdBt+Btn/summationdisplay
j=1Zj
tλj
tρj
tdt=
Btn/summationdisplay
j=1Zj
tλj
td/tildewiderWj
t+/parenleftigg
ψt+n/summationdisplay
i=1φi
tZi
t/parenrightigg
dBt+Btn/summationdisplay
j=1Zj
tλj
tρj
tdt=
Btn/summationdisplay
i,j=1Zi
tφi
t/hatwideσij
t/bracketleftig
d/tildewiderWj
t+ρj
tdt/bracketrightig
+/parenleftigg
ψt+n/summationdisplay
i=1φi
tZi
t/parenrightigg
dBt=
n/summationdisplay
i=1φi
t/bracketleftigg
BtdZi
t+Btn/summationdisplay
j=1Zi
t/hatwideσij
tρj
t+Zi
tdBt/bracketrightigg
+ψtdBt=
n/summationdisplay
i=1φi
td/parenleftbig
BtZi
t/parenrightbig
+ψtdBt=
n/summationdisplay
i=1φi
tdSi
t+ψtdBt. (371)
Here we have taken into account that
dBt=Bt/bracketleftiggn/summationdisplay
i=1ρi
td/tildewiderWi
t+/parenleftigg
rt−n/summationdisplay
i=1ρi
tγi
t/parenrightigg
dt/bracketrightigg
. (372)
Thus, as we see, the portfolio is self-ﬁnancing even though the num eraire is volatile.
18.1 Change of Numeraire
Suppose we have stocks Si
tplus two other securities BtandCteither of which can
be a numeraire. If we choose Btas the numeraire, then we need to ﬁnd a measure
Qsuch thatB−1
tSi
tandB−1
tCtare martingales. On the other hand, if we choose Ct
as the numeraire, then we need to ﬁnd a measure QCsuch thatC−1
tSi
tandC−1
tBt
are martingales.
Letζtbe the Radon-Nikodym process
ζt=/angbracketleftbiggdQC
dQ/angbracketrightbigg
Q,Ft. (373)
Then for any process Xtwe have:
ζs/an}bracketle{tXt/an}bracketri}htQC,Fs=/an}bracketle{tζtXt/an}bracketri}htQ,Fs. (374)
Thus, ifXtis aQC-martingale, then
ζsXs=/an}bracketle{tζtXt/an}bracketri}htQ,Fs. (375)
65",2014-05-07T01:13:23Z, vt  et td et td  zi  zi zi  td zi zi zi    zi  si  re   wi  e num  ire suppose si  and  eitr   as susi  are o susi   radody ft txt  xt fs xt fs  xt is xs xt fs
paper_qf_22.pdf,67,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","That is,ζtXtis aQ-martingale. The process ζtthat satisﬁes this property is given
by:
ζt=B−1
tCt. (376)
Indeed, the canonical Q-martingales are 1 ,B−1
tCt,B−1
tSi
t, and the corresponding
canonical QC-martingales are C−1
tBt,1,C−1
tSi
t.
Let us compute the price of a claim X=XTunder the measure QC:
VC
t=Ct/an}bracketle{tC−1
TX/an}bracketri}htQC,Ft=
Ctζ−1
t/an}bracketle{tζTC−1
TX/an}bracketri}htQ,Ft=
Bt/an}bracketle{tB−1
TX/an}bracketri}htQ,Ft=Vt, (377)
whereVtis the price of the claim Xunder the measure Q. Thus, the prices under
QandQCagree, that is, the prices are independent of the choice of the num eraire.
19 Foreign Exchange
Consider the Black-Scholes foreign currency model. Let Btbe the dollar cash bond,
Dtbe the sterling cash bond, and Ctbe the dollar worth of one pound. Then the
model is
Bt= exp(rt), (378)
Dt= exp(ut), (379)
Ct=C0exp(σWt+µt), (380)
where the dollar interest rate r, the sterling interest rate u, as well as the log-
volatilityσand the log-drift µfor the exchange rate are all constant.
Let us consider this model from the viewpoint of the dollar investor. The dollar
cash bond is tradable. Since the sterling cash is not dollar tradable (t his is because
there is non-zero sterling interest rate u),Ct, which is the dollar worth of one pound,
is not tradable either. On the other hand, the sterling cash bond Dtis not dollar
tradable as it is the price of a tradable instrument (the sterling cash bond), but it
is a sterling price. There is, however, a dollar tradable we can constr uct. It is given
by
St=DtCt. (381)
This is the dollar price of the sterling cash bond, so it is dollar tradable. Note that
this tradable is volatile, and has the same behavior as a US stock.
The discounted process is now Zt=B−1
tSt=B−1
tDtCt. That is,
Ct=BtD−1
tZt= exp([r−u]t)Zt= exp(/hatwidert)Zt. (382)
The quantity/hatwider≡r−ucan be thought of as the eﬀective dollar interest rate. Then
the priceFtof a sterling forward contract (that is, the price at time tfor trading
sterling at a future date T) is given by
Ft=Ctexp(/hatwider[T−t]) =Ctexp([r−u][T−t]). (383)
66",2014-05-07T01:13:23Z,that xt is t  ined  si  si  unr  ft  ft  ft vt vt is unr  and agree foreexe consir  schools   be dt be  be t dt    t since  odt is tre it st dt   note t st dt  that   t tft of ft te  te 
paper_qf_22.pdf,68,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","All option prices are then given by the corresponding Black-Scholes formulas with
the forward price given by Ft.
20 The Interest Rate Market
We can regard a promise of a dollar at the maturity time Tas an asset, which has
some worth at time tbeforeT. This asset is called a discount bond . Let its price
at time 0 ≤t≤TbeP(t,T). ThenP(T,T) = 1. A discount bond behaves like a
stock, but it has this boundary condition at the maturity.
Theyieldof a discount bond is given by:
R(t,T) =−ln(P(t,T))
T−t. (384)
This has the meaning of an average interest rate over the period of timeT−t.
Theinstantaneous rate , orshort rate , is given by:
rt=R(t,t). (385)
This is the rate of instantaneous borrowing.
Theforward rate is given by:
f(t,T) =−∂Tln(P(t,T)). (386)
This has the meaning of the forward rate of instantaneous borrow ing at time T.
We have the following relations:
f(t,T) =R(t,T)+(T−t)∂TR(t,T), (387)
rt=f(t,t), (388)
P(t,T) = exp/parenleftbigg
−/integraldisplayT
tf(t,u)du/parenrightbigg
. (389)
The latter gives the discount bond price in terms of the forward rat e.
20.1 The Heath-Jarrow-Morton (HJM) Model
In the HJM model the forward rate for each maturity Tis a stochastic process:
f(t,T) =f(0,T)+/integraldisplayt
0σ(s,T)dWs+/integraldisplayt
0α(s,T)ds,0≤t≤T , (390)
or in the diﬀerential form
dtf(t,T) =σ(t,T)dWt+α(t,T)dt , (391)
67",2014-05-07T01:13:23Z,all  schools ft t interest rate market  as   be tt yield of  t instantaneous  t forward l  t t ath narrow mortomitis ws 
paper_qf_22.pdf,69,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","where the volatilities σ(t,T) and drifts α(t,T) are previsible processes. The formula
P(t,T) = exp/parenleftbigg
−/integraldisplayT
tf(0,u)du−/integraldisplayt
0/parenleftbigg/integraldisplayT
tσ(s,u)du/parenrightbigg
dWs−
/integraldisplayt
0/parenleftbigg/integraldisplayT
tα(s,u)du/parenrightbigg
ds/parenrightbigg
(392)
then gives the price of the discount bond.
To hedge claims, we need a cash product. The simplest cash product is an
account, or a cash bond, formed by starting with $1 at t= 0 and reinvesting
continuously at the instantaneous rate rt:
dBt=rtBtdt , (393)
Bt= exp/parenleftbigg/integraldisplayt
0rsds/parenrightbigg
. (394)
Since
rt=f(t,t) =f(0,t)+/integraldisplayt
0σ(s,t)dWs+/integraldisplayt
0α(s,t)ds , (395)
we have
Bt=
exp/parenleftbigg/integraldisplayt
0f(0,u)du+/integraldisplayt
0du/integraldisplayu
0σ(s,u)dWs+/integraldisplayt
0du/integraldisplayu
0α(s,u)ds/parenrightbigg
=
exp/parenleftbigg/integraldisplayt
0f(0,u)du+/integraldisplayt
0/parenleftbigg/integraldisplayt
sσ(s,u)du/parenrightbigg
dWs+
/integraldisplayt
0/parenleftbigg/integraldisplayt
sα(s,u)du/parenrightbigg
ds/parenrightbigg
. (396)
Note that we have changed the order of integration in the last two t erms.
The discounted asset price is given by:
Z(t,T) =B−1
tP(t,T) =
exp/parenleftbigg/integraldisplayt
0Σ(s,T)dWs−/integraldisplayt
0f(0,u)du−/integraldisplayt
0/parenleftbigg/integraldisplayT
sα(s,u)du/parenrightbigg
ds/parenrightbigg
,(397)
where
Σ(t,T)≡ −/integraldisplayT
tσ(t,u)du (398)
plays the role of the log-volatility of P(t,T).
Next, we need to change the measure from PtoQso thatZ(t,T) becomes a
Q-martingale. Since
dtZ(t,T) =Z(t,T)/parenleftbigg
Σ(t,T)dWt+/bracketleftbigg1
2Σ2(t,T)−/integraldisplayT
tα(t,u)du/bracketrightbigg
dt/parenrightbigg
,(399)
68",2014-05-07T01:13:23Z,t ws to t   dt  since ws  ws ws note t ws next to so since 
paper_qf_22.pdf,70,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","the corresponding shift γtis given by
γt=1
2Σ(t,T)−1
Σ(t,T)/integraldisplayT
tα(t,u)du. (400)
We havedtZ(t,T) = Σ(t,T)Z(t,T)d/tildewiderWt, where
/tildewiderWt=Wt+/integraldisplayt
0γsds (401)
is aQ-Brownian motion. Note that the SDE for P(t,T) is given by
dtP(t,T) =BtdtZ(t,T)+Z(t,T)dBt=
P(t,T)/bracketleftig
Σ(t,T)d/tildewiderWt+rtdt/bracketrightig
(402)
under the martingale measure.
The rest (that is, the hedging, self-ﬁnancing portfolios and pricing ) is as usual.
Thus, the price of a claim X=XTis given by
Vt=Bt/an}bracketle{tB−1
TX/an}bracketri}htQ,Ft=/angbracketleftig
e−/integraltextT
trsdsX/angbracketrightig
Q,Ft. (403)
In particular, the price P(t,S) of theS-bond is the same as the price of the claim
XS= $1:
P(t,S) =/angbracketleftig
e−/integraltextS
trsds/angbracketrightig
Q,Ft, t≤S≤T . (404)
Note that this is nothing but a path integral of an exponential oper ator.
This has an important implication. Thus, for the discounted S-bond we have:
Z(t,S) =B−1
tP(t,S) =/an}bracketle{tB−1
S/an}bracketri}htQ,Ft. (405)
This implies that Z(t,S) is aQ-martingale for all S. That is, a single shift γtshould
make all discounted S-bonds into Q-martingales, i.e.,the market price of risk for
allS-bonds must be the same. It then follows that γtshould be independent of the
maturityT. Recall that we have
/integraldisplayT
tα(t,u)du=1
2Σ2(t,T)−Σ(t,T)γt,0≤t≤T . (406)
Diﬀerentiating w.r.t. Twe obtain:
α(t,T) =σ(t,T)[γt−Σ(t,T)], (407)
so theT-dependence of the P-driftsα(t,T) cannot be arbitrary.
69",2014-05-07T01:13:23Z,    brownish note  dt   t  tis vt  ft ft ift note   ft  that it recall di 
paper_qf_22.pdf,71,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Note that under the risk-neutral measure Qwe have
dtf(t,T) =σ(t,T)/bracketleftig
d/tildewiderWt−Σ(t,T)dt/bracketrightig
, (408)
rt=f(0,t)+/integraldisplayt
0σ(s,t)d/tildewiderWs−/integraldisplayt
0σ(s,t)Σ(s,t)ds . (409)
That is, these expressions no longer depend on the P-drifts, but only on the volatil-
ities.
20.2 Multi-factor HJM Models
The drawback of the single-factor HJM model is that the correlatio n of aT-bond
and anS-bond is exactly 1. This can be lifted by considering a multi-factor HJM
model:
f(t,T) =f(0,T)+n/summationdisplay
i=1/integraldisplayt
0σi(s,T)dWi
s+/integraldisplayt
0α(s,T)ds, (410)
whereWi
tare independent Brownian motions. Note that the correlation betw een
theT-bond and the S-bond
/summationtextn
i=1σi(t,T)σi(t,S)/radicalig/summationtextn
i,j=1[σi(t,T)]2[σj(t,S)]2(411)
is now generally diﬀerent from 1.
As in the single factor model, to ﬁnd a martingale measure Q, we have a restric-
tion on the drift:
α(t,T) =n/summationdisplay
i=1σi(t,T)/bracketleftbig
γi
t−Σi(t,T)/bracketrightbig
, (412)
where Σi(t,T)≡ −/integraltextT
tσi(t,u)du, andγi
tare independent of T.
Since we have nindependent Brownian motions, to hedge a claim we need a
portfolio consisting of nseparate instruments (plus the cash bond). Here we can
choose whichever ninstruments we like, and the answer will always be the same.
Thus, consider hedgingtheclaim X=XTwithdiscount bonds P(t,Ti),i= 1,...,n.
We must make sure that all Ti>T.
The value of a self-ﬁnancing strategy ( φ1
t,...,φn
t,ψt) is then
Vt=n/summationdisplay
i=1φi
tP(t,Ti)+ψtBt. (413)
Its discounted value Et=B−1
tVthas the SDE
dEt=n/summationdisplay
i=1φi
tdZ(t,Ti). (414)
70",2014-05-07T01:13:23Z,note   ws that multi mols t  wi wi brownish note as since brownish re  with diunt ti  ti t vt ti  its et vt has et ti
paper_qf_22.pdf,72,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","To read oﬀ φi
tfrom this equation (note that Et=/an}bracketle{tB−1
TX/an}bracketri}htQ,Ft), we must make sure
that the volatility matrix
At= (Aij
t)≡/parenleftbig
Σi(t,Tj)/parenrightbig
(415)
is non-singular. The rest goes as usual.
21 Short-rate Models
A short-rate model posits a risk-neutral measure Qand a short-rate process rt. The
cash bond process is then given by
Bt= exp/parenleftbigg/integraldisplayt
0rsds/parenrightbigg
, (416)
while the bond price is given by
P(t,T) =/angbracketleftbigg
exp/parenleftbigg
−/integraldisplayT
trsds/parenrightbigg/angbracketrightbigg
Q,Ft. (417)
The price at time tof a claimX=XTis
Vt=/angbracketleftbigg
exp/parenleftbigg
−/integraldisplayT
trsds/parenrightbigg
X/angbracketrightbigg
Q,Ft. (418)
One then works with a parametrized family of processes, which typic ally are Marko-
vian (but need not be), and chooses the parameters to best ﬁt th e market.
It is clear that the HJM models are short-rate models. Let us, howe ver, show
that short-rate models are HJM models. Let us focus on the case w herertis a
Markov process. Then we have
drt=ρ(rt,t)dWt+ν(rt,t)dt , (419)
where we have chosen ρ(y,t) andν(y,t) to be deterministic functions.
Let
V(x,t,T)≡/angbracketleftbigg
exp/parenleftbigg
−/integraldisplayT
trsds/parenrightbigg/angbracketrightbigg
Q, rt=x. (420)
This is nothing but the pricing function for the claim XT= 1. In particular,
V(rt,t,T) =P(t,T), andV(x,T,T) = 1. This pricing function satisﬁes a pricing
PDE. To derive this PDE, let us use the fact that the discounted bon d process
Z(t,T) =B−1
tP(t,T) =B−1
tV(rt,t,T) must be a martingale under the risk-neutral
measure Q. Thus, we have:
dtZ(t,T) =B−1
t[dtV(rt,t,T)−rtV(rt,t,T)] =
B−1
t/bracketleftig
ρ(rt,t)∂xV(rt,t,T)dWt+/parenleftig
ν(rt,t)∂xV(rt,t,T)+
1
2ρ2(rt,t)∂2
xV(rt,t,T)+∂tV(rt,t,T)−rtV(rt,t,T)/parenrightig
dt/bracketrightig
.(421)
71",2014-05-07T01:13:23Z,to et ft at air tj t short mols and t  ft t tis vt ft one mark it   mark t   i to  
paper_qf_22.pdf,73,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","The requirement that the drift term vanish then implies that
ν(rt,t)∂xV(rt,t,T)+∂tV(rt,t,T)+1
2ρ2(rt,t)∂2
xV(rt,t,T)−rtV(rt,t,T) = 0 (422)
with the boundary condition V(x,T,T) = 1.
Next, let
g(x,t,T)≡ −ln(V(x,t,T)). (423)
Note thatP(t,T) = exp(−g(rt,t,T)), and
f(t,T) =−∂Tln(P(t,T)) =∂Tg(rt,t,T). (424)
We therefore have
dtf(t,T) =ρ(rt,t)∂x∂Tg(rt,t,T)dWt+/bracketleftig
ν(rt,t)∂x∂Tg(rt,t,T)+
∂t∂Tg(rt,t,T)+1
2ρ2(rt,t)∂2
x∂Tg(rt,t,T)/bracketrightig
dt . (425)
SinceQis the risk-neutral measure, we must have
dtf(t,T) =σ(t,T)dWt−σ(t,T)Σ(t,T)dt . (426)
This gives
σ(t,T) =ρ(rt,t)∂x∂Tg(rt,t,T), (427)
Σ(t,T) =−ρ(rt,t)∂xg(rt,t,T). (428)
Matching the drift terms requires that
ν(rt,t)∂x∂Tg(rt,t,T)+∂t∂Tg(rt,t,T)+1
2ρ2(rt,t)∂2
x∂Tg(rt,t,T) =
ρ2(rt,t)∂x∂Tg(rt,t,T)∂xg(rt,t,T). (429)
Thisconditionisindeedsatisﬁed; intermsof g(x,t,T)thepricing PDEfor V(x,t,T)
reads:
ν(rt,t)∂xg(rt,t,T)+∂tg(rt,t,T)+1
2ρ2(rt,t)/bracketleftbig
∂2
xg(rt,t,T)−(∂xg(rt,t,T))2/bracketrightbig
+rt= 0.
(430)
Diﬀerentiating this equation w.r.t. Twe obtain (429). This shows that short-rate
models are indeed HJM models.
Note that in a sense the choice of the drift νtis not particularly important in the
short-rate models – indeed, Wtis aQ-Brownian motion, but we can depart from
the martingale measure Qto a “real world” measure PviaWt=W′
t+γt, whereγt
is an arbitrary previsible process. Under this new measure we have
drt=ρ(rt,t)dW′
t+ν′
tdt , (431)
where
ν′
t=ν(rt,t)+ρ(rt,t)γt (432)
can be an arbitrary previsible process under the measure P.
72",2014-05-07T01:13:23Z,t next note ltg  tg  tg tg tg since is   tg matchi tg tg tg tg  conditis ined satifor di   note  is brownish to via  unr
paper_qf_22.pdf,74,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","21.1 The Ho and Lee Model
TheHo and Lee model is given by:
drt=ρ(t)dWt+ν(t)dt . (433)
That is, neither ρnorνdepend onrt, but only on time t.
The function g(x,t,T) can be computed as follows. Note that
/integraldisplayT
trsds=srs|T
t−/integraldisplayT
tsdrs=
srs|T
t−T/integraldisplayT
tdrs+/integraldisplayT
t(T−s)drs=
(T−t)rt+/integraldisplayT
t(T−s)drs. (434)
This implies that
V(x,t,T) = exp(−x[T−t])/angbracketleftbigg
exp/parenleftbigg
−/integraldisplayT
t(T−s)drs/parenrightbigg/angbracketrightbigg
Q, rt=x.(435)
The expectation can be readily computed using the path integral te chniques:
/angbracketleftbigg
exp/parenleftbigg
−/integraldisplayT
t(T−s)drs/parenrightbigg/angbracketrightbigg
Q, rt=x= exp/parenleftbigg
−/integraldisplayT
t(T−s)ν(s)ds/parenrightbigg
×
×/integraldisplay
Dzexp(−S[z;t,T]) exp/parenleftbigg
−/integraldisplayT
t(T−s)ρ(s)˙z(s)ds/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
rt=x=
exp/parenleftbigg
−/integraldisplayT
t(T−s)ν(s)ds+1
2/integraldisplayT
t(T−s)2ρ2(s)ds/parenrightbigg
×
×/integraldisplay
Dyexp(−S[y;t,T])|rt=x=
exp/parenleftbigg
−/integraldisplayT
t(T−s)ν(s)ds+1
2/integraldisplayT
t(T−s)2ρ2(s)ds/parenrightbigg
(436)
From this it follows that
g(x,t,T) =x(T−t)−1
2/integraldisplayT
t(T−s)2ρ2(s)ds+/integraldisplayT
t(T−s)ν(s)ds . (437)
We, therefore, have
σ(t,T) =ρ(t)∂x∂Tg(rt,t,T) =ρ(t), (438)
Σ(t,T) =−ρ(t)∂xg(rt,t,T) =−ρ(t)(T−t), (439)
dtf(t,T) =ρ(t)dWt+ρ2(t)(T−t)dt , (440)
f(0,T) =∂Tg(r0,0,T) =r0−/integraldisplayT
0(T−s)ρ2(s)ds+/integraldisplayT
0ν(s)ds .(441)
Note that the volatility surface σ(t,T) is independent of Tin this model.
73",2014-05-07T01:13:23Z,t ho  mt ho   that t note  t dz e dye  from  tg  tg note tin
paper_qf_22.pdf,75,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","21.2 The Vasicek/Hull-White Model
TheVasicekmodel is given by:
drt=ρ(t)dWt+[ν(t)−α(t)rt]dt . (442)
The corresponding equation for g(x,t,T) is given by:
[ν(t)−α(t)x]∂xg(x,t,T)+∂tg(x,t,T)+
1
2ρ2(t)/bracketleftbig
∂2
xg(x,t,T)−(∂xg(x,t,T))2/bracketrightbig
+x= 0. (443)
The solution is given by:
g(x,t,T) =xη(t,T)+/integraldisplayT
tη(s,T)ν(s)ds−1
2/integraldisplayT
tη2(s,T)ρ2(s)ds , (444)
where
η(t,T)≡/integraldisplayT
tβ(t,u)du, (445)
and
β(t,T)≡exp/parenleftbigg
−/integraldisplayT
tα(s)ds/parenrightbigg
. (446)
Thus, we have
σ(t,T) =ρ(t)β(t,T), (447)
Σ(t,T) =−ρ(t)η(t,T), (448)
dtf(t,T) =ρ(t)β(t,T)dWt+ρ2(t)β(t,T)η(t,T)dt, (449)
f(0,T) =r0β(0,T)+/integraldisplayT
0β(s,T)ν(s)ds−
/integraldisplayT
0β(s,T)η(s,T)ρ2(s)ds . (450)
Note that in this model the volatility surface depends on T.
Consider the case where ρ,ν,αare all constant. Then we have
drt=ρdWt+[ν−αrt]dt . (451)
Note that the drift term pushes rtupward ifrtis belowν/α, and it pushes it
downward if rtis aboveν/α. Moreover, the magnitude of the drift is proportional
to the distance away from this mean ν/α. Processes with such “mean-reverting”
behavior are known as Ornstein-Uhlenbeck processes .
Letrt≡/hatwidertexp(−αt). Then we have
d/hatwidert= exp(αt)[ρdWt+νdt]. (452)
74",2014-05-07T01:13:23Z,t vas ice hull white mt vas ice m t t   note consir t note oprocessor nste ihlebeck  rt t
paper_qf_22.pdf,76,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","This has the solution
rt= exp(−αt)r0+ν
α[1−exp(−αt)]+ρexp(−αt)/integraldisplayt
0exp(αs)dWs.(453)
Note that the mean of this process is
/an}bracketle{trt/an}bracketri}ht= exp(−αt)r0+ν
α[1−exp(−αt)], (454)
which converges to ν/αastgets large. The variance can be computed as follows.
Let
rt=/an}bracketle{trt/an}bracketri}ht+ξt, (455)
where
ξt≡ρexp(−αt)/integraldisplayt
0exp(αs)dWs. (456)
Then
v(t)≡ /an}bracketle{tr2
t/an}bracketri}ht−(/an}bracketle{trt/an}bracketri}ht)2=/an}bracketle{tξ2
t/an}bracketri}ht. (457)
Note that
dξt=ρdWt−αξtdt . (458)
We therefore have:
d/an}bracketle{tξ2
t/an}bracketri}ht=/an}bracketle{tdξ2
t/an}bracketri}ht=
2/an}bracketle{tξtdξt/an}bracketri}ht+/an}bracketle{t(dξt)2/an}bracketri}ht=
2ρ/an}bracketle{tξtdWt/an}bracketri}ht+/parenleftbig
ρ2−2α/an}bracketle{tξ2
t/an}bracketri}ht/parenrightbig
dt=/parenleftbig
ρ2−2α/an}bracketle{tξ2
t/an}bracketri}ht/parenrightbig
dt . (459)
That is,
dv(t)
dt=ρ2−2αv(t). (460)
The solution to this equation is given by
v(t) =ρ21−exp(−2αt)
2α, (461)
where we have taken into account the initial condition v(0) = 0. Thus, the variance
converges to ρ2/2αastgets large. Note that even though the distribution for rt
converges, the process rtitself does not.
21.3 The Cox-Ingersoll-Ross Model
In the Ho and Lee as well as Vasicek models the short rate rtcan occasionally
become negative. There are various ways to rectify this.
75",2014-05-07T01:13:23Z, ws note t  ws tnote    that t  note t cox inoll ross miho  vas ice tre
paper_qf_22.pdf,77,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","TheCox-Ingersoll-Ross model is given by:
drt=√rtρ(t)dWt+[ν(t)−α(t)rt]dt . (462)
The drift term is mean-reverting, while the volatility term is set up in su ch a way
that it gets smaller as rtapproaches zero allowing the drift term to dominate and
stoprtfromgoing belowzero. Infact, aslong as ν(t)≥ρ2(t)/2, this process actually
stays strictly positive. Such processes are called autoregressive .
LetB(t,T) be the solution of the Riccati equation
∂tB(t,T) =1
2ρ2(t)B2(t,T)+α(t)B(t,T)−1 (463)
with the boundary condition B(T,T) = 1. Then we have
g(x,t,T) =xB(t,T)+/integraldisplayT
tν(s)B(s,T)ds. (464)
Indeed, this satisﬁes the corresponding equation:
[ν(t)−xα(t)]∂xg(x,t,T)+∂tg(x,t,T)+
1
2xρ2(t)/bracketleftbig
∂2
xg(x,t,T)−(∂xg(x,t,T))2/bracketrightbig
+x= 0. (465)
Let
D(t,T)≡∂TB(t,T). (466)
Then we have:
σ(t,T) =√rtρ(t)D(t,T), (467)
Σ(t,T) =−√rtρ(t)B(t,T), (468)
dtf(t,T) =√rtρ(t)D(t,T)dWt+rtρ2(t)D(t,T)B(t,T)dt, (469)
f(0,T) =r0D(0,T)+/integraldisplayT
0D(s,T)ν(s)ds . (470)
Note that the volatility surface in this model depends on the proces srt.
21.4 The Black-Karasinski Model
TheBlack-Karasinski model is given by:
rt= exp(Xt), (471)
dXt=ρ(t)dWt+[ν(t)−α(t)Xt]dt . (472)
This is another way of ensuring that rtstays positive. Note that the process Xt
follows the Vasicek model.
76",2014-05-07T01:13:23Z,t cox inoll ross  t ifa su ric cat tined  t note t  kara i mt  kara i xt xt  xt  note xt vas ice
paper_qf_22.pdf,78,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","22 Interest Rate Products
The simplest interest rate product is a forward contract. In a for ward contract at
the current time twe agree to make a payment kat a future time T1, and in return
receive $1 at a later time T2. According to the pricing formula, we have
Vt=Bt/an}bracketle{tB−1
T2/an}bracketri}htFt−Bt/an}bracketle{tB−1
T1k/an}bracketri}htFt=P(t,T2)−kP(t,T1), (473)
so the value of kthat gives this contract a nil value Vt= 0 is given by
k=P(t,T2)
P(t,T1). (474)
We then hedge ourselves as follows. At time twe buykunits of the T1-bond, and
sell one unit of the T2-bond. The initial value of this deal is zero, so it requires
no investment. At time T1we receivekdollars from the maturing T1-bonds, which
matches the payment kwe have to make according to the forward contract. At time
T2the dollar we receive then covers the short T2-bond. This is a statichedge.
22.1 Forward Measures
For the following applications it will be useful to deﬁne the notion of th e forward
measures in the interest rate markets. Thus, in the interest rate models it is often
popular to use a T-bond as the numeraire. The martingale measure for this nu-
meraire is called the T-forward measure PT, and it makes the forward rate f(t,T)
into aPT-martingale.
The new numeraire is the T-bond normalized to have unit value at t= 0:Ct=
P(t,T)/P(0,T). The corresponding Radon-Nikodym process is
ζt=Ct
Bt=P(t,T)
P(0,T)Bt. (475)
The forward price set at time tfor purchasing Xat timeTis given by its current
valueVtscaled up by the return on a T-bond:
Ft=P−1(t,T)Vt=P−1Bt/an}bracketle{tB−1
TX/an}bracketri}htQ,Ft=
/an}bracketle{tX/an}bracketri}htPT,Ft, (476)
soFtis aPT-martingale.
Note that we have:
Vt=P(t,T)/an}bracketle{tX/an}bracketri}htPT,Ft, (477)
so the price of the claim Xat timetis the conditional PT-expectation of Xup to
timet(the forward price of X) discounted by the time value of money (the T-bond).
Note that
ζt=Z(t,T)
P(0,T), (478)
77",2014-05-07T01:13:23Z,interest rate produs t iaccordi vt  ft  ft vt  at t at at  forward measurfor  t t  t radody    t at tis vt scaled ft vt  ft ft ft is note vt ft at up note
paper_qf_22.pdf,79,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","whereZ(t,T) is the discounted T-bond process: Z(t,T) =B−1
tP(t,T). This implies
that
dζt=ζtn/summationdisplay
i=1Σi(t,T)d/tildewiderWi
t, (479)
where/tildewiderWi
tare theQ-Brownian motions. This then implies that
/hatwiderWi
t=/tildewiderWi
t−/integraldisplayt
0Σi(s,T)ds (480)
are the corresponding PT-Brownian motions.
Also, note that for the forward rate we have
dtf(t,T) =n/summationdisplay
i=1σi(t,T)/bracketleftig
d/tildewiderWi
t−Σi(t,T)dt/bracketrightig
=n/summationdisplay
i=1σi(t,T)d/hatwiderWi
t,(481)
so thatf(t,T) is aPT-martingale. This, in particular, implies that
f(t,T) =/an}bracketle{tf(T,T)/an}bracketri}htPT,Ft=/an}bracketle{trT/an}bracketri}htPT,Ft, (482)
sof(t,T) is the forward rate for rT.
22.2 Multiple Payment Contracts
Most interest rateproductsdo not just make a single payment Xat timeT. Instead,
the contract speciﬁes a sequence of payments Ximade at a sequence of times Ti,
i= 1,...,n.
To price such a contract, we can treat each payment separately:
Vi(t) =Bt/an}bracketle{tB−1
TiXi/an}bracketri}htQ,Ft=P(t,Ti)/an}bracketle{tXi/an}bracketri}htPTi,Ft. (483)
Note that in this case the forward measure, if used, would have to b e changed for
eachi.
Alternatively, we can roll up the payments into a savings account as we receive
them, and keep them until the last payment date T. That is, as each payment is
made, we use them to buy a T-bond, or invest it into the bank account process Bt
until timeT. Thus, in the former case the payoﬀ is a single payment at time T:
X=n/summationdisplay
i=1Xi
P(Ti,T), (484)
and its worth at time tis
Vt=Bt/an}bracketle{tB−1
TX/an}bracketri}htQ,Ft=P(t,T)/an}bracketle{tX/an}bracketri}htPT,Ft. (485)
In this case we need only one forward measure PT.
78",2014-05-07T01:13:23Z, wi wi brownish  wi wi brownish also wi wi  ft ft multiple payment contras most at instead xi ma ti to vi  ti xi ft ti xi ti ft note alternatively that   xi ti vt  ft ft in
paper_qf_22.pdf,80,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","22.3 Bonds with Coupons
In practice zero-coupon bonds are not popular products, espec ially at the long end.
Instead, a bond usually pays not only its principal at maturity T, but also makes
smaller regular coupon payments of a ﬁxed amount cuntil then.
Thus, suppose a bond makes nregular payments at an uncompounded rate kat
timesTi=T0+iδ,i= 1,2,..., and also pays oﬀ a dollar at the maturity time T.
The amount of the actual coupon payment is kδ, whereδis the payment period.
This income stream is equivalent to owning one T-bond plus kδunits of each Ti-
bond,i= 1,...,n, wheren=I(T)−1 is the total number of payments before
maturity time T, andI(t)≡min(i:t<Ti). The price of the coupon bond at time
tthen is
Pc(t,T) =P(t,T)+kδn/summationdisplay
i=I(t)P(t,Ti). (486)
At timet=T0we have
Pc(T0,T) =P(T0,T)+kδn/summationdisplay
i=1P(T0,Ti). (487)
If we desire the coupon bond to start with its face value ( Pc(T0,T) = 1), then
k=1−P(T0,T)
δ/summationtextn
i=1P(T0,Ti)(488)
is the corresponding coupon rate.
22.4 Floating Rate Bonds
A bond might also have ﬂoatingcoupon payments. Thus, consider a bond that
pays $1 at the maturity time T, and also makes payments at times Ti=T0+iδ,
i= 1,2,...,of varying amounts. The amount of payment made at time Tiis
determined by the LIBORrate (London Interbank Oﬀer Rate ) at timeTi−1:
L(Ti−1) =1
δ/bracketleftbigg1
P(Ti−1,Ti)−1/bracketrightbigg
. (489)
The actual coupon payment is
Xi=δL(Ti−1) =1
P(Ti−1,Ti)−1, (490)
which is the amount of interest we would get by buying a dollar’s worth o fTi-bond
at timeTi−1. The value of this payment at time T0is
Vi(T0) =BT0/an}bracketle{tB−1
TiXi/an}bracketri}htQ,FT0=
BT0/an}bracketle{tB−1
TiP−1(Ti−1,Ti)/an}bracketri}htQ,FT0−BT0/an}bracketle{tB−1
Ti/an}bracketri}htQ,FT0=
BT0/angbracketleftig
/an}bracketle{tB−1
TiP−1(Ti−1,Ti)/an}bracketri}htQ,FTi−1/angbracketrightig
Q,FT0−BT0/an}bracketle{tB−1
Ti/an}bracketri}htQ,FT0,(491)
79",2014-05-07T01:13:23Z,bonds coupons iinstead  ti t  ti ti t pc ti at pc ti  pc ti floati rate bonds  ti t ti is rate londointer bank rate ti ti ti ti t xi ti ti ti ti ti t vi ti xi ti ti ti ti ti ti ti ti ti
paper_qf_22.pdf,81,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","where in the last line we are using the tower law. Note that
/an}bracketle{tB−1
TiP−1(Ti−1,Ti)/an}bracketri}htQ,FTi−1=P−1(Ti−1,Ti)/an}bracketle{tB−1
Ti/an}bracketri}htQ,FTi−1=
P−1(Ti−1,Ti)B−1
Ti−1/parenleftig
BTi−1/an}bracketle{tB−1
Ti/an}bracketri}htQ,FTi−1/parenrightig
=
P−1(Ti−1,Ti)B−1
Ti−1(P(Ti−1,Ti)) =
B−1
Ti−1. (492)
We therefore have
Vi(T0) =BT0/an}bracketle{tB−1
Ti−1/an}bracketri}htQ,FT0−BT0/an}bracketle{tB−1
Ti/an}bracketri}htQ,FT0=P(T0,Ti−1)−P(T0,Ti).(493)
The total value of the variable coupon bond is given by:
V0=P(T0,T)+n/summationdisplay
i=1Vi(T0) =
P(T0,T)+n/summationdisplay
i=1[P(T0,Ti−1)−P(T0,Ti)] =
P(T0,T)+[P(T0,T0)−P(T0,Tn)] =
1+[P(T0,T)−P(T0,Tn)]. (494)
If the maturity time Tcoincides with the last coupon payment Tn, then we have
V0= 1. That is, theinitial valueofthevariablecouponbondis itsfacevalu e. This is
because this bond is equivalent to the following sequence of trades. At timeT0take
a dollar and buy T1-bonds with it. At time T1take the interest from the T1-bonds
as theT1-coupon, and buy T2bonds with the leftover dollar principal. Repeat until
we are left with a dollar at time Tn. This has exactly the same cash ﬂows as the
variable coupon bond, so the initial prices must match.
22.5 Swaps
Swaps are popular contracts that exchange a stream of varying p ayments for a
stream of ﬁxed payments or vice versa . That is, we swap a ﬂoating interest rate for
a ﬁxed one. In practice only the net diﬀerence is exchanged at each payment date.
Consider a swap where we receive a stream of ﬁxed rate payments in exchange
for ﬂoating rate payments. This swap is simply a portfolio which is long a ﬁxed
coupon bond and short a variable coupon bond. The former is worth
P(T0,T)+kδn/summationdisplay
i=1P(T0,Ti), (495)
while the latter costs
P(T0,T)+1−P(T0,Tn). (496)
80",2014-05-07T01:13:23Z,note ti ti ti ti ti ti ti ti ti ti ti ti ti ti ti ti ti ti ti ti  vi ti ti ti ti t vi ti ti tt coincis tthat  at at repeat t swaps swaps that iconsir  t ti tn
paper_qf_22.pdf,82,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","The ﬁxed rate needed to give the swap initial null value then is
k=1−P(T0,Tn)
δ/summationtextn
i=1P(T0,Ti). (497)
Note that this rate does not depend on the maturity T.
Suppose we would like to enter into a forward swap agreement. The v alue of the
swap at time T0is
X=P(T0,Tn)+kδn/summationdisplay
i=1P(T0,Ti)−1. (498)
The price of Xat timetbeforeT0is
Vt=Bt/an}bracketle{tB−1
T0X/an}bracketri}htQ,Ft=P(t,Tn)+kδn/summationdisplay
i=1P(t,Ti)−P(t,T0),(499)
where we have taken into account that
/an}bracketle{tB−1
T0P(T0,Ti)/an}bracketri}htQ,Ft=B−1
tP(t,Ti) (500)
asZ(t,Ti) =B−1
tP(t,Ti) is aQ-martingale.
Thus, the forward ﬁxed rate kneeded to give the forward swap initial null value
at timetis
k=P(t,T0)−P(t,Tn)
δ/summationtextn
i=1P(t,Ti)=1−Ft(T0,Tn)
δ/summationtextn
i=1Ft(T0,Ti), (501)
where
Ft(T0,Ti)≡P(t,Ti)
P(t,T0)(502)
is the forward price at time tfor purchasing a Ti-bond at time T0.
22.6 Bond Options
Consider a European call option ona T-bondwith the strike price kand the exercise
dateτ. Its worth at time t<τis
Vt=Bt/an}bracketle{tB−1
τ(P(τ,T)−k)+/an}bracketri}htQ,Ft. (503)
Let us consider the Ho and Lee model:
dtf(t,T) =ρdWt+ρ2(T−t)dt (504)
with constant ρ. We then have
f(t,T) =f(0,T)+ρWt+1
2ρ2t(2T−t), (505)
81",2014-05-07T01:13:23Z,t tti note suppose t tti t at vt  ft tti ti ft ti ti ti  tti ft tft ti ft ti ti ti bond optns consir aits vt  ft  ho    
paper_qf_22.pdf,83,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","and
P(t,T) = exp/parenleftbigg
−/integraldisplayT
tf(t,u)du/parenrightbigg
=
exp/parenleftbigg
−/integraldisplayT
tf(0,u)du−ρ(T−t)Wt−1
2ρ2tT(T−t)/parenrightbigg
.(506)
Also,
rt=f(t,t) =f(0,t)+ρWt+1
2ρ2t2, (507)
so we have
Bt= exp/parenleftbigg/integraldisplayt
0rsds/parenrightbigg
=
exp/parenleftbigg/integraldisplayt
0f(0,u)du+ρ/integraldisplayt
0Wsds+1
6ρ2t3/parenrightbigg
. (508)
Since we have/integraltextt
0WsdsinBt, it is simpler to use the τ-forward measure instead of
the measure Q:
Vt=P(t,τ)/an}bracketle{t(P(τ,T)−k)+/an}bracketri}htPτ,Ft. (509)
Note that
/hatwiderWt=Wt−/integraldisplayt
0Σ(s,τ)ds=
Wt+ρ/integraldisplayt
0(τ−s)ds=
Wt+ρ/parenleftbigg
τt−1
2t2/parenrightbigg
(510)
is aPτ-Brownian motion. (Moreover, f(t,τ) is aPτ-martingale: dtf(t,τ) =ρd/hatwiderWt.)
Note that
P(τ,T) =Ft(τ,T)exp/parenleftig
−ρ(T−τ)[Wτ−Wt]−
1
2ρ2[τT(T−τ)+tτ(τ−t)−tT(T−t)]/parenrightig
=
Ft(τ,T)exp/parenleftbigg
−ρ(T−τ)/bracketleftig
/hatwiderWτ−/hatwiderWt/bracketrightig
−1
2ρ2(T−τ)2(τ−t)/parenrightbigg
=
Ft(τ,T)exp/parenleftbigg
−σ/bracketleftig
/hatwiderWτ−/hatwiderWt/bracketrightig
−1
2σ2(τ−t)/parenrightbigg
, (511)
whereFt(τ,T) =P(t,T)/P(t,τ) is the forward price at time tof purchasing a T-
bond at time τ, andσ≡ρ(T−τ) is theterm volatility . Note that the process
82",2014-05-07T01:13:23Z, also   sds since sds i vt ft note     brownish o note ft  ft  ft  ft note
paper_qf_22.pdf,84,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","/hatwiderWτ−/hatwiderWtis a normal N(0,τ−t), and is independent of Ft. It is then clear that the
price of the call option is given by the corresponding Black-Scholes f ormula
Vt=P(t,τ)
Ft(τ,T)Φ
ln/parenleftig
Ft(τ,T)
k/parenrightig
σ√τ−t+σ√τ−t
2

−kΦ
ln/parenleftig
Ft(τ,T)
k/parenrightig
σ√τ−t−σ√τ−t
2

, (512)
The reason why the Black-Scholes formula works in this model is that the latter is
actually log-normal.
22.7 Bond Options in the Vasicek Model
The most general single-factor model with log-normal bond prices is the Vasicek
model:
drt=ρ(t)dWt+[ν(t)−α(t)rt]dt . (513)
Let
β(t,T)≡exp/parenleftbigg
−/integraldisplayT
tα(s)ds/parenrightbigg
, (514)
η(t,T)≡/integraldisplayT
tβ(t,u)du. (515)
Then we have
σ(t,T) =ρ(t)β(t,T), (516)
Σ(t,T) =−ρ(t)η(t,T), (517)
dtf(t,T) =ρ(t)β(t,T)dWt+ρ2(t)β(t,T)η(t,T)dt. (518)
That is,
f(t,T) =f(0,T)+/integraldisplayt
0ρ(s)β(s,T)dWs+/integraldisplayt
0ρ2(s)β(s,T)η(s,T)ds. (519)
We therefore have:
−ln(P(t,T)) =/integraldisplayT
tf(0,u)du+
/integraldisplayT
t/parenleftbigg/integraldisplayt
0ρ(s)β(s,u)dWs/parenrightbigg
du+/integraldisplayT
t/parenleftbigg/integraldisplayt
0ρ2(s)β(s,u)η(s,u)ds/parenrightbigg
du=
/integraldisplayT
tf(0,u)du+/integraldisplayt
0ρ(s)[η(s,T)−η(s,t)]dWs+
1
2/integraldisplayt
0ρ2(s)/bracketleftbig
η2(s,T)−η2(s,t)/bracketrightbig
ds . (520)
83",2014-05-07T01:13:23Z, is ft it  schools vt ft ft ft t  schools bond optns vas ice mt vas ice   t that ws  ws ws
paper_qf_22.pdf,85,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","This gives:
P(τ,T)
Ft(τ,T)= exp/parenleftbigg
−/integraldisplayτ
tρ(s)[η(s,T)−η(s,τ)]dWs−
1
2/integraldisplayτ
tρ2(s)/bracketleftbig
η2(s,T)−η2(s,τ)/bracketrightbig
ds/parenrightbigg
. (521)
Let us now go to the τ-forward measure. The corresponding Brownian motion is
/hatwiderWt:
d/hatwiderWt=dWt−Σ(t,τ)dt=dWt+ρ(t)η(t,τ)dt . (522)
We, therefore, have:
P(τ,T)
Ft(τ,T)= exp/parenleftbigg
−/integraldisplayτ
tρ(s)[η(s,T)−η(s,τ)]d/hatwiderWs−
1
2/integraldisplayτ
tρ2(s)[η(s,T)−η(s,τ)]2ds/parenrightbigg
. (523)
Note that
η(s,T)−η(s,τ) =/integraldisplayT
τβ(s,u)du. (524)
Also, note that the process
ζ(t,τ,T)≡ −/integraldisplayτ
tρ(s)[η(s,T)−η(s,τ)]d/hatwiderWs (525)
is independent of Ft. Moreover, for any real θwe have
/an}bracketle{texp(θζ(t,τ,T))/an}bracketri}htPτ,Ft=/integraldisplay
Dxexp/parenleftbigg
−S[x;t,τ]−θ/integraldisplayτ
tρ(s)[η(s,T)−η(s,τ)] ˙x(s)ds/parenrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x(t)=x∗(t)=
exp/parenleftbiggθ2
2/integraldisplayτ
tρ2(s)[η(s,T)−η(s,τ)]2ds/parenrightbigg
, (526)
which implies that ζ(t,τ,T) is a normal N(0,v(t,τ,T)) with the variance given by
v(t,τ,T) =/integraldisplayτ
tρ2(s)[η(s,T)−η(s,τ)]2ds . (527)
This then immediately implies that the price of the call option is given by:
Vt=P(t,τ)/an}bracketle{t(P(τ,T)−k)+/an}bracketri}htPτ,Ft=
P(t,τ)
Ft(τ,T)Φ
ln/parenleftig
Ft(τ,T)
k/parenrightig
/radicalbig
v(t,τ,T)+/radicalbig
v(t,τ,T)
2

−kΦ
ln/parenleftig
Ft(τ,T)
k/parenrightig
/radicalbig
v(t,τ,T)−/radicalbig
v(t,τ,T)
2

, (528)
84",2014-05-07T01:13:23Z, ft ws  t brownish      ft ws note also ws ft oft dx e  vt ft ft ft ft
paper_qf_22.pdf,86,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","which, once again, is a Black-Scholes-like formula.
For constant ρandαwe have:
β(s,u) = exp(−α(u−s)), (529)
η(s,T)−η(s,τ) =/integraldisplayT
τβ(s,u)du=eαs
α[exp(−ατ)−exp(−αT)],(530)
v(t,τ,T) =ρ2
α2[exp(−ατ)−exp(−αT)]2/integraldisplayτ
texp(2αs)ds=
ρ2
2α3[1−exp(−α[T−τ])]2[1−exp(−2α[τ−t])], (531)
so that the dependence on T−τandτ−tfactorizes.
22.8 Options on Coupon Bonds
Suppose we have a bond with coupons:
Pc(t,T) =P(t,T)+κδn/summationdisplay
i=I(t)P(t,Ti). (532)
Here we use κfor the uncompounded rate for coupons to distinguish it from the
strike price k. Supposethezero-couponbondsfollowasingle-factorshort-ra temodel
with deterministic ρ(rt,t) andν(rt,t). Then each zero-coupon bond price can be
viewed as a deterministic function P(t,T) =V(rt,t,T). In this case we can price a
call option on the coupon bond using the trick due to Jamshidian.
The function V(rt,t,T) monotonically decreases with increasing rt. This means
thatPc(t,T) is also a decreasing function of rt. Letkbe the strike price for the call
option. Then there exists r∗such that
Pc(t,T)|rt=r∗=k . (533)
Letkτ≡V(r∗,t,τ). Then we have
(Pc(t,T)−k)+= (P(t,T)−kT)++κδn/summationdisplay
i=I(t)(P(t,Ti)−kTi)+.(534)
We can therefore price a call option on a coupon bond using the corr esponding call
options on the zero-coupon bonds of various maturities.
22.9 Caps and Floors
Suppose we are borrowing at a ﬂoating rate and want to ensure tha t it does not go
above a ﬁxed rate k. Thecapcontract pays us the diﬀerence between the LIBOR
rate and the ﬁxed rate at each payment time Ti:
δ(L(Ti−1)−k)+, (535)
85",2014-05-07T01:13:23Z, schools for optns coupobonds suppose pc ti re suppose t zero tijamshid iat  pc  be tpc  tpc ti ti  caps floors suppose t cap contra ti ti
paper_qf_22.pdf,87,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","where
L(Ti) =1
δ/parenleftbig
P−1(Ti−1,Ti)−1/parenrightbig
. (536)
An individual payment is called a caplet. If we can price caplets, then we can also
price the cap.
The caplet claim is
XTi=/parenleftbig
P−1(Ti−1,Ti)−1−kδ/parenrightbig+=K−1P−1(Ti−1,Ti)(K−P(Ti−1,Ti))+,(537)
whereK≡(1+δk)−1. The price of the caplet at time tis given by:
Vt=Bt/an}bracketle{tB−1
TiXTi/an}bracketri}htQ,Ft=
K−1Bt/angbracketleftbig
P−1(Ti−1,Ti)B−1
Ti(K−P(Ti−1,Ti))+/angbracketrightbig
Q,Ft=
K−1Bt/angbracketleftig
B−1
Ti−1(K−P(Ti−1,Ti))+/angbracketrightig
Q,Ft, (538)
where in the last line we have used the tower law. Thus, the value of a c aplet is
just the price of (1+ kδ) put options on the Ti-bond with the strike price Kand the
exercise date Ti−1.
Aﬂoorcontract works similarly, we receive a premium for agreeing to never pay
less than some ﬁxed rate k. That is, we pay an extra amount
δ(k−L(Ti−1))+(539)
at timeTi. There is a ﬂoor-cap parity . Thus, the worth of a ﬂoorlet less the cost of
a caplet with the same ﬁxed rate kis
Bt/an}bracketle{tB−1
Tiδ(k−L(Ti−1))/an}bracketri}htQ,Ft= (1+δk)P(t,Ti)−P(t,Ti−1),(540)
where, once again, we have used the tower law. Note that
n/summationdisplay
i=1[(1+δk)P(t,Ti)−P(t,Ti−1)] =P(t,Tn)−P(t,T0)+kδn/summationdisplay
i=1P(t,Ti),(541)
which for time t≤T0is the value of the forward swap with maturity T≥Tnand
the ﬁxed rate k.
22.10 Swaptions
A swaption is an option to enter into a swap on a future date at a given ratek. The
worth of this option at time T0is
/parenleftigg
P(T0,Tn)+kδn/summationdisplay
i=1P(T0,Ti)−1/parenrightigg+
, (542)
which is nothing but a call option struck at $1 on a Tn-bond with the coupon rate
k. This can be understood from the fact that a swap is just a coupon bond less a
ﬂoating bond (the latter always has par value). If you receive a ﬁxe d on a swap, you
have a long position in the bond market, so a swaption looks like a coupo n bond
option.
86",2014-05-07T01:13:23Z,ti ti ti a t ti ti ti ti ti ti ti t vt  ti ti ft  ti ti ti ti ti ft  ti ti ti ft  ti and ti that ti ti tre   ti ti ft ti ti note ti ti tti tand swap ns t tti t 
paper_qf_22.pdf,88,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","23 The General Multi-factor Log-Normal Model
Consider an HJM model with the factorizable volatility surfaces:
σi(t,T) =xi(t)yi(T). (543)
Then we have
Σi(t,T) =−xi(t)/integraldisplayT
tyi(u)du≡ −xi(t)Yi(t,T), (544)
and
dtf(t,T) =n/summationdisplay
i=1yi(T)xi(t)d/tildewiderWi
t+n/summationdisplay
i=1yi(T)[xi(t)]2Yi(t,T)dt. (545)
Themarketcompletenessconditionrequiresthatthematrix At= (Aij
t)≡(Yi(t,Tj))
be non-singular for all t<T1for every set of nmaturitiesT1,...,T n.
Note that
f(t,T) =f(0,T)+n/summationdisplay
i=1yi(T)/integraldisplayt
0xi(s)d/tildewiderWs+n/summationdisplay
i=1yi(T)/integraldisplayt
0[xi(s)]2Yi(s,T)ds.(546)
This gives:
−ln(P(t,T)) =/integraldisplayT
tf(0,u)du+n/summationdisplay
i=1Yi(t,T)/integraldisplayt
0xi(s)d/tildewiderWs+
1
2n/summationdisplay
i=1/integraldisplayt
0[xi(s)]2/parenleftbig
[Yi(s,T)]2−[Yi(s,t)]2/parenrightbig
ds .(547)
Note that
−ln(Ft(τ,T)) =/integraldisplayT
τf(0,u)du+n/summationdisplay
i=1Yi(τ,T)/integraldisplayt
0xi(s)d/tildewiderWs+
1
2n/summationdisplay
i=1/integraldisplayt
0[xi(s)]2/parenleftbig
[Yi(s,T)]2−[Yi(s,τ)]2/parenrightbig
ds . (548)
On the other hand,
−ln(P(τ,T)) =/integraldisplayT
τf(0,u)du+n/summationdisplay
i=1Yi(τ,T)/integraldisplayτ
0xi(s)d/tildewiderWs+
1
2n/summationdisplay
i=1/integraldisplayτ
0[xi(s)]2/parenleftbig
[Yi(s,T)]2−[Yi(s,τ)]2/parenrightbig
ds .(549)
87",2014-05-07T01:13:23Z,t genl multi log normal mconsir tyi wi yi t market eness conditrequirthat t matrix at air yi tj note ws yi  yi ws yi yi note ft yi ws yi yi oyi ws yi yi
paper_qf_22.pdf,89,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","This implies that
−ln/parenleftbiggP(τ,T)
Ft(τ,T)/parenrightbigg
=n/summationdisplay
i=1Yi(τ,T)/integraldisplayτ
txi(s)d/tildewiderWs+
1
2n/summationdisplay
i=1/integraldisplayτ
t[xi(s)]2/parenleftbig
[Yi(s,T)]2−[Yi(s,τ)]2/parenrightbig
ds .(550)
Let us now go to the τ-forward measure. The corresponding Brownian motions are
d/hatwiderWi
t=d/tildewiderWi
t−Σi(t,τ)dt=d/tildewiderWt+xi(t)Yi(t,τ)dt . (551)
This then implies that
−ln/parenleftbiggP(τ,T)
Ft(τ,T)/parenrightbigg
=
n/summationdisplay
i=1Yi(τ,T)/integraldisplayτ
txi(s)d/hatwiderWs+1
2n/summationdisplay
i=1[Yi(τ,T)]2/integraldisplayτ
t[xi(s)]2ds .(552)
The price of a call option then is given by
Vt=P(t,τ)/an}bracketle{t(P(τ,T)−k)+/an}bracketri}htPτ,Ft=
P(t,τ)
Ft(τ,T)Φ
ln/parenleftig
Ft(τ,T)
k/parenrightig
/radicalbig
v(t,τ,T)+/radicalbig
v(t,τ,T)
2

−kΦ
ln/parenleftig
Ft(τ,T)
k/parenrightig
/radicalbig
v(t,τ,T)−/radicalbig
v(t,τ,T)
2

, (553)
where
v(t,τ,t)≡n/summationdisplay
i=1[Yi(τ,T)]2/integraldisplayτ
t[xi(s)]2ds . (554)
Once again, we have a Black-Scholes-like formula as this model is log-n ormal.
23.1 The Brace-Gatarek-Musiela (BGM) Model
Let
L(t,T)≡1
δ/bracketleftbiggP(t,T)
P(t,T+δ)−1/bracketrightbigg
=1
δ/bracketleftbigg
exp/parenleftbigg/integraldisplayT+δ
Tf(t,u)du/parenrightbigg
−1/bracketrightbigg
.(555)
Note thatL(t,T) is theδ-period forward LIBORrate for borrowing at time T. Also,
L(T)≡L(T,T) =1
δ/bracketleftbigg1
P(T,T+δ)−1/bracketrightbigg
(556)
88",2014-05-07T01:13:23Z, ft yi ws yi yi  t brownish wi wi  yi  ft yi ws yi t vt ft ft ft ft yi once  schools t brace gat are mus ie la m note rate also
paper_qf_22.pdf,90,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","is the instantaneous LIBORrate.
In the BGM model the volatility surfaces are restricted as follows:
/integraldisplayT+δ
Tσi(t,u)du=δL(t,T)
1+δL(t,T)γi(t,T), (557)
whereγi(t,T) are some deterministic functions. Under the martingale measure Q
we have:
dtL(t,T) =1
δexp/parenleftbigg/integraldisplayT+δ
Tf(t,u)du/parenrightbigg
×
/bracketleftigg/integraldisplayT+δ
Tdtf(t,u)du+1
2/parenleftbigg/integraldisplayT+δ
Tdtf(t,u)du/parenrightbigg2/bracketrightigg
=
1
δ[1+δL(t,T)]×
n/summationdisplay
i=1/bracketleftbigg
d/tildewiderWi
t/integraldisplayT+δ
Tσi(t,u)du−Σi(t,T+δ)/parenleftbigg/integraldisplayT+δ
Tσi(t,u)du/parenrightbigg
dt/bracketrightbigg
=
L(t,T)n/summationdisplay
i=1γi(t,T)/bracketleftig
d/tildewiderWi
t−Σi(t,T+δ)dt/bracketrightig
. (558)
This implies that under the ( T+δ)-forward measure L(t,T) is a martingale:
dtL(t,T) =L(t,T)n/summationdisplay
i=1γi(t,T)d/hatwiderWi
t. (559)
Moreover, it is log-normallydistributed under PT+δ. This enables us to price certain
options.
Suppose a payment at time Ti+1depends on the instantaneous LIBORrate at
timeTi:XTi+1=f(L(Ti)). Then the value of the payment at time tis given by:
Vt=P(t,Ti+1)/an}bracketle{tf(L(Ti))/an}bracketri}htPTi+1,Ft. (560)
As an example consider a caplet payoﬀ f(L(Ti−1)) =δ(L(Ti−1)−k)+. Then we have
the following Black-Scholes-like formula for its price:
Vt=δP(t,Ti)
L(t,Ti−1)Φ
ln/parenleftig
L(t,Ti−1)
k/parenrightig
/radicalbig
ζ(t,Ti−1)+/radicalbig
ζ(t,Ti−1)
2

−kΦ
ln/parenleftig
L(t,Ti−1)
k/parenrightig
/radicalbig
ζ(t,Ti−1)−/radicalbig
ζ(t,Ti−1)
2

, (561)
where
ζ(t,T)≡n/summationdisplay
j=1/integraldisplayT
t/bracketleftbig
γj(s,T)/bracketrightbig2ds (562)
is the log-variance of L(T,T) givenFt.
89",2014-05-07T01:13:23Z,rate iunr td td wi wi  wi o suppose ti rate ti ti ti tvt ti ti ti ft as ti ti t schools vt ti ti ti ti ti ti ti ti ft
paper_qf_22.pdf,91,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","24 Foreign Currency Interest-rate Models
Suppose we have a dollar zero-coupon bond P(t,T) as well as the dollar cash bond
Bt, a sterling zero-coupon bond Q(t,T) as well as the sterling cash bond Dt, and
the exchange rate Ct, which is the value in dollars of one pound.
A multi-factor model for this market is given by:
dtf(t,T) =n/summationdisplay
i=1σi(t,T)dWi
t+α(t,T)dt , (563)
dtg(t,T) =n/summationdisplay
i=1τi(t,T)dWi
t+β(t,T)dt , (564)
dCt=Ct/bracketleftiggn/summationdisplay
i=1ρi
tdWi
t+λtdt/bracketrightigg
, (565)
wheref(t,T) andg(t,T) are the dollar respectively sterling forward rates. The
dollar tradable securities in this market consist of Bt,P(t,T),CtQ(t,T), andCtDt.
To price various derivative securities, we must make the discounted versions of these
processes into martingales under a single measure Q. This, as usual, gives certain
constraints on the drifts.
25 Quantos
Consider the following quantomodel. The sterling stock price Stand the value of
one pound in dollars Ctfollow the processes:
St=S0exp(σ1W1(t)+µt), (566)
Ct=C0exp/parenleftig
ρσ2W1(t)+/radicalbig
1−ρ2σ2W2(t)+νt/parenrightig
. (567)
In addition we have a dollar cash bond Bt= exp(rt) and a sterling cash bond
Dt= exp(ut).
The dollar tradables are Bt,CtDtandCtSt. The discounted processes for the
last two tradables are Yt=B−1
tCtDtandZt=B−1
tCtSt:
Yt=C0exp/parenleftig
ρσ2W1(t)+/radicalbig
1−ρ2σ2W2(t)+[ν+u−r]t/parenrightig
, (568)
Zt=C0S0exp(/parenleftig
[σ1+ρσ2]W1(t)+/radicalbig
1−ρ2σ2W2(t)+[µ+u−r]t/parenrightig
.(569)
Under the measure Qthat makes both of these into martingales we have:
Yt=C0exp/parenleftbigg
ρσ2/tildewiderW1(t)+/radicalbig
1−ρ2σ2/tildewiderW2(t)−1
2σ2
2t/parenrightbigg
,(570)
Zt=C0S0exp(/parenleftig
[σ1+ρσ2]/tildewiderW1(t)+/radicalbig
1−ρ2σ2/tildewiderW2(t)−
1
2/bracketleftbig
σ2
1+σ2
2+2σ1σ2ρ/bracketrightbig
t/parenrightbigg
. (571)
90",2014-05-07T01:13:23Z,forecurrency interest mols suppose  dt  wi wi   wi t    dt to  quato consir t stand  follow st  i dt t   dt and  st t  dt and  st unr that
paper_qf_22.pdf,92,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","We, therefore, have
Ct=C0exp/parenleftbigg
ρσ2/tildewiderW1(t)+/radicalbig
1−ρ2σ2/tildewiderW2(t)+/bracketleftbigg
r−u−1
2σ2
2/bracketrightbigg
t/parenrightbigg
,(572)
St=S0exp/parenleftbigg
σ1/tildewiderW1(t)+/bracketleftbigg
u−1
2σ2
1−σ1σ2ρ/bracketrightbigg
t/parenrightbigg
. (573)
In quanto contracts the stock price is quoted in the “wrong” curr ency, in this case
in dollars. We can then price quanto contracts as follows.
25.1 A Forward Quanto Contract
Consider a forward quanto contract for buying the stock at time Tfor a pre-agreed
dollar amount k. At timet= 0 the price of this contract is given by
V0=e−rT/an}bracketle{t(ST−k)/an}bracketri}htQ=e−rT(FQ−k), (574)
where
FQ≡Fexp(−σ1σ2ρT), (575)
andF≡S0exp(uT) is the forward price in the local currency (that is, in pounds).
To give the forward quanto contract zero initial value, we must set k=FQ, which
is the forward quanto price.
All other quanto contracts now have a familiar Black-Scholes form w ith the
forward price given by the forward quanto price FQ.
26 Optimal Hedge Ratio
Suppose we are hedging an asset using a futures contract. Let Sbe the spot price
of the asset, and let Fbe the price of the futures contract. The basisis deﬁned as
b=S−F.
Let ∆Sand ∆Fbe the change in spot respectively futures price during the life
of the hedge. Also, let σS,σFandρbe the standard deviation of S, the standard
deviation of Fand the correlation between SandF, respectively. Finally, let hbe
the hedge ratio (the size of the position taken in the futures contr acts to the size of
the exposure).
The change in the value of the hedger’s position during the life of the h edge is
±(∆S−h∆F), (576)
where plus stands for the position long in the asset and short in the f utures, while
minus stands for the position short in the asset and long in the futur es. In either
case the variance is
v=σ2
S+h2σ2
F−2hσSσFρ . (577)
91",2014-05-07T01:13:23Z,  st i forward quato contra consir for at e to all  schools optimal dge rat suppose  be fbe t  and fbe also and and and nally t in
paper_qf_22.pdf,93,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Minimizing this expression gives
h=ρσS
σF, (578)
which gives the optimal hedge ratio.
Typically in the hedging strategy the futures contract has delivery date close to
but later than the expiration of the hedge. It has to be close so tha t the basis risk is
minimized. It is usually chosen later so that the erratic nature of the futures prices
during the delivery months does not aﬀect the hedge. If at a given t ime there is no
liquid futures contract that matures later than the expiration of t he hedge, one can
use the strategy of rolling the hedge forward. This strategy work s well if there is a
close correlation between changes in the futures prices and the ch anges in the spot
prices.
Acknowledgments
I would like to thank everyone who took the course back in 2002 and m ade it a suc-
cess. I am especially grateful to my then Ph.D. students at the C.N. Yang Institute
for Theoretical Physics Olindo Corradini, Alberto Iglesias and Peter Langfelder for
their enthusiastic participation in the course. I am indebted to Yan V torov, among
so many other things, for introducing me to Baxter and Rennie’s boo k, which in-
spired me to give this course.
A Some Fun Questions
Question 1. Two ropesburninhomogeneously (diﬀerent lengths, thicknesses) , each
in 1 hour. You need to measure 45 minutes. How?
Answer. Light both ends of rope A and one end of rope B. Rope A will burn
out in exactly 30 minutes. At that time light the second end of rope B. When it
burns out, that’s the 45 minute mark.
Question 2. You have two jars, 5 liters (jar A) and 3 liters (jar B). How do you
pour 4 liters of water into jar A?
Answer. The following sequence does the trick:
Jar A: 5 2 2 0 5 4
Jar B: 0 3 0 2 2 3
B Quiz 1
Problem 1. If a family has two children and there is a boy in the family, what is
the probability that there is a girl?
92",2014-05-07T01:13:23Z,minii typically it it   ackledgment ph ya institute toical psics lido corr ad ialberto lesias peter la eyabaer ronnie some futns ttwo you how ansr lht rope at you how ansr t jar jar quiz problem 
paper_qf_22.pdf,94,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Answer. 2/3.
Solution. LetBstand for a boy, while Gstand for a girl. Then in a family
with two children a prioriwe have the following fourpossibilities:
B B, (579)
B G, (580)
G B, (581)
G G. (582)
Since we know that in the aforementioned family there is a boy, the las t of the above
four possibilities cannot be the case. This leaves us with the ﬁrst 3 po ssibilities,
among which we have 2 possibilities that there is a girl in this family. Thus , the
probability that there is a girl in the family is
Pcond=2
3. (583)
This is an example of a conditional probability, which diﬀers from the na ive proba-
bilityP= 1/2.
Problem 2. If you have two stocks and they both have the same expected
return, but one has volatility 20% and the other has volatility of 30%, and they
have a 50% correlation, how should I allocate a ﬁxed sum of money bet ween the two
stocks so as to minimize my risk?
Answer. 6/7 in the ﬁrst stock, 1/7 in the second stock.
Solution. Since the expected returns forthe two stocks are thesame, we a ssume
that they have the same drift. Then the risk for a portfolio contain ing these stocks
in some proportion is minimized by minimizing the volatility of the portfolio. Thus,
let the portfolio contain Xamount of stock 1 and 1 −Xamount of stock 2, where
0≤X≤1 is the fraction of stock 1 in the portfolio. Then the volatility of the
portfolio is
σ2=X2σ2
1+(1−X)2σ2
2+2X(1−X)σ1σ2ρ , (584)
whereσ1= 20% is the volatility of stock 1, σ2= 30% is the volatility of stock 2,
andρ= 50% is the correlation. Minimizing σ2we obtain:
X=σ2
2−ρσ1σ2
σ2
1+σ2
2−2ρσ1σ2. (585)
This givesX= 6/7. We should therefore invest 6 /7 of our money into stock 1, and
1/7 into stock 2.
Problem 3. Suppose there is an inﬁnite straight beach and there is a lighthouse
1 mile oﬀshore. The light rotates at 1 revolution per minute. How fast is the image
of the beam on the beach, i.e.the “white dot”, moving along the beach when that
white dot is exactly 3 miles from the lighthouse?
93",2014-05-07T01:13:23Z,ansr solut bst and gst and tsince   cond  problem  ansr solutsince t amount amount tminii   problem suppose t how
paper_qf_22.pdf,95,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","theta
Y
XR
Figure 1: Figure for Problem 3 in Quiz 1. The light emanating from the ligh thouse
is for deﬁniteness assumed to be rotating counterclockwise.
Answer. Approximately 56.5 miles/min.
Solution. To solve this problem, it is useful to visualize it via Fig.1. The
horizontal line is the beach, the center of the circle is the lighthouse , the vertical
distanceY= 1 mile, while the radius of the circle is R= 3 miles. The position
Xof the white dot along the beach is given by (the origin of the X-axis, which is
directed from left to right, is chosen so that when the beam is perpe ndicular to the
beachX= 0)
X=Ytan(θ). (586)
The angle theta is given by ( t= 0 corresponds to X= 0, and we are working
within the ﬁrst quarter of the period T= 1 min., that is, within the ﬁrst 15 seconds)
θ=ωt , (587)
whereω= 2πf= 2πrev./min is the angular velocity, and f= 1/T= 1 rev./min is
thefrequencyofthecircularmotionofthelightbeamemanatingfro mthelighthouse.
Now, the velocity of the white dot along the beach in the X-direction is given by
VX=dX
dt=Yω
cos2(θ)=R2ω
Y= 2πfR2
Y≈56.5 miles/min.(588)
94",2014-05-07T01:13:23Z,  problem quiz t ansr approximately solutto  t t of tat 
paper_qf_22.pdf,96,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","In the last line we have used the fact that cos( θ) =Y/R.
Problem 4. SupposeXis a normal random variable with mean 0 and variance
v, what is the expected value of eX?
Answer. exp/parenleftbigv
2/parenrightbig
.
Solution. The probability distribution for the variable Xis given by:
P(X) =1√
2πvexp/parenleftbigg
−X2
2v/parenrightbigg
. (589)
Note that
/integraldisplay∞
−∞dX P(X) = 1, (590)
/an}bracketle{tX2/an}bracketri}ht ≡/integraldisplay∞
−∞dX X2P(X) =v . (591)
Now, the expected value of eXis
/an}bracketle{teX/an}bracketri}ht ≡/integraldisplay∞
−∞dX eXP(X) =
=1√
2πv/integraldisplay∞
−∞dXexp/parenleftbigg
X−X2
2v/parenrightbigg
=
= exp/parenleftigv
2/parenrightig1√
2πv/integraldisplay∞
−∞dXexp/parenleftigg
−/bracketleftbiggX√
2v−/radicalbiggv
2/bracketrightbigg2/parenrightigg
=
= exp/parenleftigv
2/parenrightig1√
2πv/integraldisplay∞
−∞dYexp/parenleftbigg
−Y2
2v/parenrightbigg
=
= exp/parenleftigv
2/parenrightig
. (592)
In the last line we have used the following change of variables: Y≡X−v.
Problem 5. What is the integral of sec( x) fromx= 0 tox=π/6?
Answer.1
2ln(3)≈.55.
Solution. This integral is computed in the following standard way:
/integraldisplayπ/6
0sec(x)dx=/integraldisplayπ/6
0dx
cos(x)=/integraldisplayπ/6
0cos(x)dx
cos2(x)=/integraldisplayπ/6
0dsin(x)
1−sin2(x)=
1
2/integraldisplayπ/6
0dsin(x)/bracketleftbigg1
1−sin(x)+1
1+sin(x)/bracketrightbigg
=
1
2[−ln(1−sin(x))+ln(1+sin( x))]|π/6
0=1
2ln1+sin(x)
1−sin(x)/vextendsingle/vextendsingle/vextendsingle/vextendsingleπ/6
0=
1
2ln(3)≈.55. (593)
95",2014-05-07T01:13:23Z,iproblem suppose is ansr solutt is note  is e e e iproblem what ansr solut
paper_qf_22.pdf,97,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Problem 6. If you are solving a parabolic partial diﬀerential equation by using
the explicit ﬁnite diﬀerence method, is it worse to have too many time s teps or too
ﬁne a grid in the space dimension?
Answer. For stability of the algorithm it is worse to have too ﬁne a grid in the
space dimension.
Solution. For deﬁniteness let us consider the simplest example of a parabolic
PDE, the diﬀusion equation in one space dimension with a constant diﬀu sion coef-
ﬁcientD>0:
∂u
∂t=D∂2u
∂x2. (594)
Let us consider the FTCS (Forward Time Centered Space) represe ntation, which is
an explicit ﬁnite diﬀerence scheme:
un+1
j−un
j
∆t=D/bracketleftbiggun
j+1−2un
j+un
j−1
(∆x)2/bracketrightbigg
. (595)
Here the subscript jcorresponds to the discretized xcoordinate, while the super-
scriptncorresponds to the discretized time t.
An important point in solving such equations numerically, as is generally the
case when solving initial value (Cauchy) problems, is stability of the alg orithm.
Here the von Neumann stability analysis is particularly convenient. Th us, we look
for eigenmodes (of the diﬀerence equation) of the form:
un
j= (ξ)neikj∆x, (596)
wherekis the wave number, and ξ=ξ(k), which is called the ampliﬁcation factor,
is a complex number. The diﬀerence equation is unstable, in particular , it has
exponentially growing modes, if |ξ(k)|>1 for somek.
In the case of the diﬀusion equation (595) we have the following solut ion for the
ampliﬁcation factor in (596):
ξ= 1−4D∆t
(∆x)2sin2/parenleftbiggk∆x
2/parenrightbigg
. (597)
The stability requirement |ξ| ≤1 then implies the following condition:
2D∆t
(∆x)2≤1. (598)
An intuitive interpretation of this restriction is clear: the maximum allo wed time
step ∆t(up to a numerical factor of order 1) is the diﬀusion time across a ce ll of
width ∆x.
Thus, as we see, for a given size of the spatial grid there is a minimum a llowed
number of time steps, and if we, say, decrease the former by a fac tor of 10, then the
latter must be increased by a factor of 100. So, for the stability of the algorithm it
appears to be worse to have too ﬁne a grid in the spatial dimension.
96",2014-05-07T01:13:23Z,problem  ansr for solutfor  forward time centered space re acatc re newmath t it a so
paper_qf_22.pdf,98,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","However, in practice the conclusions one might draw from the above discussion
in general are not particularly useful. The point is that usually we are interested
in modeling accurately the evolution of features with spatial scales L≫∆x. The
diﬀusion time across a spatial scale of size Lis of order
T∼L2
D. (599)
If we are limited to time steps satisfying (598), we will need to evolve t hrough of
orderL2/(∆x)2steps before interesting things start to happen on the scale L. This
number of steps, however, is usually too large (prohibitive). This is w hy in practice
one usually appeals to either fully ( e.g., backward time) or partially implicit ( e.g.,
Crank-Nicholson) schemes that do not suﬀer from severe stability restrictions such
as (598).
Problem 7. Suppose 2 teams play a series of up to 7 games in which the ﬁrst
team to win 4 games wins the series and then no other games are playe d. Suppose
that you want to bet on each individual game in such a way that when t he series
ends you will be ahead $100 if your team wins the series, or behind by e xactly $100
if your team loses the series, no matter how many games it takes. Ho w much would
you bet on the ﬁrst game?
Answer. $125
4= 31 dollars and 25 cents.
Solution. To solve this problem we can draw a binary tree and work backwards.
There are two observations that simplify the analysis. Thus, let us p ut “+” if our
team wins, and put “ −” if our team loses. Then if we have a slot with 3 +’s and 3
−’s, no matter in what order, there is one more game to be played, whic h is deciding
for the series. It is then clear that before that game, that is, aft er the sixth game, we
must break even, and on the seventh game we must bet $100. Also, we can restrict
our attention to only a half of the binary tree, say, the half that co rresponds to our
team winning the ﬁrst game – indeed, the other half is the same as this half up to
exchanging +’s and −’s.
C Quiz 2
Problem 1. What is the expected minimum number of coin tosses you would need
to make in order to get 3 heads in a row?
Answer. 14.
Solution. For any ﬁnite number of coin tosses there is a ﬁnite probability that
we donotget 3 heads in a row. Therefore, there is no ﬁnite minimum number of
coin tosses that would guarantee 3 heads in a row.
In fact, the probability P(N) that we do get 3 heads in a row grows with the
number of coin tosses N. In particular, P(N)→1 asN→ ∞, so that the set
{P(N)}isnota measure for deﬁning an average number of coin tosses we need
to make to get 3 heads in a row. However, as we will see below, we can d eﬁne a
97",2014-05-07T01:13:23Z,t t     ank nichols problem suppose suppose ho ansr solutto tre  tit also quiz problem what ansr solutfor trefore iiver
paper_qf_22.pdf,99,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","conditional probability/tildewideP(N) such that {/tildewideP(N)}is an appropriate measure. To do
this, let us ﬁrst study some properties of the probabilities P(N).
The probability P(N) can be determined as follows. Let Q(N)≡1−P(N)
(this is the probability that we do not get 3 heads in a row). Then we ha veP(0) =
P(1) =P(2) = 0,Q(0) =Q(1) =Q(2) = 1. For N≥3 we have non-zero P(N).
Thus, for instance, P(3) = 1/8, andQ(3) = 7/8. It is then not diﬃcult to see that
P(N) =1
8/bracketleftigg
1+1
2N−4/summationdisplay
n=0Q(n)/bracketrightigg
, N≥4. (600)
Equivalently, we have
Q(N) =1
16/bracketleftigg
14−N−4/summationdisplay
n=0Q(n)/bracketrightigg
, N≥4. (601)
Note that∞/summationdisplay
n=0Q(n) = 14, (602)
so thatQ(N)→0 andP(N)→1 asN→ ∞.
Next, we deﬁne the conditional probability/tildewideP(N) as the probability of getting 3
heads in a row with Ncoin tosses such that we do notget 3 heads in a row until
the last (that is, Nth) coin toss. It is not diﬃcult to see that
/tildewideP(0) =/tildewideP(1) =/tildewideP(2) = 0, (603)
/tildewideP(3) =1
8, (604)
/tildewideP(N) =1
16Q(N−4), N≥4. (605)
Note that/tildewideP(N)→0 asN→ ∞. In fact, using (602) we have
∞/summationdisplay
N=0/tildewideP(N) =∞/summationdisplay
N=3/tildewideP(N) =1
8+1
16∞/summationdisplay
n=0Q(n) = 1. (606)
Thus, the set /tildewideP≡ {/tildewideP(N)}is an appropriate measure for computing an average
number of coin tosses.
This average number is deﬁned as
N∗≡ /an}bracketle{tN/an}bracketri}ht/tildewideP≡∞/summationdisplay
N=0N/tildewideP(N) =∞/summationdisplay
N=3N/tildewideP(N), (607)
where we took into account (603).
98",2014-05-07T01:13:23Z,to t  tfor  it equivalently note next cointh it note i 
paper_qf_22.pdf,100,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","We can rewrite (607) as follows:
N∗=3
8+∞/summationdisplay
N=4N/tildewideP(N) =
=3
8+1
16∞/summationdisplay
N=4NQ(N−4) =
=3
8+1
16/bracketleftigg∞/summationdisplay
N=4(N−4)Q(N−4)+4∞/summationdisplay
N=4Q(N−4)/bracketrightigg
=
=3
8+1
16/bracketleftigg∞/summationdisplay
n=0nQ(n)+4∞/summationdisplay
n=0Q(n)/bracketrightigg
=
=31
8+1
16∞/summationdisplay
n=0nQ(n). (608)
Next, we can compute the last term in the last line above as follows. Fr om (601) it
follows that
Q(n) =Q(n−1)−1
16Q(n−4), n≥4. (609)
Using this formula, we obtain:
∞/summationdisplay
n=0nQ(n) =Q(1)+2Q(2)+3Q(3)+∞/summationdisplay
n=4nQ(n) =
=Q(1)+2Q(2)+3Q(3)+∞/summationdisplay
n=4nQ(n−1)−1
16∞/summationdisplay
n=4nQ(n−4) =
=Q(1)+2Q(2)+3Q(3)+∞/summationdisplay
n=4(n−1)Q(n−1)+∞/summationdisplay
n=4Q(n−1)−
−1
16∞/summationdisplay
n=4(n−4)Q(n−4)−1
4∞/summationdisplay
n=4Q(n−4) =
= 3Q(3)−Q(0)−Q(1)−Q(2)+15
16∞/summationdisplay
n=0nQ(n)+3
4∞/summationdisplay
n=0Q(n).(610)
This implies that
1
16∞/summationdisplay
n=0nQ(n) = 3Q(3)−Q(0)−Q(1)−Q(2)+3
4∞/summationdisplay
n=0Q(n) =81
8.(611)
Plugging this into (608), we ﬁnally obtain:
N∗= 14. (612)
99",2014-05-07T01:13:23Z, next fr usi  plui
paper_qf_22.pdf,101,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Note.This average number N∗is the same as the sum
∞/summationdisplay
n=0Q(n), (613)
which is not a coincidence.
Problem 2. Suppose that xis a Brownian motion with no drift and unit
variance, i.e.dx=dz. Ifxstarts at 0, what is the probability that xhits 3 before
hitting−5?
Answer. 5/8.
Solution. LetP(x0;x1;x2)denotetheprobabilitythatstartingat x0theBrown-
ianmotionxhitsx1beforeithits x2, wherex1/ne}ationslash=x2. Bydeﬁnition, P(x0;x0;x2) = 1,
andP(x0;x1;x0) = 0. Clearly, we have
P(x0;x1;x2)+P(x0;x2;x1) = 1. (614)
We need to determine P(0;3;−5). According to (614), we have
P(0;3;−5) = 1−P(0;−5;3). (615)
HereP(0;−5;3) is the probability that starting at 0 xhits−5 before it hits 3. Since
Brownian motion is continuous, to hit −5xmust ﬁrst hit −3, so we have
P(0;−5;3) =P(0;−3;3)P(−3;−5;3). (616)
Note that, due to the symmetry under x→ −x, we have
P(0;−3;3) =P(0;3;−3) =1
2. (617)
On the other hand, since Brownian motion is independent of the prev ious history,
we have
P(−3;−5;3) =P(0;−2;6), (618)
so that
P(0;−5;3) =1
2P(0;−2;6). (619)
Now we can use the above trick repeatedly until we obtain a desired r esult. Thus,
we have:
P(0;−2;6) = 1 −P(0;6;−2), (620)
P(0;6;−2) =P(0;2;−2)P(2;6;−2)=1
2P(2;6;−2), (621)
P(2;6;−2) =P(0;4;−4) =1
2, (622)
P(0;6;−2) =1
4, (623)
P(0;−2;6) =3
4. (624)
100",2014-05-07T01:13:23Z,note  problem suppose brownish  starts ansr solut browby    accordi re since brownish note obrownish  
paper_qf_22.pdf,102,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Thus, we have
P(0;−5;3) =3
8, (625)
and
P(0;3;−5) =5
8. (626)
So the probability that starting at 0 xhits 3 before it hits −5 is 5/8.
Note.This result is independent of the variance vofx, which is not surprising
as the actual variance of the corresponding probability distributio n at timetisvt,
and the answer to the question stated in this problem cannot possib ly involve any
time interval. Another way of stating this is that the variance vis a dimensionful
quantity (it has dimension of inverse time assuming that xis dimensionless), so it
cannot enter into a dimensionless quantity such as probability since t here are no
other dimensionful quantities in this problem.
Problem 2a. In Problem 2, what if the drift is m,i.e.dx=m dt+dz?
Answer. The probability that starting at 0 xhits 3 before it hits −5 in this
case equals
exp(9m)+exp(5m)+exp(m)+2cosh(3 m)
8cosh(2m)cosh(3m)cosh(4m). (627)
Solution. Note that now we have two dimensionful quantities, namely, the
variancevand the drift m. Out of these we can form the following dimensionless
combination (assuming that xis dimensionless): m/v. This can now enter non-
trivially into various probabilities.
In the presence of the drift mour discussion in Problem 2 is modiﬁed as follows.
Note that in Problem 2 we used the fact that a path x(t) withx(0) = 0 and
x(T) =xTwas as probable as the path −x(t). This, in particular, implied that
P(0;x1;−x1) =P(0;−x1;x1) = 1/2. To avoid confusion, in the presence of the
driftmwe will denote all probabilities via Qinstead ofP. Then we have (here we
are taking into account that the variance of xv= 1 in the appropriate units of time)
Q(0;x1;−x1) = exp(2mx1)Q(0;−x1;x1). (628)
This can be seen by using the continuous version of the Radon-Nikod ym derivative
and the Cameron-Martin-Girsanov theorem. Thus, we have
Q(0;x1;−x1) =1
1+exp(−2mx1)=exp(mx1)
2cosh(mx1). (629)
All the other probabilities will reduce to probabilities of this type.
Before we obtain Q(0;3;−5), we would like to give a simple derivation of (628)
and (629). Thus, let us assume that xhas dimension of length. Then the variance
vofxhas dimension length2/time, while the drift mhas dimension of length /time,
101",2014-05-07T01:13:23Z, so note  anotr problem iproblem ansr t solutnote out  iproblem note problem was  to instead t radocameromartir nov  all before  tn
paper_qf_22.pdf,103,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","so that the ratio m/vhas dimension of 1 /length. Just on dimensional grounds it is
then clear that the ratio
Q(0;x1;−x1)
Q(0;−x1;x1)=f/parenleftigmx1
v/parenrightig
, (630)
wheref(y)is a dimensionless function of a dimensionless variable y. In the following
we will set v= 1 (in the appropriate units).
Now, from (630) it follows that f(−mx1) = 1/f(mx1), that is,f(−y) = 1/f(y).
This implies that
f(y) = exp[g(y)], (631)
whereg(y) is an odd function of y:g(−y) =−g(y).
To further constrain g(y), consider the following trick. Thus, we have:
Q(0;2x1;−2x1) =Q(0;x1;−2x1)Q(x1;2x1;−2x1) =
Q(0;x1;−2x1)Q(0;x1;−3x1), (632)
Q(0;x1;−3x1) = 1−Q(0;−3x1;x1), (633)
Q(0;−3x1;x1) =Q(0;−x1;x1)Q(−x1;−3x1;x1) =
Q(0;−x1;x1)Q(0;−2x1;2x1), (634)
Q(0;−2x1;2x1) = 1−Q(0;2x1;−2x1). (635)
Putting all of this together, we obtain:
Q(0;2x1;−2x1) =Q(0;x1;−2x1){1−Q(0;−x1;x1)[1−Q(0;2x1;−2x1]},(636)
that is,
Q(0;2x1;−2x1) =Q(0;x1;−2x1)[1−Q(0;−x1;x1)]
1−Q(0;x1;−2x1)Q(0;−x1;x1)=
=Q(0;x1;−2x1)Q(0;x1;−x1)
1−Q(0;x1;−2x1)Q(0;−x1;x1). (637)
This expression can be further reduced using the following trick:
Q(0;x1;−2x1) = 1−Q(0;−2x1;x1), (638)
Q(0;−2x1;x1) =Q(0;−x1;x1)Q(−x1;−2x1;x1) =
Q(0;−x1;x1)Q(0;−x1;2x1), (639)
Q(0;−x1;2x1) = 1−Q(0;2x1;−x1), (640)
Q(0;2x1;−x1) =Q(0;x1;−x1)Q(x1;2x1;−x1) =
Q(0;x1;−x1)Q(0;x1;−2x1). (641)
Putting all of this together, we obtain:
Q(0;x1;−2x1) = 1−Q(0;−x1;x1)[1−Q(0;x1;−x1)Q(0;x1;−2x1)],(642)
102",2014-05-07T01:13:23Z,just i  to  pui  pui
paper_qf_22.pdf,104,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","that is,
Q(0;x1;−2x1) =1−Q(0;−x1;x1)
1−Q(0;−x1;x1)Q(0;x1;−x1)=
=Q(0;x1;−x1)
1−Q(0;x1;−x1)Q(0;−x1;x1). (643)
Plugging this into (637), we obtain:
Q(0;2x1;−2x1) =[Q(0;x1;−x1)]2
1−2Q(0;x1;−x1)Q(0;−x1;x1). (644)
From this expression it immediately follows that
Q(0;2x1;−2x1)
Q(0;−2x1;2x1)=/bracketleftbiggQ(0;x1;−x1)
Q(0;−x1;x1)/bracketrightbigg2
. (645)
This then implies that the function f(y) has the following property:
f(2y) = [f(y)]2, (646)
that is,
g(2y) = 2g(y). (647)
In fact, the function g(y) has the property that for an arbitrary real number λ
g(λy) =λg(y), (648)
that is,g(y) is a homogeneous linear function of y:
g(y) =κy , (649)
f(y) = exp(κy), (650)
whereκis a coeﬃcient which still needs to be ﬁxed.
Finally, let us ﬁx κ. We have:
Q(0;x1;−x1) = exp(κmx1)Q(0;−x1;x1), (651)
Q(0;x1;−x1) =exp/parenleftbigκ
2mx1/parenrightbig
2cosh/parenleftbigκ
2mx1/parenrightbig. (652)
It is convenient to consider the case of small x1. Then we can consider a discrete
version of the above Brownian motion:
∆x= ∆z+m∆t , (653)
where
∆z=ǫ√
∆t . (654)
103",2014-05-07T01:13:23Z,plui from  inally  it tbrownish
paper_qf_22.pdf,105,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Hereǫis a normally distributed random variable with a mean of zero and unit
variance. Then the corresponding binomial model will, at any given va lue of discrete
time, have a step up Uand a step down Dwith the probabilities pand 1−p,
respectively. The mean and the variance of ∆ xare given by
/an}bracketle{t∆x/an}bracketri}ht=pU+(1−p)D , (655)
/an}bracketle{t(∆x)2/an}bracketri}ht=pU2+(1−p)D2. (656)
On the other hand, we know that (in the second equation below we ar e neglecting
a term of order (∆ t)2)
/an}bracketle{t∆x/an}bracketri}ht=m∆t , (657)
/an}bracketle{t(∆x)2/an}bracketri}ht= ∆t . (658)
This gives two equations for three unknowns U,D,p. We, therefore, have some
freedom in choosing our binary model. As will become clear in a moment, for our
purposes here it is convenient to choose D=−U. Then we have:
U=−D=√
∆t , (659)
p=1
2/bracketleftig
1+m√
∆t/bracketrightig
. (660)
Now consider x1=U=√
∆tin (652). In the context of the above binomial model
it is clear that Q(0;x1;−x1) in this case is nothing but the probability that xwill
make a step up (while Q(0;−x1;x1) is the probability that xwill make a step down).
That is,
Q(0;x1;−x1) =Q(0;U;−U) =p . (661)
On the other hand, from (652) we have (here we are neglecting the O(U2) terms)
Q(0;U;−U) =1
2/bracketleftig
1+κ
2mU/bracketrightig
=1
2/bracketleftig
1+κ
2m√
∆t/bracketrightig
. (662)
Comparing this with (660) we obtain κ= 2.
Finally, let us compute Q(0;3;−5). As in Problem 2 we proceed as follows. We
have
Q(0;3;−5) = 1 −Q(0;−5;3), (663)
Q(0;−5;3) =Q(0;−3;3)Q(−3;−5;3) =Q(0;−3;3)Q(0;−2;6) =
=exp(−3m)
2cosh(3m)Q(0;−2;6), (664)
Q(0;−2;6) = 1 −Q(0;6;−2), (665)
Q(0;6;−2) =Q(0;2;−2)Q(2;6;−2) =Q(0;2;−2)Q(0;4;−4) =
=exp(6m)
4cosh(2m)cosh(4m). (666)
104",2014-05-07T01:13:23Z,re tand with t o  as t ithat oari nally as problem 
paper_qf_22.pdf,106,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Putting all of this together, we obtain:
Q(0;3;−5) = 1 −exp(−3m)
2cosh(3m)/bracketleftbigg
1−exp(6m)
4cosh(2m)cosh(4m)/bracketrightbigg
,
=exp(9m)+exp(5m)+exp(m)+2cosh(3 m)
8cosh(2m)cosh(3m)cosh(4m).(667)
Note.The binary tree approach gives us an immediate answer to Problem 2
above, where we have no drift. Thus, consider the probability P(0;a;−b) (in the
notationsofProblem2) with a,b>0. Since thereisno drift, andsince the Brownian
motion has no scale, it is clear that
P(0;a;−b) =f(λ), (668)
wherefis some function and λ≡a/b. Furthermore, we have
P(0;a;−b)+P(0;−b;a) = 1, (669)
P(0;b;−a) =P(0;−b;a), (670)
so
f(λ)+f(1/λ) = 1. (671)
As above let us now consider a binary tree with a step up U(probability p), and a
step downD(probability 1 −p). The driftlessness implies that
pU+(1−p)D= 0, (672)
sop=−D/(U−D),Uis positive, and Dis negative. Furthermore, P(0;U;D) =p.
However, above we established that P(0;U;D) =f(λ), whereλ=−U/D. This
then implies that
f(λ) =1
1+λ(673)
and
P(0;a,−b) =b
a+b. (674)
So,P(0;3;−5) = 5/8.
Problem 3. IfX,YandZare 3 random variables such that XandYare 90%
correlated and YandZare 80% correlated, what is the minimum correlation that
XandZcan have?
Answer. The minimum possible correlation between XandZis approximately
45.8%. (The maximum possible correlation between XandZis approximately
98.2%.)
Solution. For notational convenience let us introduce the following notation:
X1≡X,X2≡Y,X3≡Z. We can write Xi,i= 1,2,3, as linear combinations of
some independent random variables Piwith unit variances and zero correlations:
/an}bracketle{tPiPj/an}bracketri}ht=δij, (675)
105",2014-05-07T01:13:23Z,pui note t problem  problem since brownish furtr as t is is furtr  so problem  and are and are and are and caansr t and is t and is solutfor  xi pi with pi 
paper_qf_22.pdf,107,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","where/an}bracketle{tA/an}bracketri}htdenotes the expectation value of A. Thus,
Xi= ΛijPj, (676)
where Λ ijare real coeﬃcients, and summation over repeated indices is implicit.
Then we have:
Mij≡ /an}bracketle{tXiXj/an}bracketri}ht= ΛikΛjk, (677)
or in the matrix form
M= ΛΛT, (678)
where superscript Tdenotes transposition.
From (678) we have the following condition:
det(M) = [det(Λ)]2≥0. (679)
On the other hand, we have
M=
σ2
1ρ12σ1σ2ρ13σ1σ3
ρ12σ1σ2σ2
2ρ23σ2σ3
ρ13σ1σ3ρ23σ2σ3σ2
3
, (680)
whereσ2
iis the variance of the random variable Xi, andρij,i/ne}ationslash=jis the correlation
between the variables XiandXj. In terms of σiandρijwe have:
det(M) =σ2
1σ2
2σ2
3/bracketleftbig
1+2ρ12ρ23ρ13−ρ2
12−ρ2
23−ρ2
13/bracketrightbig
. (681)
SinceMmust be positive semi-deﬁnite, we have the following condition:
1+2ρ12ρ23ρ13−ρ2
12−ρ2
23−ρ2
13≥0. (682)
The roots of the corresponding quadratic equation for ρ13
ρ2
13−2(ρ12ρ23)ρ13+/parenleftbig
ρ2
12+ρ2
23−1/parenrightbig
= 0 (683)
are given by
ρ±
13=ρ12ρ23±/radicalig
1+ρ2
12ρ2
23−ρ2
12−ρ2
23=ρ12ρ23±ρ12ρ23,(684)
where
ρ12≡/radicalig
1−ρ2
12, (685)
ρ23≡/radicalig
1−ρ2
23. (686)
106",2014-05-07T01:13:23Z, xi  tmi xi xj notfrom oxi xi and xj isince must t
paper_qf_22.pdf,108,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","It is not diﬃcult to see that for any values of ρ12andρ23between−1 and 1, we have
ρ−
13≥ −1, andρ+
13≤1.
Next, to satisfy the condition (682), we must have
ρ−
13≤ρ13≤ρ+
13, (687)
so that the minimum possible correlation ρ13is
(ρ13)min=ρ−
13. (688)
In our case ρ12=.9, andρ23=.8, soρ−
13≈.458, so that the minimum possible
correlation between XandZis approximately 45 .8%. (Similarly, ρ+
13≈.982, so
that the maximum possible correlation between XandZis approximately 98 .2%.)
Problem 4. Suppose two cylinders each with radius 1 intersect at right angles
and their centers also intersect. What is the volume of the intersec tion?
Answer. 16/3.
Solution. This problem can be solved in the following standard way. Let one
of the cylinders have its axis along the z-axis, while the other one along the y-axis.
Then the boundary B=∂Mof the intersection Mis described by the following set
of equations:
x2+y2= 1, (689)
x2+z2= 1. (690)
To ﬁnd the volume of the intersection we must compute the integral
VM=/integraldisplay
Mdxdydz . (691)
To compute this integral, it is convenient to divide the intersection int o 8 octants,
compute the volume of any one octant, and multiply the answer by 8. This is
because in computing the above integral we will encounter a square root, for which
we will have to choose an appropriate branch (which corresponds t o choosing an
octant, or, more precisely, a set of octants). To avoid this, we ca n use the symmetry
of the problem, and compute the volume of an individual octant.
Thus, let us compute the volume of the octant for which 0 ≤x,y,z≤1. The
corresponding integral is given by
V1=/integraldisplay
D1dxdy/integraldisplay√
1−x2
0dz=/integraldisplay
D1dxdy√
1−x2, (692)
whereD1is the ﬁrst quarter of the disk of unit radius in the xyplane:x2+y2≤1,
0≤x,y≤1. To compute V1let us change the x,yintegration to that over the
corresponding polar coordinates:
x=ρcos(φ), y=ρsin(φ). (693)
107",2014-05-07T01:13:23Z,it next iand is simarly and is problem suppose what ansr solut  tof mis to dx dy dz to  to  t to
paper_qf_22.pdf,109,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","In the polar coordinates D1is given by 0 ≤ρ≤1, 0≤φ≤π/2. Thus, our integral
becomes:
V1=/integraldisplayπ/2
0dφ/integraldisplay1
0dρ ρ/radicalbig
1−ρ2cos2(φ) =
=/integraldisplayπ/2
0dφ/parenleftbigg
−1
3cos2(φ)/parenrightbigg/parenleftbig
1−ρ2cos2(φ)/parenrightbig3/2/vextendsingle/vextendsingle/vextendsingle1
0=
=1
3/integraldisplayπ/2
0dφ1−sin3(φ)
cos2(φ)=1
3/integraldisplayπ/2
0dφ/bracketleftbigg1
cos2(φ)−sin(φ)
cos2(φ)+sin(φ)/bracketrightbigg
=
=1
3/bracketleftbigg
tan(φ)−1
cos(φ)−cos(φ)/bracketrightbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleπ
2−ǫ
0. (694)
In the last line we have introduced an inﬁnitesimal shift in the upper int egration
limit (ǫ>0) to carefully treat the fact that tan( φ) as well as 1 /cos(φ) blow up as
φ→π/2. At the end of the day we will take ǫ→0.
Thus, we have
V1=2
3, (695)
and the volume of the intersection is
VM= 8V1=16
3. (696)
Note that this volume is somewhat larger than the volume Vball=4π
3of a unit
ball, which is consistent with the fact that the intersection Mcontains a unit ball
centered at the center of the intersection.
Problem 5. Consider the following C program for producing Fibonacci num-
bers:
int Fibonacci(int n)
{
if (n<=0 || n==1)
return 1;
else
return Fibonacci(n-1)+Fibonacci(n-2);
}
If for some large n, it takes 100 seconds to compute Fibonacci(n) , how long will it
take to compute Fibonacci(n+1) , to the nearest second?
Answer. Approximately1+√
5
2×100 seconds ≈162 seconds.
Solution. The above program does the following. Let F(n) beFibonacci(n) .
Then the ﬁrst step sets F(n≤1) = 1. For n >1 the second step computes F(n)
via
F(n) =F(n−1)+F(n−2). (697)
108",2014-05-07T01:13:23Z,i iat  note ball cont articial intellence ns problem consir bonacci bonacci bonacci bonacci  bonacci bonacci ansr approximately solutt  bonacci tfor
paper_qf_22.pdf,110,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","So the entire process can be viewed as a binary tree where the top o f this tree is
F(n), which is computed by adding two numbers F(n−1) andF(n−2),F(n−1) is
computed by adding F(n−2) andF(n−3), whileF(n−2) is computed by adding
F(n−3) andF(n−4), and so on. A particular branch ends if we hit F(1) orF(0).
From this binary tree we see that the time T(n) that it takes to compute F(n) is
given by
T(n) =T(n−1)+T(n−2)+∆(n), (698)
where ∆(n) is the time required to call F(n−1) andF(n−2), and then add them.
We do not have enough information to determine ∆( n). However, as we will see
in a moment, we actually do notneed it. All we need is that for large nthe ratio
∆(n)/T(n) goes to zero, which is a reasonable assumption.
From (698) we have
T(n+1)
T(n)= 1+T(n−1)
T(n)+∆(n+1)
T(n), (699)
or, equivalently,
Q(n) = 1+1
Q(n−1)+∆(n+1)
T(n), (700)
where
Q(n)≡T(n+1)
T(n). (701)
The last term in (700) goes to zero for large n. It is then clear that Q(n) has a ﬁnite
non-zero limit as n→ ∞, call itQ∗. From (700) we see that
Q∗= 1+1
Q∗. (702)
Solving this equation (and keeping the positive root), we obtain
Q∗=1+√
5
2≈1.62. (703)
Thus, if for some large nit takesT(n) = 100 seconds to compute F(n), then it takes
T(n+1)≈Q∗T(n)≈162 seconds to compute F(n+1).
Problem 6. Show thatp2−1 is divisible by 24 if pis a prime number, p>3.
Solution. p= 2n+1 (n>1)⇒p2−1 = 4n(n+1)⇒p2−1 is divisible by 8.
p= 3m±1 (m≥2)⇒p2−1 = 3m(3m±2)⇒p2−1 is divisible by 3.
Problem 7. YouhaveNrandomvariablestaking valuesbetween 0and1. What
is the expected value of the smallest one.
Answer. 1/(N+1).
109",2014-05-07T01:13:23Z,so from  all from t it from solvi  problem show solutproblem you he random variabltaki what ansr
paper_qf_22.pdf,111,Phynance,"  These are the lecture notes for an advanced Ph.D. level course I taught in
Spring'02 at the C.N. Yang Institute for Theoretical Physics at Stony Brook.
The course primarily focused on an introduction to stochastic calculus and
derivative pricing with various stochastic computations recast in the language
of path integral, which is used in theoretical physics, hence ""Phynance"". I
also included several ""quiz"" problems (with solutions) comprised of
(pre-)interview questions quantitative finance job candidates were sometimes
asked back in those days. The course to a certain extent follows an excellent
book ""Financial Calculus: An Introduction to Derivative Pricing"" by M. Baxter
and A. Rennie.
","Solution. Let
IN(a)≡/integraldisplay1
0dx1...dxNmin(x1,...,x N,a). (704)
What we need to compute is IN(1). We have
IN(a) =
=/integraldisplay1
0dx1...dxNmin(x1,...,x N,a) =
=/integraldisplay1
0dx1...dxN/braceleftbig
min(x1,...,x N)−(min(x1,...,x N)−a)+/bracerightbig
=
=IN(1)−/integraldisplay1
adx1...dxN(min(x1,...,x N)−a) =
=IN(1)−/integraldisplay1−a
0d/tildewidex1...d/tildewidexNmin(/tildewidex1,...,/tildewidexN) =
=IN(1)−(1−a)N+1/integraldisplay1
0dy1...dyNmin(y1,...,y N) =
=IN(1)/bracketleftbig
1−(1−a)N+1/bracketrightbig
. (705)
Furthermore,
IN+1(1) =/integraldisplay1
0da IN(a) =IN(1)/bracketleftbigg
1−/integraldisplay1
0da(1−a)N+1/bracketrightbigg
=IN(1)N+1
N+2,(706)
which recursion relationtogether with I1(1) = 1/2then implies that IN= 1/(N+1).
Problem 7a. You haveNcars entering a one-lane highway at random speeds.
What is the expected number of clusters?
Answer. The expected number of clusters ENis given by the N-th harmonic
numberHN=/summationtextN
k=11/k= ln(N)+γ+O(1/N), whereγis the Euler constant.
Solution. If we have Ncars on the highway and an ( N+ 1)-th car enters, if
its speed is lower than the expected minimum speed of the Ncars then it will form
another cluster, otherwise it’ll join the last existing cluster. Assum ing all cars travel
at random speeds between 0 and 1, the expected minimum speed of t heNcars is
1/(N+1) (see Problem 7), so the probability that the speed of the ( N+1)-th car is
lower isPN+1= 1/(N+1), and we have EN+1=EN+PN+1, from which recursion
relation together with E1= 1 it follows that EN=HN.
Bibliography
•M. Baxter and A. Rennie, “Financial Calculus: An Introduction to De rivative
Pricing”, Cambridge University Press (1996), 233 pp; also see refe rences therein.
•Z. Kakushadze, “Path Integral and Asset Pricing”, SSRN Working Papers Series,
http://ssrn.com/abstract=2506430; also see references ther ein.
110",2014-05-07T01:13:23Z,solut nm iwhat  nm inm inm ifurtr problem you cars what ansr t is ruler solut cars cars as sum cars problem biblgrap baer ronnie nancial calculus aintrodu prici cambridge   u sha path integral asset prici worki pas series
paper_qf_23.pdf,1,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","Density analysis of non-Markovian BSDEs and applications
to biology and ﬁnance.
Thibaut Mastrolia
October 8, 2018
Abstract
In this paper, we provide conditions which ensure that stochastic Lipschitz BSDEs admit
Malliavin diﬀerentiable solutions. We investigate the problem of existence of densities for
the ﬁrst components of solutions to general path-dependent stochastic Lipschitz BSDEs
and obtain results for the second components in particular cases. We apply these results
to both the study of a gene expression model in biology and to the classical pricing
problems in mathematical ﬁnance.
Key words: BSDEs, Malliavin calculus, Nourdin-Viens’ Formula, gene expression, option
pricing.
AMS 2010 subject classiﬁcation: Primary: 60H10; Secondary: 60H07, 91G30, 92D20.
1 Introduction
The problem of existence of densities for random processes, as e.g.solutions of stochastic
diﬀerential equations (SDEs), has been a very active strand of research in the last two
decades, see among others [26, 35]. A very useful criterion to prove that the law of a
random variable admits a density is the criterion of Bouleau and Hirsch, see e.g.[35,
Theorem 2.1.2]. The analysis of densities has been the subject of several works dealing
with Stochastic Partial Diﬀerential Equations (SPDEs), among which we can mention
the study of the stochastic heat equation, the stochastic wave equation (see for instance
[33], [36], [31]), the Navier-Stokes equation [11] and recently the Landau equation for
Maxwellian molecules (see [12]). Besides, most of these papers investigate tails estimates
of the solutions to SPDEs by using the formula of Nourdin and Viens, introduced in [34],
to have a better understanding of these processes.
Although the problem of existence of densities for S(P)DEs, together with estimates on
their tails, has been a prosperous ﬁeld, the corresponding theory for Backward Stochastic
Diﬀerential Equations (BSDEs) has not received the same attention in the literature.
BSDEs were introduced for the ﬁrst time in 1973 by Bismut in [5], in order to study
stochastic control problems and their links to the Pontryagin maximum principle. The
theory of BSDEs was then formalised and developed in the 90’s, with the seminal papers
[38, 39] and [17]. In the last decades, BSDEs have been the object of an ever growing
Université Paris-Dauphine, CEREMADE UMR CNRS 7534, Place du Maréchal De Lattre De Tassigny,
75775 Paris Cedex 16, FRANCE, mastrolia@ceremade.dauphine.fr
1arXiv:1602.06101v1  [math.PR]  19 Feb 2016",2016-02-19T10:13:34Z,nsity mark iathibaut astro lia ober abstra ilipschitz alla i lipschitz  key alla ino url ivi ens  primary secondary introdut boyle au harsh torem t stochastic partial di equatns nistorland  iabesis no url ivi ens although backward stochastic di equatns is but potry agait i paris dauphine place mar  laer  assparis ce  
paper_qf_23.pdf,2,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","interest, since these equations naturally appear in ﬁnancial problems, as for instance
pricing problems (see [17]) and utility maximisation problems (see [45], [22]).
As far as we know, the existence of densities for solutions to BSDEs was studied in three
papers. Conditions ensuring that the ﬁrst component Yof the solution to a Lipschitz
BSDE admits a density were provided for the ﬁrst time in [3]. In this paper, the authors
also investigated both estimates on the existing density and its smoothness. Then, a
result ensuring existence of a density for the second component Zof the solution to a
particular BSDE, in which the generator is linear with respect to its zvariable, was
obtained in [1]. Recently, this problem was studied in [29] for both the Yand theZ
components of solutions to BSDEs with a quadratic growth generator. However, [3, 29]
only consider Markovian BSDEs, that is the case where the data and!7",2016-02-19T10:13:34Z,as conditns of lipschitz itof recently and mark iaes
paper_qf_23.pdf,3,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","the non-Markovian case. We give in Section 4.1 conditions which ensure the existence of
densities for the law of the Ycomponent of the solution to stochastic Lipschitz BSDEs,
by using Bouleau and Hirsch’s Criterion. We provide weaker conditions in Section 4.2
for theYcomponent of the solution to a non-Markovian Lipschitz BSDE. We then turn
to theZcomponent in Section 5. We ﬁrst provide in Section 5.1 conditions ensuring
that the law of the Ztcomponent has a density for a particular class of BSDE, extending
the results of [1]. We then explain in Section 5.2 why we are not able to adapt the proofs
of [29] to the non-Markovian framework for the Zcomponents of solutions to general
non-Markovian BSDEs and we indicate paths for future researches. We ﬁnally apply our
study in Sections 6 and 7 to biology and ﬁnance respectively.
In Section 6, we propose to study mathematically a model of synthesis of proteins intro-
duced in [46], with the Malliavin calculus. Indeed, in order to validate their model, the
authors of [46] need to compare the law of the protein concentration at time tobtained
by solving a BSDE with the data produced by Gillespie Method (see [20]). However,
in [46], the authors assumed implicitly that the law of the ﬁrst component Ytof the
BSDE under consideration admits a density with respect to the Lebesgue measure. The
present paper can be seen as a mathematical strengthening of the model developed in
[46] by using the so-called Nourdin and Viens’ formula to obtain Gaussian estimates of
the density. Besides, we propose to extend their model to the non-Markovian setting,
which could be quite relevant when we study the synthesis of protein in some models
(see for instance [7, 27, 18]).
In Section 7, we study classical pricing problems. As showed in [17], this problem can be
reduced to solve a stochastic linear BSDE. In this section we aim at applying the results
obtained in previous sections to Asian and Lookback options in the Vašìček Model to
obtain information on both the regularity of the value function and the regularity of
optimal strategies.
2 Preliminaries and notations
2.1 Notations
We denote by the Lebesgue measure on R. LetT > 0be a time ﬁxed horizon.
Let
 :=C0([0;T];R)be the canonical Wiener space of continuous function !from
[0;T]toRsuch that!(0) = 0. We denote by W:= (Wt)t2[0;T]the canonical Wiener
process, that is, for any time tin[0;T],Wt(!) :=!tfor any element !in
. We
setFothe natural ﬁltration of W. Under the Wiener measure P, the process Wis a
standard Brownian motion and we denote by F:= (Ft)t2[0;T]the usual right-continuous
and complete augmentation of Founder P. For the sake of simplicity, we denote all
expectations under PbyEand we set for any t2[0;T]Et[] :=E[jFt]. Besides, all
notions of measurability for elements of 
will be with respect to the ﬁltration For the
-ﬁeldFT.
We set h:=L2([0;T];R), whereB([0;T])is the Borel -algebra on [0;T], and consider
the following inner product on h
hf;gi:=ZT
0f(t)g(t)dt;8(f;g)2h2;
with associated norm kkh. Let nowHbe the Cameron-Martin space that is the space
of functions in 
which are absolutely continuous with square-integrable derivative and
which start from 0at0:
H:=
h: [0;T]",2016-02-19T10:13:34Z,mark ia seonent lipschitz boyle au harsh iter seonent mark ialipschitz  onent se seonent  semark iaonents mark ia sens isealla iined glespie method of le be gue t no url ivi ens besis mark iaiseas i look back va mpreliminarinotatns  le be gue   winner su  winner   fo t unr winner wis brownish ft founr for by and et ft besis for  bore  be cameromartin
paper_qf_23.pdf,4,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","the inner product hh1;h2iH:=h_h1;_h2ih, for any (h1;h2)2HH, and with associated
normkhk2
H:=h_h;_hih. Letp1. DeﬁneLp(K)as the set of allFT-measurable random
variablesFwhich are valued in an Hilbert space K, and such thatkFkLp(K)<+1,
where
kFkLp(K):=E[kFkp
K]1=p;
where the normkkKis the one canonically induced by the inner product on K. We
deﬁne
Lp([t;T];K) :=(
f: [t;T]",2016-02-19T10:13:34Z,  lp whbert fk lp fk lp fk is  lp
paper_qf_23.pdf,5,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","2.2 Elements of Malliavin calculus
We give in this section some results on the Malliavin calculus that we will use in this
paper. Let nowSbe the set of cylindrical functionals, that is the set of random variables
Fof the form
F=f(W(h1);:::;W (hn));(h1;:::;hn)2Hn; f2C1
b(Rn);for somen1;(2.2)
whereW(h) :=RT
0_hsdWsfor anyhinHand where C1
b(Rn)denotes the space of
bounded mappings which are inﬁnitely continuously diﬀerentiable with bounded deriva-
tives. For any FinSof the form (2.2), the Malliavin derivative rFofFis deﬁned as
the following H-valued random variable:
rF:=nX
i=1fxi(W(h1);:::;W (hn))hi; (2.3)
wherefxi:=df
dxi. It is then customary to identify rFwith the stochastic process
(rtF)t2[0;T]. Denote then by D1;pthe closure ofSwith respect to the Malliavin-Sobolev
semi-normkk 1;p, deﬁned as:
kFk1;p:= (E[jFjp] +E[krFkp
H])1=p:
We set D1;1:=T
p2D1;p. We make use of the notation DFto represent the derivative
ofrFas:
rtF=Zt
0DsFds; t2[0;T]:
To avoid any ambiguity in the non-Markovian case we will consider later on, we need to
introduce immediately some further notations. For any mapping ~ffrom [0;T]
R
intoR, we letD~f(t;y)be the Malliavin derivative, computed at the point (t;y), of
!7",2016-02-19T10:13:34Z,elements alla i alla i be of hrws for hand rfor of alla iof is it with note with alla iso bold fk jp fk   to as ds fd to mark iafor alla in
paper_qf_23.pdf,6,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","LetFsuch thatkDFkh>0,P",2016-02-19T10:13:34Z, sukh
paper_qf_23.pdf,7,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","(iv) Ifp2(1
2;1), there exists a positive constant Lsuch thatAa
T<L;P-a.s.
Remark 3.1. Notice that the case a0is excluded according to (ii). However, this
case can be studied easily since a0implies that fis constant with respect to yandz.
Then, we can provide an explicit expression for the solution to this kind of BSDE.
The main diﬃculty in this study is that the process ais not bounded and the stochastic
integral ofais not a BMO-martingale under Assumption (sLp;). We recall the following
result which can be found in [48] and extends the results in [16].
Theorem 3.1 (Theorem 4.1 together with Proposition 3.6 in [48]) .Letp >1
2and
 >maxf2=(2p",2016-02-19T10:13:34Z, suremark notice tt assumptlp  torem torem proposit
paper_qf_23.pdf,8,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","Remark 3.2. Concerning Property (ii)of Assumption (DsLp;), notice that for ﬁxed
(y;z), the process (s;!)7",2016-02-19T10:13:34Z,remark concerni proty assumptds lp
paper_qf_23.pdf,9,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","We thus have the following theorem.
Theorem 3.2. Letpbe in2",2016-02-19T10:13:34Z, torem  be
paper_qf_23.pdf,10,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","where
p;a;
"" :=Eh
epAa
Tj""",2016-02-19T10:13:34Z,eh tj
paper_qf_23.pdf,11,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","We now turn to Xz;""
T. By proceeding similarly, we have
Xz;""
T=E"" ZT
0eAa
tj~Zh
tj2Az;""
t",2016-02-19T10:13:34Z, by zh az
paper_qf_23.pdf,12,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","(i)For eacht2[0;T],(y;z)7",2016-02-19T10:13:34Z,for
paper_qf_23.pdf,13,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","We now set the following assumptions
(sD1)For anyp>1,belongs to D1;pand!7",2016-02-19T10:13:34Z, for
paper_qf_23.pdf,14,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","with
~Ay;""
r:=Z1
0fy(r;+""h;Yr+(Yr""h",2016-02-19T10:13:34Z,ay yr yr
paper_qf_23.pdf,15,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","Concerning the second approach, a priori estimates (3.12) seem to be better, since they
are similar to those obtained in the Lipschitz or quadratic case (see [9]). Notice however
that the order of these a priori estimates depends closely on the BMO-norm of the
stochastic integral of the Lipschitz constant K, which in practice, could be quite diﬃcult
to control. We provide conditions in D1;pfor anyp>1due to the control of the norm of
YandZat an order depending on this BMO-norm. Assumptions (sD1)and(sH1;1)
are not so surprising, since they are similar to conditions obtained in Section 7 in [30]
when dealing with quadratic growth BSDEs.
From now, we set the following two assumptions.
(EKHp;)Let Assumptions (sL1;)and(DsLp;)hold.
(BC)Let Assumptions (BC1)-(BC3),(sD1),(sH1;1)and(sH2;1)hold.
3.4 Example: aﬃne BSDE with unbounded coeﬃcients
We now study a particular stochastic Lipschitz BSDE in the non-Markovian case:
Yt=+ZT
tf(s;Ys;Zs)ds",2016-02-19T10:13:34Z,concerni lipschitz notice lipschitz  and at assumptns sefrom hp  assumptns ds lp  assumptns   lipschitz mark iays
paper_qf_23.pdf,16,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","need to have a BMO-property forR
0sdWs, since only Relation (2)in [8], corresponding
to(A2)(i), is used to deal with terms depending on . Hence, for aﬃne BSDE (3.19)
we can make replace Assumption (BC1)with Assumptions (A1)and(A2). We then
deduce that BSDE (3.19) admits a unique solution (Y;Z)2SpHpfor anyp>1and
(3.12) holds for any 1<p<p.
In this particular case, Assumptions (sD1),(sH1;1)and(sH2;1)become
(DA1)For anyp>1,belongs to D1;pand the stochastic processes
(t;!)7",2016-02-19T10:13:34Z,ws relatnce assumptassumptns  sp hp for iassumptns for
paper_qf_23.pdf,17,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
",(sH+ )Du0; Duf(s;Ys;Zs)0; (du),2016-02-19T10:13:34Z,du du ys
paper_qf_23.pdf,18,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
",(L) (i)Themap (y;z)7,2016-02-19T10:13:34Z,t map
paper_qf_23.pdf,19,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","We now aim at applying Bouleau and Hirsch’s Criterion (see Theorem 2.1.2 in [35])
to theYcomponent of the solution (Y;Z)of BSDE (4.1). The existence of a density
forYtwhent2(0;T]whenfis Lipschitz in its space variable was solved in [3] in the
Markovian case. We want to extend this result to the non-Markovian case. The following
theorem gives conditions which ensure that, given a time t, the ﬁrst component Ytof
the solution of the non-Markovian BSDE (4.1) admits a density under (L)and(lD).
These conditions are similar to those of [3, Theorem 3.1] in the Lipschitz Markovian case.
Following [3, 1, 29], let Abe a subset of 
such that P(A)>0. We set
d:= maxfM2R; DuM; du
P",2016-02-19T10:13:34Z, boyle au harsh itertorem onent t wlipschitz mark ia mark iat of mark iatse torem lipschitz mark iafollowi be  du
paper_qf_23.pdf,20,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","DuYtdAEQ
th
1AeRT
tfy(s;Ys;Zs)dsi
+df(t)EQ
t""ZT
teRs
tfy(;Y;Z)dds#
>0:
Thus,kDYtk2
L2([0;T])>0;P",2016-02-19T10:13:34Z,du ys rs  tk
paper_qf_23.pdf,21,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","Proof.We prove the previous theorem under Assumption (aH+ ). Lett2(0;T]. We
know from Theorem 3.5 and Theorem 3.6 that BSDE (3.19) admits a unique solution
(Yt;Zt)t2[0;T]such thatYt2D1;pandZ2L2([t;T];D1;p)for anyp>1with derivatives
(DYt;DZt)satisfying the following linear BSDE
DuYt=Du+ZT
t(Dus+DusYs+DusZs+sDuYs+sDuZs)ds
",2016-02-19T10:13:34Z,proof  assumpt  torem torem du du du du ys du du ys du
paper_qf_23.pdf,22,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","DuYt=Du+ZT
t
Du~f(s;Ys) +fy(s;Ys)DuYs+sDuZs
ds",2016-02-19T10:13:34Z,du du du ys ys du ys du
paper_qf_23.pdf,23,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","Proof.Let(Y;Z)be the unique solution to BSDE (5.2) in D1;2L2([0;T];D1;2), which
the Malliavin derivatives are solutions to BSDE (5.2).
Let Assumption (DZ+)be true together with Assumption (DY+)and(f+). We follow
the proof of [1, Theorem 4.3] by taking the advantage of the representation of the Z
process with Clark-Ocone Formula. Using now a linearization and according to Clark-
Ocone Formula, we obtain
Zt=EQ
t""
DteRT
t~fy(s;Ys)ds+ZT
tDt~f(s;Ys)eRs
t~fy(u;Yu)duds#
;
withdQ
dP= expRT
0sdWs",2016-02-19T10:13:34Z,proof  alla i assumptassumpt torem ark oc one  usi ark oc one  dt ys dt ys rs yu ws
paper_qf_23.pdf,24,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","where (rX;rY;rZ)is the solution to the following FBSDE:
8
>>>>>><
>>>>>>:rXt=Zt
0bx(s;Xs)rXsds+Zt
0x(s;Xs)rXsdWs;
rYt=g0(XT)rXT+ZT
t(fx(s;Xs;Ys;Zs)rXs+fy(s;Xs;Ys;Zs)rYs
+fz(s;Xs;Ys;Zs)rZs)ds",2016-02-19T10:13:34Z,xt xs sds xs sd ws xs ys xs xs ys ys xs ys
paper_qf_23.pdf,25,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","of this phenomenon, we consider for instance a necrotic cells model, in which we want to
control the initial protein concentration. It was showed in [46] that this problem can be
reduced to solve the following BSDE
Yt=+ZT
t(f(Ys)",2016-02-19T10:13:34Z,it ys
paper_qf_23.pdf,26,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","is a GaussianFT-measurable random variable whose mean is denoted by cand
variance is denoted by 2.
is inD1;2and there exist 0<kksuch that for any r2[0;T],0<kDrk.
According to Theorem 4.2 and Theorem 4.3 above, BSDE (6.1) admits a unique solution
(Y;Z)such that for any t2[0;T],Yt2D1;2andZ2L2([t;T];D1;2). We then have the
following proposition.
Proposition 6.2. The ﬁrst component Yof the solution of BSDE (6.1)admits a density
denoted by Ytat any time t2(0;T]. Besides, Ythas Gaussian estimates, satisfying
the following inequalities for any x2R
fi(x)Yt(x)fs(x); (6.2)
where
fi(x) =CYt
k2te",2016-02-19T10:13:34Z,dr accordi torem torem  propositt of tat besis th as n
paper_qf_23.pdf,27,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","6.3.1 Law of Ytby using statistical tests
Let(Y;Z)be the unique solution to BSDE (6.1) where has a normal distribution. In
[46], the authors study their model by assuming that Ythas a normal distribution and
compare the ﬁrst and second order moments of Ytwith those generated by a benchmark
random variable, which has a normal distribution. However, it is not clear that the
law ofYtis normal. Nevertheless, from a statistical point of view, we could validate this
assumption by using a statistical hypothesis test. In this subsection, we set the statistical
hypothesis
(H) ""Ythas a normal distribution""
and we ﬁrst test it using a Jarque-Bera test with the data of [46, A. Self-regulating gene].
Recall that the Jarque-Bera test consists in computing the sample skewness, denoting
byS, and the sample kurtosis, denoting by K, of a sample data, such that
S:=1
MPM
i=1(Yi
t",2016-02-19T10:13:34Z,law by  ith as with tis nevertless ith as jar que ber self recall jar que ber yi
paper_qf_23.pdf,28,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","meanandthevarianceofthesample. Hence, fora level = 0:05, byusingaKolmogorov-
Smirnov test, we reject the Hypothesis (H)as soon as KS > 1:36. The results are
presented in Table 2.
Table 2: A Jarque-Bera test and a Kolmogorov-Smirnov test for Hypothesis (H)with the
data of [46, A. Self-regulating gene] and M= 100000 . We write ""Not R."" for ""not rejected"".
Statistical testsTime t400 300 200 100 50
Jarque-Bera test
JB 1.91 2.61 2.08 2.31 1.72
(H) Not R. Not R. Not R. Not R. Not R.
Kolmogorov-Smirnoﬀ test
KS 0.501 0.500 0.501 0.501 0.501
(H) Not R. Not R. Not R. Not R. Not R.
Interpretation A Jarque-Bera test together with a Kolmogorov-Smirnov test cannot
invalidate the assumption (H)with a risk level = 0:05. Hence, from a statistical point
of view, the model developed in [46] seems to be relevant. However, we propose in the
next section a pure mathematical analyse of this model, by using the Malliavin calculus
and by applying results of [29] together with those obtained in Section 4.
6.3.2 Validation of the model by using the Malliavin calculus and
Nourdin-Viens Formula
Assume that =c+2WT. Then, we can use the result of Section 6.2 and we deduce
that BSDE (6.1) admits a unique solution (Y;Z)such that for any t2[0;T],Yt2D1;2
andZ2L2([t;T];D1;2). Besides, according to Proposition 6.2, for any t2[0;T],Yt
admits a density with respect to the Lebesgue measure denoted by Yt. such that Yt
has Gaussian estimates, satisfying the following inequalities for any x2R
fi(x)Yt(x)fs(x);
where
fi(x) =CYt
4te",2016-02-19T10:13:34Z,nce lmogorov mir nov sis t table table jar que ber lmogorov mir nov sis self  not statistical time jar que ber not not not not not lmogorov mir no not not not not not interpatjar que ber lmogorov mir nov nce alla isevalidatalla ino url ivi ens   tsebesis propositle be gue n
paper_qf_23.pdf,29,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","Figure 1:T;a;c;2= 1,R= 1,= 0:001, and 500 000 simulations using a method of
Monte-Carlo (see [6] for instance) to compute the solution of BSDE (6.1). We represent Yt
fort= 0:9;0:75;0:6;0:5. We provide in red (resp. in blue) the supremum bound ""fs"" of 
(resp. the inﬁmum ""ﬁ""), using Nourdin and Viens’ Formula.
Timet0.9 0.75 0.6 0.5
E[Yt]101.089 101.228 101.357 101.446
Var[Yt]0.89847 0.749607 0.599469 0.503076
Interpretation TheclosertistoT,thebettertheapproximationisusingProposition
6.2. Besides, this method guarantees Gaussian tails to control extreme events which is
fundamental to validate the model developed in [46] by comparing the obtained data
with those induced by Gillepsie Method (see [46] and [20] for more details).
Notice ﬁnally that the variance of Ytseems to be a decreasing function of the time. This
is not surprising since Y0is deterministic.
6.4 Example 2. An example in the non-Markovian case
We now propose to extend the model developed by Shamarova, Ramos and Aguiar
(see the previous Example 1) to the non-Markovian setting. This extension might be
quite relevant when we study the synthesis of protein in some kind of cells (see for
instance [7, 27, 18]). Assume that there exist 2R, > 0and0such that
=+WT+RT
0Wsds:Hence, BSDE (6.1)becomes:
Yt=+WT+ZT
0Wsds+ZT
t
RaY2
s
1 +aY2s",2016-02-19T10:13:34Z,     no url ivi ens  time var interpatt oser tis to propositbesis gl eps ie method notice ms   amark ia shamaova ramos guitar  mark ia  sds nce sds ra
paper_qf_23.pdf,30,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","6.2, for any t2[0;T],Ytadmits a density with respect to the Lebesgue measure denoted
byYtsuch thatYthas Gaussian estimates, satisfying the following inequalities for any
x2R
fi(x)Yt(x)fs(x); (6.4)
where
fi(x) =CYt
(+T)2te",2016-02-19T10:13:34Z,admits le be gue suth as n
paper_qf_23.pdf,31,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","the solution to (7.1). In this model, Assumptions (A1)and(A2) (i)above in Section
3.4 become
(H1)For anyp>0,Eh
epRT
0rsdsi
<+1and",2016-02-19T10:13:34Z,iassumptns sefor eh
paper_qf_23.pdf,32,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","In this particular model and as said in Section 5, we provide now conditions on and its
Malliavin derivatives ensuring existence of densities for both the law of the Ytcomponent
and for the law of the Ztcomponent of the solution to BSDE (7.1).
Theorem 7.1. Let2D1;pfor anyp>1. Assume that ()holds and that one of the
following two assumptions is satisﬁed for A
such that P(A)>0
(+)0,Du0; (du)",2016-02-19T10:13:34Z,isealla ionent onent torem   du
paper_qf_23.pdf,34,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","then the law of Ytis absolutely continuous with respect to the Lebesgue measure.
Remark 7.2. Since:=MTis not twice Malliavin diﬀerentiable (see [21]), i.e. does
not belong to D2;pwhateverp1, we cannot reproduce the proof of Proposition 7.2 to
study the problem of existence of density for Zt.
Acknowledgments
TheauthorthanksDylanPossamaïandAnthonyRéveillacforconversationsandprecious
advice in the writing of this paper. The author is grateful to Région Ile-De-France for
ﬁnancial support.
References
[1] O. Aboura and S. Bourguin. Density estimates for solutions to one dimensional
Backward SDE’s. Potential Analysis , 38(2):573–587, 2013.
[2] S. Ankirchner, P. Imkeller, and G. Dos Reis. Classical and variational diﬀeren-
tiability of BSDEs with quadratic growth. Electron. J. Probab , 12(53):1418–1453,
2007.
[3] F. Antonelli and A. Kohatsu-Higa. Densities of one-dimensional backward SDEs.
Potential Analysis , 22(3):263–287, 2005.
[4] C. Bender and M. Kohlmann. BSDEs with stochastic Lipschitz condition. Univer-
sität Konstanz, Fakultät für Mathematik und Informatik, 2000.
[5] JM. Bismut. Conjugate convex functions in optimal stochastic control. Journal of
Mathematical Analysis and Applications , 44(2):384–404, 1973.
[6] B. Bouchard and N. Touzi. Discrete-time approximation and Monte-Carlo simu-
lation of backward stochastic diﬀerential equations. Stochastic Processes and their
applications , 111(2):175–206, 2004.
[7] D.A Bratsun, D.N Volfson, J. Hasty, and L.S Tsimring. Non-markovian processes in
generegulation(keynoteaddress). In SPIE Third International Symposium on Fluc-
tuations and Noise , pages 210–219. International Society for Optics and Photonics,
2005.
[8] P. Briand and F. Confortola. BSDEs with stochastic Lipschitz condition and
quadratic PDEs in Hilbert spaces. Stochastic Processes and their Applications ,
118(5):818–838, 2008.
[9] Ph. Briand, B. Delyon, Y. Hu, E. Pardoux, and L. Stoica. Lpsolutions of BSDEs
with stochastic Lipschitz condition. Stochastic Processes and their Applications ,
108(1):109–129, 2003.
[10] R. Cont and DA. Fournié. Functional Itô calculus and stochastic integral represen-
tation of martingales. The Annals of Probability , 41(1):109–133, 2013.
[11] A. Debussche and M. Romito. Existence of densities for the 3d Navier-Stokes equa-
tions driven by Gaussian noise. Probability Theory and Related Fields , 158(3-4):575–
596, 2014.
[12] F. Delarue, S. Menozzi, and E. Nualart. The Landau Equation for Maxwellian
molecules and the Brownian Motion on SON(R).Electronic Journal of Probability ,
20:1–39, 2014.
[13] G. Dos Reis and Ricardo JN. Dos Reis. A note on comonotonicity and positivity of
the control components of decoupled quadratic FBSDE. Stochastics and Dynamics ,
13(04):1350005, 2013.
[14] I.Ekren, C.Keller, N.Touzi, andJ.Zhang. Onviscositysolutionsofpathdependent
PDEs.The Annals of Probability , 42(1):204–236, 2014.
34",2016-02-19T10:13:34Z,tis le be gue remark since tis alla ipropositackledgment t author thanks dylapos sama anthony t e  france referencab our borg insity backward ential analysis akirchner im kler dos rassical eleropro bab atoi ha su his nsitiential analysis benr kmanlipschitz uoconstant fa kul ma tm tik iformat ik is but conjugate journal matmatical analysis applicatns orchard to ui disete   stochastic processbrat suvsohasty tim ri noithird internatnal symposium flu noise internatnal society optics photo nice brand cofor to la lipschitz gbert stochastic processapplicatns ph brand  lyohu par ux stic lp solutns lipschitz stochastic processapplicatns cont four ni funnal it t annals probabity  bus sc rom to existence nistorprobabity tory related elds  large meoz nu al art t land equat iabrownish moteleronic journal probabity dos rricardo dos rstochastic dynamics ek rekler to ui  ovisity solutns of path pennt t annals probabity
paper_qf_23.pdf,35,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","[15] I. Ekren, N. Touzi, and J. Zhang. Viscosity solutions of fully nonlinear parabolic
path dependent PDEs: Part i,(2012). To appear in The Annals of probability, arXiv
preprint arXiv:1210.0006 , 2014.
[16] N. El Karoui and SJ. Huang. A general result of existence and uniqueness of back-
ward stochastic diﬀerential equations. Pitman Research Notes in Mathematics Se-
ries, pages 27–38, 1997.
[17] N. El Karoui, S. Peng, and MC. Quenez. Backward stochastic diﬀerential equations
in ﬁnance. Mathematical ﬁnance , 7(1):1–71, 1997.
[18] V. Fromion, E. Leoncini, and P. Robert. A stochastic model of the production of
multiple proteins in cells. arXiv preprint arXiv:1411.1572 , 2014.
[19] C. Geiss and A. Steinicke. Malliavin derivative of random functions and applications
to lévy driven bsdes. arXiv preprint arXiv:1404.4477 , 2014.
[20] DT. Gillespie. Exact stochastic simulation of coupled chemical reactions. The
journal of physical chemistry , 81(25):2340–2361, 1977.
[21] E. Gobet and A. Kohatsu-Higa. Computation of Greeks for barrier and lookback
options using Malliavin calculus. Electron. Comm. Probab , 8:51–62, 2003.
[22] Y. Hu, P. Imkeller, and M. Müller. Utility maximization in incomplete markets.
The Annals of Applied Probability , 15(3):1691–1712, 2005.
[23] C.M. Jarque and A.K. Bera. A test for normality of observations and regres-
sion residuals. International Statistical Review/Revue Internationale de Statistique ,
pages 163–172, 1987.
[24] I. Karatzas and S. Shreve. Brownian motion and stochastic calculus , volume 113.
Springer Science, 2012.
[25] N. Kazamaki. Continuous exponential martingales and BMO. Lecture notes in
mathematics , 1994.
[26] A. Kohatsu-Higa. Lower bounds for densities of uniformly elliptic random variables
on Wiener space. Probability theory and related ﬁelds , 126(3):421–457, 2003.
[27] E. Leoncini. Towards a global and systemic understanding of protein production in
prokaryotes . PhD thesis, Ecole Polytechnique X, 2013.
[28] J. Ma and J. Zhang. Representation theorems for backward stochastic diﬀerential
equations. The Annals of Applied Probability , 12(4):1390–1418, 2002.
[29] T. Mastrolia, D. Possamaï, and A. Réveillac. Density analysis of BSDEs. To appear
in Annals of Probability, preprint arXiv:1402.4416 , 2015.
[30] T. Mastrolia, D. Possamaï, and A. Réveillac. On the Malliavin diﬀerentiability
of BSDEs. To appear in Annales de l’Institut Henri Poincaré, arXiv preprint
arXiv:1404.1026 , 2015.
[31] A. Millet and M. Sanz-Solé. A stochastic wave equation in two space dimension:
smoothness of the law. Annals of Probability , pages 803–844, 1999.
[32] G. Mokobodzki. Ultraﬁltres rapides sur N. Construction d’une densité relative de
deux potentiels comparables , 1969.
[33] C.MuellerandD.Nualart. Regularityofthedensityforthestochasticheatequation.
Electron. J. Probab , 13(74):2248–2258, 2008.
[34] I. Nourdin and FG. Viens. Density formula and concentration inequalities with
Malliavin calculus. Electron. J. Probab , 14(78):2287–2309, 2009.
[35] D. Nualart. The Malliavin calculus and related topics . Probability and its Applica-
tions (New York). Springer-Verlag, Berlin, second edition, 2006.
[36] E. Nualart and L. Quer-Sardanyons. Gaussian estimates for the density of the non-
linear stochastic heat equation in any space dimension. Stochastic Processes and
their Applications , 122(1):418–447, 2012.
35",2016-02-19T10:13:34Z,ek o ui  visity part to t annals   el taro ui hu hitmaresearnotmatmatics se el taro ui pe que nez backward matmatical from leoirobert   ge is steicke alla i  glespie exa t go bet ha su his tgreek alla ielerocom pro bab hu im kler utity t annals applied probabity jar que ber internatnal statistical review revue internatnale status que kara tz as hr eve brownish  science panama ki continuous leure ha su his lor winner probabity leoitowards ph ecole polytechnique ma  representatt annals applied probabity astro lia pos sama nsity to annals probabity  astro lia pos sama oalla ito annals institut nri pointer   m sans sannals probabity mok bod ki ultra construmueller and nu al art regularity of t nsity for t stochastic at equateleropro bab no url ivi ens nsity alla ieleropro bab nu al art t alla iprobabity app lica new york  verlag berlinu al art quer ard any ostochastic processapplicatns
paper_qf_23.pdf,36,"Density analysis of non-Markovian BSDEs and applications to biology and
  finance","  In this paper, we provide conditions which ensure that stochastic Lipschitz
BSDEs admit Malliavin differentiable solutions. We investigate the problem of
existence of densities for the first components of solutions to general
path-dependent stochastic Lipschitz BSDEs and obtain results for the second
components in particular cases. We apply these results to both the study of a
gene expression model in biology and to the classical pricing problems in
mathematical finance.
","[37] M. Nutz. Pathwise construction of stochastic integrals. Electron. Commun. Probab ,
17(24):1–7, 2012.
[38] E. Pardoux and S. Peng. Adapted solution of a backward stochastic diﬀerential
equation. Systems & Control Letters , 14(1):55–61, 1990.
[39] E. Pardoux and S. Peng. Backward stochastic diﬀerential equations and quasilinear
parabolic partial diﬀerential equations. In Stochastic partial diﬀerential equations
and their applications , pages 200–217. Springer, 1992.
[40] J. Paulsson. Models of stochastic gene expression. Physics of life reviews , 2(2):157–
175, 2005.
[41] S. Peng and F. Wang. BSDE, path-dependent PDE and nonlinear Feynman-Kac
formula. arXiv preprint arXiv:1108.4317 , 2011.
[42] D. Possamaï, X. Tan, and C. Zhou. Stochastic control for a class of nonlinear kernels
and applications. arXiv preprint arXiv:1510.08439 , 2015.
[43] Z.Ren, N.Touzi, andJ.Zhang. Anoverviewofviscositysolutionsofpath-dependent
PDEs. In Stochastic Analysis and Applications 2014 , pages 397–453. Springer, 2014.
[44] DR. Rigney and WC. Schieve. Stochastic model of linear, continuous protein syn-
thesis in bacterial populations. Journal of theoretical biology , 69(4):761–766, 1977.
[45] R. Rouge and N. El Karoui. Pricing via utility maximization and entropy. Mathe-
matical Finance , 10(2):259–276, 2000.
[46] E. Shamarova, AF. Ramos, and P. Aguiar. Backward SDE approach to modelling
of gene expression. preprint arXiv:1308.6619 , 2013.
[47] H. Sugita. On a characterization of the Sobolev spaces over an abstract Wiener
space.Journal of Mathematics of Kyoto University , 25(4):717–725, 1985.
[48] J. Wang, Q. Ran, and Q. Chen. Lpsolutions of BSDEs with stochastic Lipschitz
condition. International Journal of Stochastic Analysis , 2007.
36",2016-02-19T10:13:34Z,nut path wise elerocom mupro bab par ux pe adapted tems contrters par ux pe backward istochastic  paul somols psics pe wa leymaka   pos sama tahou stochastic   o ui  aoverview of visity solutns of path istochastic analysis applicatns  r ney sc ve stochastic journal rouge el taro ui prici ma t nance shamaova ramos guitar backward  su ita oso bold winner journal matmatics kyoto  wa raclp solutns lipschitz internatnal journal stochastic analysis
paper_qf_24.pdf,1,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","arXiv:1802.01307v2  [q-fin.PR]  14 Sep 2018Asian Option Pricing with Orthogonal Polynomials∗
Sander Willems†
September 14, 2018
forthcoming in Quantitative Finance
Abstract
In this paper we derive a series expansion for the price of a continuo usly sampled arith-
metic Asian option in the Black-Scholes setting. The expansion is base d on polynomials that
are orthogonal with respect to the log-normal distribution. All te rms in the series are fully
explicit and no numerical integration nor any special functions are in volved. We provide
suﬃcient conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the ser ies, however we show
numerically that the bias can safely be ignored in practice.
JEL Classiﬁcation : C32, G13
Keywords : Asian option, option pricing, orthogonal polynomials
1 Introduction
An Asian option is a derivative contract with payoﬀ continge nt on the average value of the
underlying asset over a certain time period. Valuation of th ese contracts is not straightforward
because of the path-dependent nature of the payoﬀ. Even in the standard Black and Scholes
(1973) setting the distribution of the (arithmetic) average stoc k price is not known. In this
paper we derive a series expansion for the Asian option price in the Black-Scholes setting using
polynomials that are orthogonal with respect to the log-nor mal distribution. The terms in the
series are fully explicit since all the moments of the averag e price are known. We prove that the
series does not diverge by showing that the tails of the avera ge price distribution are dominated
by the tails of a log-normal distribution. As a consequence o f the well known moment indeter-
minacy of the log-normal distribution (see e.g., Heyde(1963)), it is not theoretically guaranteed
that the series converges to the true price. We show numerica lly that this asymptotic bias is
small for standard parameterizations and the real approxim ation challenge lies in controlling the
error coming from truncating the series after a ﬁnite number of terms.
There exists a vast literature on the problem of Asian option pricing. We give a brief overview
which is by no means exhaustive. One approach is to approxima te the unknown distribution
of the average price with a more tractable one. Turnbull and Wakeman (1991),Levy(1992),
∗I thank Damien Ackerer, Damir Filipovi´ c, participants at t he 9th International Workshop on Applied Prob-
ability in Budapest, and two anonymous referees for helpful comments. The research leading to these results
has received funding from the European Research Council und er the European Union’s Seventh Framework Pro-
gramme (FP/2007-2013) / ERC Grant Agreement n. 307465-POLY TE.
†EPFL and Swiss Finance Institute, 1015 Lausanne, Switzerla nd. Email: sander.willems@epﬂ.ch
1",2018-02-05T09:26:13Z, sep  optprici orthogonal polynomials sanrs wled september quantitative nance abstra i  schools t all  t ass keywords  introdua valuateve schools i  schools t  as y   tre   one turnbull wake malevy admicker er amir lipovi internatnal workshapplied pro budapest t aresearcounc aunseventh framework pro grant agreement swiss nance institute lausanne twier la em articial intellence
paper_qf_24.pdf,2,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","Ritchken et al. (1993),Li and Chen (2016) use an Edgeworth expansion around a log-normal
reference distribution to approximate the distribution of the arithmetic average of the geometric
Brownian motion. Ju(2002) andSun et al. (2013) use a Taylor series approach to approxi-
mate the unknown average distribution from a log-normal. Milevsky and Posner (1998) use a
moment matched inverse gamma distribution as approximatio n. Their choice is motivated by
the fact that the inﬁnite horizon average stock price has an i nverse gamma distribution. More
recently, Aprahamian and Maddah (2015) use a moment matched compound gamma distribu-
tion. Although these type of approximations lead to analyti c option price formulas, their main
drawback is the lack of reliable error estimates. A second st rand of the literature focuses on
Monte-Carlo and PDE methods. Kemna and Vorst (1990) propose to use the continuously sam-
pled geometric option price as a control variate and show tha t it leads to a signiﬁcant variance
reduction. Fu et al. (1999) argue that this is a biased control variate, but interestin gly the
bias approximately oﬀsets the bias coming from discretely co mputing the continuous average
in the simulation. Lapeyre et al. (2001) perform a numerical comparison of diﬀerent Monte-
Carlo schemes. Rogers and Shi (1995),Zvan et al. (1996),Vecer(2001,2002),Marcozzi (2003)
solve the pricing PDE numerically. Another approach is to de rive bounds on the Asian option
price, see e.g. Curran(1994),Rogers and Shi (1995),Thompson (2002), andVanmaele et al.
(2006). Finally, there are several papers that derive exact repre sentations of the Asian option
price.Yor(1992) expresses the option price as a triple integral, to be evalu ated numerically.
Geman and Yor (1993) derive the Laplace transform of the option price. Numerica l inversion of
this Laplace transform is however a delicate task, see e.g. Eydeland and Geman (1995),Fu et al.
(1999),Shaw(2002).Carverhill and Clewlow (1990) relate the density of the discrete arithmetic
average to an iterative convolution of densities, which is a pproximated numerically through the
Fast Fourier Transform algorithm. Later extensions and imp rovements of the convolution ap-
proach include Benhamou (2002),Fusai and Meucci (2008),ˇCern` y and Kyriakou (2011), and
Fusai et al. (2011).Dufresne (2000) derives a series representation using Laguerre orthogona l
polynomials. Linetsky (2004) derives a series representation using spectral expansion s involving
Whittaker functions.
The approach taken in this paper is closely related to Dufresne (2000) in the sense that both
are based on orthogonal polynomial expansions. The Laguerr e series expansion can be shown
to diverge when directly expanding the density of the averag e price, which is related to the fact
that the tails of the average price distribution are heavier than those of the Gamma distribution.
As a workaround, Dufresne (2000) proposes to work with the reciprocal of the average, for whi ch
the Laguerre series does converge. The main downside of this approach is that the moments
of the reciprocal average are not available in closed form an d need to be calculated through
numerical integration, which introduces a high computatio nal cost and additional numerical
errors.Asmussen et al. (2016) use a diﬀerent workaround and expand an exponentially tilte d
transformation of the density of a sum of log-normal random v ariables using a Laguerre series.
They show that the exponential tilting transformation guar antees the expansion to converge.
However, a similar problem as in Dufresne (2000) arises: the moments of the exponentially tilted
density are not available in closed form and have to be comput ed numerically. In contrast, our
approach is fully explicit and does not involve any numerica l integration, which makes it very
fast.
Truncating our series after only one term is equivalent to pr icing the option under the assump-
tion that the average price is log-normally distributed. Th e remaining terms in the series can
therefore be thought of as corrections to the log-normal dis tribution. This has a very similar
ﬂavour to approaches using an Edgeworth expansion around th e log-normal distribution (cfr.
2",2018-02-05T09:26:13Z,itkeli cedgeworth brownish ju sutaylor me  poster tir  drama maima although   ke mna worst fu lap eye   ro shi vave cer marco zz anotr  current ro shi thompsovamae le nally  or gem aor place numeric place ey mand gem afu shaw carhl ew low fast courier transform later beam ou fu articial intellence mecca corky ria u fu articial intellence du free la guerre linet whiaker t du free t la gu err gaa as du free la guerre t as muss ela guerre ty du free itruncati th  edgeworth
paper_qf_24.pdf,3,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","Jarrow and Rudd (1982) andTurnbull and Wakeman (1991)). The key diﬀerence with our ap-
proach is that the Edgeworth expansion can easily diverge be cause it lacks a proper theoretical
framework. In contrast, the series we present in this paper i s guaranteed to converge, possibly
with a small asymptotic bias. A thorough study of the approxi mation error reveals that the
asymptotic bias is positively related to the volatility of t he stock price process and the option
expiry. We use the integration by parts formula from Malliav in calculus to derive an upper
bound on the approximation error.
The remaining of this paper is structured as follows. Sectio n2casts the problem and derives
usefulpropertiesaboutthedistributionofthearithmetic average. Section 3describesthedensity
expansion used to approximate the option price. In Section 4we investigate the approximation
error. Section 5illustrates the method with numerical examples. Section 6concludes. All proofs
can be found in Appendix C.
2 The distribution of the arithmetic average
We ﬁx a stochastic basis (Ω ,F,(Ft)t≥0,Q) satisfying the usual conditions and let Qbe the risk-
neutral probability measure. We consider the Black-Schole s setup where the underlying stock
priceStfollows a geometric Brownian motion:
dSt=rStdt+σStdBt,
wherer∈Ris the short-rate, σ >0 the volatility of the asset, and Bta standard Brownian
motion. For notational convenience we assume S0= 1, which is without loss of generality. We
deﬁne the average price process as
At=1
t/integraldisplayt
0Sudu, t >0.
The price at time 0 of an Asian option with continuous arithme tic averaging, strike K >0, and
expiryT >0 is given by
π= e−rTE/bracketleftbig
(AT−K)+/bracketrightbig
.
The option price can not be computed explicitly since we do no t know the distribution of AT.
However, we can derive useful results about its distributio n.
We start by computing all the moments of AT. Using the time-reversal property of a Brownian
motion, we have the following identity in law (cfr. Dufresne (1990),Carmona et al. (1997),
Donati-Martin et al. (2001),Linetsky (2004)):
Lemma 2.1. The random variable TAThas the same distribution as the solution at time Tof
the following SDE
dXt= (rXt+1)dt+σXtdBt, X0= 0. (2.1)
Since the SDE in ( 2.1) deﬁnes a polynomial diﬀusion (see e.g., Filipovi´ c and Larsson (2016)),
we can compute all the moments of its solution at a given time i n closed form. By the identity
in law of Lemma 2.1we therefore also have all of the moments of ATin closed form:
3",2018-02-05T09:26:13Z,narrow rudd turnbull wake mat edgeworth i alla t se  seiseseseall  t  ft be   sc hole st follows brownish st std std  is  brownish for  at sud t  t  usi brownish du free car mona donate martilinet lea t th as of xt xt td  since lipovi larsoby lea tin
paper_qf_24.pdf,4,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","Proposition 2.2. If we denote by Hn(x) = (1,x,...,xn)⊤,n∈N, then we have
E[Hn(AT)] = eGnTHn(0),
whereGn∈R(n+1)×(n+1)is the following lower bidiagonal matrix
Gn=
0
1
Tr
......
n
T(nr+1
2n(n−1)σ2)
. (2.2)
Given that the matrix exponential is a standard built-in fun ction in most scientiﬁc comput-
ing packages, the above moment formula is very easy to implem ent. There also exist eﬃ-
cient numerical methods to directly compute the action of th e matrix exponential, see e.g.
Al-Mohy and Higham (2011) andCaliari et al. (2014). An equivalent, but more cumbersome to
implement, representation of the moments can be found in Geman and Yor (1993).
The following proposition shows that ATadmits a smooth density function g(x) whose tails are
dominated by the tails of a log-normal density function:
Proposition 2.3.
1. The random variable ATadmits an inﬁnitely diﬀerentiable density function g(x).
2. The density function g(x)has the following asymptotic properties:
g(x) =

O/parenleftbigg
exp/braceleftbigg
−3
2log(x)2
σ2T/bracerightbigg/parenrightbigg
forx→0,
O/parenleftbigg
exp/braceleftbigg
−1
2log(x)2
σ2T/bracerightbigg/parenrightbigg
forx→ ∞.
3 Polynomial expansion
Following asimilar structureas Ackerer et al. (2017) andAckerer and Filipovi´ c (2017), weusein
this section the density expansion approach described by Filipovi´ c et al. (2013) to approximate
the Asian option price. Deﬁne the weighted Hilbert space L2
was the set of measurable functions
fonRwith ﬁnite L2
w-norm deﬁned as
/bardblf/bardbl2
w=/integraldisplay∞
0f(x)2w(x)dx, w(x) =1√
2πνxexp/braceleftbigg
−(log(x)−µ)2
2ν2/bracerightbigg
, (3.1)
for some constants µ∈R,ν >0. The weight function wis the density function of a log-normal
distribution. The corresponding scalar product between tw o functions f,h∈L2
wis deﬁned
as
/an}bracketle{tf,h/an}bracketri}htw=/integraldisplay∞
0f(x)h(x)w(x)dx.
Since the measures associated with the densities gandware equivalent, we can deﬁne the
likelihood ratio function ℓsuch that
g(x) =ℓ(x)w(x), x∈(0,∞).
Using Proposition 2.3we now have the following result:
4",2018-02-05T09:26:13Z,proposit hhghtr givetre al moh hh call ari agem aor t admits propositt admits t polynomial followi cker er cker er lipovi lipovi   gbert with t t since usi propositn
paper_qf_24.pdf,5,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","Proposition 3.1. Ifν2>1
2σ2T, thenℓ∈L2
w, i.e.
/integraldisplay∞
0/parenleftbiggg(x)
w(x)/parenrightbigg2
w(x)dx <∞.
Denote by Pol( R) the set of polynomials on Rand by Pol N(R) the subset of polynomials on Rof
degree at most N∈N. Since the log-normal distribution has ﬁnite moments of any degree, we
have Pol N(R)⊂L2
wfor allN∈N. Letb0,b1,...,bNform an orthonormal polynomial basis for
PolN(R). Such a basis can, for example, be constructed numerically from the monomial basis
using a Cholesky decomposition. Indeed, deﬁne the Hankel mo ment matrix M= (Mij)0≤i,j≤N
as
Mij=/an}bracketle{txi,xj/an}bracketri}htw= eµ(i+j)+1
2(i+j)2ν2, i,j= 0,...,N, (3.2)
which is positive deﬁnite by construction. If we denote by M=LL⊤the unique Cholesky
decomposition of M, then
(b0(x),...,bN(x))⊤=L−1HN(x),
forms an orthonormal polynomial basis for Pol N(R). Alternative approaches to build an or-
thonormal basis are, for example, the three-term recurrenc e relation (see Lemma A.1for de-
tails) or the analytical expressions for the orthonormal po lynomials derived in Theorem 1.1 of
Asmussen et al. (2016).
Remark 3.2. The matrix Mdeﬁned in (3.2)can in practice be non-positive deﬁnite due to
rounding errors. This problem becomes increasingly importa nt for large Nand/or large νbecause
the elements in Mgrow very fast. Similarly, the moments of ATcan also grow very large, which
causes rounding errors in ﬁnite precision arithmetic. In App endixAwe describe a convenient
scaling technique that solves these problems in many cases.
Deﬁne the discounted payoﬀ function F(x) = e−rT(x−K)+. SinceF(x)≤e−rTxfor allx≥0,
we immediately have that F∈L2
w. Denote by Pol(R) the closure of Pol( R) inL2
w. We deﬁne
the projected option price ¯ π=E[¯F(AT)], where ¯Fdenotes the orthogonal projection of Fonto
Pol(R) inL2
w. Elementary functional analysis gives
¯π=/an}bracketle{t¯F,ℓ/an}bracketri}htw=/summationdisplay
n≥0fnℓn, (3.3)
where we deﬁne the likelihood coeﬃcients ℓn=/an}bracketle{tℓ,bn/an}bracketri}htwand payoﬀ coeﬃcients fn=/an}bracketle{tF,bn/an}bracketri}htw.
Truncating the series in ( 3.3) after a ﬁnite number of terms ﬁnally gives the following app roxi-
mation for the Asian option price:
π(N)=N/summationdisplay
n=0fnℓn, N∈N. (3.4)
The likelihood coeﬃcients are available in closed form usin g the moments of ATin Proposition
2.2:
(ℓ0,...,ℓN)⊤=L−1eGNTHN(0).
Thepayoﬀ coeﬃcients can also bederived explicitly, as show nin thefollowing proposition.
5",2018-02-05T09:26:13Z,proposit note prand pof since p form psuhole  ined hank el mi mi  hole  palternative lea torem as muss eremark t   and grow simarly caiapp   since tx for note pp notfoto pelementary truncati  t tipropositt pay
paper_qf_24.pdf,6,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","Proposition 3.3. LetΦbe the standard normal cumulative distribution function. Th e payoﬀ
coeﬃcients f0,...,fNare given by
(f0,...,fN)⊤=e−rTL−1(˜f0,...,˜fN)⊤,
with
˜fn= eµ(n+1)+1
2(n+1)2ν2Φ(dn+1)−Keµn+1
2n2ν2Φ(dn), n= 0,...,N,
dn=µ+ν2n−log(K)
ν, n= 0,...,N+1. (3.5)
Equivalently, we could also derive the approximation ( 3.4) by projecting ℓ, instead of F, on the
set of polynomials. This leads to the interpretation of ( 3.4) as the option price one obtains when
approximating the true density g(x) by
g(N)(x) =w(x)N/summationdisplay
n=0ℓnbn(x). (3.6)
The function g(N)(x) integrates to one by construction
/integraldisplay∞
0g(N)(x)dx=N/summationdisplay
n=0ℓn/an}bracketle{tbn,b0= 1/an}bracketri}htw=ℓ0= 1,
where the last equality follows from the fact that g(x) integrates to one. However, it is not a
true probability density function since it is not guarantee d to be non-negative.
4 Approximation error
The error introduced by the the approximation in ( 3.4) can be decomposed as
π−π(N)= (π−¯π)+/parenleftig
¯π−π(N)/parenrightig
.
The second term is guaranteed to converge to zero as N→ ∞. In order for the ﬁrst term
to vanish, we need F∈Pol(R) and/or ℓ∈Pol(R). It is well known (see e.g., Heyde(1963))
that the log-normal distribution is not determined by its mo ments. As a consequence, the set
of polynomials does not lie dense in L2
w:Pol(R)/subsetnoteqlL2
w. Hence, the fact that F,ℓ∈L2
wis not
suﬃcient to guarantee that the ﬁrst term in the error decompo sition is zero. One of the goals
of this paper is to quantify the importance of the ﬁrst error t erm. In this section we therefore
investigate the error associated with projecting Fandℓon the set of polynomials.
TheL2
w-distances of Fandℓto their respective orthogonal projections on Pol N(R) are given
by
ǫF
N:=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleF−N/summationdisplay
n=0bnfn/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
w=/bardblF/bardbl2
w−N/summationdisplay
n=0f2
n,
ǫℓ
N:=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleℓ−N/summationdisplay
n=0bnℓn/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
w=/bardblℓ/bardbl2
w−N/summationdisplay
n=0ℓ2
n.
6",2018-02-05T09:26:13Z,proposit th are ke equivalently  t approximatt t ippit y  as pnce one iand t and pol
paper_qf_24.pdf,7,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","TheL2
w-norm of the payoﬀ function Fcan be derived explicitly following very similar steps as
in the proof of Proposition 3.3:
/bardblF/bardbl2
w= e−2rT/parenleftig
e2µ+2ν2Φ(d2)−2Keµ+0.5ν2Φ(d1)+K2Φ(d0)/parenrightig
,
whered0,d1, andd2are deﬁned in ( 3.5). Hence, we can explicitly evaluate ǫF
N.
The computation of ǫℓ
Nis more diﬃcult since /bardblℓ/bardbl2
wdepends on the unknown density g(x).
The following lemma uses the integration by parts formula fr om Malliavin calculus to derive
a representation of g(x) in terms of an expectation, which can be evaluated by Monte- Carlo
simulation:
Lemma 4.1. For any x∈Rwe have
g(x) =E/bracketleftbigg/parenleftbig
1{AT≥x}−c(x)/parenrightbig2
σ2/parenleftbiggST−S0
TA2
T+σ2−r
AT/parenrightbigg/bracketrightbigg
, (4.1)
wherecis any deterministic ﬁnite-valued function.
Remark 4.2. The purpose of the function cis to guarantee that the simulated g(x)actually
goes to zero for x→0. Indeed, if we set c(x)≡0, theng(0)can be diﬀerent from zero due to
the Monte-Carlo error, which can lead to numerical problems w hen evaluating ℓ(x). This can be
avoided by, for example, using the indicator function c(x) = 1x≤ξ, for some ξ >0.
As adirect consequenceof ( 4.1) we get thefollowing expression forthe L2
w-normof thelikelihood
ratio.
Corollary 4.3. TheL2
w-norm of ℓis given by
/bardblℓ/bardbl2
w=E
(1{AT≥˜AT}−c(˜AT))2
σ2/parenleftig
ST−S0
TA2
T+σ2−r
AT/parenrightig
w(˜AT)
,
where the random variable ˜ATis independent from all other random variables and has the sa me
distribution as AT.
This allows us to get an estimate for ǫℓ
Nby simulating the random vector ( ST,AT,˜AT). In
Appendix Bwe describe how to use the known density function of the geome tric average as a
powerfulcontrol variatetosigniﬁcantly reducethevarian ceintheMonte-Carlo simulation.
Using the Cauchy-Schwarz inequality we also have the follow ing upper bound on the approxi-
mation error in terms of the projection errors ǫF
Nandǫℓ
K.
Proposition 4.4. For any N≥0we have
|π−π(N)| ≤/radicalig
ǫF
Nǫℓ
N(4.2)
This upper bound will therefore be small if Fand/orℓare well approximated by a polynomial
inL2
w. Computing the upper bound involves a Monte-Carlo simulati on to compute ǫℓ
N, which
makes it impractical to use as a decision rule for N. This bound should be seen as a more
conservative estimate of the approximation error compared to direct simulation of the option
price.
7",2018-02-05T09:26:13Z,t capropositke nce t is t alla i  lea for  remark t ined    as corollary t tis  by i    usi catc schwartz and propositfor  and uti   
paper_qf_24.pdf,8,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","5 Numerical examples
In this section we compute Asian option prices using the seri es expansion in ( 3.4). The or-
thonormal basis is constructed using the scaling technique described in Appendix A. We set
ν2=1
2σ2T+10−4so that Proposition 3.1is satisﬁed and choose µso that the ﬁrst moment of
wmatches the ﬁrst moment of AT. As a consequence, we always have
ℓ1=/integraldisplay∞
0b1(x)g(x)dx=/an}bracketle{tb0,b1/an}bracketri}htw= 0.
Remark 5.1. Choosing µandνso that the ﬁrst two moments of ATare matched is typically
not possible due to the restriction ν2>1
2σ2Tin Proposition 3.1. A similar problem arises in
the Jacobi stochastic volatility model of Ackerer et al. (2017), where options are priced using a
polynomial expansion with a normal density as weight functi on.Ackerer and Filipovi´ c (2017)
address this problem by using a mixture of two normal densiti es as weight function. Specifying
was a mixture of normal densities would not work in our setting since in this case ℓ /∈L2
w.1
Instead, we can use a mixture of two log-normal densities:
w(x) =cw1(x)+(1−c)w2(x),
wherec∈[0,1]is the mixture weight, and w1andw2are log-normal density functions with
mean parameters µ1,µ2∈R, and volatility parameters ν1,ν2>0, respectively. In order for
Proposition 3.1to apply, it suﬃces to choose ν2
1>1
2σ2T. The remaining parameters can then
be chosen freely and used for higher order moment matching.
We consider a set of seven parameterizations that has been us ed as a set of test cases in, among
others,Eydeland and Geman (1995),Fu et al. (1999),Dufresne (2000), andLinetsky (2004).
The ﬁrst columns of Table 1contain the parameter values of the seven cases. The cases ar e
orderedinincreasingsizeof τ=σ2T. Remarkthat S0/ne}ationslash= 1forallcases, however wenormalizethe
initial stock price to one and scale the strike and option pri ce accordingly. The columns LNS10,
LNS15, LNS20 (Log-Normal Series) contain the option price a pproximations using ( 3.4) for
N= 10,15,20, respectively. The columns LS (Laguerre Series) and EE (E igenvalue Expansion)
correspond to the series expansions of Dufresne (2000) andLinetsky (2004), respectively. The
column VEC shows the prices produced by the PDE method of Vecer(2001,2002) using a
grid with 400 space points and 200 time points.2The last column contains the 95% conﬁdence
intervals of a Monte-Carlo simulation using thegeometric A sian option priceas a control variate,
cfr.Kemna and Vorst (1990). We simulate 2 ×105price trajectories with a time step of 10−3
and approximate the continuous average with a discrete aver age.
[Table1around here]
For the ﬁrst three cases we ﬁnd virtually identical prices as Linetsky (2004), which is one
of the most accurate benchmarks available in the literature . Remarkably, our method does
not face any problems with the very low volatility in case 1. M any other existing method
1Instead of approximating the distribution of AT, it is tempting to approximate the distribution of log( AT)
and rewrite the discounted payoﬀ function accordingly. In t his case, one can show that specifying was a normal
density function gives a series approximation that converg es to the true price. The catch is that we do not know
the moments of log( AT), only those of AT, and hence the terms in the series can not be computed explici tly.
2This grid choice corresponds to the one used in Vecer(2001). By signiﬁcantly increasing the number of space
points in the grid, the PDE method can achieve the same accura cy asLinetsky (2004). However, doing so makes
the method very slow.
8",2018-02-05T09:26:13Z,numerical i t   propositas remark choosi are tipropositjacob cker er cker er lipovi specyi instead ipropositt  ey mand gem afu du free linet t table t remark that t log normal serit la guerre serieansdu free linet t ve cer t   ke mna worst  table for linet remarkably instead it  ve cer by linet ver
paper_qf_24.pdf,9,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","have serious diﬃculty with this parameterization. Indeed, the series of Dufresne (2000) does
not even come close to the true price, while Linetsky (2004) requires 400 terms to obtain an
accurate result. The price of Vecer(2001,2002) is close to the true price, but still outside
of the 95% Monte-Carlo conﬁdence interval. Methods based on numerical inversion of the
Laplacetransformof Geman and Yor (1993)alsostrugglewithlowvolatility becausetheyinvolve
numerical integration of highly oscillating integrands (s ee e.g.,Fu et al. (1999)). When using
exactarithmeticforcase1, ourserieswith20+1termsagree s withthe400 termseriesof Linetsky
(2004) to eight decimal places. When using double precision arith metic, which was used for all
numerical results in this section, the price agrees to four d ecimal places due to rounding errors.
For cases 4 to 7, the LNS prices are slightly diﬀerent from the E E benchmark. However, they are
still very close and with the exception of case 4, they are all within the 95% conﬁdence interval
of the Monte-Carlo simulation.
[Figure1around here]
Figure1plots the LNS price approximations for Nranging from 0 to 20, together with the
Monte-Carlo price and the corresponding95% conﬁdence inte rvals as a benchmark.3We observe
that the series converges very fast in all cases. In fact, tru ncating the series at N= 10 would
give almost identical results. In theory, Ncan be chosen arbitrarily large, however in ﬁnite
precision arithmetic it is inevitable that rounding errors start playing a role at some point.
Remark that the prices for N= 0 and N= 1 are identical, which is a consequence of the fact
that the auxiliary distribution matches the ﬁrst moment of AT. Figure 2plots the simulated
true density g(x) and the approximating densities g(0)(x),g(4)(x), andg(20)(x) deﬁned in ( 3.6).
The true density was simulated using ( B.2) in Appendix B, which is an extension of ( 4.1) using
the density of the geometric average as a powerful control va riate. Note that g(0)(x) =w(x),
sinceb0(x) =ℓ0= 1. We can see that the approximating densities approach the true density
as we include more terms in the expansion. In Figure 2aand2bthe approximation g(20)(x) is
virtually indistinguishable from the true density. Howeve r, in Figure 2cand2dthere remains
a noticeable diﬀerence between g(x) andg(20)(x). This is consistent with the pricing errors we
observed earlier in Table 1.
[Figure2around here]
The above results indicate that for τnot too high, the LNS provides a very accurate approx-
imation of the option price. This is not entirely surprising sinceτdetermines the volatility
parameter of the auxiliary log-normal density w, and hence how fast the tails of wgo to zero.
Loosely speaking, when τis small, projecting the payoﬀ or likelihood ratio function on the set
of polynomials in L2
wis almost like approximating a continuous function on a comp act interval
by polynomials. However, when τbecomes larger, the tails of wdecay slowly and it becomes
increasingly diﬃcult to approximate a function with a polyn omial in L2
w. In other words, for
larger values of τ, the moment indeterminacy of the log-normal distribution s tarts playing a
more prominent role. A natural question is therefore whethe r this poses a problem for option
pricing purposes. In Figure 3awe ﬁxT= 1 and plot for a range of values for σthe squared
relative error
SRE=/parenleftigg
ˆπMC−π(20)
π(20)/parenrightigg2
,
where ˆπMCdenotes the Monte-Carlo price estimate. The error starts to becomes noticeable
3Figure1and2only show cases 1, 3, 5, and 7. The plots for the other cases loo k very similar and are available
upon request.
9",2018-02-05T09:26:13Z,ined du free linet t ve cer   methods place transform of gem aor fu wlinet wfor     rai    iicaremark  t  note  i how eve   table  t  loosely ii not  t  t
paper_qf_24.pdf,10,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","aroundσ= 80%, where√
SRE≈0.5%. For higher values of σthe error increases sharply. In
Figure3bwe plot a more conservative estimate of the squared relative error using the upper
bound in ( 4.2). This plot shows that the upper bound is only signiﬁcantly d iﬀerent from zero
forσlarger than approximately 70%. Figure 4gives a more detailed insight in the extreme
case ofσ= 100%. Although the LNS series converges relatively fast, i t is clear from Figure
4athat it does not converge to the true price. The reason, as alr eady mentioned before, is
that the payoﬀ and likelihood ratio functions are not accura tely approximated by polynomials
in theL2
w-norm, as indicated by the projection errors in Figure 4cand4d. We would obtain
similar results by keeping σﬁxed and varying the maturity T, since the crucial parameter for
the asymptotic pricing error is τ=σ2T. As a rule of thumb, we suggest to use the LNS method
whenτ≤0.5.
[Figure3and4around here]
Themainadvantage ofthemethodproposedinthispaperisthe easeofofitsimplementation and
the computation speed. All terms in the series are fully expl icit and involve only simple linear
algebra operations. Table 2shows the computation times of the LNS with N∈ {10,25,20}, as
well as the computation times of the benchmark methods.4The LNS computation times are
all in the order of miliseconds. Although the EE is very accur ate, it comes at the cost of long
computation times (in the order of several seconds) caused b y the expensive evaluations of the
Whittaker function. The LS does not require calls to special functions, however the method is
slowed down by the numerical integration involved in comput ing the moments of the reciprocal
of the average. The implementation of both the EE and LS requi re the use of software that can
handle symbolic mathematics, in contrast to the implementa tion of the LNS. The VEC method
is the fastest among the benchmarks considered in this paper , but still an order of magnitude
slower than the LNS.
[Table2around here]
6 Conclusion
We have presented aseries expansionfor thecontinuously sa mpled arithmetic Asian option using
polynomials that are orthogonal with respect to the log-nor mal distribution. The terms in the
series are fully explicit and do not require any numerical in tegration or special functions, which
makes the method very fast. We have shown that the series does not diverge if the volatility of
the auxiliary log-normal distribution is suﬃciently high. However, the series is not guaranteed
to converge to the true price. We have investigated this asym ptotic bias numerically and found
that its magnitude is related to the size of τ=σ2T.
4For the LS, all symbolic calculations related to the moments of the reciprocal of the average have been
pre-computed using Matlab’s Symbolic Math Toolbox. We use 1 5+1 terms in the series, a higher number of
terms leads to severe rounding problems in double precision arithmetic. For the EE, the integral representa-
tion (16) in Linetsky (2004) has been implemented instead of the series representation (15). The implemen-
tation of the series representation involves partial deriv atives of the Whittaker function with respect to its in-
dices. These derivatives are not available in Matlab’s Symb olic Math Toolbox and numerical ﬁnite diﬀerence
approximations did not give accurate results. All numerica l integrations are performed using Matlab’s built-
in function integral . For case 1, the numerical integration in the EE did not ﬁnish in a reasonable amount
of time. For the VEC method, we use Prof. Jan Vecer’s Matlab im plementation, which can be downloaded
athttp://www.stat.columbia.edu/ ~vecer/asiancontinuous.m . All computations are performed on a desktop
computer with an Intel Xeon 3.50GHz CPU.
10",2018-02-05T09:26:13Z,for i   although  t   as  t articial intellence advantage all table t although whiaker t t t table conus  t   for mat lab symbolic math toolbox  for linet t whiaker tse mat lab sym math toolbox all mat lab for for prof jave cer mat lab all intel xeno 
paper_qf_24.pdf,11,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","There are several extensions to our method. First of all, we c an handle discretely monitored
Asian options using exactly the same setup, but replacing th e moments of the continuous av-
erage with those of the discrete average. The latter are easi ly computed using iterated expec-
tations. Secondly, we only look at ﬁxed-strike Asian option s in this paper. Since the process/parenleftig
St,/integraltextt
0Sudu/parenrightig
is jointly a polynomial diﬀusion, we can compute all of its mix ed moments. Our
method can then be extended to price ﬂoating-strike Asian op tions by using a bivariate log-
normal as auxiliary distribution. Finally, we can deﬁne the auxiliary density was a mixture
of log-normal densities, as studied in Ackerer and Filipovi´ c (2017). Using a mixture allows
to match higher order moments, which can lead to a faster conv ergence of the approximating
series.
11",2018-02-05T09:26:13Z,tre rst  t secondly  since st sud our  nally cker er lipovi usi
paper_qf_24.pdf,12,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","References
Ackerer, D. and D. Filipovi´ c (2017). Option pricing with or thogonal polynomial expansions.
Working paper .
Ackerer, D., D. Filipovi´ c, and S. Pulido (2017). The Jacobi stochastic volatility model. Finance
and Stochastics , 1–34.
Al-Mohy, A. H. and N. J. Higham (2011). Computing the action o f the matrix exponential,
with an application to exponential integrators. SIAM Journal on Scientiﬁc Computing 33 (2),
488–511.
Aprahamian, H. and B. Maddah (2015). Pricing Asian options v ia compound gamma and
orthogonal polynomials. Applied Mathematics and Computation 264 , 21–43.
Asmussen, S., P.-O. Goﬀard, and P. J. Laub (2016). Orthonorma l polynomial expansions and
lognormal sum densities. arXiv preprint arXiv:1601.01763 .
Bally, V. (2003). An elementary introduction to Malliavin calculus . Ph. D. thesis, INRIA.
Benhamou, E. (2002). Fast Fourier transform for discrete As ian options. Journal of Computa-
tional Finance 6 (1), 49–68.
Bj¨ orck, k. and V. Pereyra (1970). Solution of Vandermonde s ystems of equations. Mathematics
of Computation 24 (112), 893–903.
Black, F. and M. Scholes (1973). The pricing of options and co rporate liabilities. Journal of
Political Economy 81 (3), 637–654.
Caliari, M., P. Kandolf, A. Ostermann, and S. Rainer (2014). Comparison of software for
computing the action of the matrix exponential. BIT Numerical Mathematics 54 (1), 113–
128.
Carmona, P., F. Petit, and M. Yor (1997). On the distribution and asymptotic results for
exponential functionals of L´ evy processes. Exponential functionals and principal values related
to Brownian motion , 73–121.
Carverhill, A. and L. Clewlow (1990). Valuing average rate ( Asian) options. Risk 3(4), 25–29.
ˇCern` y, A. and I. Kyriakou (2011). An improved convolution a lgorithm for discretely sampled
Asian options. Quantitative Finance 11 (3), 381–389.
Curran, M. (1994). Valuing Asian and portfolio options by co nditioning on the geometric mean
price.Management science 40 (12), 1705–1711.
Donati-Martin, C., R. Ghomrasni, and M. Yor (2001). On certa in Markov processes attached to
exponential functionals of Brownian motion; applications to Asian options. Revista Matem-
atica Iberoamericana 17 (1), 179.
Dufresne, D. (1990). The distribution of a perpetuity, with applications to risk theory and
pension funding. Scandinavian Actuarial Journal 1990 (1), 39–79.
Dufresne, D. (2000). Laguerre series for Asian and other opt ions.Mathematical Finance 10 (4),
407–428.
12",2018-02-05T09:26:13Z,referenccker er lipovi optworki cker er lipovi pu l t jacob nance stochastic al moh hh uti journal cie nti uti drama maima prici  applied matmatics tas muss ego lau north normal   ball aalla iph beam ou fast courier as journal com put nance bj  ey ra solutvar momatmatics t schools t journal political economy call ari and os term anarticial intellence ner arisonumerical matmatics car mona petit or oeonential brownish carhl ew low val risk corky ria u a quantitative nance current val management donate marth or ani or omark brownish  revista mate libero du free t scandiniaauarial journal du free la guerre  matmatical nance
paper_qf_24.pdf,13,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","Eydeland, A. and H. Geman (1995). Domino eﬀect: Inverting the Laplace transform. Risk 8(4),
65–67.
Filipovi´ c, D. and M. Larsson (2016). Polynomial diﬀusions a nd applications in ﬁnance. Finance
and Stochastics 20 (4), 931–972.
Filipovi´ c, D., E. Mayerhofer, and P. Schneider (2013). Den sity approximations for multivariate
aﬃne jump-diﬀusion processes. Journal of Econometrics 176 (2), 93–111.
Fourni´ e, E., J.-M. Lasry, J. Lebuchoux, P.-L. Lions, and N. Touzi (1999). Applications of
Malliavin calculus to Monte Carlo methods in ﬁnance. Finance and Stochastics 3 (4), 391–
412.
Fu, M. C., D. B. Madan, and T. Wang (1999). Pricing continuous Asian options: a compar-
ison of Monte Carlo and Laplace transform inversion methods .Journal of Computational
Finance 2 (2), 49–74.
Fusai, G., D. Marazzina, and M. Marena (2011). Pricing discr etely monitored asian options by
maturity randomization. SIAM Journal on Financial Mathematics 2 (1), 383–403.
Fusai, G. and A. Meucci (2008). Pricing discretely monitore d Asian options under L´ evy pro-
cesses.Journal of Banking & Finance 32 (10), 2076–2088.
Gautschi, W. (2004). Orthogonal Polynomials: Computation and Approximation . Oxford Uni-
versity Press.
Geman, H. and M. Yor (1993). Bessel processes, Asian options , and perpetuities. Mathematical
Finance 3 (4), 349–375.
Heyde, C. (1963). On a property of the lognormal distributio n. InSelected Works of CC Heyde .
Springer Science & Business Media.
Jarrow, R. andA. Rudd(1982). Approximateoption valuation forarbitrarystochastic processes.
Journal of Financial Economics 10 (3), 347–369.
Ju, N. (2002). Pricing Asian and basket options via Taylor ex pansion. Journal of Computational
Finance 5 (3), 79–103.
Kemna, A. G. and A. Vorst (1990). A pricing method for options based on average asset values.
Journal of Banking & Finance 14 (1), 113–129.
Lapeyre, B., E. Temam, et al. (2001). Competitive Monte Carl o methods for the pricing of
Asian options. Journal of Computational Finance 5 (1), 39–58.
Levy, E. (1992). Pricing European average rate currency opt ions.Journal of International
Money and Finance 11 (5), 474–491.
Li, W. and S. Chen (2016). Pricing and hedging of arithmetic A sian options via the Edgeworth
series expansion approach. The Journal of Finance and Data Science 2 (1), 1–25.
Linetsky, V. (2004). Spectral expansions for Asian (averag e price) options. Operations Re-
search 52 (6), 856–867.
Marcozzi, M. D. (2003). On the valuation of Asian options by v ariational methods. SIAM
Journal on Scientiﬁc Computing 24 (4), 1124–1140.
13",2018-02-05T09:26:13Z,ey mand gem adomino investi place risk lipovi larsopolynomial nance stochastic lipovi mayer hoschneir journal econometrics four ni las ry le bu hou lns to ui applicatns alla i  nance stochastic fu madmawa prici    place journal tnal nance fu articial intellence ma jazz iarena prici journal nancial matmatics fu articial intellence mecca prici  journal banki nance ga teorthogonal polynomials tapproximatoxford uni  gem aor vessel  matmatical nance y  oiseleed works y   science business media narrow rudd approximate optjournal nancial economics ju prici  taylor journal tnal nance ke mna worst journal banki nance lap eye tem am etitive  carl  journal tnal nance levy prici ajournal internatnal money nance li cprici edgeworth t journal nance data science linet speral  oatns re marco zz o journal cie nti uti
paper_qf_24.pdf,14,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","Milevsky, M.A.andS.E.Posner(1998). Asianoptions, thesu moflognormals, andthereciprocal
gamma distribution. Journal of Financial and Quantitative Analysis 33 (3), 409–422.
Nualart, D. (2006). The Malliavin Calculus and Related Topics . Springer.
Ritchken, P., L. Sankarasubramanian, and A. M. Vijh (1993). The valuation of path dependent
contracts on the average. Management Science 39 (10), 1202–1213.
Rogers, L. C. G. and Z. Shi (1995). The value of an Asian option .Journal of Applied Probabil-
ity 32(4), 1077–1088.
Shaw, W. (2002). Pricing Asian options by contour integrati on, including asymptotic methods
for low volatility. Working paper .
Sun, J., L. Chen, andS.Li (2013). A quasi-analytical pricin gmodel forarithmetic Asian options.
Journal of Futures Markets 33 (12), 1143–1166.
Thompson, G. (2002). Fast narrow bounds on the value of Asian options. Technical report,
Judge Institute of Management Studies.
Turnbull, S. M. and L. M. Wakeman (1991). A quick algorithm fo r pricing European average
options. Journal of Financial and Quantitative Analysis 26 (3), 377–389.
Turner, L. R. (1966). Inverse of the Vandermonde matrix with applications.
Vanmaele, M., G. Deelstra, J. Liinev, J. Dhaene, and M. J. Goo vaerts (2006). Bounds for the
price of discrete arithmetic Asian options. Journal of Computational and Applied Mathemat-
ics 185(1), 51–90.
Vecer, J. (2001). A new PDE approach for pricing arithmetic a verage Asian options. Journal of
Computational Finance 4 (4), 105–113.
Vecer, J. (2002). Uniﬁed pricing of Asian options. Risk 15(6), 113–116.
Yor, M. (1992). On some exponential functionals of Brownian motion. Advances in Applied
Probability 24 (3), 509–531.
Zvan, R., P. A. Forsyth, and K. R. Vetzal (1996). Robust numer ical methods for PDE models
of Asian options. Technical report, University of Waterloo , Faculty of Mathematics.
14",2018-02-05T09:26:13Z,me  poster  optns journal nancial quantitative analysis nu al art t alla icalculus related topics  itkesandra suania vi t management science ro shi t  journal applied pro babel shaw prici  worki sucli  journal futurmarkets thompsofast  technical judge institute management studiturnbull wake maajournal nancial quantitative analysis turner inverse var movamae le  el stra li iev dha ene goo nds  journal tnal applied ma tm at ve cer  journal tnal nance ve cer uni  risk or obrownish advancapplied probabity vaforth vet zal robust  technical  waterloo faculty matmatics
paper_qf_24.pdf,15,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","A Scaling with auxiliary moments
Inthisappendixweshowhowtoavoid roundingerrorsbyscali ngtheproblemusingthemoments
of the auxiliary density w.
Using (L−1)⊤L−1=M−1we get from ( 3.4):
π(N)= (f0,...,fN)(ℓ0,...,ℓN)⊤
= e−rT(˜f0,...,˜fN)M−1eGNTHN(0).
DeﬁneS∈R(N+1)×(N+1)as the diagonal matrix with the moments of won its diagonal:
S= diag(s0,...,sN), si= eiµ+1
2i2ν2.
We can now write
π(N)= e−rT(f0,...,fN)S−1SM−1SS−1eGNTSS−1HN(0)
= e−rT(f0,...,fN)M−1eGNTHN(0), (A.1)
where we have deﬁned the matrices (( GN)ij)0≤i,j≤N, (Mij)0≤i,j≤Nand the vector ( f0,...,fN)⊤
as
Mij= eijν2,fi= eµ+1
2(2i+1)ν2Φ(di+1)−KΦ(di),(GN)ij=

ir+1
2i(i−1)σ2j=i
i
Te−µ+1
2(1−2i)ν2j=i−1
0 else,
fori,j= 0,...,N. The components of Mand (f0,...,fN)⊤growmuchslower for increasing
Nas their counterparts Mand (˜f0,...,˜fN)⊤, respectively. The vector eGNTHN(0) corresponds
to the moments of ATdivided by the moments corresponding to w. Since both moments grow
approximately at the same rate, this vector will have compon ents around one. This scaling is
important since for large Nthe raw moments of ATare enormous. This was causing trouble for
example in Dufresne (2000). We therefore circumvent the numerical inaccuracies comi ng from
the explosive moment behavior by directly computing the rel ative moments.
The likelihood and payoﬀ coeﬃcients can be computed by perfo rming a Cholesky decomposition
onMinstead of M:
(f0,...,fN)⊤= e−rTL−1(f0,...,fN)⊤,
(ℓ0,...,ℓN)⊤=L−1eGNTHN(0),
whereM=LL⊤is the Cholesky decomposition of M.
Remark that to compute the option price in ( A.1), we do not necessarily need to do a Cholesky
decomposition. Indeed, we only need to invert the matrix M. Doing a Cholesky decomposition
is one way to solve a linear system, but there are other possib ilities. In particular, remark
thatMis a Vandermonde matrix and its inverse can be computed analy tically (see e.g. Turner
(1966)). There also exist speciﬁc numerical methods to solve line ar Vandermonde systems, see
e.g.Bj¨ orck and Pereyra (1970). However, we have not found any signiﬁcant diﬀerences betwe en
the Cholesky method and alternative matrix inversion techn iques for the examples considered
in this paper.
For very large values of ν, even the matrix Mmight become ill conditioned. In this case it is
advisable to construct the orthonormal basis using the thre e-term recurrence relation:
15",2018-02-05T09:26:13Z,scali i   show how to oid usi   mi and mi te t and nas and t divid since  t are  du free  t hole  instead hole  remark hole  ined doi hole  imis var moturner tre var mobj  ey ra hole  for mht in
paper_qf_24.pdf,16,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","Lemma A.1. The polynomials bn∈Poln(R),n= 0,1,..., deﬁned recursively by
b0(x) = 1, b1(x) =1
β1(x−α0),
bn(x) =1
βn((x−αn−1)bn−1(x)−βn−1bn−2(x)), n= 2,3,...,
with
αn= eµ+ν2(n−1
2)/parenleftig
eν2(n+1)+eν2n−1/parenrightig
, n= 0,1,...
βn= eµ+1
2ν2(3n−2)/radicalbig
eν2n−1, n= 1,2,...,
satisfy
/integraldisplay
Rbi(x)bj(x)w(x)dx=/braceleftigg
1i=j
0else, i,j= 0,1,....
Proof.Straightforwardapplication ofthemoment-generating fun ctionofthenormaldistribution
and Theorem 1.29 in Gautschi (2004).
The above recursion suﬀers from rounding errors in double pre cision arithmetic for small ν, but
is very accurate for large ν.
B Control variate for simulating g(x)
In order to increase the eﬃciency of the Monte-Carlo simulat ion ofg(x), we describe in this
appendix how to use the density of the geometric average as a c ontrol variate. This idea is
inspired by Kemna and Vorst (1990), who report a very substantial variance reduction when
using the geometric Asian option price as a control variate i n the simulation of the arithmetic
Asian option price. Denote by QT= exp/parenleftig
1
T/integraltextT
0log(Ss)ds/parenrightig
the geometrical price average. It is
not diﬃcult to see that log( QT) is normally distributed with mean1
2(r−1
2σ2)Tand variance
σ2
3T. Hence, QTis log-normally distributed and we know its density functio n, which we denote
byq(x), explicitly. Similarly as in Lemma 4.1, we can also express q(x) as an expectation:
Lemma B.1. For any x∈R
q(x) =E/bracketleftbigg/parenleftbig
1{QT≥x}−c(x)/parenrightbig/parenleftbigg2BT
σTQT+1
QT/parenrightbigg/bracketrightbigg
, (B.1)
wherecis any deterministic ﬁnite-valued function.
We now propose the following estimator for the density of the arithmetic average:
g(x) =E/bracketleftbigg
(1{AT≥x}−c1(x))2
σ2/parenleftbiggST−S0
TA2
T+σ2−r
AT/parenrightbigg
+/parenleftbigg
q(x)−/parenleftbig
1{QT≥x}−c2(x)/parenrightbig/parenleftbigg2BT
σTQT+1
QT/parenrightbigg/parenrightbigg/bracketrightbigg
, (B.2)
16",2018-02-05T09:26:13Z,lea t poll rbi proof str articial intellence ghtforward applicattorem ga tet contri   ke mna worst   note it and nce tis simarly lea lea for 
paper_qf_24.pdf,17,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","for some deterministic ﬁnite-valued functions c1andc2. Given the typically high correlation
between the geometric and arithmetic average, the above est imator has a signiﬁcantly smaller
variance than the estimator in ( 4.1). In numerical examples the functions c1andc2are chosen
as follows:
c1(x) = 1x≤mA
1, c2(x) = 1x≤mQ
1,
wheremA
1andmQ
1denote the ﬁrst moments of ATandQT, respectively.
Finally, we use ( B.2) to express /bardblℓ/bardbl2
was an expectation that can be evaluated using Monte-Carlo
simulation:
/bardblℓ/bardbl2
w=E
(1{AT≥˜AT}−c1(˜AT))2
σ2/parenleftig
ST−S0
TA2
T+σ2−r
AT/parenrightig
w(˜AT)
+q(˜AT)−/parenleftig
1{QT≥˜AT}−c2(˜AT)/parenrightig/parenleftig
2BT
σTQT+1
QT/parenrightig
w(˜AT)
, (B.3)
where the random variable ˜ATis independent from all other random variables and has the sa me
distribution as AT. In numerical examples we ﬁnd a variance reduction of roughl y a factor
ten.
C Proofs
This appendix contains all the proofs.
C.1 Proof of Lemma 2.1
Using the time-reversal property of a Brownian motion, we ha ve the following identity in law
for ﬁxed t >0 :
tAt=/integraldisplayt
0e(r−1
2σ2)u+σBudu
law=/integraldisplayt
0e(r−1
2σ2)(t−u)+σ(Bt−Bu)du
=St/integraldisplayt
0S−1
udu.
Applying Itˆ o’s lemma to Xt:=St/integraltextt
0S−1
udugives
dXt=StS−1
tdt+/integraldisplayt
0S−1
udu(rStdt+σStdBt)
= (rXt+1)dt+σXtdBt.
17",2018-02-05T09:26:13Z,giveiand nally   tis iproofs  proof lea usi brownish at bud  bu st applyi it xt st xt st std std  xt td 
paper_qf_24.pdf,18,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","C.2 Proof of Proposition 2.2
Applying the inﬁnitesimal generator Gcorresponding to the diﬀusion in ( 2.1) to a monomial xn
gives:
Gxn=xn(nr+1
2n(n−1)σ2)+nxn−1.
Hence, we have GHn(x) =˜GnHn(x) componentwise, where ˜Gnis deﬁned as
˜Gn=
0
1r
......
n(nr+1
2n(n−1)σ2)
.
Using the identity in distribution of Lemma 2.1, we get
E[Hn(AT)] = diag( Hn(T−1))E[Hn(XT)]
= diag(Hn(T−1))/parenleftbigg
Hn(X0)+/integraldisplayT
0E[GHn(Xu)]du/parenrightbigg
= diag(Hn(T−1))Hn(0)+diag( Hn(T−1))˜Gn/integraldisplayT
0E[Hn(Xu)]du
=Hn(0)+diag( Hn(T−1))˜Gndiag(Hn(T))/integraldisplayT
0E[Hn(Au)]du
=Hn(0)+Gn/integraldisplayT
0E[Hn(Au)]du,
whereGnwas deﬁned in ( 2.2). The result now follows from solving the above linear ﬁrst o rder
ODE.
C.3 Proof of Proposition 2.3
1. We will show that the solution at time T >0 of the SDE in ( 2.1) admits a smooth density
function. The claim then follows by the identity in law.
Deﬁne the volatility and drift functions a(x) =σxandb(x) =rx+ 1. H¨ ormander’s
condition (see for example Section 2.3.2 in Nualart(2006)) becomes in this case:
a(X0)/ne}ationslash= 0 or a(n)(X0)b(X0)/ne}ationslash= 0 for some n≥1.
H¨ ormander’s condition is satisﬁed since for n= 1 we have a′(0)b(0) =σ/ne}ationslash= 0. Since
a(x) andb(x) are inﬁnitely diﬀerentiable functions with bounded partia l derivatives of
all orders, we conclude by Theorem 2.3.3 in Nualart (2006) thatXT, and therefore AT,
admits a smooth density function.
2. We start from the following two observations:
AT≤sup
0≤u≤TSuandP/parenleftigg
sup
0≤u≤TσBu≥x/parenrightigg
= 2P/parenleftbigg
Z≥x
σ√
T/parenrightbigg
,
18",2018-02-05T09:26:13Z,proof propositapplyi correspondi gx nce hghgnis gusi lea hhhhhhxu hhhghxu hhndia hhau hghau gwas t proof proposit t  senu al art since torem nu al art  su and bu
paper_qf_24.pdf,19,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","whereZ∼N(0,1). Using the fact that the exponential is an increasing func tion, we get
P(AT≥x)≤P/parenleftigg
sup
0≤u≤TSu≥x/parenrightigg
=P/parenleftigg
sup
0≤u≤T(r−1
2σ2)u+σBu≥log(x)/parenrightigg
≤

2P/parenleftig
Z≥log(x)−(r−1
2σ2)T
σ√
T/parenrightig
ifr≥1
2σ2
2P/parenleftig
Z≥log(x)
σ√
T/parenrightig
ifr≤1
2σ2.
Applying the rule of l’Hˆ opital gives
lim
x→∞g(x)/parenleftbigg
e−log(x)2
2σ2T/parenrightbigg−1
= lim
x→∞P(AT≥x)/parenleftbigg/integraldisplay∞
xe−log(y)2
2σ2Tdy/parenrightbigg−1
≤21√
2πTσlim
x→∞/integraldisplay∞
xe−(log(y)−(r−1
2σ2)+T)2
2σ2Tdy/parenleftbigg/integraldisplay∞
xe−log(y)2
2σ2Tdy/parenrightbigg−1
=/radicalbigg
2
πT1
σ.
Hence we have show that g(x) =O/parenleftig
exp/braceleftig
−1
2log(x)2
σ2T/bracerightig/parenrightig
forx→ ∞.
Since the exponential is a convex function, we have that the a rithmetic average is always
bounded below by the geometric average:
AT≥QT= exp/parenleftbigg1
T/integraldisplayT
0log(Ss)ds/parenrightbigg
.
It is not diﬃcult to see that log( QT) is normally distributed with mean1
2(r−1
2σ2)Tand
varianceσ2
3T. By similar arguments as before we therefore have
g(x) =O/parenleftbigg
exp/braceleftbigg
−3
2log(x)2
σ2T/bracerightbigg/parenrightbigg
forx→0.
C.4 Proof of Proposition 3.1
We can write the squared norm of ℓas
/bardblℓ/bardbl2
w=/integraldisplay∞
0/parenleftbiggg(x)
w(x)/parenrightbigg2
w(x)dx
=/integraldisplaya
0g(x)2
w(x)dx+/integraldisplayb
ag(x)2
w(x)dx+/integraldisplay∞
bg(x)2
w(x)dx, (C.1)
for some 0 < a < b < ∞. The second term is ﬁnite since the functiong2
wis continuous over the
compact interval [ a,b]. From Proposition 2.3we have
g(x)2=O/parenleftbigg
exp/braceleftbigg
−log(x)2
σ2T/bracerightbigg/parenrightbigg
,forx→ ∞andx→0.
19",2018-02-05T09:26:13Z,usi su bu applyi td td td nce since it and by proof proposit t from propositn
paper_qf_24.pdf,20,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","For the log-normal density we have
w(x) =O/parenleftbigg
exp/braceleftbigg
−log(x)2
2ν/bracerightbigg/parenrightbigg
,forx→ ∞andx→0.
Since 2ν > σ2Tby assumption, we are guaranteed that the ﬁrst and last term i n (C.1) are ﬁnite
for a suﬃciently small (resp. large) choice of a(resp.b).
C.5 Proof of Proposition 3.3
The payoﬀ coeﬃcients can be written as
(f0,...,fN)⊤=e−rTC(˜f0,...,˜fN)⊤,
with
˜fn=1√
2πν/integraldisplay∞
0(ex−K)+enxe−(x−µ)2
2ν2dx
=1√
2πν/parenleftigg/integraldisplay∞
log(K)e(n+1)xe−(x−µ)2
2ν2dx−K/integraldisplay∞
log(K)enxe−(x−µ)2
2ν2dx/parenrightigg
.
Completing the square in the exponent gives
1√
2πν/integraldisplay∞
log(K)enxe−(x−µ)2
2ν2dx=1√
2πνeµn+1
2n2ν2/integraldisplay∞
log(K)e−(x−(µ+ν2n))2
2ν2dx
=1√
2πeµn+1
2n2ν2/integraldisplay∞
log(K)−(µ+ν2n)
νe−1
2y2dy
= eµn+1
2n2ν2Φ(dn),
wherednis deﬁned in ( 3.5). We ﬁnally get
˜fn= eµ(n+1)+1
2(n+1)2ν2Φ(dn+1)−Keµn+1
2n2ν2Φ(dn).
C.6 Proof of Lemma 4.1
This proof is based on Malliavin calculus techniques, we ref er toNualart(2006) for an overview
of standard results in this area. A similar approach is taken byFourni´ e et al. (1999) to compute
the Greeks of an Asian option by Monte-Carlo simulation.
Denote by D:D1,2→L2(Ω×[0,T]), F/ma√sto→ {DtF,t∈[0,T]}, the Malliavin derivative operator.
By Theorem 2.2.1 in ( Nualart,2006) we have St,At∈D1,2fort∈(0,T] and
DuSt=σSt1{u≤t}, DuAt=σ
t/integraldisplayt
uSsds.
Denote by
δ:Dom(δ)→L2(Ω),{Xt,t∈[0,T]} /ma√sto→δ(X)
the Skorohod integral and by Dom(δ)⊆L2(Ω×[0,T]) the correspondingdomain. TheSkorohod
integral is deﬁned as the adjoint operator of the Malliavin d erivative and can be shown to
20",2018-02-05T09:26:13Z,for since by proof propositt i  ke proof lea  alla inu al art four ni greek    note dt alla iby torem nu al art st at du st st du at sds note dom xt oro had dom t oro had alla in
paper_qf_24.pdf,21,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","extend the Itˆ o integral to non-adapted processes. In parti cular, we have immediately that
{St,t∈[0,T]} ∈Dom(δ) and
(C.2) δ(S) =/integraldisplayT
0SsdBs.
Forφ∈C∞
cwe have φ(AT)∈D1,2and
/integraldisplayT
0(Duφ(AT))Sudu=φ′(AT)/integraldisplayT
0(DuAT)Sudu.
Using the duality relationship between the Skorohod integr al and the Malliavin derivative we
get
E[φ′(AT)] =E/bracketleftigg/integraldisplayT
0(Duφ(AT))Su/integraltextT
0(DuAT)Sududu/bracketrightigg
=E/bracketleftigg
φ(AT)δ/parenleftigg
S/integraltextT
0(DuAT)Sudu/parenrightigg/bracketrightigg
. (C.3)
By Lemma 1 in Bally(2003) (see also Proposition 2.1.1 in Nualart(2006) for a similar approach)
we obtain the following representation of the density funct ion ofAT:5
g(x) =E/bracketleftigg
1{AT≥x}δ/parenleftigg
S/integraltextT
0(DuAT)Sudu/parenrightigg/bracketrightigg
=T
σE/bracketleftigg
1{AT≥x}δ/parenleftigg
S/integraltextT
0Su/integraltextT
uSsdsdu/parenrightigg/bracketrightigg
. (C.4)
Interchanging the order of integration gives
/integraldisplayT
0Su/integraldisplayT
uSsdsdu=/parenleftbigg/integraldisplayT
0Sudu/parenrightbigg2
−/integraldisplayT
0/integraldisplayu
0SuSsdsdu
=/parenleftbigg/integraldisplayT
0Sudu/parenrightbigg2
−/integraldisplayT
0Ss/integraldisplayT
sSududs,
which gives/integraltextT
0Su/integraltextT
uSsdsdu=T2
2A2
T.Plugging this into ( C.4) gives
g(x) =2
TσE/bracketleftbigg
1{AT≥x}δ/parenleftbiggS
A2
T/parenrightbigg/bracketrightbigg
.
We use Proposition 1.3.3 in Nualart (2006) to factor out the random variable A−2
Tfrom the
Skorohod integral:
δ/parenleftbiggS
A2
T/parenrightbigg
=A−2
Tδ(S)−/integraldisplayT
0Dt/parenleftbig
A−2
T/parenrightbig
Stdt
=A−2
T1
σ/parenleftbigg
ST−S0−r/integraldisplayT
0Ssds/parenrightbigg
−/integraldisplayT
0Dt/parenleftbig
A−2
T/parenrightbig
Stdt,
5Informally speaking one applies a regularization argument in order to use ( C.3) for the (shifted) Heaviside
function φ(y) = 1{y≥x}.
21",2018-02-05T09:26:13Z,it ist dom ssd bs for du sud du sud usi oro had alla idu su du su du du sud by lea ball propositnu al art du sud su sds du interi su sds du sud su sds du sud su du su sds du plui  propositnu al art from oro had dt std sds dt std informally isi
paper_qf_24.pdf,22,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","where we used ( C.2) in the last equation. Using the chain rule for the Malliavin derivative we
get
δ/parenleftbiggS
A2
T/parenrightbigg
=A−2
T1
σ/parenleftbigg
ST−S0−r/integraldisplayT
0Ssds/parenrightbigg
+2A−3
T/integraldisplayT
0DtATStdt
=A−2
T1
σ(ST−S0−rTAT)+2A−3
T1
T/integraldisplayT
0St/integraldisplayT
tσSududt
=A−2
T1
σ(ST−S0−rTAT)+A−1
TσT
=A−2
T1
σ(ST−S0)+T
σA−1
T(σ2−r).
Putting everything back together we ﬁnally get:
g(x) =2
σ2E/bracketleftbigg
1{AT≥x}/parenleftbiggST−S0
TA2
T+σ2−r
AT/parenrightbigg/bracketrightbigg
.
Since the Skorohod integral has zero expectation we also hav e
g(x) =2
σ2E/bracketleftbigg/parenleftbig
1{AT≥x}−c(x)/parenrightbig/parenleftbiggST−S0
TA2
T+σ2−r
AT/parenrightbigg/bracketrightbigg
,
for any deterministic ﬁnite-valued function c.
C.7 Proof of Corollary 4.3
The result follows immediately from ( 4.1) and
/bardblℓ/bardbl2
w=/integraldisplay∞
0ℓ2(x)w(x)dx=/integraldisplay∞
0g(x)
w(x)g(x)dx.
C.8 Proof of Proposition 4.4
Using the Cauchy-Schwarz inequality and the orthonormalit y of the polynomials b0,...,bNwe
get
|π−π(N)|=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/an}bracketle{tF,l/an}bracketri}htw−N/summationdisplay
n=0fnℓn/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=/angbracketleftigg
F−N/summationdisplay
n=0bnfn, ℓ−N/summationdisplay
n=0bnℓn/angbracketrightigg
w
≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleF−N/summationdisplay
n=0bnfn/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
w/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleℓ−N/summationdisplay
n=0bnℓn/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
w
=/parenleftigg
/bardblF/bardbl2
w−N/summationdisplay
n=0f2
n/parenrightigg1
2/parenleftigg
/bardblℓ/bardbl2
w−N/summationdisplay
n=0ℓ2
n/parenrightigg1
2
.
22",2018-02-05T09:26:13Z,usi alla isds dt std st sud ud pui since oro had proof corollary t proof propositusi catc schwartz 
paper_qf_24.pdf,23,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","C.9 Proof of Lemma B.1
Applying the Malliavin derivative to QTgives
DuQT=QTDu/parenleftbigg1
T/integraldisplayT
0log(Ss)ds/parenrightbigg
=QTσ
T(T−u)1u≤T.
Similarly as in the proof of Lemma 4.1we can write
q(x) =E/bracketleftigg
1{QT≥x}δ/parenleftigg
1/integraltextT
0DuQTdu/parenrightigg/bracketrightigg
=E/bracketleftbigg
1{QT≥x}δ/parenleftbigg2
QTσT/parenrightbigg/bracketrightbigg
. (C.5)
Using Proposition 1.3.3 in Nualart(2006) to factor out the random variable from the Skorohod
integral gives
δ/parenleftbigg2
QTσT/parenrightbigg
=2BT
σTQT−2
σT/integraldisplayT
0Du(Q−1
T)du
=2BT
σTQT−2
σTQ−2
T/integraldisplayT
0QTσ
T(T−u)du
=2BT
σTQT+1
QT.
Plugging this back into ( C.5) ﬁnally gives
q(x) =E/bracketleftbigg
1{QT≥x}/parenleftbigg2BT
σTQT+1
QT/parenrightbigg/bracketrightbigg
.
Since the Skorohod integral has zero expectation we also hav e
q(x) =E/bracketleftbigg/parenleftbig
1{QT≥x}−c(x)/parenrightbig/parenleftbigg2BT
σTQT+1
QT/parenrightbigg/bracketrightbigg
,
for any deterministic ﬁnite-valued function c.
23",2018-02-05T09:26:13Z,proof lea applyi alla ivdu du simarly lea du td usi propositnu al art oro had du plui since oro had
paper_qf_24.pdf,24,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","Caser σ T S 0LNS10 LNS15 LNS20 LS EE VEC MC 95% CI
1 .02 .10 1 2.0 .05601 .05600 .05599 .0197 .05599 .05595 [.055 98,.05599]
2 .18 .30 1 2.0 .2185 .2184 .2184 .2184 .2184 .2184 [.2183 , .21 85]
3 .0125 .25 2 2.0 .1723 .1722 .1722 .1723 .1723 .1723 [.1722 , . 1724]
4 .05 .50 1 1.9 .1930 .1927 .1928 .1932 .1932 .1932 [.1929 , .19 33]
5 .05 .50 1 2.0 .2466 .2461 .2461 .2464 .2464 .2464 [.2461 , .24 66]
6 .05 .50 1 2.1 .3068 .3062 .3061 .3062 .3062 .3062 [.3060 , .30 65]
7 .05 .50 2 2.0 .3501 .3499 .3499 .3501 .3501 .3500 [.3494 , .35 04]
Table 1: Price approximations for diﬀerent parameterizatio ns and diﬀerent methods. The strike
price isK= 2 for all cases. The column LNS Xrefers to the method presented in this paper with
the ﬁrst 1+ Xterms of the series, LS to Dufresne (2000), EE to Linetsky (2004), VEC to Vecer
(2001,2002), and MC 95% CI to the 95% conﬁdence interval of the Monte-Car lo simulation.
Caser σ T S 0LNS10 LNS15 LNS20 LS EE VEC MC
1 .02 .10 1 2.0 .006 .008 .009 .930 - .277 6.344
2 .18 .30 1 2.0 .002 .002 .003 .666 2.901 .345 5.518
3 .0125 .25 2 2.0 .002 .002 .002 .635 3.505 .374 12.138
4 .05 .50 1 1.9 .001 .002 .003 .785 3.172 .404 6.819
5 .05 .50 1 2.0 .001 .002 .002 .701 2.768 .404 5.432
6 .05 .50 1 2.1 .001 .001 .002 .687 2.719 .398 5.452
7 .05 .50 2 2.0 .002 .002 .004 .594 2.202 .438 11.699
Table 2: Computation times in seconds. The column LNS Xrefers to the method presented in
this paper with the ﬁrst 1+ Xterms of the series, LS to Dufresne (2000), EE to Linetsky (2004),
VEC toVecer(2001,2002), and MC to the Monte-Carlo simulation.
24",2018-02-05T09:26:13Z,case table price t t refers terms du free linet ve cer  car case table tt refers terms du free linet ve cer  
paper_qf_24.pdf,25,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","0 2 4 6 8 10 12 14 16 18 200.0540.0560.0580.060.0620.0640.0660.068
Polynomial expansion
MC
MC 95%-CI
(a) Case 10 2 4 6 8 10 12 14 16 18 200.170.1750.180.1850.190.1950.20.2050.21
Polynomial expansion
MC
MC 95%-CI
(b) Case 3
0 2 4 6 8 10 12 14 16 18 200.240.250.260.270.280.290.3
Polynomial expansion
MC
MC 95%-CI
(c) Case 50 2 4 6 8 10 12 14 16 18 200.340.350.360.370.380.390.40.410.42
Polynomial expansion
MC
MC 95%-CI
(d) Case 7
Figure 1: Asian option price approximations in function of p olynomial approximation order N.
The cases correspond to diﬀerent parameterizations shown in Table1.
25",2018-02-05T09:26:13Z,polynomial case polynomial case polynomial case polynomial case   t table
paper_qf_24.pdf,26,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","0.7 0.8 0.9 1 1.1 1.2 1.3 1.4-101234567
(a) Case 10.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 2.200.511.522.5
(b) Case 3
0.5 1 1.5 2 2.500.20.40.60.811.21.41.6
(c) Case 50 0.5 1 1.5 2 2.5 3 3.500.20.40.60.811.2
(d) Case 7
Figure 2: Simulated true density function g(x) and approximated density functions g(n)(x),
n= 0,4,20. The cases correspond to diﬀerent parameterizations show n in Table 1.
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10123456710-3
(a) Squared relative error0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1-2024681012141610-3
(b) Squared relative error upper bound
Figure 3: Squared relative approximation error for diﬀerent values of σ. Dashed lines correspond
to the 95% conﬁdence intervals from the Monte-Carlo simulat ion. Parameters: T= 1,r= 0.05,
S0=K= 2.
26",2018-02-05T09:26:13Z,case case case case  simulated t table squared squared  squared dasd   meters
paper_qf_24.pdf,27,Asian Option Pricing with Orthogonal Polynomials,"  In this paper we derive a series expansion for the price of a continuously
sampled arithmetic Asian option in the Black-Scholes setting. The expansion is
based on polynomials that are orthogonal with respect to the log-normal
distribution. All terms in the series are fully explicit and no numerical
integration nor any special functions are involved. We provide sufficient
conditions to guarantee convergence of the series. The moment indeterminacy of
the log-normal distribution introduces an asymptotic bias in the series,
however we show numerically that the bias can safely be ignored in practice.
","0 2 4 6 8 10 12 14 16 18 200.440.460.480.50.520.540.56
Polynomial expansion
MC
MC 95%-CI
(a) Monte-Carlo simulated price0 0.5 1 1.5 2 2.5 3 3.5 4 4.500.20.40.60.811.2
(b) Density approximations
0 2 4 6 8 10 12 14 16 18 2010-210-1100101
(c) Payoﬀ projection error0 2 4 6 8 10 12 14 16 18 2000.020.040.060.080.10.120.140.160.180.2
(d) Likelihood projection error
Figure 4: Visualization of the projection bias in the extrem e volatility case. Dashed lines
correspondtothe95%conﬁdenceintervalsfromtheMonte-Ca rlosimulation. Parameters: σ= 1,
T= 1,r= 0.05,S0=K= 2.
27",2018-02-05T09:26:13Z,polynomial   nsity pay likelihood  visualizatdasd  ca meters
paper_qf_25.pdf,1,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","1 
  
 
Nonlinear price dynamics of S&P 100 stocks  
Gunduz Caginalp  
University of Pittsburgh  
Pittsburgh, PA 15260  
caginalp@pitt.edu  
 
Mark DeSantis  
Chapman University  
Orange, CA 92866  
desantis@chapman.edu  
July 9, 2019  
Abstract.   The methodology  presented  provides a quantitative way to characterize investor behavior 
and price dynamics within a particular asset class and time period.  The methodology is applied to a data 
set consisting of  over 2 50,000 data points of the S&P 100 stocks during 200 4-2018.  Using a two -way 
fixed -effects model, we uncover trader  motivations including evidence of both under - and overreaction 
within a unified setting.  A nonlinear relationship  is found  between return and trend suggesting a small, 
positive trend incr eases the return, while a larger one tends to decrease it.  The sha pe parameters of the 
nonlinearity  quantif y trader motivation to buy into trends or wait for bargains.  The methodology allows 
the testing of  any behavioral finance bias or technical analysis concept.  
Key Words:  S&P stocks; Trend; Nonlinear dynamics; Volume  
JEL Codes: G02, G12, G17  ",2019-07-09T21:33:49Z,nonlinear gudue agaialp  pisburgh pisburgh mark  sant is chapma orae july abstra t t usi t t key words trend nonlinear volume cos
paper_qf_25.pdf,2,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","2 
 Nonlinear price dynamics of S&P 100 stocks  
1. Introduction  
Understanding the dynamics of asset prices holds the key to establishing or refuting many of the basic 
theories of economics and finance.  Although there is no unique metho dology for determining the 
valuation of a stock or asset, the basic principles are not in dispute.  However, t he changes in prices 
beyond those that can be explained by valuation have led to a lively debate.  Classical finance provides 
for no other explanation than random fluctuations, while classical economics is largely a study of 
equilibrium conditions.  Thus the study of the relative changes in asset prices, i.e. asset price dynamics, 
is a new discipline th at can explain many of the ideas behind the motivations and strategies of traders 
and investors.  
Key problems in finance related to asset pricing and dynamics can be broadly grouped into four 
categories arranged in order of  increasing refinement and decrea sing time scale : 
I. Portfolio Optimization.  Given a set of investments with a spectrum of variance and expected 
return, the investor  strives to find an optimal portfolio of stocks, bonds and Treasury bills, etc. 
This is a classical problem in finance that us ually assumes a time period of many years.  
II. Risk Premium Analysis.  An investor attempts to d etermine how the risk premium varies, i.e., 
ascertain when investors are more risk averse and move toward safer assets such as bonds or 
Treasury bills. This  flight to or from quality  generally involves a time period that varies from 
months to years.  
III. Asset Price Dynamics.  A trader tries to d etermine , for a particular asset , the likely change in 
price depending upon changes in valuation  and the history of prices, volu me, and volati lity, 
among other factors.  Such technical analysis  is associated with a time scale of one day to several 
weeks  (see also comments in Section 3.3.2).  ",2019-07-09T21:33:49Z,nonlinear introduunrstandi although assical  key tfoptiatgivetreasury  risk premium analysis atreasury  asset price dynamics susen
paper_qf_25.pdf,3,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","3 
 IV. Market Microstructure.  A trader  attempts to d etermine the likely price change of a traded as set 
as a function of the order book.  The timescale is typically on the order of milliseconds to 
minutes for liquid securities and potentially hours for less liquid assets.  
Most of classical finance focuses on (I) Portfolio Optimization , and thus , in provi ding a 
theoretical framework for choosing the highest return given a particular risk tolerance. However, a 
resolution to this problem provides little insight into the more refined issues further down this list. In 
recent years, there has been some effort t o understand why investors as a group are more risk -averse 
during certain time periods than others , which is part of  (II) Risk Premium Analysis . Empirically these 
preferences appear to vary on time periods of several months to many years and provide the ke y to 
understanding large market moves (e.g., the high tech bubble of the late 1990s).  This is an important 
line of investigation that is relevant to understanding significant aggregate market movements . 
However, it does not impact the more detailed issue of the factors behind price movements for a 
particular asset, namely problem (III), Asset Price Dynamics . Methodologies  have been developed  to 
understand how a stock should adjust to a new equilibrium price  given changes in fundamentals, e.g., 
earnings forecasts .  However, the manner in which a stock should adjust to  its recent price history, 
volume and volatility changes, resistance (upon nearing a recent high), etc. is largely untouched by the 
methodol ogie s of (I) and (II).  Finally, the rapid price movements in actively traded stocks as orders flow 
into the marketplace have become another important issue (i.e., problem (IV) Market Microstructure ) 
that has moved to the forefront as the debate rages on the e ffects of high -frequency trading.  
Problems (I) - (IV) each illuminate different aspects of investor motivation. For example, (I) 
involves the key concept of risk tolerance in an equilibrium  or static context, while (II) invokes temporal 
changes in risk tol erance together with other factors that lead to changes in the popularity of stocks. 
Problem (III) can illuminate investor and trader behavior and reactions on a daily basis that  can provide ",2019-07-09T21:33:49Z,market m struure t most tfoptiatirisk premium analysis emically  asset price dynamics methodologinally market m struure problems for problem
paper_qf_25.pdf,4,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","4 
 valuable insight to the causes and evolution of critical periods of market instability, e.g., Fall 2008, as the 
housing crisis unfolded.  Problem (IV) can be useful in developing market trading rules and their 
implications for order flow.  
In this paper, we focus on problem (III). Behavioral finance has noted numerous di versions of 
asset prices from fundamental value and has provided evidence of the factors responsible for these 
discrepancies.  For example, although both overreaction and underreaction have been established, 
advocates of classical finance argue that withou t a way to distinguish overreaction from underreaction, 
the ideas can be viewed as philosophical rather than practical.  This study  shows that bo th of these 
effects are present  and provides clear criteria under which they can be expected.  Hence, it is poss ible to 
distinguish between under - and overreaction so that it is clear when one or the other is more likely. It 
also demonstrates a new method that can illuminate and quantify the motivations of traders using 
market data.   In particular , this methodology  can examine either one stock or a class of stocks over a 
particular time period, and conclude, for example, whether momentum trading, profit taking, or 
changes in valuation motivates traders.  Furthermore, these effects can be measured , and their relative 
importance examined.   This methodology can be used for testing any particular motivation that is 
hypothesized  and can be quantified . 
It is widely acknowledged that ""noise,"" or the randomness involved in valuation, complicates 
testing of  hypotheses re lated to market dynamics (Black , 1986).  Less well appreciated is the idea that 
examining nonlinear phenomena within the framework of linearity can obscure an effect completely.  
For example, suppose a market participant  is examining the effe ct of the recent price trend on asset 
returns.  If the empirical data is such that small uptrends have a positive effect on returns, while large 
uptrends have a negative effect, then a linear  (unli ke a nonlinear)  regression  may  fail to establish a 
signific ant effect . ",2019-07-09T21:33:49Z,fall problem ibehral for  nce it ifurtr  it  less for 
paper_qf_25.pdf,5,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","5 
 This paper  pursue s a methodology that overcomes both of these obstacles using the S&P 100  
stocks. The first key idea is that by modeling the valuation and using this as one of the independent 
variables  in a regression , much of the randomness th at is inherent in valuation  may be extracted .  
Second, the linear, square , and cubic terms of a suitably defined price trend variable  are included as 
regressors.  Thus, the effect of these terms on the return, which is the dependent variable , is obtained . 
The primary objectives  of this paper are to:  
1. Provide a precise methodology for testing and quantifying the effect of any hypothesized  
influence  beyond valuation on the dynamics of stocks by utilizing an equity valuation model 
together with non-classical effects such as trend and resistance.  
2. Demonstrate that the short -term trend variable has a non -linear contribution to the daily 
return, i.e., small uptrends  (downtrends)  increase  (decrease)  daily return , while large uptrends  
(downtrends)  decr ease  (increase)  returns.  
3. Demonstrate that this methodology can be used to examine long -standing  practitioner 
strategies  and trader behavior . 
A natural goal for practitioners is the development of tools for forecasting. Our work provides a basis for 
this en deavor, but the goal in this paper is not to determine the best model for predictions but rather to 
examine quantitatively the existence of factors in price dynamics beyond valuation.  
Closed -end funds (CEFs) and exchange -traded  funds (ETFs) have been used in order to examine 
trader motivations (see Caginalp and DeSantis , 2011a, 2011b for closed -end funds; and Caginalp et al. , 
2014 for ETFs).  The key to this approach is the inclusion of an appropriately defined valuation variable 
as an independent regressor in the model.   The net asset value (NAV) is utilized as a proxy for the fund’s 
intrinsic  value  in these earlier studies.  Since both CEFs and ETFs are special asset classes, it is important 
to determine whether investor behavior can be characterize d for a broad class of major stocks that are ",2019-07-09T21:33:49Z, t second  t provi monstrate monstrate our osed fs fs agaialp  sant is agaialp fs t t since fs fs
paper_qf_25.pdf,6,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","6 
 of interest to both the individual and institutional investor.  In particular, we co nsider the stocks in the 
S&P 100.  As there is no unique and unambiguous valuati on methodology for these stocks,  an approach 
for estimating a stock’s intrinsic value based upon market returns and forecasted earnings per share 
estimates  is developed .  With this variable defined it is then possible  to extend the methodology from 
prior works to stocks.   Thus, it opens the door  to a new way of analyzing the price change dynamics of 
individual stocks and identifying additional influences on returns.   It is then possible to test any 
quantifiable variable in terms of its effect on price changes.   
A discussion of the  implications of the results in terms of the motivations in the marketplace and 
a description of  how this  methodology may be utilized to understand add itional factors and strategies in 
the financial markets  are presented  in the Conclusion . 
2. Background and Related Literature  
2.1 Quantifying behavioral factors  
Behavioral finance suggests a number of factors that influence price dynamics, but the establishment of 
these ideas i n a quantitative  and decisive  manner is a new and evolving endeavor .  Among these 
behavioral factors are two opposing effects:  underreacti on and overreaction.  A steady trend in prices is 
usually seen as evidence of underreaction whereby investors are slow to assimilate and act upon new 
information that arrives into the market stochastically.  Overreaction is essentially the opposite effect 
whereby traders bid up prices excessively as positive information enters the market, and analogously for 
negative  information (Bremmer and Sweeney, 2001; Madura and Richie , 2004 ; Sturm , 2003) .   
Advocates of the efficient market hypothesis would argue that  if both of these phenomena exist 
with similar frequencies  in the marketplace , and there is no methodical way to distinguish them, then 
the market must be efficient  (Fama , 1998 ).  Indeed , he suggests that any alternative model to the ",2019-07-09T21:33:49Z,ias with  it conusbackground related re qualyi behral amo overreabre mer seney ma dra richie storm advocatfam ined
paper_qf_25.pdf,7,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","7 
 efficient market model  must choose between under - and overreaction.  The analysis  in Section 5 , 
however,  shows that both effects are present in this large data set of high capitalization, liquid stocks 
making up  the S&P 100.   Furthermore, this paper provide s a quantitative methodology to distinguish 
between them1.  A cubic relationship exists between the return and the recent price trend, so that an 
uptrend has an increasingly positive effect on prices up to a maximum, at which point an increase in the 
uptrend d iminishes the positive effect on return.  Beyond a particular point in the magnitude of the 
uptrend, there is a negative effect on the return.  
Thus, the analysis demonstrates in a unified setting the impact of both underreaction and 
overreaction within a d ata set of large capitalization stocks. The distinction between these is determined 
empirically . Among the other variables considered in the analysis is volatility (both short - and long -
term), which is a key concept in finance that is generally assumed to be a proxy for risk.  Both volatility 
variables have statistically significant and positive regression coefficients.  In addition, daily trading 
volume exhibits a positive and marginally significant impact on the return.  
This paper utilizes a data set comp rising daily closing prices of the S&P 100 stocks from 200 4-
2018.  While many studies of stock prices have focused on yearly or monthly data, there are a number of 
advantages to studying the shorter time periods from the perspective of understanding the mo tivations 
of traders and investors.  One of these is that the short time scale reduces the impact of long term issues 
such as economic cycles, demographics and other influences on prices.  Another involves the size of the 
data.  Since many stocks are highl y correlated with one another, using a time scale of years severely 
reduces the amount of data.  Examining this issue further, we consider  the time period 1998 to 201 8.  
Most stocks experienced a sharp rise during the first two years, then a sharp decline for three years; 
                                                           
1 Bloomfield et al.  (2000) attempt to discern between situations in which one might expect under - or overreaction 
by conducting laboratory market experiments.  Their results suggest this can be accomplished provided one knows 
the “reliability of investors’ information.”  Ind eed, as the reliability of information increases both “prices and 
investors’ value estimates tend to underreact.”  ",2019-07-09T21:33:49Z,t sefurtr beyond  t amo both i w one anotr since examini most bloomeld tir in
paper_qf_25.pdf,8,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","8 
 followed by another boom during 2003 -2007 and another sharp decline during 2007 -2009; followed by a 
strong recovery during 2009 -2018. Even if one considered the yearly dynamics of 1,000 stocks during 
this 20 year period, one would essentia lly be observing these t hree cycles and arriving at conclusions 
that describe this era, but not necessarily the fundamental forces in stock dynamics.   Thus, the use of 
yearly data is equivalent to studying a small data sample.  
2.2 Asset pricing and estimat ion of intrinsic value  
Several studies, for example the surveys of Subrahmanyam (2010) and Goyal (2012), note that a 
tremendous amount of research has been devoted to understanding why one stock might exhibit a 
higher expected return than another.  At the heart of these cross -sectional studies lies the Fama -
Macbeth (1973) regression methodology  and the development of n -factor models.  
Our goal , however, is to determine the variables beyond valuation that impact future  returns on 
a daily basis  and to develop a methodology that facilitates understanding of trader motivations and 
strateg ies.   
A first step in this process is the e stimat ion of  the value of a stock . This a classical problem in 
finance, dating back to Graham  and Dodd (1934)  [see also review by Pinto et. al. (2015) ].  In principle, 
one would like to estimate the daily valuation as with the closed -end and exchange -traded fund studies  
in which one can approximate the intrinsic value through the use of a proxy su ch as the fund’s net asset 
value.  Lee et al.  (1999) notes that “the scant attention paid to this important topic [measurement of the 
intrinsic value] reflects the standard academic view that a security’s price is the best available estimate 
of the intrins ic value.”  
One way to account for much of the daily change in valuation is by assuming that the relative 
change in a stock’s price should, on average, be proportional to the relative change in the  overall  ",2019-07-09T21:33:49Z,eve asset sevl sub rahmayam goa at fam macbeth our  graham odd pinto i one
paper_qf_25.pdf,9,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","9 
 market.  This concept of beta has long been used by  practitioners  and academics2.  Moreover, cross -
sectional regression studies of asset prices consistently control for the return on the market.  
A study by Ang and Bekaert (2007) finds that the earnings yield is a strong predictor of future 
cash flows,  but does not help to explain excess returns.   Bali et al.  (2008) find that “earnings yield is a 
significant predictor of firm -level stock returns...”  Campbell and Thompson (2008) find that several 
valuation ratios, including earnings -to-price, serve as bette r out -of-sample predictors for returns than 
historical return data.  Lee et al.  (1999) find that the use of consensus analyst earnings forecasts yield 
better predictive results than using time series methods on historical data.  Thus, rather than us ing the 
current earnings yield which does not contain new or forward -looking information, the  relative change 
in the  expected earnings per share is utilized.  Although Ang and Bekaert (2007) find that the short -term 
interest rate is a “robust predictive variable for future excess returns …at short horizons”, we utilize a 
long -term rate.  While the Ang and Bekaert (2007) data set includes d ata up to 2001, our data set 
consists of data from December 2004 to July 201 8.  Note that for much of our time period of intere st 
short -term interest rates were held artificially low (near zero).  Thus, we believe that relative changes in 
the longer term rate are more informative as a 5% change in the short -term rate  (e.g., 0.21% versus 
0.20% are essentially the same for an equity  investor)  is not as significant as a 5% change in the long -
term rate3 which was  more significant during this time period . While long -term rates were also targeted 
by the Federal Reserve during this time period, the se rates were generally higher  and provided some 
competition for stocks.  Finally,  Vassalou (2003) shows that news regarding upcoming gross domestic 
product (GDP) g rowth can help to explain the cross -section of returns. So relative changes in the 
forecasted GDP are utilized (see below).  
                                                           
2 In addition to the numerous academic articles regarding beta, standard Finance textbooks such as Bodie et al. 
(2008) and Luenberger (1998) a lso cover this topic.  From a practitioner’s perspective, beta’s importance is evident 
from its inclusion on the standard equity summary screens/pages of, for example, Bloomberg and Value Line.  
3 Further support for the long -term bond yield is provided by Neely , et al. (2014)  who find that it helps to predict 
the equity risk premium.  ",2019-07-09T21:33:49Z, oa be ka ert bali campbell thompso  although a be ka ert w a be ka ert cember july note  w fl reserve nally vassal ou so inance bodilu eberger from bloomberg value line furtr newly
paper_qf_25.pdf,10,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","10 
 In order to quantify the changes in valuation4, we consider a set of variables that practitioners 
have long known to be relevant and recent studies have established as statistically significan t.  There are 
numerous effects on stocks. These can be divided generally into two sets: those that  impact the overall 
market  and those that are firm specific. Since the former are already incorporated into the price 
changes in the S&P 500, we can utilize these changes rather than consider each of the numerous 
indicators that impact the market.   The change in the v aluation of a particular stock can be separated 
into three parts:  
(i) Changes due to adjustments in the overall economy.  Any modifications to the GDP forecast 
will have an impact on expected output, and hence profits for the aggregate market. 
Changes in inte rest rates affect borrowing costs and consumer purchases.  Each company 
will feel the influence of these two key indicators, and the magnitude of the impact will 
depend on the type of industry.  When determining each stock’s valuation, the regressions 
include a coefficient for each of these two variables to measure that magnitude.   
(ii) Changes to the aggregate market due to exogenous events. There are many other factors 
that influe nce the stock market beyond GDP, interest rates , and similar variables .  These 
include political events, extreme weather, earthquakes, tsunamis, etc. that tend to push the 
entire market up or down.  By using the S&P 500 as a proxy for the aggregate market and 
including a coefficient for the change in the S&P 500, we can compensate for  this effect , as 
well as the standard effects described in (i) .  We use the S&P 500 rather than the S&P 100 
since it is a broader measure of market activity.  
(iii) Changes due to firm -specific events.  Company -specific news will impact a particular stock 
beyond that of the overall market.  While there are many events that can change the 
perceived value of a company, the forecast of future earnings is the most important, as this 
                                                           
4 A detailed description of the valuation methodology is included in Section 3 .2.1.  ",2019-07-09T21:33:49Z,itre tse since t any eas tre tse by  any w sen
paper_qf_25.pdf,11,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","11 
 essentially determines the value of the company by classical measures .  Analysts are 
constantly updating their forecasts for the companies they cover, and for each stock there is 
(on average)  a couple of dozen analysts whose average forecast can easily b e found in 
financial news services on the internet5.  By including a coefficient in the regression for the 
earnings, we captu re this key factor for the firm -specific valuation.  
As such, the effects of the overall market , interest rates, and both  forecasted  earnings per share as well 
as forecasted  GDP are incorporated into the valuation variable6.  
3. Data Set and Variable Definitions  
3.1 Data Set  
The data set is a balanced panel of 279,565  daily observations corresponding to the date range 
December 28, 2004 through January 18 , 201 8.  The observations are for 85 of the S&P 100 companies  
(as of January 20, 2018 ) with 3,289  records per stock , i.e., all stock for which complete data was 
availab le in this set.  Given the  definitions of certain variables7, and the absence of data in some cases,  
the regressions are run on a balanced panel of 257,635  observations ( 3,031  records per stock covering 
the time period January 3, 2006  through January 17, 20 18).  These are large, highly liquid firms.  Indeed, 
the median firm has an average daily market capitalization of $96,039,310,000  and an average daily 
                                                           
5 While these forecasts do not change on a daily basis, a pproximately 27% of the daily obser vations in our data set 
contain revisions of earnings’ forecasts.  The fact that there is no change on the remaining 73% of the days does 
not diminish the importance of this information in the regressions.  
6 There exists a long history of practitioners’ us e of these variables in addition to the academic support noted in 
Section 2.2.  If any of these variables are not significant factors in the stock’s valuation, then the regressions will 
produce zero coefficients.  
7 Due to insufficient data, we exclude CHTR  from all regression analyses.   GOOG and FOXA are excluded, while 
GOOGL and FOX are included.   The resulting unbalanced data set consists of 283,153 daily observations 
corresponding to 98 companies.   To produce our primary analyses with a balanced data set , we also exclude the 
following two groupings of stocks.   First, as the Long Term Trend and Volatility variables require a year of data to 
calculate, we excluded the following stocks due to a lack of pricing data: ABBV, AMZN, CELG, FB, GM, KHC, KMI, 
MA, PC LN, PM, PYPL, SPG, and V (earliest records are post December 28, 2004).   Second, we excluded DWDP due 
to a lack of forecasted earnings per share data.   Refer to Table 3 for company names.  ",2019-07-09T21:33:49Z,analysts by as data set variable nitns data set t cember uary t uary giveuary uary tse ined w t tre se due t to rst lo term trend volatity cember second refer table
paper_qf_25.pdf,12,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","12 
 dollar  trading volum e of $577,130,000  (see Table 1).  Hence, this set comprises an unbiased collection of  
large capitalization and liquid stocks. The set is large enough to obtain broad results while still being 
within the computing capacity . Note that since we are examining daily (rather than yearly) changes , it is 
unlikely that neglecting those companies fo r which adequate records are available would  introduce  
survivorship issues.  This would likely be a more relevant issue if we were ex amining mutual fund returns 
over years.   In that case, one might have, for example, underperformance in the funds that have been 
terminated .  
<< Insert Table 1  here >>  
3.2 Variable Definitions  
This paper’s methodology, which is described in Section 4 , consists of running two -way fixed effects 
regressions with the following day’s return as the dependent variable.  That is, if today is day t, then the 
dependent variable is defined as  
𝑅𝑖,𝑡+1=𝑃𝑖,𝑡+1−𝑃𝑖,𝑡
𝑃𝑖,𝑡 
where 𝑃𝑖,𝑡 is the daily adjusted close price for firm 𝑖.  This price, which accounts for dividends, is 
obtained from Bloomberg’s “Total Return Index Gross Dividends” field.  The regressions’ independent 
variables are based upon this closing price and are described in the remainder of this section.  
3.2.1 Valuation  
The determination  of the Valuation variable is a two -step process.  Given a stock 𝑖, for each day  𝑡 an 
ordinary linear regression is performed for the time period [𝑡−188 ,𝑡] using the relative changes in the 
(Bloomberg) forecasted earnings per share for stock 𝑖 for the c urrent year , 𝐸𝑃𝑆𝑖,𝑡; the return of the S&P 
500 Index (SPX), 𝑀𝐾𝑇 𝑡; the yield on 10 -year U.S. Treasury bonds, 𝐼𝑁𝑇 𝑡; and the forecasted gross ",2019-07-09T21:33:49Z,table nce t note  iinsert table variable nitns  sethat  bloomberg total urinx gross divinds t valuatt valuatgivebloomberg inx treasury
paper_qf_25.pdf,13,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","13 
 domestic product for the current8 year, 𝐺𝐷𝑃 𝑡, as independent variables9. Note that we are using the 
relative changes in these quantities since it is the change in the expectation on which traders focus . The 
earnings per share variables reflect the forecasted earnings per share data as of day 𝑡.  Thus, earnings 
updates/revision s are reflected in this variable.  The 𝑀𝐾𝑇 𝑡 variable is a proxy for the market return.  The 
stock’s return, 𝑅𝑖,𝑡, for stock 𝑖 on day 𝑡 is utilized as the dependent variable.  The regression is thus of 
the form  
𝑅𝑖,𝑡~𝛼𝑖(0)+𝛼𝑖(1)𝐸𝑃𝑆𝑖,𝑡+𝛼𝑖(2)𝑀𝐾𝑇𝑡+𝛼𝑖(3)𝐼𝑁𝑇 𝑡+𝛼𝑖(4)𝐺𝐷𝑃 𝑡+𝜃𝑖,𝑡     (1) 
where  𝜃𝑖,𝑡 is an error term and the parameters 𝛼𝑖,𝑡(𝑗) correspond to stock 𝑖 on day 𝑡 for independent 
variable 𝑗. Further analysis of the error terms, which are close to Gaussian between the 1% an d 99% 
levels, can be found in the Appendix.  The nine -month period was chosen as on optimal time period as a 
short time period does not provide enough data for a reliable set of regression coefficients. Examining 
data that is much older can provide data fro m a different era. For example, in 2009, the data from the 
crisis may be less relevant than the post -crisis data.  
These coe fficients for this rolling nine -month period are used to determine a valuation for time 𝑡 
using the linear equation:  
𝑉𝑎𝑙 𝑖,𝑡=(𝛼̂𝑖,𝑡(0)+𝛼̂𝑖,𝑡(1)𝐸𝑃𝑆𝑖,𝑡+𝛼̂𝑖,𝑡(2)𝑀𝐾𝑇 𝑡+𝛼̂𝑖,𝑡(3)𝐼𝑁𝑇 𝑡+𝛼̂𝑖,𝑡(4)𝐺𝐷𝑃 𝑡+1)∗𝑃𝑖,𝑡−1                  (2) 
                                                           
8 Although the GDP estimates are less volatile toward the end of the  year, changes in forecasts for the last quarter 
continue to dominate expectations relating to growth or slowdown. The role of GDP changes is minor in any case 
as the outlook for S&P earnings is quickly incorporated into the MKT variable.  
9 The yield on the 10-year  U.S. Treasury bill was taken from Bloomberg (U.S. Generic Government 10 Year Yield).  
Forecasted GDP data for the current calendar year was obtained from The Economist  magazine.  ",2019-07-09T21:33:49Z,note t  t t t furtr  t examini for tse although t t treasury bloomberg generic government year yield forecast t economist
paper_qf_25.pdf,14,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","14 
 where 𝛼̂𝑖,𝑡(𝑗) represents the estimated regr ession coefficient from equation (1).  Thus, equation (2) 
provides an estimate of the value of stock 𝑖 on day 𝑡 based on its relationship with the market index and 
its earnings outlook.10 Note that the information used at time 𝑡 involves only the days pr ior. 
The Valuation variable , which is the relative excess value  (and measures the extent of 
undervaluation) is defined via the expression  
𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛 𝑖,𝑡=𝑉𝑎𝑙 𝑖,𝑡−𝑃𝑖,𝑡
𝑉𝑎𝑙 𝑖,𝑡 
This affords the opportunity to test the hypothesis that the da ily return is greater when this 
independent variable is positive.  This is consistent with the earlier  regression studies cited above .  
The nature of the marketplace is that traders are always trying to obtain the most recent 
information about the present and future in order to gain an edge in making profits. For example, the 
earnings per share for the previous year is already incorporated into the market, and , thus  is not as 
important as the earnings report that will be issued in the future.  Thus, traders  are focused on the 
prospects for future earnings and any signs of changes.  In utilizing information about the earnings for 
each company, the change in the prevailing forecasts on the day of the trade is considered.  It would 
appear that taking  a weighted average of the forecasts for the current year and the following year could 
yield a more precise estimate. In other words, on day d of the 252 trading days, the w eight given to the 
current year would be (252 – d)/252, with the remainder of the w eighting assigned to the following 
year. We re -ran regression models 1V, 2, 3, and 4 with the valuation variable calculated with weighted 
averages for EPS and GDP. The results are qualitatively similar to those reported in Table 2. This is not 
surprising a s the two valuation variables are highly correlated (with the correlation computed on a per 
firm basis). Indeed, the median correlation is 0.99 and the minimum correlation is 0.94 . 
                                                           
10 If a coefficient is not statistically significant (p-value >= 0.10), then it is set to zero in equation (2).  ",2019-07-09T21:33:49Z, note t valuat  t for  iit i t table  ined 
paper_qf_25.pdf,15,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","15 
 3.2.2 Price Trend  
Several prior empirical studies have noted the existence of a trend in asset prices.  See, for example, 
Carhart (1997), Jegadeesh and Titman (1993), Moskowitz and Grinblatt (1999), and George and Hwang 
(2004).  In addition, several theoretical models,  starting with Cagina lp and Ermen trout ( 1990) using 
differenti al equations  have been developed to model price behavior when traders are motivated by 
both trend and valuation. In addition , Barberis, Shleifer, and Vishny (1993), Daniel, Hirshleifer, and 
Subrahmanyam (1998), and Hong and Stein (1999), have developed mod els to relate intermediate -term 
trend coupled with long term reversals . 
Moreover, studies have also identified phenomena such as underreaction and overreaction in 
asset prices. Bremer and Sweeney (1991),  Madura and Richie (2004) , Sturm (2003)  define underr eaction 
(overreaction) as positive (negative) returns following large positive price movements and negative 
(positive) returns following large negative price movements.  
Practitioners also recognize the impact of the recent trend in price on asset prices t hrough 
sayings such as “The trend is your friend.”  Indeed, a survey by Menkhoff (2010) suggests that 
practitioners frequently use technical analysis ( including, in particular, trend following) for trading 
horizons of weeks. While the efficient market hypo thesis and technical analysis seem to be in conflict, 
Sturm (2013) concludes that the latter “ …attempts to measure changes in these beliefs to predict stock 
prices and should have value given the  evidence in behavioral finance… ”  In other words, the proces s of 
price discovery as new information enters the market provides an opportunity for technical analysis to 
be useful.  Moreover, traders may be inclined to act on deviations from the recent trend rather than the 
trend itself11.  For example, suppose return s have been trending upward at 0.5% per day over the past 
two weeks.  If today’s relative change in price is consistent with that trend, then traders may not 
                                                           
11 Various technical analysis methods focus on the differences of moving averages that, in effect, are a measure of 
a smoothed out second derivative, i.e., the change in the trend over a particular time period.  ",2019-07-09T21:33:49Z,price trend sevl  car hart je ma it mamos  with griblast george ha ica gina er meibarber is sh leaf er vis ny daniel harsh leaf er sub rahmayam ho steio seney ma dra richie storm praitners t ined mekh off w storm iofor  varus
paper_qf_25.pdf,16,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","16 
 respond.  However, if today’s relative change in price surges to, say, 1%, then traders may act.  In this 
case traders are responding to the “acceleration” in returns rather than the “derivative” of returns.   
Given these two factors, i.e. that stock prices tend to exhibit a trend and traders may respond to 
deviations from the recent trend more so than  the trend itself, the trend variable is defined12 as follows:  
𝑇𝑟𝑒𝑛𝑑 𝑖,𝑡=𝑃𝑖,𝑡−𝑃𝑖,𝑡−1
𝑃𝑖,𝑡−1−1
0.58195∑𝑒−𝑘10
𝑘=1𝑃𝑖,𝑡−𝑘−𝑃𝑖,𝑡−𝑘−1
𝑃𝑖,𝑡−𝑘−1. 
This Price Trend variable for firm 𝑖 is defined by subtracting an exponentially weighted average of the 
past ten days' relative changes in price from the current day's relative price change.  This modeling 
approach is similar to the manner in which the valuation variable is defined in prior studies13. 
A key aspect of this paper’s analysis involves the nonlinear relationship between the recent 
trend and the following day's return.  This is achieved by including the square and cube of this trend 
variable in the regressions described in Section 4 . 
3.2.3 Volatility  
In classical finance, risk is often associated with the standard deviation of returns.  In order to 
understand the impact of volatility without the trend effect, the Volatility variable for firm 𝑖 is defined as 
the standard deviation of  returns over the past 𝑋+1 days, i.e.,  
𝑉𝑜𝑙𝑎𝑡𝑖𝑙𝑖𝑡𝑦 𝑖,𝑡={1
𝑋∑[𝑅𝑖,𝑘−𝑀𝑒𝑎𝑛 (𝑅𝑖,[𝑡−𝑋,𝑡])]2𝑡−𝑋
𝑘=𝑡}12⁄
 
                                                           
12 The fraction, 10.58195⁄ , is a normalization factor equal to the inverse of the sum ∑ 𝑒−𝑘 10
𝑘=1 . 
13 The price trend definition is reminiscent of the moving averages utilized in prior works (see, for example, N eely, 
et al., 2014 ), though these m oving average definitions typically assign equal weight to each observation . ",2019-07-09T21:33:49Z,ive price trend   sevolatity iivolatity t t
paper_qf_25.pdf,17,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","17 
 where 𝑋=10 for short -term volatility and 𝑋=251 for long -term volatility.  Previous studies on closed -
end (Caginalp and DeSan tis, 2011a, 2011b)  and exchange -traded (Caginalp et al. , 2014) funds have 
shown an ambiguous role for Volatility.  With respect to closed -end funds Long Term Volatility tended to 
depress returns, while an increase in Short -Term Volatility boosted closed -end fund returns.  For ETFs, 
both long - and shor t-term volatility increased returns (though the longer term volatility was only 
marginally significant).  These variables are included in the current study to examine their effect on 
stock returns.  
3.2.4 Long Term Trend  
Several  studies (e.g. , Poterba and Summers, 1988 ) have found that returns tend to regress to the mean 
or experience price reversals over longer time periods.  Theoretical justification has been shown in Leung 
and Wang 2018 .  We test for the existence  of this pheno menon.  Indeed, to test this hypothesis an 
annual trend variable is defined as the slope of the straight line fitted to returns over the past year 
scaled to annual units.  That is, the variable, 𝐿𝑜𝑛𝑔  𝑇𝑒𝑟𝑚  𝑇𝑟𝑒𝑛𝑑 𝑖,𝑡, is the slope of the straigh t line fitted 
to (𝑃𝑖,𝑡−𝑃𝑖,𝑡−1)𝑃𝑖,𝑡−1 ⁄  multiplied by 25 1, the number of trading days per year. Hence this is an average 
of the price changed during the previous 251 trading days.  
3.2.5 Resistance  
Consider the scenario in which individuals experience regret upon failing to capitalize by selling at the 
recent high price.  These individuals might be inclined to sell if the price recovers to again approach this 
recent value that has become ""anchored"" i n their minds (in analogy with Tversky and Kahneman, 1974).  
This selling exerts a downward pressure on the price.  If this downward force keeps the price from 
attaining its recent high, then the price has encountered resistance.  That is, the notion that an increase 
in price tends to slow or even retreat as the price approaches a recent high price is termed Resistance.  
Studies by George and Hwang (2004) and Sturm (2008) found evidence of Resistance in stock prices on ",2019-07-09T21:33:49Z,us agaialp  saagaialp volatity with lo term volatity short term volatity for fs tse lo term trend sevl  er ba suers toical lu wa  ined that nce resistance consir tse o kane ma  that resistance studeorge ha storm resistance
paper_qf_25.pdf,18,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","18 
 monthly and annual time scales by form ing portfolios based on the difference between the current 
stock price and a recent high price.  
The binary Resistance variable is defined as follows :  
(i) Let 𝐻𝑖,𝑡≔𝑚𝑎𝑥 [𝑃𝑖,𝑠] 𝑓𝑜𝑟 𝑠 𝑖𝑛 [𝑡−63,𝑡−16] and  
(ii) Set the Resistance variable if the following hold  
(1) 𝑓𝑜𝑟 𝑠 𝑖𝑛 [𝑡−15,𝑡−10],𝑃𝑖,𝑠≤0.85𝐻𝑖,𝑡 and 
(2) 0.85𝐻𝑖,𝑡≤𝑃𝑖,𝑡≤𝐻𝑖,𝑡. 
That is, Resistance is set for day 𝑡 if the recent price has been less than 85%  of the recent quarterly high 
price and the current price is between 85%  of this  recent high price and the high price.  Approximately 
1% of the records in the data set ( 2,139 out of 257,635 ) satisfy these  criteria.  
3.2.6 Volume  
Market participants have long recognized the significance of trading volume in financial markets.  As 
noted in Karpoff (1987), “It takes volume to make prices move.”  Numerous theoretical (e.g., Blume, 
Easley, and O’Hara, 1994; Harris and Raviv, 1993; Wang 1994) papers have developed models in which 
volume and the magnitude of price changes are positively linked .  Karpoff (1987) provides a survey of 
papers and provides empirical evidence that volume is positively related to the magnitude of price 
changes and to the price change itself (the latter holding only in equity markets).  Antoniou et al. , (1997) 
finds evi dence suggesting that volume might help to predict returns.  
Given the extensive prior literature and the practitioners’ viewpoint, the recent trend in daily 
dollar trading volume for firm 𝑖  
𝑉𝑜𝑙𝑢𝑚𝑒 𝑖,𝑡=1
0.58195∑𝑇𝑢𝑟𝑛𝑜𝑣𝑒𝑟 𝑖,𝑡−𝑘+1−𝑇𝑢𝑟𝑛𝑜𝑣𝑒𝑟 𝑖,𝑡−𝑘
𝑇𝑢𝑟𝑛𝑜𝑣𝑒𝑟 𝑖,𝑡−𝑘𝑒−𝑘10
𝑘=1 ",2019-07-09T21:33:49Z,t resistance  set resistance that resistance approximately volume market as harp off it numerous   hara harris iv wa harp off antogiven
paper_qf_25.pdf,19,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","19 
 is utilized as the volume variable.  𝑇𝑢𝑟𝑛𝑜𝑣𝑒𝑟 𝑖,𝑡 corresponds to Bloomberg’s dollar trading volume for 
firm 𝑖 on day 𝑡.  This variable is defined as the sum over all transactions for day  𝑡 of the  product of 
shares traded multiplied by the trading price for each transaction.  Dollar trading volume has been 
utilized in prior studies as a proxy for liquidity (see, for example, Chordia et al. , 2001).  
4. Methodology  
The da ta set consists of 85 time series - one for each company in the study.  These time series are 
appended to one another and grouped by stock.  Thus, the data set may be characterized as a time -
series -cross -sectional data set or a panel data set (though, it h as a large number of both cross -sections, 
85 firms, and time periods, 3,031  days).  Ass et pricing models are typically tested with time series or 
cross -sectional methods (Goyal, 2012).  For example, the Fama -Macbeth regressions account for 
contemporaneous correlations but not serial correlation (over time) in the error terms.  Bali et al.  (2008) 
utilize a fixed -effect regression with the firm as the sole fixed effect to consider the predictive 
relationship between earnings and stock returns at the firm -level.   
Both firm heterogeneity and contemporaneous correlations are accounted for by utilizing a two -
way fixed effects model with both firm and time fixed effects14.  An advantage of the fixed -effects model 
is that it mitigates the impact of omitted variable bias.  In addition, the unobserved firm -specific effects 
may be correlated with the independent variables.  Although use of this model restricts any inferences 
to the subset of stocks considered (S&P 100) in contrast to a random -effects model, it should be  noted 
that as the number of time periods per firm increases, the random effects estimator approaches the 
fixed effects estimator (Hsiao, 1996).  As the data set co ntains 3,031  time observati ons per firm, these 
estimators should be similar.  The applicabil ity of this method, particularly for contemporaneous 
correlations, is described in Caginalp et al.  (2014).   
                                                           
14 This is accomplished by utilizing the FIXTWO option of the SAS PANEL procedure.  ",2019-07-09T21:33:49Z,bloomberg  dollar chord methodology t tse  as goa for fam macbeth bali both aialthough asia as t agaialp 
paper_qf_25.pdf,20,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","20 
 Moreover, as noted in Section 2 , most asset pricing studies account for the three Fama -French 
factors (and frequently the momentum factor as well).   As these factors vary over time but not across 
firms, the model is not enhanced by their inclusion.  However, use of the time fixed effect controls for 
these (and any other firm invariant) factors.  Thus, this paper’s results are robust to any time and/o r firm 
invariant factors.  
The primary goal of this study is to analyze and better understand the factors that underlie the 
dynamics of stock prices (i.e., the daily changes in price).  This paper considers the effect of the variables 
discussed in Section 3  defined at time 𝑡 on the following day's return, 𝑅𝑖,𝑡+1.  To that end, we consider 
regressions of the form  
𝑅𝑖,𝑡+1=𝛼0+𝛼1𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛 𝑖,𝑡+𝛼2𝑇𝑟𝑒𝑛𝑑 𝑖,𝑡+⋯+𝑢𝑖,𝑡 
where  
𝑢𝑖,𝑡=𝜇𝑖+𝛾𝑡+𝜀𝑖,𝑡 
with 𝜇𝑖 representing the firm -specific e ffect, 𝛾𝑡 the time -specific effect, and 𝜀𝑖,𝑡 the idiosyncratic error 
term .15   
To minimize the impact of outliers in the data, all independent variables (with the exception of 
the Resistance variable) are winsorized at the 99th percentile16.  As stock p rices commonly exhibit ""fat 
tails,"" this helps to minimize the im pact of a few extreme observation s. Without winsorizing the data, it 
would be possible for a few values to distort the motivations that dominate trading most of the time. A 
future topic of st udy would be to focus on these extreme events that often occur in the midst  of a crisis.  
                                                           
15 We report the R -square measure of Theil (1961)  for all regressions .  The R -square values are fa irly large, which is 
not surprising given the fixed effects for firm and time.  However, what is most important to our analyses is the 
significance of the coefficients of the primary explanatory variables, e.g. Valuation, Price Trend, etc. , rather than 
goodness of fit of the model.  
16 That is, all outliers above (below) the 99th (1st) percentile are set to the value corresponding to the 99th (1st) 
percentile.  ",2019-07-09T21:33:49Z,osefam frenas  t  seto to resistance as without  t  t valuatprice trend that
paper_qf_25.pdf,21,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","21 
 For the most comprehensive model, the results without winsorizing (reported in Table 4 in the 
Appendix ) retain  the vast majority of the coefficients  and the basic features (i.e., sign and statistical 
significance) with different coefficients, of course . 
Values for different variables may differ by orders of magnitude.  Consequently, after 
winsorizing the data , each independent variable (with the exception  of the indicator Resistance 
variable17) is standardized by firm.  This is accomplished by first subtracting the firm's mean value and 
then dividing by the standard deviation.  This facilitates comparisons of regression coefficients, which 
reflect a variabl e's effect on the following day's return, by utilizing the standard deviation as a natural 
scale of measurement.  For example, consideration of the coefficients for Long Term Volatility ( 0.261 ) 
and Long Term Trend ( -0.290 ) from Model 4 in Table 2  show s tha t a one standard deviation increase in 
the Long Term Trend is large enough to counteract the effect of a one standard deviation increase in the 
Long Term Volatility variable on the following day’s return.   
We utilize robust standard errors to account for heteroscedasticity18.  We also cluster the 
standard errors to account for the fact that the regression residuals might be correlated by firm and/or 
time (Petersen, 2009; Thompson, 2011).  Thompson (2011) notes that if the firm and time dimension s 
are not equivalent, then it is preferable to cluster along the dimension with fewest observations.  As 
each of the 85 firms in this study has 3,031  daily observa tions, clustering is performed along the firm 
dimension to account for within -firm correlatio ns. 
                                                           
17 Resistance is a highly skewed binary vari able.  That is, only 2,139 out of 257,635 observations me et the 
Resistance criteria.   Thus, standardization by a single standard deviation may not be sufficient to obtain regression 
coefficients comparable to those of the other variables (Gelman, 2008).  Therefore this variable is left  
unstandardized.  As such, note that the regression coefficient of this variable should be interpreted with care when 
comparing to the coefficients of other variables.  
18 There exist several heteroscedasticity -consistent covariance matrix estimators.  Davidson and MacKinnon (19 93) 
describe four such alternatives and infer that option 3 may perform best.  As such, we employ this estimator which 
utilizes the squared residuals divided by (1+ℎ𝑡)2 to estimate the diagonal entries of the error covariance matrix.  
The variable, ℎ𝑡, is defined as 𝑋𝑡(𝑋𝑇𝑋)−1𝑋𝑡𝑇 where 𝑋 is the regressor matrix with 𝑡th row denoted by 𝑋𝑡. ",2019-07-09T21:33:49Z,for table  valuconsequently resistance   for lo term volatity lo term trend mtable lo term trend lo term volatity   petersothompsothompsoas resistance that resistance  gel matrefore as tre didsomac d oas t
paper_qf_25.pdf,22,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","22 
 5. Regression Results  
To ensure our valuation variable is appropriately defined, we first regress the dependent variable, return 
on day 𝑡+1, against the change in valuation on day 𝑡, using the definition  of  𝑅𝑖,𝑡+1 in Section 3.2.   
Intuitively, we expect the coefficient of the valuation variable to be positive .  To confirm this we 
consider Model 1V  
𝑅𝑖,𝑡+1=𝛼0+𝛼1𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛 𝑖,𝑡+𝑢𝑖,𝑡 
where 𝑢𝑖,𝑡 is as defined in Section 4 .  The results of this model are reported in Table 2 .  Consistent with 
our expectation the coefficient of the valuation variable is positive . 
Since our primary interest is the nature of the relationship between the return and the recent 
trend in price, we next consider Model 1 T 
𝑅𝑖,𝑡+1=𝛼0+𝛼1𝑇𝑟𝑒𝑛𝑑𝑖,𝑡+𝑢𝑖,𝑡. 
 The coefficient of the sole independent variable, Trend , is positive as one might intuitively expect, and 
statistically significant.   
 << Insert Table 2  here >>  
 As described in Section 2 .2, it is hypothesized that the inclusion of an appropriately defined 
valuation variable will account for much of the “noise” in returns and allow the effect of other variables, 
for example the recent trend, to be noted and measured.  To this end, we combine  Models 1V and 1T 
and consider Model 2  
𝑅𝑖,𝑡+1=𝛼0+𝛼1𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛 𝑖,𝑡+𝛼2𝑇𝑟𝑒𝑛𝑑 𝑖,𝑡+𝑢𝑖,𝑡. 
The results of Model 2 are given in Table 2 .  The coefficient of the valuation variable is both statistically 
significant and positive.  Consistent with the results from Model  1T, the Price Trend’s coefficient is still ",2019-07-09T21:33:49Z,regressresults to seintuitively to mset table consistent since mt trend insert table as seto mols mt mtable t consistent mprice trend
paper_qf_25.pdf,23,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","23 
 positive  and statistically significant.   These results also suggest  that a small increase (decrease) in the 
price trend corresponds, on average, to a small increase (decrease) in the fo llowing day’s return.  
However, it remains to be determined if this effect is truly linear19. 
We next consider  the nature of the relationship between the following day’s return  and the 
valuation and  trend  variables .  As previously noted,  using a purely line ar regression would likely mask 
important effects that can only be observed with nonlinear terms .  Indeed, if a straight line is fit to the 
curve 𝑦=𝑥3, then a coefficient of zero  would be obtained  for the linear term.  While this coefficient 
may be signi ficant, the linear approximation is certainly inappropriate  and misleading .  Thus, we seek to 
determine whether the relationship between trend and return is nonlinear, similar to those of closed -
end and exchanged -traded funds.  To this end Model 3 builds upon Model 2 by incorporating the square 
and cube of the trend and valuation variables along with their higher order cross terms (up to third 
order).  Model 3 has the form  
𝑅𝑖,𝑡+1=𝛼0+𝛼1𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛 𝑖,𝑡+𝛼2𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛𝑖,𝑡2+𝛼3𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛𝑖,𝑡3+𝛼4𝑇𝑟𝑒𝑛𝑑 𝑖,𝑡+𝛼5𝑇𝑟𝑒𝑛𝑑𝑖,𝑡2
+𝛼6𝑇𝑟𝑒𝑛𝑑𝑖,𝑡3+𝛼7𝑇𝑟𝑒𝑛𝑑 𝑖,𝑡∗𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛 𝑖,𝑡+𝛼8𝑇𝑟𝑒𝑛𝑑𝑖,𝑡2∗𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛 𝑖,𝑡
+𝛼9𝑇𝑟𝑒𝑛𝑑 𝑖,𝑡∗𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛𝑖,𝑡2+𝑢𝑖,𝑡. 
Using the quadratic and cubic terms p rovides enough degrees of freedom to describe the 
general shape of the dependent variable as a function of the independent.  This is analogous to using the 
first three terms of a Taylor series for a function.   
The results of Model 3, which are included in Table 2 , indicate that the coefficients of the 
Valuation and Trend variables remain significant and positive.  The coefficient of the quadratic Valuation 
                                                           
19 Augmenting Model 2 with an interaction term (Valuation*Price Trend) has a negligible effect on the significance, 
magnitude, and sign of the coefficients for the  Valuation and Price Trend variables.  Indeed, the Valuation 
coefficient is 0.000399 with a p -Value of 0.0004, while the Price Trend coefficient is 0.000577 with a p -Value less 
than 0.0001.  The interaction term’s coefficient, -0.00009, is not significant with a p -value of 0.2572.  ",2019-07-09T21:33:49Z,tse  as ined w  to mmmusi  taylor t mtable valuattrend t valuataugmenti mvaluatprice trend valuatprice trend ined valuatvalue price trend value t
paper_qf_25.pdf,24,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","24 
 term is also significant and positive suggesting that large changes in the daily trend correspond to 
positive returns the following day.  Although the coefficient of the quadratic Trend variable is not 
significant, the coefficient of the cubic Trend variable is both significant and negative.  This suggests that 
large positive (negative) changes in the daily  trend tend to lead to lower (higher) returns.  Indeed, 
consider Figure 1, which displays the return as a function of both the Valuation and the Trend.  That is, 
using the statistically significant coefficients from Model 3, the following function  
𝑅𝑡+1(𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛 𝑡,𝑇𝑟𝑒𝑛𝑑 𝑡)
=0.615 𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛 𝑡+0.112 𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛 𝑡2+0.721 𝑇𝑟𝑒𝑛𝑑 𝑡−0.090 𝑇𝑟𝑒𝑛𝑑 𝑡3
+0.151 𝑇𝑟𝑒𝑛𝑑 𝑡∗𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛 𝑡2. 
is plotted.  
<< Insert Figure 1 here >>  
 Restricting focus to the scenario in which the change in Trend is assumed to be zero20, the cross -
section of Figure 1 displayed in Figure 2 is obtained.  This curve represents the following day’s return, 
𝑅𝑡+1, as a function of the Valuation variable.  This curve is fairly linear for changes in Valuati on greater 
than negative one standard deviation and somewhat flat for smaller changes.  Consistent with intuitive 
expectations, returns tend to increase with increasing valuation.  Though, larger negative changes in 
valuation tend to correspond to similar changes in return21.  
<< Insert Figure 2 here >>  
                                                           
20 As the trend and valuation variables are standardized, considering a trend or valuation of zero is equivalent to 
considering the variable assumes its average value.  Figures 2 and 3 are included as representative graphical 
depi ctions of the relationship between the return and valuation or trend.  
21 For clarity all figures are plotted without an intercept as this term would not change the shape of the curve 
(surface), but would merely shift it vertically .  Thus, the focus is on lo cal maximum and minimum values as well as 
the return at different values of the independent variables.  For example, in Figure 3 a return, 𝑅𝑡+1, of 0.25 is 
achieved at Price Trend values of -2.99, 0.352,  and 2.64 , respectively.  ",2019-07-09T21:33:49Z,although trend trend  ined  valuattrend that minsert  restrii trend    valuat val at consistent though insert  as s for  for  price trend
paper_qf_25.pdf,25,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","25 
 As noted above, both academics and practitioners concede the recent trend in price plays a key 
role in asset returns.  Consider the Price Trend terms in Model 3 of Table 2 .  Note the coefficient of the 
linear  Price Trend term is statistically significant and positive, consistent with the intuition that rising 
prices tend to continue to rise.  The cubic trend term’s negative coefficient suggests that larger (smaller) 
price trends lead to smaller (larger) return s.  Indeed, taking a cross -section of Figure 1 with the 
Valuation variable set to zero yields the cubic curve in Figure 3, which depicts 𝑅𝑡+1 as a function of the 
Trend variable.   
This curve has a local minimum of -0.785  at a trend value of -1.634  stand ard deviations and 
attains a local maximum of 0.785  at a trend value of 1.634 .  Thus, as trend values increase from -1.634  
to 1.634  standard deviations, the following day’s return also increases.  However, once the trend 
increases [decreases] beyond 1.634  [-1.634 ] standard deviations, then the return decreases [increases].  
Thus, the data exhibit evidence supporting both underreaction and overreaction to price changes.  Here 
underreaction is defined as a continuation of the trend and overreaction as a rever sal of the trend.  
<< Insert Figure 3 here >>  
Earlier studies on different time periods revealed an asymmetry in the local minimum and 
maximum, suggesting that traders were more eager to take profits than buy on dips  (Caginalp  et al., 
2014) . However, Figure 3 is quite symmetric in this regard. The symmetry may be a consequence of the 
long time period of this study that includes both bull and bear markets . The level of symmetry may be 
an indication of general investor caution or optimism duri ng a particular period.  
As noted in Section 3 , numerous variables have been hypothesized to impact returns.  Model 4 
augments Model 3 with a subset of these factors.  Model 4 has the form  ",2019-07-09T21:33:49Z,as consir price trend mtable note price trend t ined  valuat trend    re insert  earlier agaialp  t t as semmmol
paper_qf_25.pdf,26,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","26 
 𝑅𝑖,𝑡+1=𝛼0+𝛼1𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛 𝑖,𝑡+𝛼2𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛𝑖,𝑡2+𝛼3𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛𝑖,𝑡3+𝛼4𝑇𝑟𝑒𝑛𝑑 𝑖,𝑡+𝛼5𝑇𝑟𝑒𝑛𝑑𝑖,𝑡2
+𝛼6𝑇𝑟𝑒𝑛𝑑𝑖,𝑡3+𝛼7𝑇𝑟𝑒𝑛𝑑 𝑖,𝑡∗𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛 𝑖,𝑡+𝛼8𝑇𝑟𝑒𝑛𝑑𝑖,𝑡2∗𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛 𝑖,𝑡
+𝛼9𝑇𝑟𝑒𝑛𝑑 𝑖,𝑡∗𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛𝑖,𝑡2+𝛼10𝑆ℎ𝑜𝑟𝑡 𝑇𝑒𝑟𝑚  𝑉𝑜𝑙𝑎𝑡𝑖𝑙 𝑖𝑡𝑦 𝑖,𝑡
+𝛼11𝐿𝑜𝑛𝑔  𝑇𝑒𝑟𝑚  𝑉𝑜𝑙𝑎𝑡𝑖𝑙𝑖𝑡𝑦 𝑖,𝑡+𝛼12𝐿𝑜𝑛𝑔  𝑇𝑒𝑟𝑚  𝑇𝑟𝑒𝑛𝑑 𝑖,𝑡+𝛼13𝑉𝑜𝑙𝑢𝑚𝑒 𝑖,𝑡
+𝛼14𝑅𝑒𝑠𝑖𝑠𝑡𝑎𝑛𝑐𝑒 𝑖,𝑡+𝑢𝑖,𝑡. 
Whether volatility in returns, longer term price trend, dollar trading volume, and proximity to a  recent 
high price have an effect on the return is tested.  While the Long Term Volatility variable exhibit s a 
statistically significant, positive coefficient , the Short Term Volatility variabl e’s coefficient is not 
significant .  This is not consistent wit h the results on exchange traded funds in Caginalp et al.  (2014) in 
which the Short Term Volatility variable is significant with a positive coefficient, and the Long Term 
Volatility is not significant.  The discrepancy may reflect the greater use of ETFs for rapid trading and re -
adjustment of positions.  
Mean rev ersion in stock prices is a commonly accepted idea.  Consistent with this notion, the 
Long Term Trend variable, which is modeled to account for this phenomenon, is statistically significant 
with a negative coefficient.  This suggests that if the trend over  the past year is positive (negative), then 
(holding all other variables constant at zero) the following day’s return will be negative (positive).  
The coefficient of the Volume variable is not statistically different from zero.   Inclusion of the 
Volume var iable thus confirms that the observed effects are not artifacts of changes in volume.  
Lastly, strong support for the existence of resistance is exhibited in this data set.  Indeed, the 
regression coefficient for the Resistance variable is both statisticall y significant and negative suggesting 
that a downward pressure is exerted on prices as the y approach a recent high price . 
6. Conclusion  ",2019-07-09T21:33:49Z,r w lo term volatity short term volatity  agaialp short term volatity lo term volatity t fs meaconsistent lo term trend  t volume inusvolume lastly ined resistance conusn
paper_qf_25.pdf,27,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","27 
 This paper utilizes  a methodology whereby the effects of price trend, resistance, volume changes, 
volatility and other v ariables on return are determined together.  Without adjust ing for valuation  (thus 
using  the raw data of individual S&P 100 stocks ) most of  these non -valuation effects would be masked 
by the noise inherent in the changes in valuation.  
Another  important aspe ct of this methodology that is neglected in many studies is that 
nonlinear effects are often invisible using linear methods, or they appear as small terms  possibly  with 
the opposite (or unexpected) sign.  In particular, raw data of major stocks usually sho ws a nearly 
negligible trend effect.  However, the combination of adjusting for changes in valuation and the 
examination of nonlinear terms establishes a nonlinear trend term with strong statistical significance 
and a magnitude that is comparable to the co efficient of valuation.  This suggests that for short term 
movements, the trend has an influence that is comparable to that due to changes in valuation.  
The results show that the return is a cubic function of the trend.  In particular, the return is 
positively influenced by the t rend, so long as the trend is one which is small as measured by standard 
deviation. Thus an uptrend that is observed sufficien tly often that it is within 1.634  standard deviations 
tends to have a positive influence ( see Figure 3), but beyond this point any increase in trend tends to 
reduce returns.  This effect can be attributed to underreaction, since it indicates investors are slow to 
react to a positive development.  For sufficiently large trends, there is a negati ve effect on return, 
providing evidence of overreaction.  Thus this methodology addresses these two key issues related to 
behavioral finance and provides answers for questions that critics have posed:  (1) Can one establish 
over - and underreaction?  (2) Ho w does one distinguish between over - and underreaction?  The 
methodology indicates that traders are usually eager to observe an uptrend and to buy into it, but are 
also eager to take profits when the uptrend is significantly stronger than the typical level . ",2019-07-09T21:33:49Z, without anotr i t i   for  caho t
paper_qf_25.pdf,28,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","28 
 Another i ssue is whether this cubic is symmetric or asymmetric. Symmetry indicates a balance 
between buying on dips or taking profits, and is reflected in  Figure 3 . Asymmetry in the form of a 
negative root that is larger in magnitude than the positive root would indicate that traders are less eager 
to buy on dips than they are to take profits during that time period . This behavior could be an indication 
of a cautiou s or pessimistic outlook. The opposite situation would indicate  a euphoric market.  By 
examining different groups of stocks and eras, the shape of the cubic polynomial  can be used  to display 
motivations of traders and the temporal evolution of these motivat ions. 
An important idea in behavioral finance involves anchoring, or using a particular number related 
to the trader’s  experience as a benchmark.  Traders often utilize this concept in a practical context, and 
feel that a stock nearing its quarterly high, for example, will encounter resistance that will slow or 
terminate an uptrend.  The methodology proposed and utilized in this paper shows that this is indeed 
the case.  
This methodology facilitates the  examin ation of  a broad range of questions in a quantita tive 
manner, and the independent variables are not limited to those considered  above . Whether the 
hypothesized effect is fundamental or behavioral, it is possible to include it among the variables so long 
as it can be quantified.   Hence, any theoretical mo del can be examined in terms of the predictions it 
renders, allowing researchers to accept, modify or reject facets of the model.  A paradigm for behavioral 
finance thus involves an expanding and verifiable set of motivations that can be quantified and 
integrated into a model of market dynamics, much as physics has evolved with a growing list of 
experimentally established forms of energy.  
An interesting application of this would be to examine different time periods e.g., the crisis 
period of 2008 -2009, to d etermine how the cubic function formed by the regression coefficients ",2019-07-09T21:33:49Z,anotr syetry  asyetry  t by atrars t  r nce an
paper_qf_25.pdf,29,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","29 
 changes. For example, are traders more eager to lock in profits, or are they more reluctant to buy on 
dips. Analysis of this type can provide a useful tool for trading and risk analysis.  
The ideas can also be implemented  in a different direction  back -tested for trading purposes. At 
each time t, the data up until t may be used in order to provide the optimal coefficients up to that time 
period. These coefficients can then  be used to  render a (out -of-sample) forecast for time t+1. The profit 
can then be  determine d based o n these forecasts. Other tests such as the fraction of time in which the 
forecast has the right direction (i.e., positive or negative) can also be performed. Neverthe less, the 
purpose of this paper is not to develop a profitable trading strategy but rather to understand the 
motivations of traders for a particular class of assets during a particular time period.   
Implementation of such a trading application would be imp roved by focusing on a specific group 
of stocks for which valuation can be estimated more precisely. For example, investors in companies 
manufacturing high -tech consumer goods react quickly to sales data and announcements of new 
products. Thus , quantifying  and refining  changes in valuation for a smaller set of stocks may lead to  
useful forecast ing, in addition to an understanding of trader motivations.  
 
Acknowledgment s:  The author s are grateful to  Professor Akin Sayrak for valuable comments.   A careful 
reading and numerous useful suggestions by three anonymous referees are also greatly appreciated.  
 
References  
Ang, A. & Bekaert, G. (2007),  ""Stock return predictability: Is it there?"", Review of Financial Studies , Vol. 
20 No. 3, 651 -707.  ",2019-07-09T21:33:49Z,for analysis t at tse t otr net implementatfor  ackledgment t professor akisay rak referenca be ka ert stock is review nancial studivno
paper_qf_25.pdf,30,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","30 
 Antoniou, A., Ergul, N., Holmes, P. & Priestley, R. (1997),  ""Technical analysis, trading volume and market 
efficiency: evidence from an emerging market"", Applied Financial Economics , Vol. 7 No. 4, 361 -365.  
Bali, T. G., Demirtas, K. O. & Tehranian, H. (2008),""Aggregate earnings, firm -level earnings, and expected 
stock returns"", Journal of Financial and Quantitative Analysis , Vol. 43 No. 3 , 657 -684.  
Barberis, N., Shleifer, A. and Vishny, R. (1998),""A model of investor sentiment"", Journal o f Financial 
Economics , Vol. 49 No. 3, 307 -343.  
Black, F. (1986),""Noise"", Journal of Finance , Vol. 41 No. 3, 529 -543.  
Bloomfield, R., Libby, R. & Nelson, M. W. (2000),""Underreactions, overreactions and moderated 
confidence"", Journal of Financial Markets , Vol. 3 No. 2, 113 -137.  
Blume, L., Easley, D. & O’Hara, M. (1994),  ""Market statistics and technical analysis: The role of volume"", 
Journal of Finance , Vol. 49 No. 1, 153 -181.  
Bodie, Z., Kane, A. & Marcus, A.J. (2008), Investments , 7th ed. McGraw -Hill Educatio n, Boston.  
Brem er, M., & Sweeney, R. J. (1991),  “The reversal of large stock ‐price decreases ”, The Journal of 
Finance , Vol. 46 No. 2 , 747 -754.  
Caginalp, G. & Ermentrout, B. (1990),  “A kinetic thermodynamics approach to the psychology of 
fluctuations in financial markets”,  Applied Mathematics Letters , 3, No. 4, 17 -19. 
Caginalp, G. & DeSantis, M. (2011),""Nonlinearity in the dynamics of financial markets"", Nonlinear 
Analysis: Real World Applications , Vol. 12 No. 2, 1140 -1151.  
Caginalp, G. & DeSantis, M. (2011),""Stock price dynamics: nonlinear trend, volume, volatility, resistance 
and money supply"", Quantitative Finance , Vol. 11 No. 6, 849 -861.  ",2019-07-09T21:33:49Z,antoer gul holmpriestly technical applied nancial economics vno bali mi as tehraiaaregate journal nancial quantitative analysis vno barber is sh leaf er vis ny journal nancial economics vno  noise journal nance vno bloomeld lobby nelsounr reans journal nancial markets vno   hara market t journal nance vno bodikane marcus investments mc raw hl du  bostobr em seney t t journal nance vno agaialp er metrout applied matmatics ters no agaialp  sant is nolinonlinear analysis real world applicatns vno agaialp  sant is stock quantitative nance vno
paper_qf_25.pdf,31,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","31 
 Caginalp, G., DeSantis, M. & Sayrak, A. (2014),""The nonlinear price dynamics of US equity ETFs"", Journal 
of Econometrics , Vol. 183, 193 -201.  
Cam pbell, J. & Thompson, S.  (2008),""Predicting excess stock returns out of sample: Can anything beat 
the historical  average?"", Review of Financial Studies , Vol. 21 No. 4 , 1509 -1531.  
Carhart, M.  (1997),  ""On persistence in mutual fund performance"", Journal of Finance , Vol. 52 No. 1 , 57-
82. 
Chordia, T., Subrahmanyam, A. & Anshuman, V. (2001),  ""Trading activity and expec ted stock returns"", 
Journal of Financial E conomics , Vol. 59, 3 -32. 
Daniel, K., Hirshleifer, D. & Subrahmanyam, A. (1998),  ""Investor psychology and security market under‐
and overeactions"", Journal of Finance , Vol. 53 No. 6 , 1839 -1885.  
Davidson, R. & MacKinnon , J. G. (1993),  Estimatio n and inference in econometrics , Oxford University 
Press, New York.  
Fama, E. (1998),""Market efficiency, long -term returns, and b ehavioral finance"", Journal of F inancial 
Economics , Vol. 49 No. 3 , 283 -306.  
Fama, E. & MacBeth, J. D. ( 1973),""Risk, return, and equ ilibrium: Empirical tests"", Journal of Political 
Economy , Vol. 81, 607-636.  
Gelman, A. (2008),""Scaling regression inputs by dividing by two standard deviations"", Statistics in 
Medicine , Vol. 27 No. 15 , 2865 -2873.  
George, T. J. & Hwang, C. Y. (2004),""The 52‐week high and momentum investing"", Journal of Finance , 
Vol. 59 No. 5 , 2145 -2176.  ",2019-07-09T21:33:49Z,agaialp  sant is say rak t fs journal econometrics vcam thompsopredii careview nancial studivno car hart ojournal nance vno chord sub rahmayam ahumatradi journal nancial vdaniel harsh leaf er sub rahmayam investor journal nance vno didsomac d otima t oxford   new york fam market journal economics vno fam mac beth risk emical journal political economy vgel mascali statistics medicine vno george ha t journal nance vno
paper_qf_25.pdf,32,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","32 
 Goyal, A. (2012),""Empirical cross -sectional asset pricing: a survey"", Financial Markets and Portfolio 
Management , Vol. 26 No. 1 , 3-38. 
Graham, B. & Dodd, D. L. (1934),  Security analysis , McGraw Hill, New York.  
Harris, M. & Raviv, A. (1993),  ""Differen ces of opinion make a horse race"", Review of Financial Studies , 
Vol. 6 No. 3 , 473 -506.  
Hong, H. & Stein, J. C. (1999),  ""A unified theory of underreaction, momentum trading, and overreaction 
in asset markets"", Journal of Finance , Vol. 54 No. 6 , 2143 -2184.  
Hsiao, C. (2003),  Analys is of Panel Data , Cambridge University Press , New York . 
Jegadeesh, N. & Titman, S. (1993),""Returns to buying winners and selling losers: Implications for stock 
market efficiency"", Journal of Finance , Vol. 48 No. 1, 65 -91. 
Karpoff, J.  (1987),  ""The relation between price changes and trading volume: A survey"", Journal of 
Financial and Quantitative Analysis , Vol. 22 No. 1, 109 -126.  
Lee, C., Myers, J. & Swaminathan, B. (1999),""What is the Intrinsic Value of the Dow?"", Journal of Finance , 
Vol. 54 No. 5, 1693 -1741. 
Leung, T. and Wang, Z., “Optimal risk -averse timing of an asset sale: Trending vs mean -reverting price 
dynamics. (2018).   SSRN: https://ssrn.co m/abstract=2786276.  
Luenberger, D.G. (1998 ), Investment Science , Oxford University Press, New York.  
Madura, J. & Richie, N. (2004),  ""Overreaction of exchange -traded funds durin g the bubble of 1998 -
2002"", Journal of Behavioral Finance , Vol. 5 No. 2, 91-104.  ",2019-07-09T21:33:49Z,goa emical nancial markets tfmanagement vno graham odd security mc raw hl new york harris iv dfer ereview nancial studivno ho steijournal nance vno asia alys panel data cambridge   new york je ma it maurns implicatns journal nance vno harp off t journal nancial quantitative analysis vno  myers swami nathawhat intrinsic value dow journal nance vno lu wa optimal trendi lu eberger investment science oxford   new york ma dra richie overreajournal behral nance vno
paper_qf_25.pdf,33,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","33 
 Menkhoff, L. (2010), “The use of technical analysis by fund managers: International evidence ”, Journal of 
Banking & Finance , Vol. 34, 2573 -2586.  
Moskowitz, T.  & Grinblatt, M. (1999),""Do industries explain momentum?"", Journal of Finance , Vol. 54 
No. 4, 1249 -1290.  
Neely, C., Rapach, D., Tu, J. & Zhou, G. (2014) , “Forecasting the equity risk premium: The role of 
technical indicators”, Management Science , Vol. 60 No. 7, 1772 -1791.  
Petersen,  M. (2009),  ""Estimating standard errors in finance panel data se ts: Comparing approaches"", 
Review of Financial Studies , Vol. 22 No. 1, 435 -480.  
Pinto, J.E., Robinson, T.,  and Stowe, J (2015) , “Equity valuation: a survey of professional 
practice.   SSRN:   http://dx.doi.org/10.2139/ssrn.2657717  
Poterba, J. M. & Summers, L . H. (1988),""Mean reversion in stock prices: Evidence and implications"", 
Journal of Financial Economics , Vol. 22 No. 1, 27 -59. 
Sturm, R. R. (2003),""Investor confidence and returns following large one -day price changes"", Journal of 
Behavioral Finance , Vol. 4 No. 4 , 201 -216.  
Sturm, R. R. (2008),  ""The 52 -week high strategy: momentum and overreaction in large firm stocks"", The 
Journal of Investing , Vol. 17 No. 2, 55 -67. 
Sturm, R. R. (2013) , ""Market Efficiency and Technical Analysis : Can they Coexist? "", Research in Applied 
Economics  5, No. 3, 1 -16. 
Subrahmanyam, A. (2010), “The cross -section of expected stock returns: What have we learnt from the 
past twenty -five years of researc h?”, European Financial Management , Vol. 16, 27 -42. ",2019-07-09T21:33:49Z,mekh off t internatnal journal banki nance vmos  with griblast do journal nance vno newly rape tu hou forecasti t management science vno petersoestimati ari review nancial studivno pinto robinsostore equity  er ba suers meaevince journal nancial economics vno storm investor journal behral nance vno storm t t journal investi vno storm market efciency technical analysis cacoexist researapplied economics no sub rahmayam t what anancial management vol
paper_qf_25.pdf,34,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","34 
 Thompson, S. (2011), “Sim ple formulas for standard errors that cluster by both firm and time”,  Journal 
of Financial Economics , Vol. 99, 1 -10. 
Vassalou, M. (2003), “News related to future GDP growth as a risk factor in equity returns”, Journal of 
Financial Economics , Vol. 68, 47-73. 
Wang, J. (1994), “A model of competitive stock trading volume”, Journal of Political Economy , Vol. 102, 
127-168.  
  ",2019-07-09T21:33:49Z,thompsosim journal nancial economics vvassal ou news journal nancial economics vwa journal political economy vol
paper_qf_25.pdf,35,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","35 
 Appendix. Winsori zation and Error Terms.  
The data discusse d above involves winsori zed data at the 1% level. In other words, data  poin ts that are 
outside of the (1%, 99%) interval are replaced by the last data points that are within that interval. The 
reasons for this are that we are interested in motivations of traders in normal times rather than in 
extreme times. In other words, it is possible that events that occur within 1% of the time would skew th e 
regressions. Also, financial data are known to exhibit fat tails which could increase the problem.  
Nevertheless, the same regressions were performed for the non -winsorized data, and the results are 
quite similar, although the coefficients differ. The interpretation is that the regressions done with 
winsorized data  (Table 2)  reflect the trader motivations during trading days that are typical while the 
non-winsorized data (Table 4) take all  points into account.  
The choice of winsorization at 1% is typical of asset price change studies. It is also consistent 
with the distribution of the error terms in the regressions in that they are very close to the normal 
distribution between the 1% and 9 9% levels. In particular, an examination of the 257,683 error terms 
shows the following. For the normal distribution with mean 0 and standard deviation 0.0149 2, the 
values at the 1% and 99% levels are -0.0347 and +0.0347, respectively. For the error terms of the non -
winsorized data, the corresponding values are -0.035 and +0.035. For the 10% and 90% levels, the values 
for the normal are ± 0.0191. The corresponding values for the error terms are ± 0.019. For the 0.1% and 
99.9% the normal distribution values are ± 0.0461. The corresponding values for the error terms are        
± 0.046.  Finally, for the 0.01% level, the normal values are ± 0.0555 while the corresponding numbers 
for the error terms are ± 0.0550.  
A plot of the cumulative distribution functions for the error terms and the normal distributions 
shows some deviation between the 99% and the 99.99% levels.  The direction of the diff erence is toward 
fat tails as one might expect.   The Ryan -Joiner normality test on the non -winsorized error terms (with 
N=257,365) shows a correlation of 0.888 between the error terms and the normal distribution.  ",2019-07-09T21:33:49Z, windsor error terms t it ialso nevertless t table table t it ifor for for t for t nally t t ryajoined
paper_qf_25.pdf,36,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","36 
 Another way to look at the data is to note the value attained  at the kth percentile, and to see 
how this differs from the theoretical (perfect Gaussian) kth percentile. For k=10% the observed data 
value is 0.019. For the Gaussian we have the corresponding value, V, is V(0.019)  = 0.10143. The ratio of 
the two is 0.10143/0.1= 1. 0143 , i.e., 1.43% difference from the ideal. At k=0.01%, the observed value  is 
0.055, while V(0.055)  = 1. 1376×10⁻⁴ differing from the ideal by 13.76%.  
 
 
  ",2019-07-09T21:33:49Z,anotr for for t at
paper_qf_25.pdf,37,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","37 
 Tables  
Table 1. Descriptive Statistics for Data Set  
The data set consi sts of daily data  for 85 firms corresponding to the  time period Decembe r 28, 2004 
through January 19 , 201 8.  It is a balanced panel data set with 3,289  records per firm.  Mean v alues are 
calculated on a per firm basis , and values reported in the table  are calculated across all firms.  
 Mean  Min First 
Quartile  Median  Third 
Quartile  Max  
Mean Dollar Volume 
(Millions)  577.13  77.98  288.42  383.09  637.28  5,056.11  
Mean Volume (# of 
shares in Millions)  14.35  0.59  4.39  7.02  13.62  134.59  
Mean Market 
Capitalization 
(Millions)  96,039.31  25,285.21  43,598.37  65,751.50  130,651.52  384,078.04  
 
  ",2019-07-09T21:33:49Z,tabltable siptive statistics data set t c em be uary it meameamirst quartz e mediathird quartz e max meadollar volume mlns meavolume mlns meamarket capitalizatmlns
paper_qf_25.pdf,38,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","38 
 Table 2 . Regression Results  
Two -way fixed effects regressions with robust standard errors (in parentheses) clustered by firm are run 
on the balanced panel data set described in Section 4 .  The fixed effects are firm and time.   
  Model 1V  Model 1 T Model 2 Model 3  Model 4  
𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛  0.055  0.39 5*** 0.61 5*** 0.560*** 
(0.08 7; 0.63 )   (0.11 4; 3.46 ) (0.16 6; 3.7) (0.16 5; 3.39 ) 
(𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛 )2    0.11 2* 0.09 3 
   (0.065 ; 1.72 ) (0.068 ; 1.36 ) 
(𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛 )3    -0.01 -0.010 
   (0.02 8; -0.36 ) (0.028 ; -0.36 ) 
𝑃𝑟𝑖𝑐𝑒  𝑇𝑟𝑒𝑛𝑑   0.180** 0.58 4*** 0.721*** 0.65 4*** 
 (0.09 ;1.99 ) (0.09 8;5.96 ) (0.14 6; 4.94 ) (0.14 5; 4.51 ) 
(𝑃𝑟𝑖𝑐𝑒  𝑇𝑟𝑒𝑛𝑑 )2    0.108 0.08 1 
   (0.100; 1.08 ) (0.10 3; 0.79 ) 
(𝑃𝑟𝑖𝑐𝑒  𝑇𝑟𝑒𝑛𝑑 )3    -0.09 0*** -0.09 0** 
   (0.035 ; 2.57 ) (0.035 ; 2.57 ) 
𝑃𝑟𝑖𝑐𝑒  𝑇𝑟𝑒𝑛𝑑  ×𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛     0.10 3 0.09 1 
   (0.15 7; 0.65 ) (0.15 8; 0.58 ) 
𝑃𝑟𝑖𝑐𝑒  𝑇𝑟𝑒𝑛𝑑  2×𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛     0.043 0.04 5 
   (0.061 ; 0.7) (0.06 2; 0.73 ) 
𝑃𝑟𝑖𝑐𝑒  𝑇𝑟𝑒𝑛𝑑  ×𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛2    0.151***  0.153***  
   (0.033 ; 4.58 ) (0.03 3; 4.64 ) 
𝑆ℎ𝑜𝑟𝑡 𝑇𝑒𝑟𝑚  𝑉𝑜𝑙𝑎𝑡𝑖𝑙𝑖𝑡𝑦      0.101 
    (0.09 0; 1.1 ) 
𝐿𝑜𝑛𝑔  𝑇𝑒𝑟𝑚  𝑉𝑜𝑙𝑎𝑡𝑖𝑙𝑖𝑡𝑦      0.261***  
    (0.09 9; 2.63 ) 
𝐿𝑜𝑛𝑔  𝑇𝑒𝑟𝑚  𝑇𝑟𝑒𝑛𝑑      -0.29 0*** 
    (0.042 ; -6.9) 
𝑉𝑜𝑙𝑢𝑚𝑒      0.038 
    (0.04 9; 0.78 ) 
𝑅𝑒𝑠𝑖𝑠𝑡𝑎𝑛𝑐𝑒      -1.31***  
    (0.48 7; -2.7) 
𝐼𝑛𝑡𝑒𝑟𝑐𝑒𝑝𝑡  0.0043  0.0043  0.0043  0.0043  0.0043  
R-Square  0.40 29 0.40 29 0.40 31 0.40 37 0.40 39 
No. Observations  257,635  257,635  257,635  257,635  257,635  
No. Groups/Firms  85 85 85 85 85 
No. Days (per Firm)  3,031  3,031  3,031  3,031  3,031  
F Test for No Fixed Effects  55.15*** 55.13***  55.16*** 55.15 *** 55.07*** 
 ",2019-07-09T21:33:49Z,table regressresults two set mmmmmsquare no observatns no groups rms no days rm test no xed effes
paper_qf_25.pdf,39,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","39 
 Note s: 
a. *, **, *** indicates significance at the 90%, 95%, and 99% level, respectively.  
b. For each coefficient, the standard error  (multiplied by 1,000)  and t -value are denoted by (  ;  ).  
c. Coefficient values have been multiplied by 1,000 for exposition.  
d. The intercept term, which is not corrected for heteroscedasticity, is calculated as the grand 
mean over all firms of the daily mean of the reported intercept and fixed -effects terms.  
e. The R-Square measure developed by Theil (1961)  is reported . 
 
Table 3 . List of firms  
This table provides a listing of the firms used in this  study.  The first 85 firms were used  in the primary 
analyses (balanced panel of data).  The final 13 firms were excluded from the primary analysis due to 
lack of data, but are included for robustness . 
No. Ticker  Name  Sector  
1 AAPL  APPLE INC  Information Technology  
2 ABT ABBOTT LABORATORIES  Health Care  
3 ACN  ACCENTURE PLC CLASS A  Information Technology  
4 AGN  ALLERGAN  Health Care  
5 AIG AMERICAN INTERNATIONAL GROUP INC  Financials  
6 ALL ALLSTATE CORP  Financials  
7 AMGN  AMGEN INC  Health Care  
8 AXP AMERICAN EXPRESS  Financials  
9 BA BOEING  Industrials  
10 BAC BANK OF AMERICA CORP  Financials  
11 BIIB BIOGEN INC  Health Care  
12 BK BANK OF NEW YORK MELLON CORP  Financials  
13 BLK BLACKROCK INC  Financials  
14 BMY  BRISTOL MYERS SQUIBB  Health Care  
15 BRKB  BERKSHIRE HATHAWAY INC CLASS B  Financials  
16 C CITIGROUP INC  Financials  
17 CAT CATERPILLAR INC  Industrials  ",2019-07-09T21:33:49Z,note for coefcient t t square t  table t  t t no ticker name seor informattechnology alth care informattechnology alth care nancial nancial alth care nancial industrial nancial alth care nancial nancial alth care nancial nancial industrial
paper_qf_25.pdf,40,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","40 
 18 CL COLGATE -PALMOLIVE  Consumer Staples  
19 CMCSA  COMCAST A CORP  Consumer Discretionary  
20 COF CAPITAL ONE FINANCIAL CORP  Financials  
21 COP  CONOCOPHILLIPS  Energy  
22 COST  COSTCO WHOLESALE CORP  Consumer Staples  
23 CSCO  CISCO SYSTEMS INC  Information Technology  
24 CVS CVS HEALTH CORP  Consumer Staples  
25 CVX CHEVRON CORP  Energy  
26 DHR  DANAHER CORP  Health Care  
27 DIS WALT DISNEY  Consumer Discretionary  
28 DUK  DUKE ENERGY CORP  Utilities  
29 EMR  EMERSON ELECTRIC  Industrials  
30 EXC EXELON CORP  Utilities  
31 F F MOTOR  Consumer Discretionary  
32 FDX FEDEX CORP  Industrials  
33 FOX TWENTY -FIRST CENTURY FOX INC CLASS  Consumer Discretionary  
34 GD GENERAL DYNAMICS CORP  Industrials  
35 GE GENERAL ELECTRIC  Industrials  
36 GILD  GILEAD SCIENCES INC  Health Care  
37 GOOGL  ALPHABET INC CLASS A  Information Technology  
38 GS GOLDMAN SACHS GROUP INC  Financials  
39 HAL HALLIBURTON  Energy  
40 HD HOME DEPOT INC  Consumer Discretionary  
41 HON  HONEYWELL INTERNATIONAL INC  Industrials  
42 IBM INTERNATIONAL BUSINESS MACHINES CO  Information Technology  
43 INTC  INTEL CORPORATION CORP  Information Technology  
44 JNJ JOHNSON & JOHNSON  Health Care  
45 JPM JPMORGAN CHASE & CO  Financials  
46 KO COCA -COLA  Consumer Staples  
47 LLY ELI LILLY  Health Care  
48 LMT  LOCKHEED MARTIN CORP  Industrials  
49 LOW  LOWES COMPANIES INC  Consumer Discretionary  
50 MCD  MCDONALDS CORP  Consumer Discretionary  
51 MDLZ  MONDELEZ INTERNATIONAL INC CLASS A  Consumer Staples  
52 MDT  MEDTRONIC PLC  Health Care  
53 MET  METLIFE INC  Financials  
54 MMM  3M Industrials  
55 MO ALTRIA GROUP INC  Consumer Staples  
56 MON  MONSANTO  Materials  
57 MRK  MERCK & CO INC  Health Care  ",2019-07-09T21:33:49Z,consumer staplconsumer disetnary nancial energy consumer staplinformattechnology consumer staplenergy alth care consumer disetnary utitiindustrial utiticonsumer disetnary industrial consumer disetnary industrial industrial alth care informattechnology nancial energy consumer disetnary industrial informattechnology informattechnology alth care nancial consumer staplalth care industrial consumer disetnary consumer disetnary consumer staplalth care nancial industrial consumer staplmaterials alth care
paper_qf_25.pdf,41,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","41 
 58 MS MORGAN STANLEY  Financials  
59 MSFT  MICROSOFT CORP  Information Technology  
60 NEE NEXTERA ENERGY INC  Utilities  
61 NKE NIKE INC CLASS B  Consumer Discretionary  
62 ORCL  ORACLE CORP  Information Technology  
63 OXY OCCIDENTAL PETROLEUM CORP  Energy  
64 PEP PEPSICO INC  Consumer Staples  
65 PFE PFIZER INC  Health Care  
66 PG PROCTER & GAMBLE  Consumer Staples  
67 QCOM  QUALCOMM INC  Information Technology  
68 RTN RAYTHEON  Industrials  
69 SBUX  STARBUCKS CORP  Consumer Discretionary  
70 SLB SCHLUMBERGER NV  Energy  
71 SO SOUTHERN  Utilities  
72 T AT&T INC  Telecommunications  
73 TGT TARGET CORP  Consumer Discretionary  
74 TWX  TIME WARNER INC  Consumer Discretionary  
75 TXN TEXAS INSTRUMENT INC  Information Technology  
76 UNH  UNITEDHEALTH GROUP INC  Health Care  
77 UNP  UNION PACIFIC CORP  Industrials  
78 UPS UNITED PARCEL SERVICE INC CLASS B  Industrials  
79 USB US BANCORP  Financials  
80 UTX UNITED TECHNOLOGIES CORP  Industrials  
81 VZ VERIZON COMMUNICATIONS INC  Telecommunications  
82 WBA  WALGREEN BOOTS ALLIANCE INC  Consumer Staples  
83 WFC  WELLS FARGO  Financials  
84 WMT  WALMART STORES INC  Consumer Staples  
85 XOM  EXXON MOBIL CORP  Energy  
86 ABBV  ABBVIE INC  Health Care  
87 AMZN  AMAZON COM INC  Consumer Discretionary  
88 CELG  CELGENE CORP  Health Care  
89 FB FACEBOOK CLASS A INC  Information Technology  
90 GM GENERAL MOTORS  Consumer Discretionary  
91 KHC KRAFT HEINZ  Consumer Staples  
92 KMI KINDER MORGAN INC  Energy  
93 MA MASTERCARD INC CLASS A  Information Technology  
94 PCLN  THE PRICELINE GROUP INC  Consumer Discretionary  
95 PM PHILIP MORRIS INTERNATIONAL INC  Consumer Staples  
96 PYPL  PAYPAL HOLDINGS INC  Information Technology  ",2019-07-09T21:33:49Z,nancial informattechnology utiticonsumer disetnary informattechnology energy consumer staplalth care consumer staplinformattechnology industrial consumer disetnary energy utititelecounicatns consumer disetnary consumer disetnary informattechnology alth care industrial industrial nancial industrial telecounicatns consumer staplnancial consumer staplenergy alth care consumer disetnary alth care informattechnology consumer disetnary consumer staplenergy informattechnology consumer disetnary consumer staplinformattechnology
paper_qf_25.pdf,42,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","42 
 97 SPG SIMON PROPERTY GROUP REIT INC  Real Estate  
98 V VISA INC CLASS A  Information Technology  
   ",2019-07-09T21:33:49Z,real estate informattechnology
paper_qf_25.pdf,43,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","43 
 Table 4. Regression Results for Non -winsorized data  
Two -way fixed effects regressions  for the non -winsorized data  are run on the balanced panel data set 
described in Section 4.  The fixed effects are firm and time.  This is done for Model 4 which has the most 
complete set of variables.  
  Model  
𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛  0.205  
(0.132; 1.56 )  
(𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛 )2 0.232**  
(0.11; 2.11)  
(𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛 )3 0.038  
(0.024; 1.61)  
𝑃𝑟𝑖𝑐𝑒  𝑇𝑟𝑒𝑛𝑑  0.24  
(0.16; 1.5)  
(𝑃𝑟𝑖𝑐𝑒  𝑇𝑟𝑒𝑛𝑑 )2 0.322***  
(0.079; 4.1)  
(𝑃𝑟𝑖𝑐𝑒  𝑇𝑟𝑒𝑛𝑑 )3 -0.00007  
(0.01; -0.01)  
𝑃𝑟𝑖𝑐𝑒  𝑇𝑟𝑒𝑛𝑑  ×𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛  0.046  
(0.029; 1.57)  
𝑃𝑟𝑖𝑐𝑒  𝑇𝑟𝑒𝑛𝑑  2×𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛  0.101**  
(0.04; 2.52)  
𝑃𝑟𝑖𝑐𝑒  𝑇𝑟𝑒𝑛𝑑  ×𝑉𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛2 0.513**  
(0.201; 2.55)  
𝑆ℎ𝑜𝑟𝑡 𝑇𝑒𝑟𝑚  𝑉𝑜𝑙𝑎𝑡𝑖𝑙𝑖𝑡𝑦  0.063  
(0.109; 0.58)  
𝐿𝑜𝑛𝑔  𝑇𝑒𝑟𝑚  𝑉𝑜𝑙𝑎𝑡𝑖𝑙𝑖𝑡𝑦  0.211**  
(0.098; 2.15)  
𝐿𝑜𝑛𝑔  𝑇𝑒𝑟𝑚  𝑇𝑟𝑒𝑛𝑑  -0.35***  
(0.052; -6.87)  
𝑉𝑜𝑙𝑢𝑚𝑒  0.019  
(0.065; 0.29)  
𝑅𝑒𝑠𝑖𝑠𝑡𝑎𝑛𝑐𝑒  -1.33***  
(0.475; -2.8) 
R-Square  0.4039  
No. Observations  257,635  
No. Groups/Firms  85 
No. Days (per Firm)  3,031  
F Test for No Fixed Effects  55.07***  
 
 ",2019-07-09T21:33:49Z,table regressresults notwo set  mmsquare no observatns no groups rms no days rm test no xed effes
paper_qf_25.pdf,44,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","44 
 Note s: 
a. *, **, *** indicates significance at the 90%, 95%, and 99% level, respectively.  
b. For each coefficient, the standard error  (multiplied by 1,000)  and t -value are denoted by (  ;  ).  
c. Coefficient values have been multiplied by 1,000 for exposition.  
d. The R-Square measure developed by Theil (1961)  is reported . 
 
  ",2019-07-09T21:33:49Z,note for coefcient t square t 
paper_qf_25.pdf,45,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","45 
 Figures  
Figure 1. Relationship between the following day’s return and the Valuation and Price Trend variables.   
Using the (statistically significant) regression coefficients from Model 3, the return is plotted as a 
function of the Valuation and Trend.  
 
 
 
  
",2019-07-09T21:33:49Z,s  relatnship valuatprice trend usi mvaluattrend
paper_qf_25.pdf,46,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","46 
 Figure 2.  Plot of the following day's return versus valuation.   Using the (statistically significant) 
regression coefficients from Model 3 with the change in Trend set to zero, the return is plotted as a 
function of the Valuation.  The relationship between return and valuation is consistent with intuitive 
expectations of higher (lower) valuations correspon ding to higher (lower) returns.  
 
 
 
 
 
  
",2019-07-09T21:33:49Z, plot usi mtrend valuatt
paper_qf_25.pdf,47,Nonlinear price dynamics of S&P 100 stocks,"  The methodology presented provides a quantitative way to characterize
investor behavior and price dynamics within a particular asset class and time
period. The methodology is applied to a data set consisting of over 250,000
data points of the S&P 100 stocks during 2004-2018. Using a two-way
fixed-effects model, we uncover trader motivations including evidence of both
under- and overreaction within a unified setting. A nonlinear relationship is
found between return and trend suggesting a small, positive trend increases the
return, while a larger one tends to decrease it. The shape parameters of the
nonlinearity quantify trader motivation to buy into trends or wait for
bargains. The methodology allows the testing of any behavioral finance bias or
technical analysis concept.
","47 
 Figure 3.  Plot of the following day's return versus trend in price.   Using the (statistically significant) 
regression coefficients from Model 3 with the change in Valuation set to zero, the return is plotted as a 
function of the trend in price. Increasing price trends lead to greater returns for trends between -1.634  
and 1.634  standard deviations.  As the trend increases beyond 1.634  standard deviations, the return 
begins to diminish.  Conversely, the return begins t o increase as the trend decreases below -1.634  
standard deviations.  
 
 
 
 
 
 
 
 
",2019-07-09T21:33:49Z, plot usi mvaluatineasi as conversely
paper_qf_26.pdf,1,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","Computational method for probability distribution on recursive
relationships in nancial applications
Jong Jun Park, Kyungsub Leey
Abstract
In quantitative nance, it is often necessary to analyze the distribution of the sum of
specic functions of observed values at discrete points of an underlying process. Examples
include the probability density function, the hedging error, the Asian option, and statis-
tical hypothesis testing. We propose a method to calculate such a distribution, utilizing
a recursive method, and examine it using various examples. The results of the numerical
experiment show that our proposed method has high accuracy.
1 Introduction
This paper introduces a recursive method to compute interesting quantities related to proba-
bility distributions in various nancial applications. The method is versatile, and hence, with
slight modications, it is easy to apply the basic framework to various applications. More pre-
cisely, the method is based on a convolution-like formula, applied to compute the distribution of
the sum of values of one-dimensional processes observed at discrete points. Financial applica-
tions include numerical densities of asset price or volatility models, hedging error distributions,
arithmetic Asian option prices, and statistical hypothesis tests.
Various kinds of stochastic processes are used in quantitative nance, such as the Cox-
Ingersoll-Ross model (CIR), the constant elasticity of variance model (CEV), stochastic volatil-
ity, and GARCH models. The probability distributions of the processes in nancial models
can be used for risk management, asset pricing, hedging analysis, parameter estimation, and
statistical hypothesis testing. In many cases, the closed form formulas for the density function
of stochastic models are not known, and it is advantageous to develop a numerical procedure
to compute the probability distributions or density functions.
When trading a nancial option, the investor usually performs a hedging procedure to
reduce risk. In general, continuous models of asset price movements assume a continuous
hedging process. However, in practice, because continuous trading is not applicable, a discrete
Department of Mathematical Sciences, KAIST, 291 Daehak-ro, Daejeon 34141, Republic of Korea
yDepartment of Statistics, Yeungnam University, Gyeongsan, Gyeongbuk 38541, Republic of Korea, Corre-
sponding author, Email: ksublee@yu.ac.kr
1arXiv:1908.04959v1  [q-fin.ST]  14 Aug 2019",2019-08-14T05:22:32Z,tnal jo jupark kyu sub  abstra is   t introdu t  nancial  varus cox inoll ross t iwipartment matmatical sciencdae hak daejeorepublic rea partment statistics e nam  yeo sagyeobu republic rea core em articial intellence  aug
paper_qf_26.pdf,2,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","time hedging strategy is applied, and hence, the discrete time hedging error occurs even in
the complete market model. Many studies examine the discrete time hedging error in nancial
options. Sepp (2012) derived a numerically approximated distribution of the delta-hedging error
based on the characteristic function for a jump diusion model. Park et al. (2016) computed
the delta-hedging error based on a recursive method for a jump diusion model, which is the
same framework this study proposes. This study extends this to a L evy model and shows it is
possible to easily adapt the method to not only delta-hedging processes but also other trading
strategies, such as minimum variance hedging.
The arithmetic Asian option is a nancial derivative whose payo is the arithmetic average
of the underlying asset prices observed at future times. Asian options are safer with respect to
the manipulation of underlying asset prices that may occur when they are close to maturity than
European options and nancial instruments suitable for less frequently traded assets (Musiela
and Rutkowski, 2006). Since the closed-form formula is not available for the Asian option price,
we study numerical approximation and simulation methods (Kemna and Vorst, 1990; V ec er,
2002). Our example is consistent with Lee (2014), which computes European option based
Asian option prices; however, we directly apply risk-neutral probability density in this study.
We also examine an example of the statistical hypothesis test. In general, a parametric
statistical test depends on the probability distribution of a test statistic. For typical sample
mean tests, the test statistics are generally approximated by a t distribution; however, if the
corresponding random variable is far from the normal distribution, it might undermine the
accuracy of the test. Therefore, a more exact distribution will be helpful in performing a more
reliable test. We provide an example of a skewness test where the recursive method is applied.
In general, nancial asset return distributions are negatively skewed (Fama, 1965; French et al.,
1987; Cont, 2001) and the third moment of nancial asset distribution has been extensively
studied (Kraus and Litzenberger, 1976; Harvey and Siddique, 2000; Christoersen et al., 2006;
Choe and Lee, 2014; Lee, 2016). Our example demonstrates a method to compute critical values
and statistical power.
The rest of the paper is organized as follows: Section 2 explains the basic recursive method
and the process of applying the numerical procedure to compute the probability density func-
tions. Section 3 applies the proposed method to the examples. Section 4 concludes the paper.
2 Basic method
2.1 Derivation
LetXbe a continuous stochastic process dened on the time horizon [0 ;T] or a discrete stochas-
tic process dened on time indexes 0 = t0;t1;;tN=T. IfXis continuous, we are particu-
larly interested in the behaviors of Xtifor the discrete observation times 0 = t0;t1;;tN=T.
When necessary, we can introduce a complete ltered probability space (
 ;F;P) over [0;T],
with ltrationfFtgt2[0;T].
2",2019-08-14T05:22:32Z,many sep park  t   amus ie la rut   since  ke mna worst our  a  ifor trefore  ifam frencont trans litzenberger harvey si que christ cho   our t sesesebasic rivat be  is ti for wft gt
paper_qf_26.pdf,3,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","In numerous nancial applications, it is advantageous to examine the distribution of some
nite summation of
Y=NX
i=1h(Xi",2019-08-14T05:22:32Z,ixi
paper_qf_26.pdf,4,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","we have
fYjX0(yjx0) =@2g0(yjx0)
@y2:
Or, by dierentiating both sides of Eq. (2) for y, we can express
Fn(yjxn) =Z
RFn+1(y",2019-08-14T05:22:32Z,yj or eq ffn
paper_qf_26.pdf,5,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","Alternatively, we can use the cumulative distribution function to express
Fn(y) =Z
RFn+1y",2019-08-14T05:22:32Z,alternatively ffn
paper_qf_26.pdf,6,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","2.2 Numerical procedure
This subsection describes the numerical algorithm when applying the recursive method in com-
puting a probability distribution. There are specic considerations in applying the numerical
method for every application; however, in this subsection, we examine the common concerns in
applying the computational procedure.
Selection of object function
For the numerical procedure, we determine whether to use function Fnorfn, although this
distinction has no signicant eect on the results. Both Fnandfnhave nice properties that
stabilize the numerical procedure. Since Fnis theoretically bounded between 0 and 1, it can
be easily corrected, even if Fnis outside the bounded region, owing to numerical error. The
density function fnconverges to 0 as ygoes1or",2019-08-14T05:22:32Z,numerical  tre selefor for fboth fand fhe since fis fis t
paper_qf_26.pdf,7,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","Another concern is referencing previous function values of Fn+1orfn+1at stepn, as we
compute
Fn(yjxn) =Z
RFn+1(y",2019-08-14T05:22:32Z,anotr fffn
paper_qf_26.pdf,8,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","Figure 1: Reference criteria for Fn(left) andfn(right) outside the numerical domains
Algorithm 1 Recursive method with distribution function
Initial setting for FN",2019-08-14T05:22:32Z, reference falgorithm recursive initial
paper_qf_26.pdf,9,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","3.1 Numerical density for diusion models
This subsection demonstrates the computation of probability densities, or likelihood functions
of various diusion models numerically based on the recursive method.
3.1.1 CIR model
Consider a square-root process X, also known as the Cox-Ingersoll-Ross model (Cox et al.,
1985), dened by:
dXt=(",2019-08-14T05:22:32Z,numerical  consir cox inoll ross cox xt
paper_qf_26.pdf,10,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","-0.5 0 0.5 101234
numerical
closed-form
simulation0
0.62
20.4410.26
0
0-1Figure 2: Probability density function of the CIR model (left) and fYjX0(yjx0) (right)
200 400 600 800 100000.050.10.150.2
RMSE200 400 600 800 100000.050.10.150.2
RMSE
Figure 3: Global error for probability density function of the CIR model
Y=XN",2019-08-14T05:22:32Z, probabity yj  global
paper_qf_26.pdf,11,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","0.6 0.8 1 1.2 1.40123450.502
1.54
y06
18
0.5 -0.5Figure 4: Probability density function of the CEV model (left) and fYjX0(yjx0) (right)
= 0:05;= 0:2;= 0:7. For the numerical procedure,  t= 1=1250;N= 200;[xmin;xmax] =
[0:5;1:5] with x= 0:005, the number of intervals on the y-axis are 200 and the tolerance for
the dynamic allocation is 10",2019-08-14T05:22:32Z, probabity yj for
paper_qf_26.pdf,12,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","0 0.4 0.801234
numerical
closed-form
simulation(a)a= 0;b= 1=2
0 0.4 0.80123numerical
simulation0 0.4 0.80123numerical
simulation (b)a= 1;b= 1=2
0 0.2 0.4 0.602468
numerical
simulation0 0.2 0.4 0.602468
numerical
simulation
(c)a= 0;b= 1
0 0.2 0.4 0.60246 numerical
simulation0 0.2 0.4 0.60246 numerical
simulation (d)a= 1;b= 1
0.1 0.2 0.3051015numerical
simulation0.1 0.2 0.3051015numerical
simulation
(e)a= 0;b= 3=2
0.1 0.2 0.3 0.404812 numerical
simulation0.1 0.2 0.3 0.404812 numerical
simulation (f)a= 1;b= 3=2
Figure 5: Probability density function of Y=VT",2019-08-14T05:22:32Z, probabity
paper_qf_26.pdf,13,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","the probability density functions of the integrated variance under various stochastic models are
presented in Figure 6. The basic setting is the same as in previous cases, except for the formula
ofh. Compared with the simulation results, the numerical density functions are precise for all
stochastic volatility models.
3.2 The GARCH model
We can calculate the numerical density function for not only continuous models but also discrete
time models. Consider the GARCH model (Bollerslev, 1986) for the variance 2and log-return
"":
2
n+1=!+2
n+""2
n; ""nN(0;2
n):
We are interested in the distributions of both time Nvariance,2
N, and total return,PN
i=1""i.
The conditional distribution of ""2
nwith givennis represented by the gamma distribution,
such that
""2
njn",2019-08-14T05:22:32Z, t ared t  consir roller lev  variance t
paper_qf_26.pdf,14,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","0 0.1 0.2 0.3 0.4 0.5 0.60246
numerical
simulation(a)a= 0;b= 1=2
0 0.2 0.4 0.6024numerical
simulation0 0.2 0.4 0.6024numerical
simulation (b)a= 1;b= 1=2
0.1 0.2 0.3 0.404812 numerical
simulation0.1 0.2 0.3 0.404812 numerical
simulation
(c)a= 0;b= 1
0 0.1 0.2 0.3 0.4 0.5048numerical
simulation0 0.1 0.2 0.3 0.4 0.5048numerical
simulation (d)a= 1;b= 1
0.16 0.2 0.24 0.280102030
numerical
simulation0.16 0.2 0.24 0.280102030
numerical
simulation
(e)a= 0;b= 3=2
0.1 0.2 0.301020numerical
simulation0.1 0.2 0.301020numerical
simulation (f)a= 1;b= 3=2
Figure 6: Probability density function of integrated variances Y=IVTwith various stochastic
volatility models
14",2019-08-14T05:22:32Z, probabity with
paper_qf_26.pdf,15,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","-0.01 0 0.01 0.02 0.03040801200
0.06
0.04
x0.10.5
0.020.050
01Figure 7: Probability density function of the GARCH variance (left) and F(yjx0) (right)
0.2 0.3 0.4 0.5 0.6 0.7 0.802468100.2 0.3 0.4 0.5 0.6 0.7 0.80246810
-3 -2 -1 0 1 2 300.20.40.6-3 -2 -1 0 1 2 300.20.40.6
Figure 8: Probability density function of the sum of GARCH variance (left) and return (right)
3.3 Hedging error
Many researchers such as Sepp (2012), have studied errors occurring from hedging strategies
of an option under time-discretization. We consider hedging errors in a framework in which
the underlying process follows the exponential L evy model, as in Madan et al. (1998). This
subsection can be regarded as an extension of Park et al. (2016).
Lettbe a gamma process, which is a L evy process with independent and gamma distributed
increments, with a mean rate parameter of 1 and variance parameter . In other words, the
L evy measure of is represented by z",2019-08-14T05:22:32Z, probabity  probabity edgi many sep  madma park  in
paper_qf_26.pdf,16,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","Madan et al. (1998) derived the probability density function of St, withS0= 1, as follows:
f(x) =Z1
01
p2gexp
",2019-08-14T05:22:32Z,madmast
paper_qf_26.pdf,17,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","simulation histograms. The European call options have strike price K= 0:9;1;1:05 withS0= 1
listed from top to bottom in Figure 9. The parameter setting is = 0:2;= 1:2;= 0:001;r=
0:02;t= 1=250.
3.4 Arithmetic Asian option
As mentioned, an Asian option is a nancial derivative that is more robust to the manipulation of
underlying asset prices than the European option. Since the closed form formula for arithmetic
Asian option prices is not known, simulation, approximation, or computational methods are
generally used. The recursive method can also be applied to compute the arithmetic Asian
option prices.
LetSbe an underlying asset price process. The arithmetic Asian option price over obser-
vation points t1;;tN=Twith strike price Kis represented by
EQ2
4 
1
NNX
i=1Si",2019-08-14T05:22:32Z,t a t arithmetic  as  asince  t   be t  with is si
paper_qf_26.pdf,18,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","Figure 9: Probability density functions of delta hedging errors (left) and minimum variance
hedging errors (right) of European call options with strike K= 0:9;1;1:05 from top to bottom
withS0= 1
18",2019-08-14T05:22:32Z, probabity an
paper_qf_26.pdf,19,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","0.8 0.9 1 1.1 1.200.050.10.150.20.25pricenumerical
simulationFigure 10: Comparison between the Asian option price computed by numerical method and
Monte Carlo simulation
thatRbecomes a martingale (with respect to suitable ltration). Under this assumption, the
statistical hypothesis can be modied as follows: H 0:J= 0 and H 1:J<0. The test
is similar to a simple t-test; however, the distribution of ( R)3does not follow the normal
distribution, and hence, it is advantageous to compute the exact distribution of ( R)3using
the recursive method.
We compute the critical values that determine the rejection of the null hypothesis for sample
sizes with given signicance level, = 0:05 under the null hypothesis (see Figure 11). The
null hypothesis is rejected when the test statistic; the sample mean of ( R)3is less than
the corresponding critical value with given sample size. To compute the critical values, the
numerical probability density function of ( R)3is computed to a h(xn;xn+1) = (xn+1",2019-08-14T05:22:32Z, ariso   becomunr t   t to
paper_qf_26.pdf,20,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","20 40 60 80 100-5-4-3-2-1critical value×10-620 40 60 80 1000.50.60.70.80.91statistical powerFigure 11: Critical values for ( R)3(left) and power curve (right) for testing H0:E[(R)3] = 0
versusH1:E[(R)3]<0 with= 0:05
4 Conclusion
This study proposed a recursive formula for the distribution of specic functions and detailed
the application of the numerical procedure. Various examples, including the numerical density
function, the hedging error distribution, the arithmetic Asian option pricing, and statistical
hypothesis testing, showed that the proposed method is quite precise. The method is versatile,
and we expect that more applications will become available not only in nance but also in
various probabilistic analysis. This study applied the method to a one-dimensional model, and
future studies can extend the method to the two-dimensional process model.
20",2019-08-14T05:22:32Z, itical conus varus  t 
paper_qf_26.pdf,21,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","References
Barndor-Nielsen, O. E. (2002). Econometric analysis of realized volatility and its use in estimat-
ing stochastic volatility models. Journal of the Royal Statistical Society: Series B (Statistical
Methodology) , 64:253{280.
Bollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal of
econometrics , 31:307{327.
Breeden, D. T. and Litzenberger, R. H. (1978). Prices of state-contingent claims implicit in
option prices. Journal of business , pages 621{651.
Broadie, M. and Kaya, O. (2006). Exact simulation of stochastic volatility and other ane
jump diusion processes. Operations Research , 54:217{231.
Carr, P. and Madan, D. (1999). Option valuation using the fast Fourier transform. Journal of
computational nance , 2:61{73.
Choe, G. H. and Lee, K. (2014). High moment variations and their application. Journal of
Futures Markets , 34:1040{1061.
Christoersen, P., Heston, S., and Jacobs, K. (2006). Option valuation with conditional skew-
ness. Journal of Econometrics , 131:253{284.
Christoersen, P., Jacobs, K., and Mimouni, K. (2010). Volatility dynamics for the S&P500:
evidence from realized volatility, daily returns, and option prices. Review of Financial Studies ,
23:3141{3189.
Cont, R. (2001). Empirical properties of asset returns: stylized facts and statistical issues.
Quantitative Finance , 1:223{236.
Cont, R., Tankov, P., and Voltchkova, E. (2007). Hedging with options in models with jumps.
InStochastic analysis and applications , pages 197{217. Springer.
Cox, J. C., Ingersoll Jr, J. E., and Ross, S. A. (1985). A theory of the term structure of interest
rates. Econometrica , 53:385{407.
Cox, J. C. and Ross, S. A. (1976). The valuation of options for alternative stochastic processes.
Journal of nancial economics , 3:145{166.
Fama, E. F. (1965). The behavior of stock-market prices. The Journal of Business , 38:34{105.
F ollmer, H. and Sondermann, D. (1986). Hedging of non-redundant contingent claims. In
Contributions to Mathematical Economics: In Honor of G erard Debreu , pages 205{224. North
Holland.
21",2019-08-14T05:22:32Z,referencbardor nielseeconometric journal royal statistical society seristatistical methodology roller lev genlized journal breed elitzenberger pricjournal broad ie kaya exa oatns researcarr madmaoptcourier journal cho  hh journal futurmarkets christ stojacobs optjournal econometrics christ jacobs imo uni volatity review nancial studicont emical quantitative nance cont tank ov volt v edgi istochastic  cox inoll jr ross econometric cox ross t journal fam t t journal business so nr manedgi icontributns matmatical economics ihonor re north holland
paper_qf_26.pdf,22,"Computational method for probability distribution on recursive
  relationships in financial applications","  In quantitative finance, it is often necessary to analyze the distribution of
the sum of specific functions of observed values at discrete points of an
underlying process. Examples include the probability density function, the
hedging error, the Asian option, and statistical hypothesis testing. We propose
a method to calculate such a distribution, utilizing a recursive method, and
examine it using various examples. The results of the numerical experiment show
that our proposed method has high accuracy.
","French, K. R., Schwert, G. W., and Stambaugh, R. F. (1987). Expected stock returns and
volatility. Journal of Financial Economics , 19:3{29.
Harvey, C. R. and Siddique, A. (2000). Conditional skewness in asset pricing tests. Journal of
Finance , 55:1263{1295.
Heston, S. L. (1993). A closed-form solution for options with stochastic volatility with applica-
tions to bond and currency options. Review of Financial Studies , 6:327{343.
Kemna, A. G. and Vorst, A. (1990). A pricing method for options based on average asset values.
Journal of Banking & Finance , 14:113{129.
Kraus, A. and Litzenberger, R. H. (1976). Skewness preference and the valuation of risk assets.
Journal of Finance , 31:1085{1100.
Lee, K. (2014). Recursive formula for arithmetic Asian option prices. Journal of Futures
Markets , 34:220{234.
Lee, K. (2016). Probabilistic and statistical properties of moment variations and their use in
inference and estimation based on high frequency return data. Studies in Nonlinear Dynamics
& Econometrics , 20:19{36.
Madan, D. B., Carr, P. P., and Chang, E. C. (1998). The variance gamma process and option
pricing. European nance review , 2:79{105.
Musiela, M. and Rutkowski, M. (2006). Martingale methods in nancial modelling , volume 36.
Springer Science & Business Media.
Park, M., Lee, K., and Choe, G. H. (2016). Distribution of discrete time delta-hedging error
via a recursive relation. East Asian Journal on Applied Mathematics , 6:314{336.
Sepp, A. (2012). An approximate distribution of delta-hedging errors in a jump-diusion model
with discrete trading and transaction costs. Quantitative Finance , 12:1119{1141.
V ec er, J. (2002). Unied Asian pricing. Risk, 15:113{116.
22",2019-08-14T05:22:32Z,frensre st alba gh eeed journal nancial economics harvey si que conditnal journal nance storeview nancial studike mna worst journal banki nance trans litzenberger skew ness journal nance  recursive  journal futurmarkets  probabistic studinonlinear dynamics econometrics madmacarr  t amus ie la rut   martale  science business media park  cho distributeast  journal applied matmatics sep aquantitative nance uni  risk
paper_qf_27.pdf,1,A Quantum algorithm for linear PDEs arising in Finance,"  We propose a hybrid quantum-classical algorithm, originated from quantum
chemistry, to price European and Asian options in the Black-Scholes model. Our
approach is based on the equivalence between the pricing partial differential
equation and the Schrodinger equation in imaginary time. We devise a strategy
to build a shallow quantum circuit approximation to this equation, only
requiring few qubits. This constitutes a promising candidate for the
application of Quantum Computing techniques (with large number of qubits
affected by noise) in Quantitative Finance.
","A QUANTUM ALGORITHM FOR LINEAR PDES ARISING IN FINANCE
FILIPE FONTANELA, ANTOINE JACQUIER, AND MUGAD OUMGARI
Abstract. We propose a hybrid quantum-classical algorithm, originated from quantum chemistry, to price
European and Asian options in the Black-Scholes model. Our approach is based on the equivalence between
the pricing partial dierential equation and the Schr odinger equation in imaginary time. We devise a
strategy to build a shallow quantum circuit approximation to this equation, only requiring few qubits. This
constitutes a promising candidate for the application of Quantum Computing techniques (with large number
of qubits aected by noise) in Quantitative Finance.
1.Introduction
Pricing nancial derivatives accurately and eciently is one of the most dicult and exciting challenges
in Mathematical Finance. While the existence of closed-form solutions are known in very few simple cases,
the vast majority of models do not admit one, and computing derivatives rely on computational techniques,
either using Monte Carlo simulation methods or via numerical methods for partial dierential equations, such
as nite dierences or nite elements. Unfortunately, these techniques can rapidly become computationally
intensive when the model becomes complicated, or when the dimension of the problem (in the case of Basket
options for example) becomes large. While huge progress has been made over the past decades, these
techniques have inherent limitations which cannot be overcome when run on classical computers.
Recently, the emergence of small-scale quantum computers has caught the eyes of mathematicians and
nancial engineers [16, 18] as a a potential goose laying golden eggs. Indeed, while Quantum Computing
has been around since Benio [1], Deutsch [8], Feynman [11] and Manin [15] suggested that a quantum
computer could perform tasks out of reach for classical computers, it really started getting some traction
when Shor [20] unearthed a polynomial-time quantum algorithm to factor integers. Since then, huge eorts
have been made to actually build quantum hardware; only recently though have researchers, in a partnership
between Google AI and the US NASA, managed to stabilise a quantum chip over a short period of time,
eectively starting what has already been called the Quantum Revolution. Several companies are now
proposing online quantum capabilities, using a few qubits. We are still far from being able to use those
in production mode on a large scale, but the revolution is fast marching, and tools are urgently needed to
embrace it. Quantitative Finance is traditionally quick to respond to such calls, and several attempts have
recently been made to use quantum techniques for option pricing, in particular [19, 21], focusing on the
so-called Amplitude Estimation algorithm [5], which allows a quadratic speed-up for simulation methods
compared to classical Monte Carlo schemes.
We propose a new route to investigate the use of Quantum techniques in Quantitative Finance, and develop
a hybrid quantum-classical algorithm to solve the Schr odinger equation, which by simple transformations
is equivalent to the PDE satised by the option price. The paper is organised as follows. In Section 2,
we introduce the main tools and notations, clarifying the link between the Black-Scholes PDE [4] and its
Schr odinger counterpart along the imaginary axis, while providing a short reminder on Quantum mechanics.
In Section 3, we borrow an idea developed in quantum chemistry [13], and propose an algorithm to solve
the aforementioned Schr odinger equation using a quantum computer. The methodology develops a hybrid
algorithm, where part of the computations can be run on a quantum computer, while the remaining tasks are
solved on a classical machine. We describe in detail in Section 4 the actual implementation of the algorithm,
Date : February 8, 2021.
2010 Mathematics Subject Classication. 35Q40, 91G20, 91G80.
Key words and phrases. quantum algorithms, option pricing, PDE, Schr odinger equation.
The views and opinions expressed here are the authors' and do not represent the opinions of their employers. They are not
responsible for any use that may be made of these contents. No part of this presentation is intended to inuence investment
decisions or promote any product or service. The authors would like to thank Alexei Kondratyev for stimulating discussions.
1arXiv:1912.02753v2  [q-fin.CP]  4 Feb 2021",2019-12-05T17:42:51Z,abstra  a  schools our scar   quantum uti quantitative nance introduprici matmatical nance w   unfortunately basket w recently ined quantum uti be utsleymamaishow since  quantum revolutsevl  quantitative nance amplitu estimat   quantum quantitative nance scar t ise schools scar quantum isescar t  sedate ruary matmatics subje ass key scar t ty no t alexa d rat ye  
paper_qf_27.pdf,2,A Quantum algorithm for linear PDEs arising in Finance,"  We propose a hybrid quantum-classical algorithm, originated from quantum
chemistry, to price European and Asian options in the Black-Scholes model. Our
approach is based on the equivalence between the pricing partial differential
equation and the Schrodinger equation in imaginary time. We devise a strategy
to build a shallow quantum circuit approximation to this equation, only
requiring few qubits. This constitutes a promising candidate for the
application of Quantum Computing techniques (with large number of qubits
affected by noise) in Quantitative Finance.
","2 FILIPE FONTANELA, ANTOINE JACQUIER, AND MUGAD OUMGARI
with a particular emphasis on the quantum circuit. The numerical results are presented in Section 5, and
we show that the shallow quantum circuit developed here is able to price European and arithmetic Asian
options in the Black-Scholes model with very good accuracy.
2.Black-Scholes and the Schr odinger equation
The Black-Scholes model is at the core of nancial modelling and assumes that, under a given risk-neutral
measure, the underlying stock price process ( St)t0satises the stochastic dierential equation
(2.1)dSt
St=rdt+dWt; fort0;
whereWis a standard Brownian motion on a given ltered probability space, r2Ris the instantaneous
risk-free rate, and  > 0 is the instantaneous volatility. There is a vast literature extending this model in
many dierent directions and using a plethora of numerical techniques, ranging from PDEs to Monte Carlo
and Fourier methods. We are here chiey interested in developing a quantum-based algorithm for PDEs,
and will hence ignore numerical methods apart from the PDE approach, considering only European nancial
derivatives, with no early exercise.
2.1.Pricing PDEs. For a European Call option with payo f(ST) = max(ST",2019-12-05T17:42:51Z,t sea  schools  schools scar t  schools st st st  wis brownish is tre   courier  aprici for acall
paper_qf_27.pdf,3,A Quantum algorithm for linear PDEs arising in Finance,"  We propose a hybrid quantum-classical algorithm, originated from quantum
chemistry, to price European and Asian options in the Black-Scholes model. Our
approach is based on the equivalence between the pricing partial differential
equation and the Schrodinger equation in imaginary time. We devise a strategy
to build a shallow quantum circuit approximation to this equation, only
requiring few qubits. This constitutes a promising candidate for the
application of Quantum Computing techniques (with large number of qubits
affected by noise) in Quantitative Finance.
","A QUANTUM ALGORITHM FOR LINEAR PDES ARISING IN FINANCE 3
2.2.Schr odinger's formulation. Similarly to [7], we now translate the pricing PDEs (2.4) and (2.6) into
linear Schr odinger equations, classical in quantum mechanics. The Wick rotation =",2019-12-05T17:42:51Z,scar simarly scar t wick
paper_qf_27.pdf,4,A Quantum algorithm for linear PDEs arising in Finance,"  We propose a hybrid quantum-classical algorithm, originated from quantum
chemistry, to price European and Asian options in the Black-Scholes model. Our
approach is based on the equivalence between the pricing partial differential
equation and the Schrodinger equation in imaginary time. We devise a strategy
to build a shallow quantum circuit approximation to this equation, only
requiring few qubits. This constitutes a promising candidate for the
application of Quantum Computing techniques (with large number of qubits
affected by noise) in Quantitative Finance.
","4 FILIPE FONTANELA, ANTOINE JACQUIER, AND MUGAD OUMGARI
a 2-qubit system, corresponding to a Hilbert space of dimension 4 can be viewed as the tensor product of two
Hilbert spaces, each of dimension 2, and its basis being spanned by fj0i
j0i;j0i
j1i;j1i
j0i;j1i
j1ig,
usually written in the denser form fj00i;j01i;j10i;j11ig.
Quantum logic gates are reversible quantum circuits operating on quantum states, and are represented
as unitary matrices. We shall here make use of several particular gates repeatedly, which can be easily
represented in a 1-qubit state as the following matrices in M2(the set of square 2 2 matrices in C):
(2.11)X: X-Pauli gate:0 1
1 0
Y: Y-Pauli gate:0",2019-12-05T17:42:51Z,gbert gbert quantum  paul paul
paper_qf_27.pdf,5,A Quantum algorithm for linear PDEs arising in Finance,"  We propose a hybrid quantum-classical algorithm, originated from quantum
chemistry, to price European and Asian options in the Black-Scholes model. Our
approach is based on the equivalence between the pricing partial differential
equation and the Schrodinger equation in imaginary time. We devise a strategy
to build a shallow quantum circuit approximation to this equation, only
requiring few qubits. This constitutes a promising candidate for the
application of Quantum Computing techniques (with large number of qubits
affected by noise) in Quantitative Finance.
","A QUANTUM ALGORITHM FOR LINEAR PDES ARISING IN FINANCE 5
Assumption 3.1.
(i) Every unitary gate in the algorithm depends on a single parameter.
(ii) The decomposition bH=PN
i=1ihiholds, for real numbers iand tensor products hiof Pauli matrices.
Assumption 3.1(i) is purely for convenience, as unitary gates with multiple parameters can be decomposed
into single-parameter ones, but it allows us to write the derivative of the unitary gates
@kUk(k) =NX
i=1fk;iUk(k)k;i; for everyk= 1:::;N;
wherek;iare one-qubit or two-qubit unitary operators and fk;ia scalar parameter, so that
@kj()i=NX
i=1fk;iek;ij initi; whereek;i=S",2019-12-05T17:42:51Z,assumptevery t paul assumptuk uk
paper_qf_27.pdf,6,A Quantum algorithm for linear PDEs arising in Finance,"  We propose a hybrid quantum-classical algorithm, originated from quantum
chemistry, to price European and Asian options in the Black-Scholes model. Our
approach is based on the equivalence between the pricing partial differential
equation and the Schrodinger equation in imaginary time. We devise a strategy
to build a shallow quantum circuit approximation to this equation, only
requiring few qubits. This constitutes a promising candidate for the
application of Quantum Computing techniques (with large number of qubits
affected by noise) in Quantitative Finance.
","6 FILIPE FONTANELA, ANTOINE JACQUIER, AND MUGAD OUMGARI
4.Hybrid Quantum-Classical Algorithm
We now design the quantum imaginary time algorithm described in Section 3 to price nancial derivatives.
The main challenge is to design an ansatz circuit able to represent the value of the option from expiry down
to valuation date. We employ the sequence of gates described by the quantum circuit depicted in Figure 1.
There, a line represents a qubit, whereas a squared box denotes the action of a quantum gate on the qubit on
..|0⟩...l6
..X....l5
. Ry(θ1) ............l4
......l1
.. Ry(θ8) .............l3
....................l2
...............|0⟩...l6
..X....l5
. Ry(θ1) .
|0⟩....
H....
Ry(θ2)...l4
..
Ry(θ5)....l1
.. Ry(θ8) ..
|0⟩....
H....
Ry(θ3)...l3
..
Ry(θ6).....
Ry(θ9)..
|0⟩....
H....
Ry(θ4)...l2
..
Ry(θ7).....
Ry(θ10).....
Ry(θ11).
Figure 1. Ansatz implemented for European and Asian Call options. The vector is composed
by the angles of each rotation gate. The notations l1;:::;l 6indicate the dierent layers.
this line. The circuit is to be read from left (inputs) to right (outputs). A controlled gate (for example Ry(5))
is shown with a lled black circle on the control qubit line, and the usual symbol for the gate is written
on the target qubit, with a line connecting the two. The quantum circuit has 4 qubits only, and thus our
strategy is able to recover Call prices with a resolution of 24= 16 points. The ansatz in Figure 1 has a
layerl6composed by HandXgates (2.11). In the next layer l5, a sequence of Rygates is applied to each
qubit. Each such gate has a free parameter that needs to be calculated given some desired initial condition,
and these parameters vary in time, as described above. The layers l4,l3andl2, composed by controlled Rc
y
gates, and a new layer l1, similar to l5, are added in order to give more exibility to the approximation, and
thus achieve dierent and more involved values of j()i. In the present analysis, we increase or reduce
the complexity of the ansatz circuit by adding or removing new unit cells, i.e. layers l4,l3,l2andl1. The
optimal conguration is obtained when j(0iis able to represent j (0)iwith good accuracy, and with as
few free parameters as possible. One should note that the ansatz is composed of gates with rotations along
theyaxis only. The motivation for using Rygates is due to its real-valued representation, which is desirable
asj ()iin (3.1) is itself real. However, other approaches might also be valid, using for instance RxandRz
gates, at the cost of computing the real part of j()iat the end of the simulations.
Mathematically, a quantum circuit is described by means of a series of matrix operations; the ansatz in
Figure 1 is composed of X,H, and Rysingle-qubits gates, and their mathematical representations are given
by the matrices in (2.11). The controlled rotation gate Rc
yis implemented using two qubits, and its matrix
representation reads
Rc
y() :=0
BB@1 0 0 0
0 1 0 0
0 0 cos",2019-12-05T17:42:51Z,brid quantum assical algorithm  set   tre ry ry ry ry ry ry ry ry ry ry ry ry ry  aat a call t t t ry t call t  hand gatiry gateat rc it one t ry gatand rz matmatically  ry se t rc rc
paper_qf_27.pdf,7,A Quantum algorithm for linear PDEs arising in Finance,"  We propose a hybrid quantum-classical algorithm, originated from quantum
chemistry, to price European and Asian options in the Black-Scholes model. Our
approach is based on the equivalence between the pricing partial differential
equation and the Schrodinger equation in imaginary time. We devise a strategy
to build a shallow quantum circuit approximation to this equation, only
requiring few qubits. This constitutes a promising candidate for the
application of Quantum Computing techniques (with large number of qubits
affected by noise) in Quantitative Finance.
","A QUANTUM ALGORITHM FOR LINEAR PDES ARISING IN FINANCE 7
and the combined eect of El1andEl2is the matrix multiplication El1;l2:=El1El2. This process is
repeated through the whole quantum circuit, and the resulting eect of the six layers in Figure 1 reads
El1;:::;l 6=6Y
i=1Eli:
The nal output j Fiof the quantum circuit due to the input j initiis then computed as
j Fi=El1;:::;l 6j initi:
For example, the input j initiof the ansatz circuit in Figure 1 is a quantum state where all four qubits are j0i,
so that its vectorial representation is j initi=j0i
j0i
j0i
j0i=j0000i. Finally, note that if = 0, the
resulting statej()ibecomes a step function, since each gate Rybehaves as the identity operator I2. As
discussed above, the payo of each nancial product needs to be represented by the ansatz j(0)ibefore
starting the actual simulation. In practice, this representation is implemented by computing the values of 0
which best describes the initial conditions of the algorithm, that is
(4.1) 0= arg min
2RNfkj()i",2019-12-05T17:42:51Z,el el el el el   el eli t  of  el for  nally ry behas inf kj
paper_qf_27.pdf,8,A Quantum algorithm for linear PDEs arising in Finance,"  We propose a hybrid quantum-classical algorithm, originated from quantum
chemistry, to price European and Asian options in the Black-Scholes model. Our
approach is based on the equivalence between the pricing partial differential
equation and the Schrodinger equation in imaginary time. We devise a strategy
to build a shallow quantum circuit approximation to this equation, only
requiring few qubits. This constitutes a promising candidate for the
application of Quantum Computing techniques (with large number of qubits
affected by noise) in Quantitative Finance.
","8 FILIPE FONTANELA, ANTOINE JACQUIER, AND MUGAD OUMGARI
..|0⟩....H....X..............X.
|0⟩....
X....
Ry(θ1).............
Y.....H.....
Ry(θ8)..............................................|0⟩....H....X.
|0⟩....
H....
Y.....X.
|0⟩....
X....
Ry(θ1)....
Ry(θ2)....
Ry(θ5).....
Y.....H.....
Ry(θ8)..
|0⟩....
H....
Ry(θ3)....
Ry(θ6).....
Ry(θ9)..
|0⟩....
H....
Ry(θ4)....
Ry(θ7).....
Ry(θ10).....
Ry(θ11).
Figure 2. Quantum circuit able to measure A 2;8in (3.3). The symbol on the top right of the
Figure denotes measurement of the quantum state.
Hamiltonian bHrst needs to be decomposed according to Assumption 3.1(ii). Table 2 below summarises the
structure of the hybrid quantum-classical algorithm, where the labels (CC) and (QC) indicate which part of
the algorithm should be computed on a classical and on a quantum computer respectively.
Hybrid quantum-classical algorithm
input Ansatz circuit and initial condition 0
initialisation:
Dene the initial condition for the ansatz j(0)i
Dene the Hamiltonian bHand decompose it as bH=P
iihi
end
while <2T:
Compute the matrix A( ) (QC)
ifbHis time-dependent:
Update the Hamiltonian bH() and its decomposition bH() =P
iihi
end
Compute the vector C( ) (QC)
Update the new values of  (CC)
end
output!Price of the underlying derivative (in the original coordinates)
Table 2. Hybrid quantum-classical algorithm.
The current methodology only checks the quantum approximation accuracy at the payo level, i.e. veri-
fyingkj(0)i",2019-12-05T17:42:51Z,ry ry ry ry ry ry ry ry ry ry ry ry ry  quantum t  hamtoniaassumpttable brid aat   hamtoniahand ute his update hamtoniaute update price table brid t
paper_qf_27.pdf,9,A Quantum algorithm for linear PDEs arising in Finance,"  We propose a hybrid quantum-classical algorithm, originated from quantum
chemistry, to price European and Asian options in the Black-Scholes model. Our
approach is based on the equivalence between the pricing partial differential
equation and the Schrodinger equation in imaginary time. We devise a strategy
to build a shallow quantum circuit approximation to this equation, only
requiring few qubits. This constitutes a promising candidate for the
application of Quantum Computing techniques (with large number of qubits
affected by noise) in Quantitative Finance.
","A QUANTUM ALGORITHM FOR LINEAR PDES ARISING IN FINANCE 9
5.1.European Call Option. In the Black-Scholes model (2.1) with = 20%,S0=K= 100,T= 1, and
zero interest rate, we discretise the state space on logarithmic scale on an equidistant grid [ xmin;xmax]
[3:9;5] (or [Smin;Smax] = [50;150]). With four qubits, the discretisation represents j iusing 24= 16 points,
where the statesj Fi=j0000iandj Fi=j1111irepresent the solution respectively at xminandxmax. The
evolution ofj()ifrom expiry down to inception is computed using the approach described in Section 3.
The Hamiltonian is bH=1
2@xx(Section 2.2), and we discretise it using second-order nite dierences
1
22x0
BBBBBBB@",2019-12-05T17:42:51Z,acall opti schools sm imax with   t set hamtoniasen
paper_qf_27.pdf,10,A Quantum algorithm for linear PDEs arising in Finance,"  We propose a hybrid quantum-classical algorithm, originated from quantum
chemistry, to price European and Asian options in the Black-Scholes model. Our
approach is based on the equivalence between the pricing partial differential
equation and the Schrodinger equation in imaginary time. We devise a strategy
to build a shallow quantum circuit approximation to this equation, only
requiring few qubits. This constitutes a promising candidate for the
application of Quantum Computing techniques (with large number of qubits
affected by noise) in Quantitative Finance.
","10 FILIPE FONTANELA, ANTOINE JACQUIER, AND MUGAD OUMGARI
5.2.Asian Call Option. We use the same parameters as in Section 5.1, and the ansatz circuit in Figure 1.
The system (2.6) is computed on the equidistant grid [ ymin;ymax] = [",2019-12-05T17:42:51Z, call opt se t
paper_qf_27.pdf,11,A Quantum algorithm for linear PDEs arising in Finance,"  We propose a hybrid quantum-classical algorithm, originated from quantum
chemistry, to price European and Asian options in the Black-Scholes model. Our
approach is based on the equivalence between the pricing partial differential
equation and the Schrodinger equation in imaginary time. We devise a strategy
to build a shallow quantum circuit approximation to this equation, only
requiring few qubits. This constitutes a promising candidate for the
application of Quantum Computing techniques (with large number of qubits
affected by noise) in Quantitative Finance.
","A QUANTUM ALGORITHM FOR LINEAR PDES ARISING IN FINANCE 11
price to a Quantum-State which is eectively a wave-function. The wave function is solved by the Hybrid
Quantum and Classical Algorithm where a Quantum Circuit of imaginary time evolution is build with the
help of McLachlan's invariance Principle. The strategy is based on the PDE representation of the pricing
problem, and the link between the latter and its Schr odinger counterpart from quantum mechanics. The
results show that a shallow quantum circuit is able to represent European and Asian Call option prices
accurately, and therefore suggest that this approach is a promising candidate for the application of quantum
computing in Finance.
The main challenge of the present methodology is the requirement for an ansatz circuit and the corre-
sponding solution of an optimisation problem. More work is needed in the future to design an ecient ansatz
for more complex nancial products, or in the development of an ansatz-free approach. One of the most
promising application of this technique is for basket options, based on several stocks, and hence multidi-
mensional. Classical PDE methods suer from the so-called curse of dimensionality, making such pricing
problem cumbersome, or at least computationally intensive. Since its mathematical formulation is similar
to the simulation of large-scale quantum mechanical systems governed by systems of Schr odinger equations,
we believe that a modied version of our algorithm is applicable there, leveraging the power of quantum
computing, and we leave this investigation for the next step.
Appendix A.Initial and terminal conditions of 
European Call Option Asian Call Option
= 0=2T  = 0=2T
13.142 3.203 3.141 3.458
24.173 4.149 3.387 3.153
31.392 2.278 1.282 1.108
43.713 3.512 0.927 0.683
52.399 2.400 4.946 4.858
60.935 0.512 0.257 0.130
72.196 2.578 2.937 4.062
85.014 5.024 6.283 6.759
92.736 2.405 4.304 3.993
101.477 2.406 1.632 2.423
114.472 4.271 5.103 4.859
123.415 3.375 4.959 4.824
136.283 6.331 4.409 3.063
144.244 4.224 0.592 0.236
154.711 4.347 6.283 6.599
160.717 0.510 0.133 -0.693
171.741 2.357 3.957 3.923
181.158 0.956 0.334 0.090
192.531 2.443 0.489 2.035
205.705 5.232 4.940 4.582
213.525 3.184 0.909 0.935
224.582 4.716 3.141 3.118
232.465 2.706 5.993 4.807
240.098 -1.154 2.807 3.324
255.018 4.817 2.783 2.539
Table 3. Values of at initial and terminal times.",2019-12-05T17:42:51Z,quantum state t brid quantum assical algorithm quantum circuit mc lachlaprinciple t scar t a call nance t  one assical since scar  initial acall opt call opttable values
paper_qf_27.pdf,12,A Quantum algorithm for linear PDEs arising in Finance,"  We propose a hybrid quantum-classical algorithm, originated from quantum
chemistry, to price European and Asian options in the Black-Scholes model. Our
approach is based on the equivalence between the pricing partial differential
equation and the Schrodinger equation in imaginary time. We devise a strategy
to build a shallow quantum circuit approximation to this equation, only
requiring few qubits. This constitutes a promising candidate for the
application of Quantum Computing techniques (with large number of qubits
affected by noise) in Quantitative Finance.
","12 FILIPE FONTANELA, ANTOINE JACQUIER, AND MUGAD OUMGARI
Appendix B.Complete ansatz circuit
..|0⟩....X....Ry(θ1) .
|0⟩....
H....
Ry(θ2)....
Ry(θ5).....Ry(θ8) .
|0⟩....
H....
Ry(θ3)....
Ry(θ6).....
Ry(θ9)....
Ry(θ12).....Ry(θ15) .
|0⟩....
H....
Ry(θ4)....
Ry(θ7).....
Ry(θ10)....
Ry(θ13).....
Ry(θ16)....
Ry(θ19).....Ry(θ22) .....
Ry(θ11)....
Ry(θ14).....
Ry(θ17)....
Ry(θ20).....
Ry(θ23).....
Ry(θ18)....
Ry(θ21).....
Ry(θ24).....
Ry(θ25)...|0⟩....X....Ry(θ1) .
|0⟩....
H....
Ry(θ2)....
Ry(θ5).....Ry(θ8) .
|0⟩....
H....
Ry(θ3)....
Ry(θ6).....
Ry(θ9)....
Ry(θ12).....Ry(θ15) .
|0⟩....
H....
Ry(θ4)....
Ry(θ7).....
Ry(θ10)....
Ry(θ13).....
Ry(θ16)....
Ry(θ19).....Ry(θ22) .....
Ry(θ11)....
Ry(θ14).....
Ry(θ17)....
Ry(θ20).....
Ry(θ23).....
Ry(θ18)....
Ry(θ21).....
Ry(θ24).....
Ry(θ25).
Figure 5. Ansatz circuit explained in Section 5. The circuit consists of 25 Rygates.",2019-12-05T17:42:51Z, e ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry ry  aat set ry gates
paper_qf_27.pdf,13,A Quantum algorithm for linear PDEs arising in Finance,"  We propose a hybrid quantum-classical algorithm, originated from quantum
chemistry, to price European and Asian options in the Black-Scholes model. Our
approach is based on the equivalence between the pricing partial differential
equation and the Schrodinger equation in imaginary time. We devise a strategy
to build a shallow quantum circuit approximation to this equation, only
requiring few qubits. This constitutes a promising candidate for the
application of Quantum Computing techniques (with large number of qubits
affected by noise) in Quantitative Finance.
","A QUANTUM ALGORITHM FOR LINEAR PDES ARISING IN FINANCE 13
References
[1] P. Benio. The computer as a physical system: a microscopic Quantum Mechanical Hamiltonian model of computers as
represented by Turing machines. Journal of Statistical Physics ,22(563), 1980.
[2] S.C. Benjamin and Y. Li. Ecient variational quantum simulator incorporating active error minimization. Physical Re-
view X ,7(2), 2017.
[3] X. Yuan, S. Endo, Q. Zhao, Y. Li, S.C. Benjamin. Theory of variational quantum simulation. Quantum ,3(1): 191, 2019.
[4] F. Black and M. Scholes. The pricing of options and corporate liabilities. Journal of Political Economy ,81(3): 637-654,
1973.
[5] G. Brassard, P. Hoyer, M. Mosca and A. Tapp. Quantum amplitude amplication and estimation. AMS Contemporary
Mathematics ,305: 53-74, 2002.
[6] C. Brown, J.C. Handley, C.T. Lin and K.J. Palmer. Partial dierential equations for Asian option prices. Quantitative
Finance ,16(3), 447-460, 2016.
[7] M. Contreras,, R. Pellicer, M. Villena and A. Ruiz. A quantum model of option pricing: When Black{Scholes meets
Schr odinger and its semi-classical limit. Physica A: Statistical Mechanics and its Applications ,389(23): 5447-5459, 2010.
[8] D. Deutsch. Quantum theory, the Church-Turing principle and the universal quantum computer. Proceedings of the Royal
Society A ,400(1818): 97-117, 1985.
[9] P. Dirac. Note on exchange phenomena in the Thomas atom. Math. Proc. Cambridge Phil. Society ,26(3): 376-385, 1930.
[10] S. Endo, I. Kurata and Y.O. Nakagawa. Calculation of the Green's function on near-term quantum computers. Phys. Rev.
Research ,2(033281), 2020 .
[11] R.P. Feynman. Simulating physics with computers. International Journal of Theoretical Physics ,21: 467-488, 1982.
[12] J. Frenkel. Wave mechanics - advanced general theory. Oxford University Press, 1934.
[13] S. McArdle, T. Jones, S. Endo, Y. Li, S.C. Benjamin and X. Yuan. Variational ansatz based quantum simulation of
imaginary time evolution. Quantum Information ,5(1), 2019.
[14] A. McLachlan. A variational solution of the time-dependent Schr odinger equation. Molecular Physics ,8(1), 39-44, 1964.
[15] Y.I. Manin. Computable and Noncomputable. Sov.Radio : 13-15, 1980.
[16] A. Martin, B. Candelas, A. Rodriguez-Rozas, J.D. Martin-Guerrero, X. Chen, L. Lamata, R. Orus, E. Solano and M. Sanz.
Towards pricing nancial derivatives with an IBM quantum computer. arXiv: 1904.05803, 2019.
[17] M. Motta, C. Sun, A. T.K. Tan, M.J. O'Rourke, E. Ye, A.J. Minnich, F. Brandao and G.K Chan. Determining eigenstates
and thermal states on a quantum computer using quantum imaginary time evolution. Nature Physics , 2019.
[18] R. Orus, S. Mugel and E. Lizaso. Quantum computing for nance: Overview and prospects. Reviews in Physics ,4, 2019.
[19] P. Rebentrost, B. Gupt and T.R. Bromley. Quantum computational nance: Monte Carlo pricing of nancial derivatives.
Physical Review A ,98(2), 2018.
[20] P.W. Shor. Algorithms for quantum computation: discrete logarithms and factoring. Proceedings 35th Annual Symposium
on Foundations of Computer Science , 1994.
[21] N. Stamatopoulos, D.J. Egger, Y. Sun, C. Zoufal, R. Iten, N. Shen and S. Woerner. Option pricing using quantum
computers. Quantum ,4, 2020.
[22] J. Vecer. A new PDE approach for pricing arithmetic average Asian options. Journal of Comp. Finance ,4: 105-113, 2001.
Lloyds Banking Group plc, Commercial Banking, 10 Gresham Street, London, EC2V 7AE, UK
Email address :Filipe.Fontanela@lloydsbanking.com
Department of Mathematics, Imperial College London, and Alan Turing Institute
Email address :a.jacquier@imperial.ac.uk
Lloyds Banking Group plc, Commercial Banking, 10 Gresham Street, London, EC2V 7AE, UK
Email address :Mugad.Oumgari@lloydsbanking.com",2019-12-05T17:42:51Z,referencbe t quantum meical hamtonia journal statistical psics benjamili psical re yuaendo hao li benjamitory quantum  schools t journal political economy brass ard homos ca app quantum contemary matmatics browhandle lipalmer partial  quantitative nance contrs ll ice vle na ruiz w schools scar psics statistical meics applicatns utsquantum chur proceedis royal society dfrac note thomas math proc cambridge ph society endo karate nakagawa calculatgreeph ys rev researleymasimulati internatnal journal toical psics fr enne  oxford   mc ard le jonendo li benjamiyuavariatal quantum informatmc lachlascar molecular psics maiarable noarable sov rad marticancels rodruez road martuerrero cla mata or us solo sans towards  moo sutasource ye inibrand chatermini nature psics or us mug el la so quantum overview reviews psics re bent most up trolley quantum   psical review show algorithms proceedis annual symposium foundatns uter science st math poll os  er suzo fal it ewwoe rner optquantum ve cer  journal  nance lloyd banki group coercial banki graham street londoem articial intellence felipe montane la partment matmatics imial college londoala institute em articial intellence lloyd banki group coercial banki graham street londoem articial intellence mug ad ou gary
paper_qf_28.pdf,1,Deep Stochastic Optimization in Finance,"  This paper outlines, and through stylized examples evaluates a novel and
highly effective computational technique in quantitative finance. Empirical
Risk Minimization (ERM) and neural networks are key to this approach. Powerful
open source optimization libraries allow for efficient implementations of this
algorithm making it viable in high-dimensional structures. The free-boundary
problems related to American and Bermudan options showcase both the power and
the potential difficulties that specific applications may face. The impact of
the size of the training data is studied in a simplified Merton type problem.
The classical option hedging problem exemplifies the need of market generators
or large number of simulations.
","Deep Stochastic Optimization in Finance
A. Max Reppen∗H. Mete Soner†Valentin Tissot-Daguette‡
May 11, 2022
Abstract
This paper outlines, and through stylized examples evaluates a novel and highly eﬀective
computational technique in quantitative ﬁnance. Empirical Risk Minimization (ERM) and
neural networks are key to this approach. Powerful open source optimization libraries allow
for eﬃcient implementations of this algorithm making it viable in high-dimensional structures.
The free-boundary problems related to American and Bermudan options showcase both the
power and the potential diﬃculties that speciﬁc applications may face. The impact of the
size of the training data is studied in a simpliﬁed Merton type problem. The classical option
hedging problem exempliﬁes the need of market generators or large number of simulations.
Key words: ERM, Neural Networks, Hedging, American Options.
Mathematics Subject Classiﬁcation: 91G60, 49N35, 65C05.
1 Introduction
Readily available and eﬀective optimization libraries such as Tensorﬂow or Pytorch now
make previously intractable regression type of algorithms over hypothesis spaces with large
number of parameters computationally feasible. In the context of stochastic optimal control
and nonlinear parabolic partial diﬀerential equations which have such representations, these
exciting advances allow for a highly eﬃcient computational method. This algorithm, which
we call deep empirical risk minimization , proposed by E & Han [ 21] and E, Jentzen & Han
[22], uses artiﬁcial neural networks to approximate the feedback actions which are then
trained by empirical risk minimization. As stochastic optimal control is the unifying umbrella
for almost all hedging, portfolio or risk management problems, and many models in ﬁnancial
economics, this method is also highly relevant for quantitative ﬁnance.
Although artiﬁcial neural networks as approximate controls are widely used in optimal
control and reinforcement learning [ 6], deep empirical risk minimization simulates directly the
system dynamics and does not necessarily use dynamic programming. It aims to construct
optimal actions and values oﬄine by using the assumed dynamics and the rewards structure,
and often uses market generators to simulate large training data sets. This key diﬀerence
between reinforcement learning and the proposed algorithm ushers in essential changes to
their implementations and analysis as well.
∗Questrom School of Business, Boston University, Boston, MA, 02215, USA, email: amreppen@bu.edu .
†Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ, 08540,
USA, email: soner@princeton.edu
‡Department of Operations Research and Financial Engineering, Princeton University, Princeton, NJ, 08540,
USA, email: v.tissot-daguette@princeton.edu
1arXiv:2205.04604v1  [q-fin.CP]  9 May 2022",2022-05-09T23:57:47Z,ep stochastic optiatnance max re ppemeta so ner valentine tis sot da guet te may abstra  emical risk miniatful t bermuda t mortot key neural networks edgi optns matmatics subje ass introduready tensor py tori hajetzehaas although it  t rom schobusiness bosto bostopartment oatns researnancial eineeri princeto princetopartment oatns researnancial eineeri princeto princeto may
paper_qf_28.pdf,2,Deep Stochastic Optimization in Finance,"  This paper outlines, and through stylized examples evaluates a novel and
highly effective computational technique in quantitative finance. Empirical
Risk Minimization (ERM) and neural networks are key to this approach. Powerful
open source optimization libraries allow for efficient implementations of this
algorithm making it viable in high-dimensional structures. The free-boundary
problems related to American and Bermudan options showcase both the power and
the potential difficulties that specific applications may face. The impact of
the size of the training data is studied in a simplified Merton type problem.
The classical option hedging problem exemplifies the need of market generators
or large number of simulations.
","Reppen, Soner & Tissot-Daguette Deep Stochastic Optimization
Our goal is to outline this demonstrably eﬀective methodology, assess its strengths
and potential shortcomings, and also showcase its power through representative examples
from ﬁnance. As veriﬁed in its numerous applications, deep empirical risk minimization
is algorithmically quite ﬂexible and handles well a large class of high-dimensional models,
even non-Markovian ones, and adapts to complex structures with ease. To further illustrate
and evaluate these properties, we also study three classical problems of ﬁnance with this
approach. Additional examples from nonlinear partial diﬀerential equations and stochastic
optimal control are given in the recent survey articles of Fecamp, Mikael & Warin [ 15] and
Germain, Pham & Warin [19]. They also provide an exhaustive literature review.
Our ﬁrst class of examples is the American and Bermudan options. The analysis of these
instruments oﬀer many-faceted complex experiments through which one appreciates the
potentials and the challenges. In a series of papers, Becker et al., [4,5] bring forth a complete
analysis with computable theoretical upper bounds through its known convex dual. They
also obtain inspiring computational results in high dimensional problems such as Bermudan
max-call options with 500underlyings. Akin to deep empirical risk minimization is the
seminal regression on Monte-Carlo methods that were developed for the American options
by Longstaﬀ & Schwartz [ 29] and Tsitsiklis & van Roy [ 38]. Many of their reﬁnements, as
delineated in the recent article of Ludkovski [ 30], make them not only textbook topics but
also standard industrial tools. Still, the deep empirical risk minimization approach to optimal
stopping has some advantages over them, including its eﬀortless ability to incorporate market
details and frictions, and to operate in high-dimensions as caused by state enlargements
needed for path-dependent claims. An example of the latter is the American options with
rough volatility models as studied by Chevalier at. al.[11]. They require inﬁnite-dimensional
spaces and their numerical analysis is given in Bayer et al.[3]. Other similar examples can
be found in [4, 5].
For interpretability of our results, we base the stopping decisions on a surface separating
the ‘continuation’ and ‘stopping’ regions, and approximate directly this boundary - often
called the free boundary - by an artiﬁcial neural network. Similarly for the same reason,
Ciocan & Mi˘ sic [ 12] compute the free boundary directly, by using tree based methods. An
additional beneﬁt of this geometric approach to American options is to construct a tool
that can also be eﬀectively used for ﬁnancial problems with discontinuous decisions such
as regime-switching or transaction costs, as well as non-ﬁnancial applications. Indeed, the
computation of the free-boundary is an interesting problem independent of applications to
ﬁnance. Recently, deep Galerkin method [ 37] is used to compute the free boundary arising in
the classical Stephan problem of melting ice [39].
Our numerical results, reported in the subsections 4.5 and 4.6 below, show that nat-
ural problem speciﬁc modiﬁcations enable the general approach to yield excellent results
comparable to the ones achieved in [ 4,5]. The free boundaries that we compute for the
two-dimensional max-call options also compare to the results by Broadie & Detemple [ 8] and
by Detemple [ 14]. An important step in our approach is to replace the stopping rule given
by the sharp interface by a relaxed stopping rule given by a fuzzy boundary as described in
the subsection 4.4. Further analysis and the results of our free-boundary methodology are
given in our future manuscript [32].
Our second example of classical quadratic hedging [ 36] is undoubtedly one of the most
compelling benchmark for any computational technique in quantitative ﬁnance. Thus, the
evaluation of the deep empirical risk minimization algorithm on this problem, imparts valuable
insights. Readily, Bühler et al., [9,10] use this approach for multidimensional Heston type
models, delivering convincing evidence for the ﬂexibility and the scope of the algorithm,
particularly in high-dimensions. Huré et al., [25] and Bachouch et al., [2] also obtain equally
remarkable results for the stochastic optimal control using empirical minimization as well
as other hybrid algorithms partially based on dynamic programming. Extensive numerical
2",2022-05-09T23:57:47Z,re ppeso ner tis sot da guet te ep stochastic optiatour as mark iato additnal fe camp michael war erm articial intellence ham war ity our bermuda t ibecker ty bermuda aki  lo st schwartz ts its  roy many lud v  stl acvalier ty bayer otr for simarly c cami ained recently gale rk istepour t broad ie  temple  temple afurtr our  ready stohur basuextensive
paper_qf_28.pdf,3,Deep Stochastic Optimization in Finance,"  This paper outlines, and through stylized examples evaluates a novel and
highly effective computational technique in quantitative finance. Empirical
Risk Minimization (ERM) and neural networks are key to this approach. Powerful
open source optimization libraries allow for efficient implementations of this
algorithm making it viable in high-dimensional structures. The free-boundary
problems related to American and Bermudan options showcase both the power and
the potential difficulties that specific applications may face. The impact of
the size of the training data is studied in a simplified Merton type problem.
The classical option hedging problem exemplifies the need of market generators
or large number of simulations.
","Reppen, Soner & Tissot-Daguette Deep Stochastic Optimization
experimentations are also carried out by Fecamp et. al. [15] in an incomplete market that
models the electricity markets containing a non-tradable volume risk [ 40]. Ruf & Wang [ 33]
apply this approach to market data of S&P 500 and Euro Stoxx 50 options. In all these
applications, variants of the quadratic hedging error is used as the loss function.
To highlight the essential features, we focus on a simple frictionless market with Heston
dynamics, and consider a vanilla Call option with quadratic loss. In this setting, we analyze
both the pure hedging problem by ﬁxing the price at a level lower than its known value and
also the pricing and hedging problem by training for the price as well. By the well-known
results of Schweizer [ 35,36] and Föllmer & Schweizer [ 18], we know that the minimizer of
the analytical problem in the continuous time is equal to the price obtained by Heston [ 23]
as the discounted expected value under the risk neutral measure with the chosen market risk
of volatility risk. Our numerical computations verify these results as well.
As the ﬁnal example, we report the results of an accompanying paper of the ﬁrst two
authors [ 31] for a stylized Merton type problem. With simulated data, the numerical results
once again showcase the ﬂexibility and the scope of the algorithm, in this problem as well. We
also observe that in data-poor environments, the artiﬁcial neural networks have an amazing
capability to over-learn the data causing poor generalization. This is one of the key results
of [31] which was also observed in [ 28]. Despite this potential, as demonstrated by our
experiments, continual data simulation can overcome this diﬃculty swiftly.
In this paper, we only discuss the properties of the algorithms that are variants of the
deep empirical risk minimization. The use of artiﬁcial neural networks or statistical machine
learning is of course not limited to this approach. Indeed, starting from [ 26] and especially
recently, artiﬁcial neural networks have been extensively employed in quantitative ﬁnance.
In particular, kernel methods are applied to portfolio valuation in [ 7], and to the density
estimation in [16]. Gonon et. al. [20] use the methodology to study an equilibrium problem
in a market with frictions. For further results and more information, we refer to the recent
survey of Ruﬀ & Wang [34] and the references therein.
The paper is organized as follows. The next section formulates the control problem
abstractly covering many important ﬁnancial applications. The description of the algorithm
follows. Section 4 is about the American and Bermudan options. The quadratic hedging
problem is the topic of Section 5. Finally, the numerical examples related to the simple
Merton problem are discussed in Section (6).
Acknowledgements. Research of the second and the third authors was partially supported
by the National Science Foundation grant DMS 2106462.
2 Abstract problem
Following the formulation of [ 31], we start with a Z⊂Rdvalued stochastic process Zon a
probability space Ω. This process drives the dynamics of the problem, and in all ﬁnancial
examples that we consider it is the related to the stock returns. For that reason, in the
sequel, we refer to Zas the returns process , although they may be logarithmic returns in
some cases. Investment or hedging decisions are made at Nuniformly spaced discrete time
points labeled by k= 0,1,...,Nand let
T:={0,1,...,N−1},/hatwideT:={0,1,...,N}.
We use the notation Z= (Z1,...,ZN)and setZ0= 0. We further let F= (Ft)t=0,...,Nbe the
ﬁltration generated by the process Z. The F-adapted controlled state process Xtakes values
in another Euclidean space Xand it may include all or some components of the uncontrolled
returns process Z.
3",2022-05-09T23:57:47Z,re ppeso ner tis sot da guet te ep stochastic optiatfe camp ru wa euro to xx ito stocall iby schiz schiz stoour as mortowith   spite it ined o nofor ru wa t t t sebermuda t senally mortoseackledgement researnatnal science foundatabstra followi rd valued o for as investment unormly and   ft be t takeuiaand
paper_qf_28.pdf,4,Deep Stochastic Optimization in Finance,"  This paper outlines, and through stylized examples evaluates a novel and
highly effective computational technique in quantitative finance. Empirical
Risk Minimization (ERM) and neural networks are key to this approach. Powerful
open source optimization libraries allow for efficient implementations of this
algorithm making it viable in high-dimensional structures. The free-boundary
problems related to American and Bermudan options showcase both the power and
the potential difficulties that specific applications may face. The impact of
the size of the training data is studied in a simplified Merton type problem.
The classical option hedging problem exemplifies the need of market generators
or large number of simulations.
","Reppen, Soner & Tissot-Daguette Deep Stochastic Optimization
In the ﬁnancial examples, the state includes the marked-to-market value of the portfolio
and maybe other relevant quantities. In a path-dependent structure, we would be forced to
include not only the current value of the portfolio and the return, but also some past values
as well (theoretically, we need to keep all past values but in practice one stops at a ﬁnite
point). In illiquid markets, the portfolio composition is also included into the state and even
the order-book might be considered. We assume that the state is appropriately chosen so
that the relevant decisions are feedback functions of the state alone and we optimize over
feedback decisions or controls. Thus, even if the original problem is low dimensional but
non-Markov, one is forced to expand the state resulting in a high-dimensional problem.
We denote the set of possible actions or decisions by A. While the main decision variable
is the portfolio composition, several other quantities such as the speed of the change of the
portfolio could be included. Then, a feedback decision is a continuous function
π:T×X/mapsto→A .
We letCbe the set of all such functions. Given π∈C, the time evolution of the state vector
is then completely described as a function of the returns process Z. Hence, all optimization
problems that we consider have the following form,
minimize v(π) :=E[/lscript(π,Z) ],over allπ∈C,
where/lscriptis a nonlinear function. We refer the reader to [ 31] for a detailed derivation of the
above formulation and several examples. Although the cost function /lscriptcould be quite complex
to express analytically, it can be easily evaluated by simply mimicking the dynamics of the
ﬁnancial market. Hence, computationally they are straight-forward to compute and all details
of the markets can be easily coded into it.
The goal is to compute the optimal feedback decision, π∗, and the optimal value v∗,
π∗∈argminπ∈Cv(π), v∗:= inf
π∈Cv(π) =v(π∗).
When the underlying dynamics is Markovian and the cost functional has an additive structure,
the above formulation of optimization over feedback controls is equivalent to the standard
formulation which considers the larger class of all adapted processes, sometimes called
open loop controls [ 17]. However, even without this equivalence, the minimization over the
smaller class of feedback controls is a consistent and a well-deﬁned problem, and due to their
tractability, feedback controls are widely used. In this manuscript, we implicitly assume that
the problem is well chosen and the goal is to construct the best feedback control.
3 The algorithm
In this section, we describe the deep empirical minimization algorithm proposed by Weinan
E, Jiequn Han , and Arnulf Jentzen in [21, 22].
AbatchB:={Z1,...,Zm}, with a size of m, is an i.i.d. realization of the returns process
Z, whereZi= (Zi
1,...,Zi
N)for eachi. We set
L(π,B) :=1
mm/summationdisplay
i=1/lscript(π,Zi),
and consider a set of artiﬁcial neural networks parametrized by,
N={Φ(·;θ) :T×X/mapsto→A :θ∈Θ}⊂C.
4",2022-05-09T23:57:47Z,re ppeso ner tis sot da guet te ep stochastic optiatiii  mark  w t cbe givence  although nce t cv cv wmark iait i iajie qu haarulf jetzebatzm zi zi zi  zi
paper_qf_28.pdf,5,Deep Stochastic Optimization in Finance,"  This paper outlines, and through stylized examples evaluates a novel and
highly effective computational technique in quantitative finance. Empirical
Risk Minimization (ERM) and neural networks are key to this approach. Powerful
open source optimization libraries allow for efficient implementations of this
algorithm making it viable in high-dimensional structures. The free-boundary
problems related to American and Bermudan options showcase both the power and
the potential difficulties that specific applications may face. The impact of
the size of the training data is studied in a simplified Merton type problem.
The classical option hedging problem exemplifies the need of market generators
or large number of simulations.
","Reppen, Soner & Tissot-Daguette Deep Stochastic Optimization
Instead of searching for a minimizer in C, we look for a computable solution in the smaller
setN. That is, numerically we approximate the following quantities:
θ∗:=θ∗
N∈argminθ∈Θv(Φ(·;θ)),
vN:= inf
θ∈Θv(Φ(·;θ)) =v(Φ(·;θ∗)).
The classical universal approximation result for artiﬁcial neural networks [ 13,24] imply,
under some natural structural assumptions on the function /lscript, thatvNapproximates v∗as the
networks gets larger as proved in [ 31](Theorem 5). This also implies that the performance of
the trained feedback control Φ(·;θ∗)is almost optimal.
The pseudocode of the algorithm to compute θ∗andv∗is the following,
•Initializeθ∈Θ;
•Optimize by stochastic gradient descent: forn= 0,1,...:
–Generate a batch B:={Z1,...,Zm},
–Compute the derivative d:=∇θL(Φ(·;θ),B);
–Updateθ←θ−κd.
•Stopifnis suﬃciently large and the improvement of the value is ‘small’.
In the above κis the learning rate and the stochastic gradient step is done through an
optimization library.
The data generation can be done through either an assumed and calibrated model, namely
a market generator, or by random samples from a ﬁxed ﬁnancial market data when suﬃcient
and relevant historical data is available. Although these two settings look similar, one may
get quite diﬀerent results in these two cases, even when the ﬁxed data set is large. One of
our goals is to better understand this dichotomy between these two data regimes and the
size of the data needed for reliable results. Theoretically, when the simulation capability is
not limited and data is continually generated, the above algorithm should yield the desired
minimizerθ∗and the corresponding optimal feedback decision Φ(·,θ∗). However, with a
ﬁxed data set, the global minimum over Nis almost always strictly less than v∗, and the
large enough networks will eventually gravitate towards this undesirable extreme point which
would be over-learning the data as already observed and demonstrated in [31].
4 Exercise boundary of American type options
American and Bermudan options are particularly central to any computational study in
quantitative ﬁnance as they pose diﬃcult and deep challenges, and they serve as an important
benchmarkforanynewnumericalapproach. Methodssuccessfulinthissettingoftengeneralize
to other problems as well. Indeed, the seminal regression on Monte-Carlo methods that were
developed for the American options by Longstaﬀ & Schwartz [ 29] and Tsitsiklis & van Roy
[38] have not only become industry standards in few years, but they have also shed insight into
other problems as well. Together with rich improvements developed over the past decades,
they can now handle many Markovian problem with ease. However, the key feature of these
algorithms is a projection onto a linear subspace, and this space must grow exponentially
with the dimension of the ambient space, making high-dimensional problems out of reach of
this otherwise powerful technique. Examples of such high-dimensional problems are ﬁnancial
instruments on many underlyings modeled with many parameters, path-dependent options,
or non-Markovian models, all requiring state enlargements and resulting in vast state spaces.
5",2022-05-09T23:57:47Z,re ppeso ner tis sot da guet te ep stochastic optiatinstead that t approximate torem  t initiative optie gente zm ute update st is it although one toically is exercise bermuda methods successful i sei oftegenlize ined   lo st schwartz ts its  roy togetr mark ias mark ian
paper_qf_28.pdf,6,Deep Stochastic Optimization in Finance,"  This paper outlines, and through stylized examples evaluates a novel and
highly effective computational technique in quantitative finance. Empirical
Risk Minimization (ERM) and neural networks are key to this approach. Powerful
open source optimization libraries allow for efficient implementations of this
algorithm making it viable in high-dimensional structures. The free-boundary
problems related to American and Bermudan options showcase both the power and
the potential difficulties that specific applications may face. The impact of
the size of the training data is studied in a simplified Merton type problem.
The classical option hedging problem exemplifies the need of market generators
or large number of simulations.
","Reppen, Soner & Tissot-Daguette Deep Stochastic Optimization
4.1 Problem
As well known the problem is to decide when to stop and collect the pay-oﬀ of a ﬁnancial
contract. Mathematically, for t∈/hatwideT={0,...,N}, letSt∈Rd
+be the stock value at the t-th
trading date and ϕ:Rd
+/mapsto→Rbe the pay-oﬀ function. With a given interest rate r>0, the
problem is
maximizev(τ) :=E/bracketleftbig
e−rτϕ(Sτ)/bracketrightbig
,
over all/hatwideT-valued stopping times τ. We use the ﬁltration generated by the stock price process
to deﬁne the stopping times. It is classical that the expectation is taken under the risk
neutral measure.
We assume that Sis Markov and the pay-oﬀ is a function of the current stock value.
When it is not, then we need to enlarge the state space. In factor models like Heston or SABR,
factor process is included. In non-Markovian models like the fractional Brownian motion,
past values the stock are added as in [ 3,4,5]. In look-back type options, the minimum or
the maximum of the stock process must be included in the state. We refer to [ 32] for the
details of these extensions.
We continue by deﬁning the price at all future points. Recall that the ﬁltration Fis
generated by the stock price process. Let Ξtbe the set of all F-stopping times with values
in{t,...,N}. At anyt∈/hatwideT,s∈Rd
+, letv(t,s)be the maximum value or the price of this
option when St=s, i.e.,
v(t,s) := max
τ∈ΞtE[e−r(τ−t)ϕ(Sτ)|St=s].
Then,v(N,·) =ϕand the the stopping region is given by
S:={(t,s) :v(t,s) =ϕ(s)}. (4.1)
Then the optimal stopping time is the ﬁrst time to enter the region S, i.e., the following
stopping time in Ξtis a maximizer of the above problem:
τ∗:= min{u∈{t,...,N}: (u,Su)∈S}.
Notice that as v(N,·) =ϕ, we always have (N,SN)∈S. This implies that τ∗is well-deﬁned
and is bounded by N.
Clearly, standard call or put options are the main examples. Many other examples that
are also covered in the above abstract setting, including the max-call option discussed below.
Example 4.1 (Max-Call) .LetSt= (S(1)
t,...,S(d)
t)∈Rd
+be a the stock process of dmany
dividend bearing stocks. We model it by a d-dimensional geometric Brownian motion with
constant mean-return rate and a covariance matrix. The pay-oﬀ of the max-call is given by,
ϕ(St) =/parenleftbigg
max
i=1,...,dS(i)
t−K/parenrightbigg+
,
where the strike Kis a given constant. We study this example numerically in subsection
4.6 below. One can also consider max-call options with factor models with an extended
state-space.
6",2022-05-09T23:57:47Z,re ppeso ner tis sot da guet te ep stochastic optiatproblem as matmatically st rd rd be with  it  is mark wistoimark iabrownish i  recall is  at rd st st ttsu notice   many  max call  st rd  brownish t st is  one
paper_qf_28.pdf,7,Deep Stochastic Optimization in Finance,"  This paper outlines, and through stylized examples evaluates a novel and
highly effective computational technique in quantitative finance. Empirical
Risk Minimization (ERM) and neural networks are key to this approach. Powerful
open source optimization libraries allow for efficient implementations of this
algorithm making it viable in high-dimensional structures. The free-boundary
problems related to American and Bermudan options showcase both the power and
the potential difficulties that specific applications may face. The impact of
the size of the training data is studied in a simplified Merton type problem.
The classical option hedging problem exemplifies the need of market generators
or large number of simulations.
","Reppen, Soner & Tissot-Daguette Deep Stochastic Optimization
4.2 Relaxed stopping
Quite recently, in a series of papers, Becker et al., [4,5] use deep empirical risk minimization
in this context. As the control variable is discrete (i.e., at any point in space, the decision is
either ‘stop’ or ‘go’) and as the training or optimization is done through a stochastic gradient
method, one has to relax the problem before applying the general procedure. We continue by
ﬁrst outlining this relaxation.
In the relaxed version, we consider an adapted control process p= (p0,...,pN)with
values in [0,1]which is the probability of stopping at that time conditioned on the event that
the process has not stopped before t. Because one has to stop at maturity, we have pN= 1.
Given the process p, letξp
tbe the probability of stopping strictly before t. Clearly,ξ0= 0
and at other times it is deﬁned recursively by,
ξp
t+1=ξp
t+pt(1−ξp
t) =pt+ (1−pt)ξp
t, t∈T.
It is immediate that ξp
t∈[0,1]and is non-decreasing. Also, if pt= 1, thenξp
s= 1for all
s > t. The quantity (1−ξp
t)is the unused “stopping budget”, and the relaxed stopping
problem is deﬁned by,
maximizevr(p) :=E/bracketleftBiggN/summationdisplay
t=0pt(1−ξp
t)ertϕ(St)/bracketrightBigg
, (4.2)
over all [0,1]-valued, adapted processes p. The original problem of stopping is included in
the relaxed one, as for any given stopping time τ,pτ
t:=χ{t=τ}yieldsξτ
t=χ{t>τ}and
consequently, v(τ) =vr(pτ). It is also known that this relaxation does not change the value.
Becker et al., [4,5] study the problem through this relaxation and implement the deep
empirical risk minimization exactly as described in the earlier section. Additionally, using
the known convex dual of the stopping problem, they are able to obtain computable upper-
bounds. For many ﬁnancial products of interest, they obtain remarkable results in very
high-dimensions. They also consider a fractional Brownian motion model for the stock price.
As for this example there is no Markovian structure, in their calculations the state is all
the past yielding an enormous state space. Still the algorithm is tractable with computable
guarantees.
4.3 The free boundary
In most examples, the optimal stopping rule is derived from a surface called the free boundary .
For instance, the continuation region of a one-dimensional American Put option is the
epigraph of a function of time. The stopping region of an American max-call option on the
other hand, is obtained by comparing the maximum of the stock values to a scalar-valued
function as proved in Proposition 4.4 below. These stopping rules have the advantage of
being interpretable [12] and easy to implement. Additionally, free-boundary problems of
this type appear often in ﬁnancial economics as well as problems from other disciplines.
Thus numerical methods developed for the free-boundary of an American option could have
implications elsewhere as well.
To be able to apply this method, we assume that the stopping region Shas a certain
structure. Namely, we assume that there exists two functions
α:Rd
+/mapsto→R,andF:/hatwideT×Rd
+/mapsto→R,
(recall that/hatwideT={0,...,N}) so that the stopping region of (4.1) is given by,
S={(t,s) :α(s)≤F(t,s)}.
7",2022-05-09T23:57:47Z,re ppeso ner tis sot da guet te ep stochastic optiatrelaxed quite becker as  ibecause give it also t b st b t it becker additnally for ty brownish as mark iastl t ifor put t proposittse additnally  to has namely rd rd
paper_qf_28.pdf,8,Deep Stochastic Optimization in Finance,"  This paper outlines, and through stylized examples evaluates a novel and
highly effective computational technique in quantitative finance. Empirical
Risk Minimization (ERM) and neural networks are key to this approach. Powerful
open source optimization libraries allow for efficient implementations of this
algorithm making it viable in high-dimensional structures. The free-boundary
problems related to American and Bermudan options showcase both the power and
the potential difficulties that specific applications may face. The impact of
the size of the training data is studied in a simplified Merton type problem.
The classical option hedging problem exemplifies the need of market generators
or large number of simulations.
","Reppen, Soner & Tissot-Daguette Deep Stochastic Optimization
More importantly, we also assume that αis given by the problem and we only need to
determineFwhich we call the free boundary . The following examples clariﬁes this assumption
which holds in a large class of problems.
Example 4.2. It is known that the stopping region of an American Put option with a
Markovian stock process is given by
S={(t,s) :s≤f(t)},
for some function f: [0,T]/mapsto→R+. In this case, α(s) =sandF(t,s) =f(t).
In the case of the max-call option, we show in Proposition 4.4 below that for any
s= (s1,...,sd)∈Rd
+withα(s) = max{s1,...,sd}, there exists a free boundary F.
Given the above structure of the stopping region through the pair (α,F)the optimal
stopping time is given by τ∗=τF, where for any free boundary F,
τF:= min{t∈/hatwideT:α(St)≤F(t,St)}.
In this approach, the output of the artiﬁcial neural network is a scalar valued function
Φ(·;θ)of time and the state values, and it approximates the free boundary F. Then for any
parameterθ, the stopping time is
τθ:=τΦ(·;θ)= min{t∈/hatwideT:α(St)≤Φ(t,St;θ)}.
4.4 Fuzzy boundary
A sharp free-boundary has the same problem of zero-gradients as the original problem and
its remedy is again a relaxation to allow for partial stopping. Indeed, given a free-boundary
Φ(·;θ)and a tuning-parameter /epsilon1>0, we deﬁne a fuzzy boundary region given by,
FΦ,/epsilon1:={(t,s) :−/epsilon1≤Φ(t,s;θ)−α(s)≤/epsilon1}.
IfΦ−α≤−/epsilon1we stop, and if Φ−α≥/epsilon1we continue, and we do these with probability one in
each case. But if the process falls into the fuzzy region FΦ,/epsilon1, then as in the relaxed problem,
we assign a stopping probability as a function of the normalized distance dθ
tto the sharp
boundary{Φ−α= 0}, i.e.,
pθ
t:=g(dθ
t),wheredθ
t=Φ(t,St;θ)−α(St)
/epsilon1,
andg: [−1,1]/mapsto→[0,1]is a ﬁxed increasing, onto function. Linear or sigmoid-like functions
are the obvious choices. Once we compute the process pθ
t, the value corresponding to the
parameterθisvr(pθ)withvras in(4.2). Hence, the relaxed free boundary problem is to
train the network to
minimizeθ∈Θ/mapsto→vr(pθ).
The resulting trained artiﬁcial neural network is an approximation of the optimal free
boundary.
4.5 American Put in one-dimension
As in [4,5] we run the algorithm for an American put on a non-dividend paying stock whose
price process is modeled by a standard geometric Brownian motion with parameters
S0=K= 40,T= 1,σ= 0.4,r= 0.06,
8",2022-05-09T23:57:47Z,re ppeso ner tis sot da guet te ep stochastic optiat whit  it put mark iaiipropositrd givest st itst st fuz ined  but st st linear once nce t put as brownish
paper_qf_28.pdf,9,Deep Stochastic Optimization in Finance,"  This paper outlines, and through stylized examples evaluates a novel and
highly effective computational technique in quantitative finance. Empirical
Risk Minimization (ERM) and neural networks are key to this approach. Powerful
open source optimization libraries allow for efficient implementations of this
algorithm making it viable in high-dimensional structures. The free-boundary
problems related to American and Bermudan options showcase both the power and
the potential difficulties that specific applications may face. The impact of
the size of the training data is studied in a simplified Merton type problem.
The classical option hedging problem exemplifies the need of market generators
or large number of simulations.
","Reppen, Soner & Tissot-Daguette Deep Stochastic Optimization
where as usual S0is the initial stock value, Kis the strike, σis the volatility, and ris the
risk-free rate. In this example, the state process is simply the stock process.
We are able to obtain accurate results for the value as well as for the free boundary. One
typical result is given in Figure 4.5 below. As the free boundary has a large derivative near
maturity, we use a non-uniform mesh near maturity. Figure 4.5 uses 500 time points. We
also employ important sampling to ensure more crossings of the free boundary. After the
training is completed, the value corresponding to this trained free boundary is computed by
using the corresponding sharp interface. Accurate price values are obtained rather easily. All
of these calculations are implemented by python in a personal laptop.
0 1
t040Initial
0 1
t040Trained
Optimal Neural Net  corridor
Figure 1: Left ﬁgure is a random initialization and the right one is the ﬁnal trained boundary. Dashed
line is the optimal calculated through a ﬁnite-diﬀerence scheme. The price is 5.311.
4.6 Max-Call Options
In this subsection, we consider the max-call option studied in the seminal paper by Broadie
& Detemple [ 8] and also in the book by Detemple [ 14]. LetSt∈Rdbe the price process of a
dividend bearing stock. The pay-oﬀ the max-call option at time τis
ϕ(Sτ) = (m(Sτ)−K)+,
where the function m:Rd
+/mapsto→R+is given by,
m(s) := max
i=1,...,dsi, s = (s1,...,sd)∈Rd
+.
The main structural assumption needed is the natural sub-linear dependence of the stock
prices on their initial values.
Assumption 4.3 (Sublinearity) .Fort∈T,s∈Rd
+, non-decreasing function φ:Rd
+/mapsto→R,
λ≥1and a stopping time τ≥t,
E[φ(Sτ)|St=λs]≤E[φ(λSτ)|St=s].
Above assumption is satisﬁed in all examples. In fact, in most models the dependency
on the initial data is linear. Although in our numerical calculations, we use a geometric
Brownian motion model for the stock price process, the method also applies more generally
to all factor models.
9",2022-05-09T23:57:47Z,re ppeso ner tis sot da guet te ep stochastic optiatis i one  as   after accurate all initial tr articial intellence ned optimal neural net  left dasd t max call optns ibroad ie  temple  temple  st rd be t rd rd t assumptsub lifort rd rd st st above ialthough brownish
paper_qf_28.pdf,10,Deep Stochastic Optimization in Finance,"  This paper outlines, and through stylized examples evaluates a novel and
highly effective computational technique in quantitative finance. Empirical
Risk Minimization (ERM) and neural networks are key to this approach. Powerful
open source optimization libraries allow for efficient implementations of this
algorithm making it viable in high-dimensional structures. The free-boundary
problems related to American and Bermudan options showcase both the power and
the potential difficulties that specific applications may face. The impact of
the size of the training data is studied in a simplified Merton type problem.
The classical option hedging problem exemplifies the need of market generators
or large number of simulations.
","Reppen, Soner & Tissot-Daguette Deep Stochastic Optimization
We use this assumption to show that the stopping region has a certain geometric structure
which we exploit. The following result is already proved in [ 8] and more generaly in [ 32]. We
provide its proof for completeness. Let Sbe as in (4.1) and set
K:=/braceleftbig
s∈Rd
+:m(s) = 1/bracerightbig
.
Note that for any s∈Rd
+,s
m(s)∈K.
Proposition 4.4. Consider the max-call option in a market satisfying the Assumption 4.3.
Then, if (t,s)∈S, then (t,λs)∈Sfor anyλ≥1. In particular,
S={(t,s) :m(s)≥F(t,s/m (s))},
whereF:/hatwideT×K/mapsto→ R+is given by,
F(t,k) := inf{ρ>0 : (t,ρk)∈S}, m∈M.
Above result can be equivalently stated as the t-sectionSt:={s∈Rd
+: (t,s)∈S}of
the continuation region being star-shaped for everyt.
Proof.Suppose that (t,s)∈Sandλ≥1. As{(N,s) :s∈Rd
+}⊂S, ift=N, clearly
(t,λs) = (N,λs )∈S. So we assume that t<N. Then, a point (t,s/prime)is inSif and only if
s/prime>Kand the following inequality is satisﬁed for every τ∈Ξt:
E/bracketleftbig
e−r(τ−t)(Sτ−K)+|St=s/prime/bracketrightbig
≤s/prime−K.
By Assumption 4.3,
E/bracketleftbig
e−r(τ−t)(Sτ−K)+|St=λs/bracketrightbig
≤E/bracketleftbig
e−r(τ−t)(λSτ−K)+|St=s/bracketrightbig
=E/bracketleftbig
e−r(τ−t)(λ[Sτ−K] + (λ−1)K)+|St=s/bracketrightbig
≤λE/bracketleftbig
e−r(τ−t)(Sτ−K)+|St=s/bracketrightbig
+ (λ−1)K
≤λ(s−K) + (λ−1)K
= (λs−K).
Hence, we conclude that (t,λs)∈S.
4.6.1 Numerical Experiments
We consider a max-Call option and in a geometric Brownian motion model under the risk
neutral measure,
St=S0exp/parenleftBig
(r−div)t+σWt−1
2σ2t/parenrightBig
,
with parameters
K= 100, S0= 90,100,110, σ= 0.2, r= 0.05, div = 0.1,
where the notation is as in the previous subsection and divis the dividend rate. We take the
maturity to be 3years andN= 9. Thus, each time interval corresponds to four months. All
these parameters are taken from [ 4,5] to allow for comparison. We also make qualitative
comparison to the results of [8].
Table 1 shows the results with d= 2,S0= 90, batch size of B= 213and7,000iterations.
The corresponding price is computed after the training is completed with 223Monte-Carlo
simulations using the sharp boundary instead of the fuzzy one. Important sampling is used
with a 1.4%downward drift. We repeated the experiment ten times in a personal computer.
10",2022-05-09T23:57:47Z,re ppeso ner tis sot da guet te ep stochastic optiat t   be rd note rd propositconsir assumpttfor iabove st rd proof suppose and as rd so t and st by assumptst st st st nce numerical eximents  call brownish st b  b   all  table t   imtant 
paper_qf_28.pdf,11,Deep Stochastic Optimization in Finance,"  This paper outlines, and through stylized examples evaluates a novel and
highly effective computational technique in quantitative finance. Empirical
Risk Minimization (ERM) and neural networks are key to this approach. Powerful
open source optimization libraries allow for efficient implementations of this
algorithm making it viable in high-dimensional structures. The free-boundary
problems related to American and Bermudan options showcase both the power and
the potential difficulties that specific applications may face. The impact of
the size of the training data is studied in a simplified Merton type problem.
The classical option hedging problem exemplifies the need of market generators
or large number of simulations.
","Reppen, Soner & Tissot-Daguette Deep Stochastic Optimization
Runs 1 2 3 4 5 6
Price 8.0747 8.0757 8.0710 8.0684 8.0670 8.0731
Stdev 0.00305 0.00315 0.00310 0.00310 0.00309 0.00311
Runs 7 8 9 10 Mean Stdev
Price 8.0686 8.0707 8.0620 8.0679 8.0699 0.0040
Stdev 0.00306 0.00308 0.00307 0.00311 - -
Table 1: Ten experiments with S0= 90, batch size 213,7000iterations. Prices are calculated with
223simulations. Stdev in the third and sixth rows refer to the standard deviations of the Monte-Carlo
simulations, while Stdev at the end is the standard deviation of the calculated ten prices.
All of the results are within the 95%conﬁdence interval [8.053,8.082]computed in [ 1]. The
standard deviation of each price computation is quite low. Hence, the maximum of the values
is a lower bound for the price.
We also repeated the experiments of [ 4,5] in space dimensions d= 5,10,100with the
above parameters. For each parameter set, we computed ten prices exactly as described
above. The results reported in Table 2 below are in agreement with the results of [ 5] (Table
9). We should also note when dis large, the maximum of many stocks have a very strong
upward drift making the standard deviation of the rewards quite high.
Dim.S0Price Std Price in [5] Max Price Its Std
2 90 8.0699 0.0031 8.068 8.0757 0.0040
2 100 13.9086 0.0059 13.901 13.9128 0.0033
2 110 21.3434 0.0059 21.341 21.3541 0.0104
5 90 16.6187 0.0040 16.631 16.6238 0.0045
5 100 26.1194 0.0259 26.147 26.1644 0.0057
5 110 36.7176 0.0078 36.774 36.7408 0.0078
10 90 26.2130 0.0182 26.196 26.2362 0.0069
10 100 38.2735 0.0538 38.272 38.3351 0.0089
10 110 50.8350 0.0397 50.812 50.8685 0.0081
100 90 66.2460 0.4946 66.359 66.6163 0.0223
100 100 82.5475 0.6463 83.390 83.6563 0.0272
100 110 98.9868 0.0366 100.421 99.0575 0.0353
Table 2: Each price is the mean of ten experiments with parameters as in Table 1. Max price is the
maximum of ten experiments with a standard deviation of the price calculation with 223Monte-Carlo
simulations.
The above table reports the average values for ten runs to be able to asses the possible
variations. However, the maximum value among these ten runs is in fact a lower bound the
actual price. As we computed these values with 223(roughly eight million) simulations, the
standard devision of this price value is small.
In two dimensions, the stopping region can be visualized eﬀectively. Figures 2, 4 are
stopping regions in two space dimensions obtained with initial data S0= 90andS0= 100.
Clearly the free boundary is independent of the initial condition and the below numerical
results verify it. Also they are similar to those obtained in [8].
11",2022-05-09T23:57:47Z,re ppeso ner tis sot da guet te ep stochastic optiatruns price st v runs meast v price st v table tepricst v   st v all t nce  for t table table  dim price std price max price its std table eatable max   t as is  also
paper_qf_28.pdf,12,Deep Stochastic Optimization in Finance,"  This paper outlines, and through stylized examples evaluates a novel and
highly effective computational technique in quantitative finance. Empirical
Risk Minimization (ERM) and neural networks are key to this approach. Powerful
open source optimization libraries allow for efficient implementations of this
algorithm making it viable in high-dimensional structures. The free-boundary
problems related to American and Bermudan options showcase both the power and
the potential difficulties that specific applications may face. The impact of
the size of the training data is studied in a simplified Merton type problem.
The classical option hedging problem exemplifies the need of market generators
or large number of simulations.
","Reppen, Soner & Tissot-Daguette Deep Stochastic Optimization
50 100 150 200
s150100150200
s2t = 0.0
50 100 150 200
s150100150200t = 1.0
50 100 150 200
s150100150200t = 2.0
50 100 150 200
s150100150200t = 3.0American max call with maturity T = 3.0
Stop Continue
Figure 2: Evolution of the Free Boundary with S0= 90
50 100 150 200
s150100150200
s2t = 0.0
50 100 150 200
s150100150200t = 1.0
50 100 150 200
s150100150200t = 2.0
50 100 150 200
s150100150200t = 3.0American max call with maturity T = 3.0
Stop Continue
Figure 3: Evolution of the Free Boundary with S0= 100
5 Valuation and Hedging
We consider a European option with stock process Sand pay-oﬀ ϕ(ST). We consider the
Heston dynamics,
dSt=St(µdt+√vtdWt),
dvt= (κ(θ−vt)−λvt) dt+σvtd˜Wt,
whereW,˜Ware one-dimensional Brownian motions with constant correlation of ρ, and the
ﬁve Heston parameters (µ,κ,θ,σ,ρ )are chosen satisfying the Feller condition. In particular,
we choose the market price of volatility risk parameter λ.
Letp∗be the price of this claim, and Zbe the return process, i.e.,
Zt+1:=St+1−St
St, t∈T. (5.1)
Further, let the feedback actions be the continuous functions
π:T×R+×R/mapsto→R,
12",2022-05-09T23:57:47Z,re ppeso ner tis sot da guet te ep stochastic optiatstcontinue  evolutfree ndary stcontinue  evolutfree ndary valuatedgi  aand  stost st   ware brownish stoseller i be st st st furtr
paper_qf_28.pdf,13,Deep Stochastic Optimization in Finance,"  This paper outlines, and through stylized examples evaluates a novel and
highly effective computational technique in quantitative finance. Empirical
Risk Minimization (ERM) and neural networks are key to this approach. Powerful
open source optimization libraries allow for efficient implementations of this
algorithm making it viable in high-dimensional structures. The free-boundary
problems related to American and Bermudan options showcase both the power and
the potential difficulties that specific applications may face. The impact of
the size of the training data is studied in a simplified Merton type problem.
The classical option hedging problem exemplifies the need of market generators
or large number of simulations.
","Reppen, Soner & Tissot-Daguette Deep Stochastic Optimization
representing the dollar amount invested in the stock. The corresponding wealth process is
given by,
Xπ,x
t+1= (1 +r)Xπ,x
t+π(t,Xπ,x
t,Zt) (Zt+1−r), t∈T, (5.2)
with initial data Xπ,x
0=x.
We ﬁrst ﬁx an initial wealth of x<p∗and consider the following pure-hedging problem of
minimizing the square hedging error, i.e.,
v∗(x) := min
π∈Cv(x,π),wherev(x,π) :=E/bracketleftbig
(ϕ(ST)−Xπ,x
T)2/bracketrightbig
.(5.3)
In the second problem, we minimize over xas well, i.e,
v∗:= min
x∈Rv∗(x) = min
(x,π)∈R×Cv(x,π). (5.4)
As proved by Föllmer & Schweizer [ 18], it is well-known that in continuous time the solution to
the second problem, v∗, is equal to the Heston price. Thus, for suﬃciently ﬁne discretization
v∗is close to zero, x∗is close to the known continuous-time Heston price. Also the numerical
hedgeπ∗must be equal to the continuous time hedge.
Ifr= 0, then,Xπ,x
t=x+Xπ,0
tand the initial wealth xonly inﬂuences the mean of the
hedging error. Therefore, we expect that after an initial adjustment to minimize the mean,
the networks would minimize the variance which is independent of the initial wealth. This
approximate reasoning indicates that after an initial transient region, both minimization
problems may behave similarly when there is large data.
5.1 Numerical results
We implemented the above hedging problem in Julia’s Flux [ 27] by parameterizing the
portfolio at each time point, including the initial wealth level. In particular, we hedge a call
option with strike K, i.e.,ϕ(x) = (x−K)+=max{x−K,0}. Our implementation follows
the scheme in Section 3, which we here describe in greater detail for this particular problem.
We see in (5.4)that the two quantities we optimize over are xandπ. Asxis a scalar, we
directly parameterize it with a 1-element tensor, which after optimization is the option price.
The policy π, however, can be approximated in various ways. We here opt for a very direct
method in which we represent it by a single neural network with time and stock data as
inputs. This contrasts [9], where the authors discretize time and design one neural network
per time point. As we shall see, our implementation of a single neural network also performs
well, with the additional beneﬁt of allowing changes to the time discretization during training.
There are also other training diﬀerences between the two parameterizations, as, for instance,
the one used here accomplishes a large degree of parameter sharing. Nevertheless, a thorough
account of these diﬀerences is outside the scope of the present paper.
Another detail of our implementation is that we write πas a function of tandStinstead
of the formulation in (5.2). It is clear that the two are mathematically equivalent, although
they could diﬀer in training performance. Ours is a naïve choice and we make it because
we ﬁnd it more natural, not because it necessarily leads to better performance. The neural
network is designed with two hidden layers of width 20 and with ReLU activation. In-between
layers, batch normalization is employed.1
The results of our computations are presented in Table 3. We compare our numerical
solution to the Heston prices from https://www.quantlib.org/ . No signiﬁcant tuning has
1Although we believe that the following parameters are not crucial for replicating our results (because they
were not tuned), we list them here for completeness: batch size: 512; optimizer: Adam with the Flux default
parameters (η,β1,β2) = (0.001,0.9,0.999); and the number of epochs was a ﬁxed value for which the training error
of a typical run had plateaued.
13",2022-05-09T23:57:47Z,re ppeso ner tis sot da guet te ep stochastic optiatt  cv irv cv as schiz sto stoalso  trefore  numerical  julia flux iour se as is t   as tre nevertless anotr st instead it ours t re it table  stono although adam flux
paper_qf_28.pdf,14,Deep Stochastic Optimization in Finance,"  This paper outlines, and through stylized examples evaluates a novel and
highly effective computational technique in quantitative finance. Empirical
Risk Minimization (ERM) and neural networks are key to this approach. Powerful
open source optimization libraries allow for efficient implementations of this
algorithm making it viable in high-dimensional structures. The free-boundary
problems related to American and Bermudan options showcase both the power and
the potential difficulties that specific applications may face. The impact of
the size of the training data is studied in a simplified Merton type problem.
The classical option hedging problem exemplifies the need of market generators
or large number of simulations.
","Reppen, Soner & Tissot-Daguette Deep Stochastic Optimization
gone into producing our values, and it is nevertheless clear that accurate prices are consistently
attained. We see, for instance, that the absolute error is approximately the same for all
three strikes, which we argue is a consequence of (i) not tuning the training parameters to
each individual problem and (ii) our hedging is in discrete time, which introduces a time
discretization error. Although this is only a one-dimensional problem, it gives credence to the
method’s eﬀectiveness, eﬀectiveness that does translate into higher-dimensional performance,
as we illustrated for the American options problems.
K QuantLib price Price Avg. abs. error Error std. dev.
90 10.076508 10.078163 0.001869 0.001174
100 2.295405 2.295211 0.002018 0.001065
110 0.128136 0.127069 0.001971 0.001793
Table 3: Hedging performance of a call option with strike Kin a Heston model with parameters
S0= 100,v0= 0.04,κ= 0.9,θ= 0.04,r=λ= 0andσ= 0.2. The maturity T= 1/12and the time
interval is discretized in 22 steps. Each row lists the deep hedging price average over 100 runs along with
the standard deviation over the same 100 runs.
Theoretically, in continuous-time, the optimal hedge is independent of the initial wealth.
We also studied this by ﬁxing the initial portfolio value to the price and also to its half value.
One simulation of the trained hedge is given in the Figure 4 shows that the dependence is
minimal.
Figure 4: Optimal Hedges for the Heston model. The orange curve is the trained feedback hedge with
an initial wealth half the option price, while the red curve is trained with initial wealth equal to the
option price. The stock price is rescaled to start at S0= 1.
6 Merton Problem and Overlearning
In this section, we summarize the results of [ 31] by the ﬁrst two authors. As in that paper,
to emphasize the essential features of the algorithm, a simple ﬁnancial market without any
frictions and constant coeﬃcients is considered. Additionally, consumption is not taken into
account. All these details can incorporated into the model and problems with complex market
structures have already been studied extensively by Buehler et.al.[9, 10].
14",2022-05-09T23:57:47Z,re ppeso ner tis sot da guet te ep stochastic optiat although quant lib price g error table edgi kistot eatoically  one   optimal dgstot t mortoproblem olearni ias additnally all bu hler
paper_qf_28.pdf,15,Deep Stochastic Optimization in Finance,"  This paper outlines, and through stylized examples evaluates a novel and
highly effective computational technique in quantitative finance. Empirical
Risk Minimization (ERM) and neural networks are key to this approach. Powerful
open source optimization libraries allow for efficient implementations of this
algorithm making it viable in high-dimensional structures. The free-boundary
problems related to American and Bermudan options showcase both the power and
the potential difficulties that specific applications may face. The impact of
the size of the training data is studied in a simplified Merton type problem.
The classical option hedging problem exemplifies the need of market generators
or large number of simulations.
","Reppen, Soner & Tissot-Daguette Deep Stochastic Optimization
Consider a stock price process St∈Rd
+in discrete time and assume a constant interest
rate ofr. Let the return process Zbe as in(5.1)andXπ=Xπ,xbe as in(5.2). We suppress
the dependence of the initial wealth xfor simplicity. Then, the classical investment problem
is to maximize v(π) :=E[U(Xπ
T)]with a given utility function U.
In [31] it is proved that the deep empirical risk minimization algorithm converges as the
size of the training data gets larger. On the other hand, it is also shown that for ﬁxed training
data sets, larger and deeper neural networks have the capability of overlearning the data,
however large it might be. In such situations, the trained neural networks while substantially
over-perform the theoretical optimum on the training set, they do not generalize and perform
poorly on other data sets.
These theoretical results are demonstrated in the following stylized example with an
explicit solution in [ 31](Section 8). In that example, the utility function is taken to be the
exponential with parameter one, and as the decisions are independent of the initial value
for these class of utilities, the initial value is ﬁxed as one dollar. To simplify even further,
for one period this amount is invested uniformly on all stocks. Then, with 1:= (1,..., 1),
π0=1/dandX1= (Z1·1)/d−rare uncontrolled, and the investment problem is to choose
the feedback portfolio π1(Z1)∈Rdso as to maximize
v(π) =E/bracketleftbig
1−exp(−Xπ
2)/bracketrightbig
,
whereXπ
2= (1 +r)X1+a(Z1)·(Z2−r1). The certainty equivalent of a utility value v<1
given by
ce(v) := ln(1−v)⇐⇒v=U(ce(v))
is a more standard way of comparing diﬀerent utility values. Indeed, agents with expected
utility preferences would be indiﬀerent between an action πand a cash amount of ce(v(π))
because the utilities of both positions are equal to each other. Thus, for these agents the
cash equivalent of the action πisce(v(π)).
The following table [ 31](Table 1) clearly demonstrates overlearning. In this experiments
the training data of size N= 100,000and an artiﬁcial neural network with three hidden
layers of width 10 is trained on this set for four or ﬁve epochs. For each dimension the
algorithm is run thirty times and Table 4 below reports the mean and the standard. deviation.
Althoughconservativestoppingrulesareemployedin[ 31], thereissubstantialoverperformance
increasing with dimension.
dimspin(%) pin−pout(%)
µ σ µ σ
100 10.12820 1.09290 23.67080 2.01177
85 8.38061 1.35575 20.16440 2.30489
70 7.32720 0.86458 15.62060 1.94043
55 5.05783 0.81518 10.93950 1.54431
40 3.74648 0.62588 7.91105 1.32581
25 2.11501 0.43845 4.58954 0.88461
10 0.53982 0.34432 1.46138 0.39078
Table 4: Average relative in-sample performance, and its comparison to the out-of-sample performance
with the above described conservative stopping rule. Everything is in % with training size of N= 100,000
and three hidden layers of width 10. The µvalue is the average of 30 runs and σis the standard deviation.
15",2022-05-09T23:57:47Z,re ppeso ner tis sot da guet te ep stochastic optiatconsir st rd  be  tioitse seito trd so t ined  t table ifor table although conservative stoppi rulare employed itable ge everythi t
paper_qf_28.pdf,16,Deep Stochastic Optimization in Finance,"  This paper outlines, and through stylized examples evaluates a novel and
highly effective computational technique in quantitative finance. Empirical
Risk Minimization (ERM) and neural networks are key to this approach. Powerful
open source optimization libraries allow for efficient implementations of this
algorithm making it viable in high-dimensional structures. The free-boundary
problems related to American and Bermudan options showcase both the power and
the potential difficulties that specific applications may face. The impact of
the size of the training data is studied in a simplified Merton type problem.
The classical option hedging problem exemplifies the need of market generators
or large number of simulations.
","Reppen, Soner & Tissot-Daguette Deep Stochastic Optimization
7 Conclusion
The deep empirical risk minimization proposed by E, Han & Jentzen [ 21,22] provides a ﬂexible
and a highly eﬀective tool for stochastic optimization problems arising in computational
ﬁnance. Recent development of optimization libraries make this algorithm tractable in very
high dimensions allowing to include important market details such as factors and frictions,
as well as models with long memory. Once a large training set is given, the algorithm mimics
the market dynamics with all its details. This simple description together with powerful
new computational tools are keys to the power of the algorithm. We have demonstrated
above properties in three diﬀerent classes of problems. As it is always the case, each requires
problem speciﬁc but natural modiﬁcations. Moreover, the output can be designed to be
exactly the decision rule that is under investigation.
The method on the other hand needs large data sets for reliable results. In the ﬁnancial
setting this essentially limits its scope to model driven markets with an unlimited simulation
capability. However, due to its seamless transition to more complex structures, more
interesting parametric models are now feasible. Thus, on-going research on market generators
will be an important factor on further developments.
References
[1]L. Andersen and M. Broadie. Primal-dual simulation algorithm for pricing multidimen-
sional American options. Management Science , 50(9):1222–1234, 2004.
[2]A. Bachouch, C. Huré, N. Langrené, and H. Pham. Deep neural networks algo-
rithms for stochastic control problems on ﬁnite horizon, part 2: Numerical applications.
arXiv:1812.05916 , 2018.
[3]C. Bayer, R. Tempone, and S. Wolfers. Pricing American options by exercise rate
optimization. Quantitative Finance , 20(11):1749–1760, 2020.
[4]S. Becker, P. Cheridito, and A. Jentzen. Deep optimal stopping. Journal of Machine
Learning Research , 20(4):1–25, 2019.
[5]S. Becker, P. Cheridito, A. Jentzen, and T. Welti. Solving high-dimensional optimal
stopping problems using deep learning. European Journal of Applied Mathematics , 32
(3):470–514, 2021.
[6] D. Bertsekas and J. Tsitsiklis. Neuro-dynamic programming . Athena Scientiﬁc, 1996.
[7]L. Boudabsa and D. Filipović. Machine learning with kernels for portfolio valuation and
risk management. Finance and Stochastics , pages 1–42, 2021.
[8]M. Broadie and J. Detemple. The valuation of American options on multiple assets.
Mathematical Finance , 7(3):241–286, 1997.
[9]H. Buehler, L. Gonon, J. Teichmann, and B. Wood. Deep hedging. Quantitative Finance ,
19(8):1271–1291, 2019.
[10]H. Buehler, L. Gonon, J. Teichmann, B. Wood, and B. Mohan. Deep hedging: hedging
derivatives under generic market frictions using reinforcement learning. Technical report,
Swiss Finance Institute, 2019.
[11]E. Chevalier, S. Pulido, and E. Zúñiga. American options in the Volterra Heston model.
arXiv:2103.11734 , 2021.
16",2022-05-09T23:57:47Z,re ppeso ner tis sot da guet te ep stochastic optiatconust hajetzerecent once   as ot i referencanrsobroad ie primal management science basuhur la reham ep numerical  bayer temp one wolf ers prici quantitative nance becker cr di to jetzeep journal machine learni researbecker cr di to jetzelt solvi ajournal applied matmatics bert sek as ts its  neuro atns cie nti  dabs lipovi machine nance stochastic broad ie  temple t matmatical nance bu hler go noeichmanwood ep quantitative nance bu hler go noeichmanwood moraep technical swiss nance institute cvalier pu l volterra sto
paper_qf_28.pdf,17,Deep Stochastic Optimization in Finance,"  This paper outlines, and through stylized examples evaluates a novel and
highly effective computational technique in quantitative finance. Empirical
Risk Minimization (ERM) and neural networks are key to this approach. Powerful
open source optimization libraries allow for efficient implementations of this
algorithm making it viable in high-dimensional structures. The free-boundary
problems related to American and Bermudan options showcase both the power and
the potential difficulties that specific applications may face. The impact of
the size of the training data is studied in a simplified Merton type problem.
The classical option hedging problem exemplifies the need of market generators
or large number of simulations.
","Reppen, Soner & Tissot-Daguette Deep Stochastic Optimization
[12]D. Ciocan and V. Mišić. Interpretable optimal stopping. Management Science , 68(3):
1616–1638, 2022.
[13]G. Cybenko. Approximations by superpositions of a sigmoidal function. Mathematics
of Control, Signals and Systems , 2:183–192, 1989.
[14]J. Detemple. American-style derivatives: Valuation and computation . CRC Press, 2005.
[15]S. Fecamp, J. Mikael, and X. Warin. Deep learning for discrete-time hedging in
incomplete markets. Journal of computational Finance , 2020.
[16]D. Filipovic, M. Multerer, and P. Schneider. Adaptive joint distribution learning.
arXiv:2110.04829 , 2021.
[17]W. H. Fleming and H. M. Soner. Controlled Markov processes and viscosity solutions ,
volume 25. Springer Science & Business Media, 2006.
[18]H. Föllmer and M. Schweizer. Hedging of contingent claims under incomplete information.
Applied stochastic analysis , 5(389-414):19–31, 1991.
[19]M. Germain, H. Pham, and X. Warin. Deep backward multistep schemes for nonlinear
pdes and approximation error analysis. arXiv:2006.01496 , 2020.
[20]L. Gonon, J. Muhle-Karbe, and X. Shi. Asset pricing with general transaction costs:
Theory and numerics. Mathematical Finance , 31(2):595–648, 2021.
[21]J. Han and W. E. Deep learning approximation for stochastic control problems. In Deep
Reinforcement Learning Workshop, NIPS , 2016.
[22]J. Han, A. Jentzen, and W. E. Solving high-dimensional partial diﬀerential equations
using deep learning. Proceedings of the National Academy of Sciences , 115(34):8505–8510,
2018.
[23]S. L. Heston. A closed-form solution for options with stochastic volatility with appli-
cations to bond and currency options. The Review of Financial Studies , 6(2):327–343,
1993.
[24]K. Hornik. Approximation capabilities of multilayer feedforward networks. Neural
networks , 4(2):251–257, 1991.
[25]C. Huré, H. Pham, A. Bachouch, and N. Langrené. Deep neural networks algo-
rithms for stochastic control problems on ﬁnite horizon, part I: convergence analysis.
arXiv:1812.04300 , 2018.
[26]J. M. Hutchinson, A. W. Lo, and T. Poggio. A nonparametric approach to pricing
and hedging derivative securities via learning networks. The Journal of Finance , 49(3):
851–889, 1994.
[27]M. Innes. Flux: Elegant machine learning with julia. Journal of Open Source Software ,
3(25):602, 2018.
[28]M. Laurière, G. Pagès, and O. Pironneau. Performance of a Markovian neural network
versus dynamic programming on a ﬁshing control problem. arXiv:2109.06856 , 2021.
[29]F. A. Longstaﬀ and E. S. Schwartz. Valuing American options by simulation: a simple
least-squares approach. The Review of Financial Studies , 14(1):113–147, 2001.
17",2022-05-09T23:57:47Z,re ppeso ner tis sot da guet te ep stochastic optiatc cami interp able management science cy bank approximatns matmatics contrsnals tems  temple valuat fe camp michael war iep journal nance lipovic mul terr schneir adaptive  flemi so ner controlled mark  science business media schiz edgi applied germ articial intellence ham war iep  go nomule kar be shi asset tory matmatical nance haep iep reinforcement learni workshhajetzesolvi proceedis natnal acamy sciencstot review nancial studihorik approximatneural hur ham basula reep  hutchinsolo pt journal nance inner flux elegant journal opesource software laura pag pi tonne au formance mark ia lo st schwartz valt review nancial studies
paper_qf_28.pdf,18,Deep Stochastic Optimization in Finance,"  This paper outlines, and through stylized examples evaluates a novel and
highly effective computational technique in quantitative finance. Empirical
Risk Minimization (ERM) and neural networks are key to this approach. Powerful
open source optimization libraries allow for efficient implementations of this
algorithm making it viable in high-dimensional structures. The free-boundary
problems related to American and Bermudan options showcase both the power and
the potential difficulties that specific applications may face. The impact of
the size of the training data is studied in a simplified Merton type problem.
The classical option hedging problem exemplifies the need of market generators
or large number of simulations.
","Reppen, Soner & Tissot-Daguette Deep Stochastic Optimization
[30]M. Ludkovski. mlosp: Towards a uniﬁed implementation of regression Monte Carlo
algorithms. arXiv:2012.00729 , 2020.
[31]A. M. Reppen and H. M. Soner. Deep empirical risk minimization in ﬁnance: an
assessment. in preparation , 2020.
[32]A. M. Reppen, H. M. Soner, and V. Tissot-Daguette. Neural optimal stopping boundary.
in preparation , 2022.
[33] J. Ruf and W. Wang. Hedging with neural networks. arXiv:2004.08891 , 2020.
[34]J. Ruf and W. Wang. Neural networks for option pricing and hedging: a literature
review. Journal of Computational Finance , 24(1), 2020.
[35]M. Schweizer. Option hedging for semimartingales. Stochastic Processes and Their
Applications , 37(2):339–363, 1991.
[36]M. Schweizer. A guided tour through quadratic hedging approaches. Technical report,
SFB 373 Discussion Paper, 1999.
[37]J. Sirignano and K. Spiliopoulos. Dgm: A deep learning algorithm for solving partial
diﬀerential equations. Journal of Computational Physics , 375:1339–1364, 2018.
[38]J. N. Tsitsiklis and B. Van Roy. Regression methods for pricing complex American-style
options. IEEE Transactions on Neural Networks , 12(4):694–703, 2001.
[39]S. Wang and P. Perdikaris. Deep learning of free boundary and Stefan problems. Journal
of Computational Physics , 428:109914, 2021.
[40]X. Warin. Variance optimal hedging with application to electricity markets. Journal of
Computational Finance , 23(3):33–59, 2019.
18",2022-05-09T23:57:47Z,re ppeso ner tis sot da guet te ep stochastic optiatlud v  towards    re ppeso ner ep re ppeso ner tis sot da guet te neural ru wa edgi  ru wa neural journal tnal nance schiz optstochastic processtir applicatns schiz technical discusspa sir no so  poll os gm journal tnal psics ts its  varoy regresstransans neural networks wa  dik arts ep stefajournal tnal psics war ivariance journal tnal nance
paper_qf_29.pdf,1,"FinRL-Podracer: High Performance and Scalable Deep Reinforcement
  Learning for Quantitative Finance","  Machine learning techniques are playing more and more important roles in
finance market investment. However, finance quantitative modeling with
conventional supervised learning approaches has a number of limitations. The
development of deep reinforcement learning techniques is partially addressing
these issues. Unfortunately, the steep learning curve and the difficulty in
quick modeling and agile development are impeding finance researchers from
using deep reinforcement learning in quantitative trading. In this paper, we
propose an RLOps in finance paradigm and present a FinRL-Podracer framework to
accelerate the development pipeline of deep reinforcement learning (DRL)-driven
trading strategy and to improve both trading performance and training
efficiency. FinRL-Podracer is a cloud solution that features high performance
and high scalability and promises continuous training, continuous integration,
and continuous delivery of DRL-driven trading strategies, facilitating a rapid
transformation from algorithmic innovations into a profitable trading strategy.
First, we propose a generational evolution mechanism with an ensemble strategy
to improve the trading performance of a DRL agent, and schedule the training of
a DRL algorithm onto a GPU cloud via multi-level mapping. Then, we carry out
the training of DRL components with high-performance optimizations on GPUs.
Finally, we evaluate the FinRL-Podracer framework for a stock trend prediction
task on an NVIDIA DGX SuperPOD cloud. FinRL-Podracer outperforms three popular
DRL libraries Ray RLlib, Stable Baseline 3 and FinRL, i.e., 12% \\\\sim 35%
improvements in annual return, 0.1 \\\\sim 0.6 improvements in Sharpe ratio and 3
times \\\\sim 7 times speed-up in training time. We show the high scalability by
training a trading agent in 10 minutes with $80$ A100 GPUs, on NASDAQ-100
constituent stocks with minute-level data over 10 years.
","FinRL-Podracer: High Performance and Scalable Deep
Reinforcement Learning for Quantitative Finance
Zechu Li
zl2993@columbia.edu
Columbia UniversityXiao-Yang Liu∗
xl2427@columbia.edu
Columbia UniversityJiahao Zheng
jh.zheng@siat.ac.cn
Shenzhen Inst. of Advanced Tech.
Zhaoran Wang
zhaoranwang@gmail.com
Northwestern UniversityAnwar Walid†
anwar.i.walid@gmail.com
Amazon & Columbia UniversityJian Guo‡
guojian@idea.edu.cn
IDEA Research
ABSTRACT
Machine learning techniques are playing more and more important
roles in finance market investment. However, finance quantitative
modeling with conventional supervised learning approaches has a
number of limitations, including the difficulty in defining appropri-
ate labels, lack of consistency in modeling and trading execution,
and lack of modeling the dynamic nature of the finance market. The
development of deep reinforcement learning techniques is partially
addressing these issues. Unfortunately, the steep learning curve and
the difficulty in quick modeling and agile development are imped-
ing finance researchers from using deep reinforcement learning in
quantitative trading. In this paper, we propose an RLOps in finance
paradigm and present a FinRL-Podracer framework to accelerate the
development pipeline of deep reinforcement learning (DRL)-driven
trading strategy and to improve both trading performance and train-
ing efficiency. FinRL-Podracer is a cloud solution that features high
performance and high scalability and promises continuous training ,
continuous integration , and continuous delivery of DRL-driven trad-
ing strategies, facilitating a rapid transformation from algorithmic
innovations into a profitable trading strategy. First, we propose a
generational evolution mechanism with an ensemble strategy to
improve the trading performance of a DRL agent, and schedule
the training of a DRL algorithm onto a GPU cloud via multi-level
mapping. Then, we carry out the training of DRL components with
high-performance optimizations on GPUs. Finally, we evaluate the
FinRL-Podracer framework for a stock trend prediction task on an
NVIDIA DGX SuperPOD cloud. FinRL-Podracer outperforms three
popular DRL libraries Ray RLlib ,Stable Baseline 3 andFinRL , i.e.,
12%∼35%improvements in annual return, 0.1∼0.6improvements
in Sharpe ratio and 3×∼ 7×speed-up in training time. We show the
high scalability by training a trading agent in 10minutes with 80
A100 GPUs, on NASDAQ-100 constituent stocks with minute-level
data over 10years.
CCS CONCEPTS
•Computing methodologies →Machine learning ;Neural net-
works ;Markov decision processes ;Reinforcement learning .
∗Equal contribution.
†A. Walid finished this project at Bell labs, before joining Amazon.
‡Corresponding author.KEYWORDS
RLOps in finance, deep reinforcement learning, stock trend predic-
tion, scalability, GPU cloud
1 INTRODUCTION
Algorithmic trading is increasingly deployed in the financial in-
vestment process. A conventional supervised learning pipeline
consists of five stages [ 31,40], as shown in Fig. 1 (left), namely
data pre-process, modeling and trading signal generation, portfolio
optimization, trade execution, and post-trade analysis. Recently,
deep reinforcement learning (DRL) [ 34,36,37] has been recognized
as a promising alternative for quantitative finance [ 2,7,16,17],
since it has the potential to overcome some important limitations
of supervised learning, such as the difficulty in label specification
and the gap between modeling, positioning and order execution.
We advocate extending the principle of MLOps [1]1to the RLOps
in finance paradigm that implements and automates the continuous
training (CT), continuous integration (CI), and continuous delivery
(CD) for trading strategies. We argue that such a paradigm has vast
profits potential from a broadened horizon and fast speed, which is
critical for wider DRL adoption in real-world financial tasks.
The RLOps in finance paradigm, as shown in Fig. 1 (right), inte-
grates middle stages (i.e., modeling and trading signal generation,
portfolio optimization, and trade execution) into a DRL agent. Such
a paradigm aims to help quantitative traders develop an end-to-end
trading strategy with a high degree of automation, which removes
the latency between stages and results in a compact software stack.
The major benefit is that it can explore the vast potential prof-
its behind the large-scale financial data, exceeding the capacity
of human traders; thus, the trading horizon is lifted into a poten-
tially new dimension. Also, it allows traders to continuously update
trading strategies, which equips traders with an edge in a highly
volatile market. However, the large-scale financial data and fast
iteration of trading strategies bring imperative challenges in terms
of computing power.
Existing works are not satisfactory with respect to the usage of
large-scale financial data and the efficiency of agent training. For
DRL strategy design, existing works studied algorithmic trading
on a daily time-frame [2, 25, 26, 42–45] or hourly time-frame [14],
which is hard to fully capture the dynamics of a highly volatile mar-
ket. For DRL library/package development, existing works may not
1MLOps is an ML engineering culture and practice that aims at unifying ML system
development (Dev) and ML system operation (Ops).arXiv:2111.05188v1  [q-fin.CP]  7 Nov 2021",2021-11-07T01:03:15Z, racer hh formance scalable ep reinforcement learni quantitative nance ze chu li columbia   ya  columbia  jia hao c snzist advanced tehao rawa northster near valid amazocolumbia  jaguo researmachine t unfortunately iops  racer  racer rst tus nally  racer su  racer ray lli stable baseline share  us uti machine neural mark reinforcement equal valid bell amazocorrespondi ops algorithmic  recently  ops ops  t ops  sut also existi for for ops v ops  nov
paper_qf_29.pdf,2,"FinRL-Podracer: High Performance and Scalable Deep Reinforcement
  Learning for Quantitative Finance","  Machine learning techniques are playing more and more important roles in
finance market investment. However, finance quantitative modeling with
conventional supervised learning approaches has a number of limitations. The
development of deep reinforcement learning techniques is partially addressing
these issues. Unfortunately, the steep learning curve and the difficulty in
quick modeling and agile development are impeding finance researchers from
using deep reinforcement learning in quantitative trading. In this paper, we
propose an RLOps in finance paradigm and present a FinRL-Podracer framework to
accelerate the development pipeline of deep reinforcement learning (DRL)-driven
trading strategy and to improve both trading performance and training
efficiency. FinRL-Podracer is a cloud solution that features high performance
and high scalability and promises continuous training, continuous integration,
and continuous delivery of DRL-driven trading strategies, facilitating a rapid
transformation from algorithmic innovations into a profitable trading strategy.
First, we propose a generational evolution mechanism with an ensemble strategy
to improve the trading performance of a DRL agent, and schedule the training of
a DRL algorithm onto a GPU cloud via multi-level mapping. Then, we carry out
the training of DRL components with high-performance optimizations on GPUs.
Finally, we evaluate the FinRL-Podracer framework for a stock trend prediction
task on an NVIDIA DGX SuperPOD cloud. FinRL-Podracer outperforms three popular
DRL libraries Ray RLlib, Stable Baseline 3 and FinRL, i.e., 12% \\\\sim 35%
improvements in annual return, 0.1 \\\\sim 0.6 improvements in Sharpe ratio and 3
times \\\\sim 7 times speed-up in training time. We show the high scalability by
training a trading agent in 10 minutes with $80$ A100 GPUs, on NASDAQ-100
constituent stocks with minute-level data over 10 years.
","Conference’17, July 2017, Washington, DC, USA Zechu Li, Xiao-Yang Liu, Jiahao Zheng, Zhaoran Wang, Anwar Walid, and Jian Guo
Figure 1: Software stack for an algorithmic trading process:
conventional approach vs. RLOps in finance.
be able to meet the intensive computing requirement of relatively
high frequency trading tasks, large-scale financial data processing
and tick-level trade execution. We evaluate the training time of
three popular DRL libraries, FinRL [ 25,26], RLlib [ 19] and Stable
Baseline3 [ 9], on NASDAQ-100 constituent stocks with minute-
level data (described in Section 5.2) in Table 1, which shows that it
is difficult for them to effectively train a profitable trading agent in
a short cycle time.
In recent years, distributed DRL frameworks and massively par-
allel simulations have been recognized as the critical software de-
velopment for the RLOps paradigm [ 12,19,20,27]. It is promis-
ing to utilize extensive computing resources, e.g., a GPU cloud, to
accelerate the development of trading strategies, including both
financial simulation and model training. Therefore, we investigate
a candidate solution on a GPU cloud, an NVIDIA DGX SuperPOD
cloud [ 38] that is the most powerful AI infrastructure for enterprise
deployments.
In this paper, we propose a FinRL-Podracer framework as a high-
performance and scalable solution for RLOps in finance . At a high
level, FinRL-Podracer schedules the training process through a
multi-level mapping and employs a generational evolution mecha-
nism with an ensemble strategy. Such a design guarantees scala-
bility on a cloud platform. At a low level, FinRL-Podracer realizes
hardware-oriented optimizations, including parallelism encapsula-
tion, GPU acceleration, and storage optimization, thus achieving
high performance. As a result, FinRL-Podracer can effectively ex-
ploit the supercomputing resources of a GPU cloud, which provides
an opportunity to automatically design DRL trading strategies with
fast and flexible development, deployment and production.
Our contributions can be summarized as follows
•We propose a FinRL-Podracer framework built on two previ-
ous projects, FinRL [ 25,26] and ElegantRL [ 23]2, to initiate
a paradigm shift from conventional supervised learning ap-
proaches to RLOps in finance .
•FinRL-Podracer employs a generational evolution mecha-
nism during the training of DRL agents and provides high-
performance optimizations for financial tasks.
•We show the training efficiency by obtaining a trading agent
in10minutes on an NVIDIA DGX SuperPOD cloud [ 38] with
80A100 GPUs, for a stock trend prediction task on NASDAQ-
100 constituent stocks with minute-level data over 10years.
2ElegantRL is a scalable and elastic deep reinforcement learning library. It supports
general robotic and game playing applications.Sharpe ratio Max dropdown Training time
RLlib [19] 1.67 -23.248% 110 min
SB3 [13] 1.82 -23.750% 150 min
FinRL [44] 1.35 -27.267% 345 min
QQQ 1.25 -28.559% –
Table 1: Evaluations of existing DRL libraries on an NVIDIA
DGX A100 server [5]. We evaluate on NASDAQ-100 con-
stituent stocks with minute-level data by training from
01/01/2009 to 05/12/2019 and backtesting from 05/13/2019 to
05/26/2021. Invesco QQQ ETF is a market benchmark.
We evaluate the trained agent for one year and show its
trading performance outperforms three DRL libraries, FinRL
[25,26],Ray RLlib [19] and Stable Baseline3 [13], i.e., 12%∼
35%improvements in annual return, 0.1∼0.6improvements
in Sharpe ratio and 3×∼7×speed-up in training time.
The remainder of this paper is organized as follows. Section 2
describes related works. Section 3 models a typical stock trend pre-
diction task as a Markov Decision Process. In Section 4, we present
the FinRL-Podracer framework and describe its evolution and train-
ing layers, respectively. In Section 5, we describe the experimental
setup for the trading task and present experimental results. We
conclude this paper and discuss future directions in Section 6.
2 RELATED WORKS
This section summarizes related works from two aspects: DRL
applications in quantitative finance and the MLOps development.
2.1 DRL in Finance
With the successes of DRL in game playing, e.g., Atari games [ 30]
and GO games [ 35], more and more finance researchers show their
interests in this area, and they have done some early attempts to
applying DRL in quantitative finance investment. In this paper, we
take the stock trend prediction (STP) task as an example to introduce
existing works and show great potentials of DRL in finance area.
Stock trend prediction task is often considered a challenging
application of machine learning in finance due to its noisy and
volatile nature. Traditionally, the STP task is formulated as a su-
pervised learning problem, where features of stocks are extracted
from a past time window of technical indices, fundamental data and
alternative data [ 4], and labels are usually extracted from a future
time window of concerned criteria such as rise/fall, returns, excess
returns or Sharpe ratios. Recently, deep reinforcement learning has
been applied to solving STP tasks. Zhang et al. [45] constructed a
trading agent using three DRL algorithms, DQN, PG, and A2C, for
both discrete and continuous action spaces. Yang et al. [44] used
an ensemble strategy to integrate different DRL algorithms, A2C,
DDPG, and PPO based on the Sharpe ratio. They applied the idea of
a rolling window, where the best algorithm is picked to trade in the
following period. Recently, many researchers provide more DRL
solutions for STP tasks [ 3,8,18]. However, most existing works
are based on several assumptions, which limits the practicality. For
example, the backtesting has no impacts on the market; there are al-
most no other complex actions besides buying, holding, and selling;
only one stock type is supported for each agent.",2021-11-07T01:03:15Z,conference july washitoze chu li  ya  jia hao c hao rawa near valid jaguo  software ops  lli stable baseline setable iops it trefore su i racer ops at  racer suat  racer as  racer our   racer elegant ops  racer  su us elegant it share max tr articial intellence ni lli table evaluatns  investor  ray lli stable baseline share t sesemark cisprocess ise racer ise se ops nance with atari istock traditnally share recently  ya share ty recently for
paper_qf_29.pdf,3,"FinRL-Podracer: High Performance and Scalable Deep Reinforcement
  Learning for Quantitative Finance","  Machine learning techniques are playing more and more important roles in
finance market investment. However, finance quantitative modeling with
conventional supervised learning approaches has a number of limitations. The
development of deep reinforcement learning techniques is partially addressing
these issues. Unfortunately, the steep learning curve and the difficulty in
quick modeling and agile development are impeding finance researchers from
using deep reinforcement learning in quantitative trading. In this paper, we
propose an RLOps in finance paradigm and present a FinRL-Podracer framework to
accelerate the development pipeline of deep reinforcement learning (DRL)-driven
trading strategy and to improve both trading performance and training
efficiency. FinRL-Podracer is a cloud solution that features high performance
and high scalability and promises continuous training, continuous integration,
and continuous delivery of DRL-driven trading strategies, facilitating a rapid
transformation from algorithmic innovations into a profitable trading strategy.
First, we propose a generational evolution mechanism with an ensemble strategy
to improve the trading performance of a DRL agent, and schedule the training of
a DRL algorithm onto a GPU cloud via multi-level mapping. Then, we carry out
the training of DRL components with high-performance optimizations on GPUs.
Finally, we evaluate the FinRL-Podracer framework for a stock trend prediction
task on an NVIDIA DGX SuperPOD cloud. FinRL-Podracer outperforms three popular
DRL libraries Ray RLlib, Stable Baseline 3 and FinRL, i.e., 12% \\\\sim 35%
improvements in annual return, 0.1 \\\\sim 0.6 improvements in Sharpe ratio and 3
times \\\\sim 7 times speed-up in training time. We show the high scalability by
training a trading agent in 10 minutes with $80$ A100 GPUs, on NASDAQ-100
constituent stocks with minute-level data over 10 years.
","FinRL-Podracer: High Performance and Scalable Deep Reinforcement Learning for Quantitative Finance Conference’17, July 2017, Washington, DC, USA
2.2 Principle of MLOps
Recently, Google trends put Machine Learning Operations (MLOps)
as one of the most promisingly increasing trends [ 41]. MLOps is a
practice in developing and operating large-scale machine learning
systems, which facilitates the transformation of machine learning
models from development to production [ 6,28]. In essence, MLOps
entails cloud computing power to integrate and automate a stan-
dard machine learning pipeline: 1) data pre-processing; 2) feature
engineering; 3) continuous ML model training; 4) continuous ML
model deployment; 5) output production, thus building applications
that enable developers with limited machine-learning expertise to
train high-quality models specific to their domain or data [22, 39].
However, the DRL is quite different from conventional machine
learning approaches. For example, training data of DRL is not pre-
pared in advance compared with conventional supervised learn-
ing but collected through an agent-environment interaction inside
the training process. Such a significant difference requires a re-
integration of the automated pipeline and a re-scheduling of the
cloud computing resources with respect to the conventional MLOps
principle. Therefore, we advocate extending the principle of MLOps
to the RLOps in finance paradigm to seek an opportunity for the
wider DRL adoption in production-level financial services.
3 STOCK TREND PREDICTION TASK
We describe the problem formulation of a typical financial task,
stock trend prediction, which locates at the task layer of Fig. 2. Our
setup follows a similar setting in [26, 44].
A stock trend prediction task is modeled as a Markov Decision
Process (MDP): given state 𝑠𝑡∈Sat time𝑡, an agent takes an action
𝑎𝑡∈A according to policy 𝜋𝜃(𝑠𝑡), transitions to the next state 𝑠𝑡+1
and receives an immediate reward 𝑟(𝑠𝑡,𝑎𝑡,𝑠𝑡+1)∈R. The policy
𝜋𝜃(𝑠)with parameter 𝜃is a function that maps a state to an action
vector over 𝑛stocks. The objective is to find an optimal policy 𝜋∗
𝜃
(a policy network parameterized by 𝜃) that maximizes the expected
return (the fitness score used in Fig. 2) over 𝑇times slots
𝜋∗
𝜃=argmax
𝜃𝐽(𝜋𝜃),where𝐽(𝜋𝜃)=E""𝑇∑︁
𝑡=0𝛾𝑡𝑟(𝑠𝑡,𝑎𝑡,𝑠𝑡+1)#
,(1)
where𝛾∈(0,1]is a discount factor.
Then, for the stock trend predictiont task with 𝑛stocks, we spec-
ify the state spaceS, action spaceA, reward function 𝑟(𝑠𝑡,𝑎𝑡,𝑠𝑡+1),
and the state transition, as in [26][44].
State spaceSdescribes an agent’s perception of a market envi-
ronment. We summarize various features that are used by human
trader and use them to construct the state space:
•Balance𝑏𝑡∈R+: the account balance at time 𝑡.
•Shares h𝑡∈Z𝑛+: the number of shares for 𝑛stocks at𝑡.
•Closing price p𝑡∈R𝑛+: the closing prices of 𝑛stocks at𝑡.
•Technical indicators help the agent make decisions. Users can
select existing indicators or add new indicators. E.g., Moving
Average Convergence Divergence (MACD) M𝑡∈R𝑛, Relative
Strength Index (RSI) R𝑡∈R𝑛+, Commodity Channel Index (CCI)
C𝑡∈R𝑛+, etc.
Action spaceAdescribes the allowed actions an agent can take
at states𝑠𝑡,𝑡=1,...,𝑇 . For one stock, action is 𝑎∈{−𝑘,...,−1,0,1,...,𝑘},where𝑘∈Zor−𝑘∈Zdenotes the number of shares to buy or sell,
respectively, and 𝑎=0means to hold. Users can set a maximum
number of shares ℎmaxfor a transaction, i.e., 𝑘≤ℎmax, or set a
maximum ratio of capital to allocate on each stock.
Reward𝑟𝑡for taking action 𝑎𝑡at state𝑠𝑡and arriving at state
𝑠𝑡+1. Reward is the incentive for an agent to improve its policy for
the sake of getting higher rewards. A relatively simple reward can
be defined as the change of the account value, i.e.,
𝑟𝑡=(𝑏𝑡+1+p𝑇
𝑡+1h𝑡+1)−(𝑏𝑡+p𝑇
𝑡h𝑡)−𝑐𝑡, (2)
where the first and second terms are the account values at 𝑠𝑡+1and
𝑠𝑡, and𝑐𝑡denotes the transaction cost (market friction).
Transition(𝑠𝑡,𝑎𝑡,𝑟𝑡,𝑠𝑡+1). Taking action 𝑎𝑡at state𝑠𝑡, the en-
vironment steps forward and arrives at state 𝑠𝑡+1. A transition
involves the change of balance, number of shares, and the stock
prices due to the market changes. We split the stocks into three
sets: selling set 𝑆, buying set 𝐵and holding set 𝐻, respectively. The
new balance is
𝑏𝑡+1=𝑏𝑡+(p𝑆
𝑡)𝑇k𝑆
𝑡−(p𝐵
𝑡)𝑇k𝐵
𝑡, (3)
where p𝑆∈R𝑛andk𝑆∈R𝑛are the vectors of prices and number
of selling shares for the selling stocks, and p𝐵∈R𝑛andk𝐵∈R𝑛
for the buying stocks. The number of shares becomes
h𝑡+1=h𝑡−k𝑆
𝑡+k𝐵
𝑡≥0. (4)
The supercomputing power is necessary to achieve the massively
parallel simulations for an STP task. During the training, a DRL
agent keeps observing and trading on the historical market data
to sample trajectories (one trajectory is a series of transitions).
However, the historical data has to be significantly large in order to
provide a broadened horizon. For example, the historical data could
scale up in two dimensions: the data volume anddata type [16]:
•The data volume varies with respect to:
– the length of data period : from several months up to more
than ten years.
– the time granularity : from daily-level to minute-level, second-
level or microsecond-level.
– the number of stocks : from thirty (Dow 30) to hundreds
(NASDAQ 100 or S&P 500), or even covers the whole market.
•The data type varies with respect to:
– the raw market data includes data points of open-high-low-
close-volume (OHLCV) for each stock, which provides a direct
understanding of a stock’s market performance.
– the alternative data usually refers to the large-scale collec-
tion of both structured and unstructured data, e.g., market
news, academic graph data [ 4], credit card transactions and
GPS traffic. The agent could employ different encoders to an-
alyze the insights of investment techniques provided by the
alternative data.
– the indexes and labels could be directly given as a kind of
powerful technical indicator, which helps the agent make deci-
sions.
In practice, the market simulation, alternative data processing
and index analyzing are computationally expensive, therefore, a
cloud-level solution is critical to a fast iteration of a trading strategy.",2021-11-07T01:03:15Z, racer hh formance scalable ep reinforcement learni quantitative nance conference july washitoprinciple ops recently  machine learni oatns ops ops iops for suops trefore ops ops   our mark cisprocess at t t  tstate sib balance sharosi technical users movi ge convergence divergence relative sth inx coodity nel inx asibfor or notusers reward reward transittaki  t t t  for t dow t t in
paper_qf_29.pdf,4,"FinRL-Podracer: High Performance and Scalable Deep Reinforcement
  Learning for Quantitative Finance","  Machine learning techniques are playing more and more important roles in
finance market investment. However, finance quantitative modeling with
conventional supervised learning approaches has a number of limitations. The
development of deep reinforcement learning techniques is partially addressing
these issues. Unfortunately, the steep learning curve and the difficulty in
quick modeling and agile development are impeding finance researchers from
using deep reinforcement learning in quantitative trading. In this paper, we
propose an RLOps in finance paradigm and present a FinRL-Podracer framework to
accelerate the development pipeline of deep reinforcement learning (DRL)-driven
trading strategy and to improve both trading performance and training
efficiency. FinRL-Podracer is a cloud solution that features high performance
and high scalability and promises continuous training, continuous integration,
and continuous delivery of DRL-driven trading strategies, facilitating a rapid
transformation from algorithmic innovations into a profitable trading strategy.
First, we propose a generational evolution mechanism with an ensemble strategy
to improve the trading performance of a DRL agent, and schedule the training of
a DRL algorithm onto a GPU cloud via multi-level mapping. Then, we carry out
the training of DRL components with high-performance optimizations on GPUs.
Finally, we evaluate the FinRL-Podracer framework for a stock trend prediction
task on an NVIDIA DGX SuperPOD cloud. FinRL-Podracer outperforms three popular
DRL libraries Ray RLlib, Stable Baseline 3 and FinRL, i.e., 12% \\\\sim 35%
improvements in annual return, 0.1 \\\\sim 0.6 improvements in Sharpe ratio and 3
times \\\\sim 7 times speed-up in training time. We show the high scalability by
training a trading agent in 10 minutes with $80$ A100 GPUs, on NASDAQ-100
constituent stocks with minute-level data over 10 years.
","Conference’17, July 2017, Washington, DC, USA Zechu Li, Xiao-Yang Liu, Jiahao Zheng, Zhaoran Wang, Anwar Walid, and Jian Guo
Figure 2: Overview of FinRL-Podracer that has three layers: trading task layer, evolution layer and training layer.
4 FINRL-PODRACER FRAMEWORK
We propose a FinRL-Podracer framework to utilize the supercomput-
ing power of a GPU cloud for training DRL-driven trading strategies.
We first present an overview of FinRL-Podracer and then describe
its layered architecture.
4.1 Overview
Based on the experiments in Table 1, we found that existing DRL li-
braries/packages [ 13,19,25,26] have three major issues that restrict
the trading performance and training efficiency:
•There is no criteria to determine overfitting or underfitting
of models (trading strategies) during the training process. It
is critical to overcome underfitting by utilizing more computing
power and avoid overfitting that wastes computing power, while
both cases would lead to suboptimal models.
•The training process of a trading strategy is sensitive to hyper-
parameters , which may result in unstable trading performance
in backtesting and trading. However, it is tedious for human
traders to search for a good combination of hyper-parameters,
and thus an automatic hyper-parameter search is favored.
•Computing power is critical to effectively explore and exploit
large-scale financial data. Sufficient exploration guarantees a
good trading performance, and then smart exploitation results
in good training efficiency. A strong computing power helps
achieve a balance between exploration and exploitation.
Therefore, we provide a high performance and scalable solution
on a GPU cloud, FinRL-Podracer , to develop a profitable DRL-driven
trading strategy within a small time window. To fully utilize a
GPU cloud, say an NVIDIA DGX SuperPod cloud [ 38], we organize
FinRL-Podracer into a three-layer architecture in Fig. 2, a tradingtask layer on the top, an evolution layer in the middle and a training
layer at the bottom.
In the evolution layer, we employ a generational evolution mech-
anism with the ensemble strategy and address the issues of over-
fitting and hyper-parameter sensitivity through the synergy of an
evaluator and a selector . The evaluator computes the fitness scores
𝐽(𝜋𝜃)in (1) of a population of 𝑁agents and mitigates the perfor-
mance collapse caused by overfitting. The hyper-parameter search
is automatically performed via agent evolution , where the selector
uses the fitness scores to guide the search direction. An effective
cloud-level evolution requires a high-quality and scalable schedul-
ing, therefore we schedule a population of parallel agents through
a multi-level mapping.
In the training layer, we realize high-performance GPU-oriented
optimizations of a decomposable DRL training pipeline. We lo-
cally optimize each component (a container within a pod), namely
explorer, replay buffer, and learner, through parallelism encapsu-
lation, GPU acceleration, efficient parameter fusion, and storage
optimization. Thus, we maximize the hardware usage and minimize
the communication overhead, which allows each component to be
efficiently executed on a GPU cloud.
Such an evolution-and-training workflow pipelines the devel-
opment of a trading strategy on a GPU cloud. It enjoys great per-
formance and scalability, which promotes fast and flexible devel-
opment, deployment and production of profitable DRL trading
strategies.
4.2 Scalable Evolution Layer
FinRL-Podracer exploits a generational evolution mechanism with
an ensemble strategy to coordinate the parallel agents and to auto-
matically search the best hyper-parameters. For each generation,",2021-11-07T01:03:15Z,conference july washitoze chu li  ya  jia hao c hao rawa near valid jaguo  overview  racer   racer   racer overview based table tre it t uti sufcient trefore  racer to su   racer  it t ai  suit scalable evolutlayer  racer for
paper_qf_29.pdf,5,"FinRL-Podracer: High Performance and Scalable Deep Reinforcement
  Learning for Quantitative Finance","  Machine learning techniques are playing more and more important roles in
finance market investment. However, finance quantitative modeling with
conventional supervised learning approaches has a number of limitations. The
development of deep reinforcement learning techniques is partially addressing
these issues. Unfortunately, the steep learning curve and the difficulty in
quick modeling and agile development are impeding finance researchers from
using deep reinforcement learning in quantitative trading. In this paper, we
propose an RLOps in finance paradigm and present a FinRL-Podracer framework to
accelerate the development pipeline of deep reinforcement learning (DRL)-driven
trading strategy and to improve both trading performance and training
efficiency. FinRL-Podracer is a cloud solution that features high performance
and high scalability and promises continuous training, continuous integration,
and continuous delivery of DRL-driven trading strategies, facilitating a rapid
transformation from algorithmic innovations into a profitable trading strategy.
First, we propose a generational evolution mechanism with an ensemble strategy
to improve the trading performance of a DRL agent, and schedule the training of
a DRL algorithm onto a GPU cloud via multi-level mapping. Then, we carry out
the training of DRL components with high-performance optimizations on GPUs.
Finally, we evaluate the FinRL-Podracer framework for a stock trend prediction
task on an NVIDIA DGX SuperPOD cloud. FinRL-Podracer outperforms three popular
DRL libraries Ray RLlib, Stable Baseline 3 and FinRL, i.e., 12% \\\\sim 35%
improvements in annual return, 0.1 \\\\sim 0.6 improvements in Sharpe ratio and 3
times \\\\sim 7 times speed-up in training time. We show the high scalability by
training a trading agent in 10 minutes with $80$ A100 GPUs, on NASDAQ-100
constituent stocks with minute-level data over 10 years.
","FinRL-Podracer: High Performance and Scalable Deep Reinforcement Learning for Quantitative Finance Conference’17, July 2017, Washington, DC, USA
it is composed of model ensemble andpopulation ranking , as
shown in the middle layer of Fig. 2. At present, we utilize an eval-
uator and a selector to schedule the agent evolution, where more
modules can be incorporated, e.g., a monitor, an allocator, etc.
The evaluator evaluates agents and provides their fitness scores
as the metric for the future ranking, as shown in the ranking stage
of Fig. 2. From our observations, it is difficult for users to use ex-
isting libraries [ 13,19,25,26] to train a profitable trading strategy
because the overfitting agent may be treated as the best agent as
the training process moves forward. When the dataset scales up,
we need to increase the training time/steps to fully explore the
large-scale data, making it harder to set appropriate stop criteria,
and the resulting agent may hardly be the best one. The evaluator
effectively mitigates the performance collapse brought by overfit-
ting: in the course of the training, it evaluates the agent at each
iteration, outputs a fitness score, and keeps track of the best agent
so far; when the fitness score in (1) drops, the evaluator would
stop the training process using the early stopping mechanism and
output the best agent as the final agent.
The selector acts as a central controller to perform the selection
strategy as in a genetic algorithm (GA) [ 29]. GA is an optimization
algorithm inspired by natural evolution: at every generation, a pop-
ulation of N agents is trained, and the evaluator calculates their
fitness scores in (1) based on an objective function; then the selector
redistributes the agents with the highest scores to form a new pop-
ulation for the next generation. Since the agents are parallel and
replicable, the concept of natural selection scales up well on a GPU
cloud. As shown in the evolution layer of Fig. 2, there are 𝑁agents
with different hyper-parameters in a population. The synergy of the
evaluator and selector enables FinRL-Podracer to naturally select
the best agent for the future generation and eliminates the poten-
tial negative impact from poorly evolved agents, which effectively
improves the stability and efficiency of the training.
FinRL-Podracer achieves the ensemble training of an agent by
concurrently running K pods (training processes) in parallel and
fusing the trained models from K pods at each epoch. All parallel
pods for each agent are initialized with same hyper-parameters but
different random seeds. Such a design, as shown in the ensemble
stage of the evolution layer in Fig. 2, guarantees randomness and
stabilizes the learning process of the agent. The experiment results
in Section 5 will perform an ablation study of the improvement
brought by the generational evolution mechanism.
4.3 Packaging Worker-Learner into Pod
FinRL-Podracer achieves effective and efficient allocation of cloud
resources through a multi-level mapping, which follows the prin-
ciple of decomposition -and- encapsulation . We employ a worker-
learner decomposition [ 10–12] that splits a DRL training process
(pod) into three components (containers):
•Worker (exploration) : samples transitions through the
actor-environment interactions.
•Replay buffer : stores transitions from a worker and feeds
a batch of transitions to a learner.
•Learner (exploitation) : consumes transitions and trains
the neural networks.
Figure 3: Two implementations of a training process: a) the
environment simulation (green), action inference (yellow),
and model update (red) are all located on GPUs. b) the envi-
ronment simulation is executed on CPUs, and action infer-
ence and model update are on GPUs. An NVIDIA DGX-A100
server [38][5] contains 8A100 GPUs.
Each training process of an agent consists of the three types of
components, which are packaged into a suite that is mapped into a
GPU pod. In addition, We run those components separately where
each component is mapped to a GPU container. Such a two-level
mapping is natural since a GPU pod consists of multiple containers,
while correspondingly a training process of an agent consists of
different components.
The above pod-container structure enables scalable allocation
of GPU computing resources. We take advantage of a GPU cloud
software stack and use the Kubernetes (K8S) software to scalably
coordinate pods among severs. Consider a cloud with 10servers
(i.e., 80A100 GPUs), we encapsulate a package of components into a
pod, replicate it 80times, and send them to a K8S server. Then, K8S
distributes these 80pod replicas to computing nodes that carry out
the training process. The pod replication reflects strong parallelism,
and it is highly scalable since a GPU cloud can support a large
number of pods.
4.4 High Performance Training Layer
The optimization of each component is critical to the overall perfor-
mance. We describe the hardware-oriented optimizations of com-
ponents, including parallelism encapsulation, GPU acceleration,
efficient parameter fusion and storage optimization.
Market simulation with GPU-acceleration . The market sim-
ulation is both computing- and communication-intensive. We pro-
pose a batch mode to perform massively parallel simulations, which
maximizes the hardware utilization (either CPUs or GPUs). We in-
stantiate multiple independent sub-environments in a batched envi-
ronment, and a batched environment is exposed to a rollout worker
that takes a batch of actions and returns a batch of transitions.
Fig. 3 a) illustrates a GPU-accelerated environment. Environ-
ments of financial tasks are highly suitable to GPUs because finan-
cial simulations involve ""simple"" arithmetics, where a GPU with",2021-11-07T01:03:15Z, racer hh formance scalable ep reinforcement learni quantitative nance conference july washito at t  from  t since as  t  racer  racer all su t sepackagi worker learner   racer  worker replay learner  two us us us aus eai sut  ku borne tconsir us tt hh formance tr articial intellence ni layer t  market t  us us   eirous
paper_qf_29.pdf,6,"FinRL-Podracer: High Performance and Scalable Deep Reinforcement
  Learning for Quantitative Finance","  Machine learning techniques are playing more and more important roles in
finance market investment. However, finance quantitative modeling with
conventional supervised learning approaches has a number of limitations. The
development of deep reinforcement learning techniques is partially addressing
these issues. Unfortunately, the steep learning curve and the difficulty in
quick modeling and agile development are impeding finance researchers from
using deep reinforcement learning in quantitative trading. In this paper, we
propose an RLOps in finance paradigm and present a FinRL-Podracer framework to
accelerate the development pipeline of deep reinforcement learning (DRL)-driven
trading strategy and to improve both trading performance and training
efficiency. FinRL-Podracer is a cloud solution that features high performance
and high scalability and promises continuous training, continuous integration,
and continuous delivery of DRL-driven trading strategies, facilitating a rapid
transformation from algorithmic innovations into a profitable trading strategy.
First, we propose a generational evolution mechanism with an ensemble strategy
to improve the trading performance of a DRL agent, and schedule the training of
a DRL algorithm onto a GPU cloud via multi-level mapping. Then, we carry out
the training of DRL components with high-performance optimizations on GPUs.
Finally, we evaluate the FinRL-Podracer framework for a stock trend prediction
task on an NVIDIA DGX SuperPOD cloud. FinRL-Podracer outperforms three popular
DRL libraries Ray RLlib, Stable Baseline 3 and FinRL, i.e., 12% \\\\sim 35%
improvements in annual return, 0.1 \\\\sim 0.6 improvements in Sharpe ratio and 3
times \\\\sim 7 times speed-up in training time. We show the high scalability by
training a trading agent in 10 minutes with $80$ A100 GPUs, on NASDAQ-100
constituent stocks with minute-level data over 10 years.
","Conference’17, July 2017, Washington, DC, USA Zechu Li, Xiao-Yang Liu, Jiahao Zheng, Zhaoran Wang, Anwar Walid, and Jian Guo
Hyper-parameters Value
Total #GPUs 80
#Agent (𝑁) 10
#Pods per agent ( 𝐾) 8
Optimizer Adam
Learning rate 2−14
Discount factor 𝛾=0.99
Total steps 220
Batch size 210
Repeat times 23
Replay buffer Size 212
Ratio clip (PPO) 0.25
Lambda entropy (PPO) 0.02
Evaluation interval (second) 64
Table 2: Hyper-parameter settings in our experiments.
thousands of cores has the natural advantages of matrix compu-
tations and parallelism. Then, financial environments written in
CUDA can speed up the simulation. The GPU-accelerated envi-
ronment also effectively reduces the communication overhead by
bypassing CPUs, as supported by a GPU cloud [ 38]. The output
transitions are stored as a tensor in GPU memory, which can be
directly fetched by learners, avoiding the data transfer between
CPU and GPU back and forth.
Fig. 3 b) presents an environment on CPUs. There are some
financial simulations with frequent CPU usage (addressing trading
constraints), making it inefficient to compute on GPUs. In our
experiments, some environments run much slower on GPUs than
CPUs. Thus, we simulate those environments on CPUs.
Replay buffer on GPU . We allocate the replay buffer on the
contiguous memory of GPUs, which increases the addressing speed
and bypasses CPUs for faster data transfer. As the worker and
learner are co-located on GPUs, we store all transitions as tensors
on the contiguous memory of GPUs. Since the collected transitions
are packed together, the addressing speed increases dramatically
well when a learner randomly samples a batch of transitions to
update network parameters.
Learner with optimizations . To better support the ensemble
training in the evolution layer, we propose a novel and effective
way for learners of each pod to communicate, i.e., sending the net-
work parameters rather than the gradients. Most existing libraries
[13,19,25,26] send the gradients of learners by following a tradi-
tional synchronization approach on supervised learning. Such an
approach is inefficient for DRL algorithms since the learner will
update the neural network hundreds of times within each training
epoch, namely it needs to send gradients hundreds of times. By
taking advantage of the soft update [ 21], we send the model pa-
rameters rather than the gradients. The parameter of the models
is amenable to communication because model size in DRL is not
comparable to that in other deep learning fields. Here, communica-
tion happens once at the end of each epoch, which is a significantly
lower frequency of communication.5 PERFORMANCE EVALUATION
We describe the GPU cloud platform, the performance metrics and
compared methods, and then evaluate the performance of FinRL-
Podracer for a stock trend prediction task.
5.1 GPU Cloud Platform
All experiments were executed on NVIDIA DGX-2 servers [ 5] in
an NVIDIA DGX SuperPOD platform [ 38], a cloud-native super-
computer. We use 256 CPU cores of Dual AMD Rome 7742 running
at 2.25GHz for each experiment. An NVIDIA DGX-2 server has 8
A100 GPUs and 320GB GPU memory [5].
5.2 Performance Metrics
We evaluate the trading performance and training efficiency of the
FinRL-Podracer for a stock trend prediction task.
Data pre-processing . We select the NASDAQ-100 constituent
stocks as our stock pool, accessed at 05/13/2019 (the starting time of
our testing period), and use the datasets with two time granularities:
minute-level and daily. The daily dataset is directly downloaded
from Yahoo!Finance, while the minute-level dataset is first down-
loaded as raw data from the Compustat database through the Whar-
ton Research Data Services (WRDS) [ 33] and then pre-processed
to an open-high-low-close-volume (OHLCV) format. We split the
datasets into training period and backtesting period: the daily data
from 01/01/2009 to 05/12/2019 for training; the minute-level data
from 01/01/2016 to 05/12/2019 for training; For both datasets, we
backtest on the same period from 05/13/2019 to 05/26/2021.
Evaluation metrics . Six common metrics are used to evaluate
the experimental results:
•Cumulative return : subtracting the initial value from the final
portfolio value, then dividing by the initial value.
•Annual return and volatility : geometric average return in a
yearly sense, and the corresponding deviation.
•Sharpe ratio : the average return earned in excess of the risk-free
rate per unit of volatility.
•Max drawdown : the maximum observed loss from a historical
peak to a trough of a portfolio, before a new peak is achieved.
Maximum drawdown is an indicator of downside risk over a time
period.
•Cumlative return vs. training time : the cumulative return
during the testing period, achieved by an agent trained within a
certain amount of time.
Compared methods . For trading performance evaluation, we
compare FinRL-Podracer and vanilla FinRL-Podracer (without agent
evolution) with FinRL [ 25,26], RLlib [ 19], Stable Baseline3 [ 9], and
NASDAQ Composite/Invesco QQQ ETF. We use Proximal Policy
Optimization (PPO) [ 32] as the DRL algorithm in the reported re-
sults and fine-tune each library to maximize its performance. Each
library is allowed to use up to 80 GPUs.
For training efficiency evaluation, the experiments are conducted
on multiple GPUs. We compare with RLlib [ 19] since it has high
performance on distributed infrastructure. However, both FinRL
[25] and Stable Baseline 3 [ 9] do not support the training on mul-
tiple GPUs, thus we do not compare with them. We keep hyper-
parameters and computing resources the same to guarantee fair",2021-11-07T01:03:15Z,conference july washitoze chu li  ya  jia hao c hao rawa near valid jaguo  value total us agent s optied adam learni diunt total batrepeat replay size rat lambda evaluattable  tt us t  us tre us ius us  us replay  us us as us us since learner to most suby t re   racer oud platform all su  dual rome  aus formance metrics   racer data  t yahoo nance  ust at wha researdata servic for evaluatsix cumulative annual share max maximum cum lat ive ared for  racer  racer lli stable baseline osite investor  proximal policy optiateaus for us  lli stable baseline us 
paper_qf_29.pdf,7,"FinRL-Podracer: High Performance and Scalable Deep Reinforcement
  Learning for Quantitative Finance","  Machine learning techniques are playing more and more important roles in
finance market investment. However, finance quantitative modeling with
conventional supervised learning approaches has a number of limitations. The
development of deep reinforcement learning techniques is partially addressing
these issues. Unfortunately, the steep learning curve and the difficulty in
quick modeling and agile development are impeding finance researchers from
using deep reinforcement learning in quantitative trading. In this paper, we
propose an RLOps in finance paradigm and present a FinRL-Podracer framework to
accelerate the development pipeline of deep reinforcement learning (DRL)-driven
trading strategy and to improve both trading performance and training
efficiency. FinRL-Podracer is a cloud solution that features high performance
and high scalability and promises continuous training, continuous integration,
and continuous delivery of DRL-driven trading strategies, facilitating a rapid
transformation from algorithmic innovations into a profitable trading strategy.
First, we propose a generational evolution mechanism with an ensemble strategy
to improve the trading performance of a DRL agent, and schedule the training of
a DRL algorithm onto a GPU cloud via multi-level mapping. Then, we carry out
the training of DRL components with high-performance optimizations on GPUs.
Finally, we evaluate the FinRL-Podracer framework for a stock trend prediction
task on an NVIDIA DGX SuperPOD cloud. FinRL-Podracer outperforms three popular
DRL libraries Ray RLlib, Stable Baseline 3 and FinRL, i.e., 12% \\\\sim 35%
improvements in annual return, 0.1 \\\\sim 0.6 improvements in Sharpe ratio and 3
times \\\\sim 7 times speed-up in training time. We show the high scalability by
training a trading agent in 10 minutes with $80$ A100 GPUs, on NASDAQ-100
constituent stocks with minute-level data over 10 years.
","FinRL-Podracer: High Performance and Scalable Deep Reinforcement Learning for Quantitative Finance Conference’17, July 2017, Washington, DC, USA
May 13Oct 03 Feb 27Jul 21 Dec 10 May 61.01.21.41.61.82.02.22.42.6Cumulative ReturnFinRL-Podracer
FinRL-Podracer (vanilla)
RLlib
Stable Baseline 3
FinRL
QQQMay 13
2019Oct 03 Feb 27
2020Jul 21 Dec 10 May 6
20211.01.21.41.61.82.02.22.42.6Cumulative ReturnFinRL-Podracer
FinRL-Podracer (vanilla)
RLlib
Stable Baseline 3
FinRL
QQQ
Figure 4: Cumulative returns on daily dataset during
05/13/2019 to 05/26/2021. Initial capital $1,000,000, transac-
tion cost percentage 0.2%, and Invesco QQQ ETF is a market
benchmark.
May 13Oct 03 Feb 27Jul 21 Dec 10 May 261.02.03.04.05.0Cumulative ReturnFinRL-Podracer
FinRL-Podracer (vanilla)
RLlib
Stable Baseline 3
FinRL
QQQMay 13
2019Oct 03 Feb 27
2020Jul 21 Dec 10 May 26
20211.02.03.04.05.0Cumulative ReturnFinRL-Podracer
FinRL-Podracer (vanilla)
RLlib
Stable Baseline 3
FinRL
QQQFigure 5: Cumulative returns on minute dataset during
05/13/2019 to 05/26/2021. Initial capital $1,000,000, transac-
tion cost percentage 0.2%, Invesco QQQ ETF is a market
benchmark.
Cumul. return Annual return Annual volatility Max drawdown Sharpe ratio
FinRL-Podracer ( Ours )149.553% /362.408% 56.431% /111.549% 22.331% /33.427% -13.834% /-15.874% 2.12 /2.42
FinRL-Podracer (vanilla) 73.546%/231.747% 30.964%/79.821% 23.561%/31.024% -18.428%/-21.002% 1.27/2.05
RLlib [19] 58.926%/309.54% 25.444%/99.347% 30.009%/31.893% -23.248%/-22.292% 0.91/2.33
Stable Baseline3 [13] 85.539%/218.531% 35.316%/76.28% 31.592%/34.595% -24.056%/-23.75% 1.12/1.82
FinRL [25] 78.255%/169.975% 32.691%/62.576% 37.641%/42.908% -26.774%/-27.267% 0.94/1.35
Invesco QQQ ETF 89.614% 36.763% 28.256% -28.559% 1.25
Table 3: Performance of stock trading on NASDAQ-100 constituent stocks with daily (red) and minute-level (blue) data.
comparisons, and a general hyper-parameter setting is given in
Table 2.
5.3 Trading Performance
We backtest the trading performance from 05/13/2019 to 05/26/2021
on both daily and minute-level datasets. From Fig. 4 and Fig. 5, all
DRL agents are able to achieve a better or equal performance than
the market in cumulative return, which demonstrates the profit po-
tentials of DRL-driven trading strategies. Comparing Fig. 4 with Fig.
5, we observe that all methods have a much better performance on
the minute-level dataset than that on the daily dataset. The trading
performance of most agents is almost the same as that of the mar-
ket on daily dataset, however, all agents significantly outperform
the market if they have a larger dataset to explore. With a higher
granularity data, the Sharpe ratios are also lifted up to a new level.
From Table 3, agents achieve Sharpe ratios of 2.42, 2.05, 2.33, 1.82,
1.35 on the minute-level dataset, which are 0.3, 0.78, 1.42, 0.7, and
0.41 higher than those on the daily dataset. Therefore, we conclude
that the capability to process large-scale financial data is critical
for the development of a profitable DRL-driven trading strategy
since the agent can better capture the volatility and dynamics of
the market.
From Table 3, Fig. 4, and Fig. 5, we also observe that our FinRL-
Podracer outperforms other baselines on both datasets, in terms of
expected return, stability, and Sharpe ratio. As can be seen from
Table 3, our FinRL-Podracer achieves the highest cumulative returnsof 149.533% and 362.408%, annual returns of 56.431% and 111.549%,
and Sharpe ratios of 2.12 and 2.42, which are much higher than the
others. Furthermore, FinRL-Podracer also shows an outstanding
stability during the backtesting: it achieves Max drawdown -13.834%
and -15.874%, which is much lower than other methods. Consider
the vanilla FinRL-Podracer as a direct comparison, we find that
vanilla FinRL-Podracer has a similar trading performance with other
baseline frameworks, which is in consistent with our expectation
since the settings are the same. Such a performance improvement
of FinRL-Podracer over vanilla FinRL-Podracer demonstrates the
effectiveness of the generational evolution mechanism, as further
verified by Fig. 7.
5.4 Training Efficiency
We compare the training efficiency of FinRL-Podracer with RLlib
[19] on a varying number of A100 GPUs, i.e., 8, 16, 32, and 80.
We store the model snapshots at different training time, say every
100seconds, then later we use each snapshot model to perform
inference on the backtesting dataset and obtain the generalization
performance, namely, the cumulative return.
In Fig. 6, as the number of GPUs increases, both FinRL-Podracer
and RLlib achieve a higher cumulative return with the same training
time (wall-clock time). FinRL-Podracer with 80GPUs has a much
steeper generalization curve than others, e.g., it can achieve a cumu-
lative return of 4.0at800s, which means it learns in a much faster
speed. However, FinRL-Podracer with 32GPUs and 16GPUs need",2021-11-07T01:03:15Z, racer hh formance scalable ep reinforcement learni quantitative nance conference july washitomay   jul c may cumulative ur racer  racer lli stable baseline may   jul c may cumulative ur racer  racer lli stable baseline  cumulative initial investor may   jul c may cumulative ur racer  racer lli stable baseline may   jul c may cumulative ur racer  racer lli stable baseline  cumulative initial investor cum ul annual annual max share  racer ours  racer lli stable baseline investor table formance table tradi formance  from   ari   t with share from table share trefore from table    racer share as table  racer share furtr  racer max consir  racer  racer su racer  racer  tr articial intellence ni efciency   racer lli us  i us  racer lli  racer us  racer us us
paper_qf_29.pdf,8,"FinRL-Podracer: High Performance and Scalable Deep Reinforcement
  Learning for Quantitative Finance","  Machine learning techniques are playing more and more important roles in
finance market investment. However, finance quantitative modeling with
conventional supervised learning approaches has a number of limitations. The
development of deep reinforcement learning techniques is partially addressing
these issues. Unfortunately, the steep learning curve and the difficulty in
quick modeling and agile development are impeding finance researchers from
using deep reinforcement learning in quantitative trading. In this paper, we
propose an RLOps in finance paradigm and present a FinRL-Podracer framework to
accelerate the development pipeline of deep reinforcement learning (DRL)-driven
trading strategy and to improve both trading performance and training
efficiency. FinRL-Podracer is a cloud solution that features high performance
and high scalability and promises continuous training, continuous integration,
and continuous delivery of DRL-driven trading strategies, facilitating a rapid
transformation from algorithmic innovations into a profitable trading strategy.
First, we propose a generational evolution mechanism with an ensemble strategy
to improve the trading performance of a DRL agent, and schedule the training of
a DRL algorithm onto a GPU cloud via multi-level mapping. Then, we carry out
the training of DRL components with high-performance optimizations on GPUs.
Finally, we evaluate the FinRL-Podracer framework for a stock trend prediction
task on an NVIDIA DGX SuperPOD cloud. FinRL-Podracer outperforms three popular
DRL libraries Ray RLlib, Stable Baseline 3 and FinRL, i.e., 12% \\\\sim 35%
improvements in annual return, 0.1 \\\\sim 0.6 improvements in Sharpe ratio and 3
times \\\\sim 7 times speed-up in training time. We show the high scalability by
training a trading agent in 10 minutes with $80$ A100 GPUs, on NASDAQ-100
constituent stocks with minute-level data over 10 years.
","Conference’17, July 2017, Washington, DC, USA Zechu Li, Xiao-Yang Liu, Jiahao Zheng, Zhaoran Wang, Anwar Walid, and Jian Guo
0 500 1000 1500 2000 2500 3000 35001.02.03.04.0Cumulative ReturnFinRL-Podracer 8 GPUs
FinRL-Podracer 16 GPUs
FinRL-Podracer 32 GPUs
FinRL-Podracer 80 GPUs
RLlib 8 GPUs
RLlib 16 GPUs
RLlib 32 GPUs
RLlib 80 GPUs0 500 1000 1500 2000 2500 3000 3500
Training Time (Seconds)1.02.03.04.0Cumulative ReturnFinRL-Podracer 8 GPUs
FinRL-Podracer 16 GPUs
FinRL-Podracer 32 GPUs
FinRL-Podracer 80 GPUs
RLlib 8 GPUs
RLlib 16 GPUs
RLlib 32 GPUs
RLlib 80 GPUs
Figure 6: Generalization performance on backtesting dataset,
using the model snapshots of FinRL-Podracer and RLlib [19]
at different training time (wall clock time).
0 800 1600 2400 3200 36000.01.02.03.04.0Cumulative Return
800 1600 2400 32000.01.02.03.04.00 800 1600 2400 3200 3600
Training Time (Seconds)0.01.02.03.04.0Cumulative Return
800 1600 2400 32000.01.02.03.04.0Figure 7: Generalization performance of a model along the
agent evolution in the training process of FinRL-Podracer.
2,200s and 3,200s to achieve the same cumulative return, respec-
tively. The generalization curves of RLlib with different numbers of
GPUs are relatively similar, and we do not observe much speed-up.
For example, FinRL-Podracer needs approximately 300s to achieve
a cumulative return of 3.5, however, RLlib needs 2,200s to achieve
the same cumulative return. FinRL-Podracer is 3×∼ 7×faster than
RLlib.
It is counter-intuitive that the increase of GPU resources
not only makes FinRL-Podracer have a fast training, but also
improves the trading performance over RLlib [19] . We know
from Fig. 4 and Fig. 5 that the generational evolution mechanism
promotes the trading performance of FinRL-Podracer, therefore, we
empirically investigate the agent evolution process. Fig. 7 explicitly
demonstrates an evolution of 𝑁=10agents, where the selector
chooses the best model to train in the next generation every 800s.
The inner figure of Fig. 7 depicts the generalization curves of the
ten agents in the first generation (without using the agent evolution
mechanism). The curve with the evolution mechanism (the thick
green curve) is substantially higher than the other ten curves.
6 DISCUSSION AND CONCLUSION
In this paper, we have proposed a high-performance and scalable
deep reinforcement learning framework, FinRL-Podracer , to initiate
a paradigm shift from conventional supervised learning approaches
toRLOps in finance . FinRL-Podracer provides a highly automated
development pipeline of DRL-driven trading strategies on a GPU
cloud, which aims to help finance researchers and quantitative
traders overcome the steep learning curve and take advantage of
supercomputing power from the cloud platforms.
FinRL-Podracer achieved promising performance on a cloud plat-
form, mainly by following the two principles, the virtues of nested
hierarchies andgetting smart from dumb things [15]. For low-level
training, FinRL-Podracer realizes nested hierarchies by empolying
hardware-oriented optimizations, including parallelism encapsula-
tion, GPU acceleration, and storage optimizations. As a high level
scheduling, FinRL-Podracer obtains a smart agent from hundreds
of weak agents, which is the essence of ensemble methods, byemploying a generational evolution mechanism. We further inves-
tigate the evolution and training layers in a followup work [ 24] for
a cloud-native solution. We believe that ensemble multiple weak
agents is preferable to aiming to train one strong agent. Thus we
propose a new orchestration mechanism, a tournament-based en-
semble training method [ 24] with asynchronous parallelism, which
involves relatively low communication overhead. Also, we observe
the great potential of massively parallel simulation, which lifts the
exploration capability up into a potentially new dimension.
FinRL-Podracer is our first step from building a standard DRL
pipeline of financial tasks to using DRL agents to understand the
dynamics of the markets. We believe that FinRL-Podracer is critical
for the ecosystem of the FinRL community [ 25,26] because it of-
fers opportunities for many future directions. First, FinRL-Podracer
provides a way to take advantage of large-scale financial data. It
is possible to allow DRL agents to work in second or microsecond
level and cover all stocks in the market, which is meaningful for
the exploration and understanding of the dynamics of the mar-
ket. Moreover, training on the cloud makes DRL agents adapt to
much more complex financial simulations and neural networks,
thus achieving wider DRL applications to various financial tasks,
e.g., portfolio allocation, fraud detection, DRL-driven insights for
yield improvement and optimization. Furthermore, the low-level
optimizations in FinRL-Podracer could be also useful for the future
development of financial simulators, such as using GPU-accelerated
techniques to reduce latency.
ACKNOWLEDGEMENT
This research used computational resources of the GPU cloud plat-
form [38] provided by the IDEA Research institute.
REFERENCES
[1]Sridhar Alla and Suman Kalyan Adari. 2021. What Is MLOps? In Beginning
MLOps with MLFlow . Springer, 79–124.
[2] H. Buehler, L. Gonon, J. Teichmann, and B. Wood. 2019. Deep hedging. Quanti-
tative Finance 19 (2019), 1271 – 1291.
[3]L. Chen and Qiang Gao. 2019. Application of deep reinforcement learning
on automated stock trading. IEEE 10th International Conference on Software
Engineering and Service Science (ICSESS) (2019), 29–33.",2021-11-07T01:03:15Z,conference july washitoze chu li  ya  jia hao c hao rawa near valid jaguo cumulative ur racer us  racer us  racer us  racer us lli us lli us lli us lli us tr articial intellence ni time seconds cumulative ur racer us  racer us  racer us  racer us lli us lli us lli us lli us  genlizat racer lli cumulative urtr articial intellence ni time seconds cumulative ur genlizat racer t lli us for  racer lli  racer lli it  racer lli     racer  t  t i racer ops  racer  racer for  racer as  racer    also  racer   racer rst  racer it ofurtr  racer  researrida all sum akal yadari what is ops ibeginni ops flow  bu hler go noeichmanwood ep quant nance clia gao applicatinternatnal conference software eineeri service science
paper_qf_29.pdf,9,"FinRL-Podracer: High Performance and Scalable Deep Reinforcement
  Learning for Quantitative Finance","  Machine learning techniques are playing more and more important roles in
finance market investment. However, finance quantitative modeling with
conventional supervised learning approaches has a number of limitations. The
development of deep reinforcement learning techniques is partially addressing
these issues. Unfortunately, the steep learning curve and the difficulty in
quick modeling and agile development are impeding finance researchers from
using deep reinforcement learning in quantitative trading. In this paper, we
propose an RLOps in finance paradigm and present a FinRL-Podracer framework to
accelerate the development pipeline of deep reinforcement learning (DRL)-driven
trading strategy and to improve both trading performance and training
efficiency. FinRL-Podracer is a cloud solution that features high performance
and high scalability and promises continuous training, continuous integration,
and continuous delivery of DRL-driven trading strategies, facilitating a rapid
transformation from algorithmic innovations into a profitable trading strategy.
First, we propose a generational evolution mechanism with an ensemble strategy
to improve the trading performance of a DRL agent, and schedule the training of
a DRL algorithm onto a GPU cloud via multi-level mapping. Then, we carry out
the training of DRL components with high-performance optimizations on GPUs.
Finally, we evaluate the FinRL-Podracer framework for a stock trend prediction
task on an NVIDIA DGX SuperPOD cloud. FinRL-Podracer outperforms three popular
DRL libraries Ray RLlib, Stable Baseline 3 and FinRL, i.e., 12% \\\\sim 35%
improvements in annual return, 0.1 \\\\sim 0.6 improvements in Sharpe ratio and 3
times \\\\sim 7 times speed-up in training time. We show the high scalability by
training a trading agent in 10 minutes with $80$ A100 GPUs, on NASDAQ-100
constituent stocks with minute-level data over 10 years.
","FinRL-Podracer: High Performance and Scalable Deep Reinforcement Learning for Quantitative Finance Conference’17, July 2017, Washington, DC, USA
[4]Qian Chen and Xiao-Yang Liu. 2020. Quantifying ESG alpha using scholar big
data: an automated machine learning approach. Proceedings of the First ACM
International Conference on AI in Finance (2020).
[5]Jack Choquette, Wishwesh Gandhi, Olivier Giroux, Nick Stam, and Ronny
Krashinsky. 2021. NVIDIA A100 tensor core GPU: Performance and innova-
tion. IEEE Micro 41, 2 (2021), 29–35.
[6]Google Cloud. 2020. MLOps: Continuous delivery and automation pipelines
in machine learning. https://cloud.google.com/architecture/mlops-continuous-
delivery-and-automation-pipelines-in-machine-learning#mlops_level_0_
manual_process. Google Cloud , Jan. 07, 2020.
[7]Jacomo Corbo, Oliver Flemin, and Nicolas Hohn. 2021. It’s time for businesses to
chart a course for reinforcement learning. https://www.mckinsey.com/business-
functions/mckinsey-analytics/our-insights/its-time-for-businesses-to-chart-a-
course-for-reinforcement-learning. McKinsey Analytics , Apirl. 01, 2021.
[8]Quang-Vinh Dang. 2019. Reinforcement learning in stock trading. ICCSAMA
(2019).
[9]DLR-RM. 2021. Stable-baseline 3. https://github.com/DLR-RM/stable-baselines3.
[10] Lasse Espeholt, Raphaël Marinier, Piotr Stanczyk, Ke Wang, and Marcin Michalski.
2020. SEED RL: scalable and efficient deep-RL with accelerated central inference.
InProceedings of the International Conference on Learning Representations (ICLR) .
[11] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih,
Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg,
and Koray Kavukcuoglu. 2018. IMPALA: scalable distributed deep-RL with im-
portance weighted actor-learner architectures. In Proceedings of the International
Conference on Machine Learning (ICML) .
[12] Matteo Hessel, Manuel Kroiss, Aidan Clark, Iurii Kemaev, John Quan, Thomas
Keck, Fabio Viola, and Hado van Hasselt. 2021. Podracer architectures for scalable
reinforcement learning. ArXiv abs/2104.06272 (2021).
[13] Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto,
Rene Traore, Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol,
Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu.
2018. Stable baselines. https://github.com/hill-a/stable-baselines.
[14] Zhengyao Jiang, Dixing Xu, and Jinjun Liang. 2017. A deep reinforcement
learning framework for the financial portfolio management problem. ArXiv
abs/1706.10059 (2017).
[15] Kevin Kelly. 1994. Out of control: The rise of neo-biological civilization . Addison-
Wesley Longman Publishing Co., Inc.
[16] Marko Kolanovic and Rajesh T. Krishnamachari. 2017. Big data and AI strategies:
machine learning and alternative data approach to investing. https://www.
cognitivefinance.ai/single-post/big-data-and-ai-strategies. J.P. Morgan Securities
LLC, May. 18, 2017.
[17] Petter N. Kolm and G. Ritter. 2019. Modern perspectives on reinforcement learning
in finance. Econometrics: Mathematical Methods & Programming eJournal (2019).
[18] Xinyi Li, Yinchuan Li, Yuancheng Zhan, and Xiao-Yang Liu. 2019. Optimistic bull
or pessimistic bear: Adaptive deep reinforcement learning for stock portfolio
allocation. ICML Workshop on Applications and Infrastructure for Multi-Agent
Learning (2019).
[19] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Joseph
Gonzalez, Ken Goldberg, and Ion Stoica. 2017. Ray RLLib: a composable and
scalable reinforcement learning library. ArXiv abs/1712.09381 (2017).
[20] Jacky Liang, Viktor Makoviychuk, A. Handa, N. Chentanez, M. Macklin, and
D. Fox. 2018. GPU-accelerated robotic simulation for distributed reinforcement
learning. In Conf. on Robot Learning (CoRL) .
[21] T. Lillicrap, Jonathan J. Hunt, A. Pritzel, N. Heess, T. Erez, Yuval Tassa, D. Silver,
and Daan Wierstra. 2016. Continuous control with deep reinforcement learning.
CoRR abs/1509.02971 (2016).
[22] Paul Lipton, Derek Palma, Matt Rutkowski, and Damian Andrew Tamburri. 2018.
TOSCA solves big problems in the cloud and beyond! IEEE Cloud Computing
(2018), 1–1. https://doi.org/10.1109/MCC.2018.111121612
[23] Xiao-Yang Liu, Zechu Li, Zhaoran Wang, and Jiahao Zheng. 2021. ElegantRL: A
Scalable and Elastic Deep Reinforcement Learning Library. https://github.com/
AI4Finance-Foundation/ElegantRL.
[24] Xiao-Yang Liu, Zechu Li, Zhuoran Yang, Jiahao Zheng, Zhaoran Wang, Anwar
Walid, Jiang Guo, and Michael Jordan. 2021. ElegantRL-Podracer: Scalable and
elastic library for cloud-native deep reinforcement learning. Deep Reinforcement
Learning Workshop at NeurIPS (2021).
[25] Xiao-Yang Liu, Hongyang Yang, Qian Chen, Runjia Zhang, Liuqing Yang, Bowen
Xiao, and Christina Dan Wang. 2020. FinRL: a deep reinforcement learning
library for automated stock trading in quantitative finance. Deep Reinforcement
Learning Workshop at NeurIPS (2020).
[26] Xiao-Yang Liu, Hongyang Yang, Jiechao Gao, and Christina Dan Wang. 2021.
FinRL: Deep reinforcement learning framework to automate trading in quantita-
tive finance. ACM International Conference on AI in Finance (ICAIF) (2021).
[27] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey,
Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, et al .
2021. Isaac Gym: High performance GPU-based physics simulation for robot
learning. arXiv preprint arXiv:2108.10470 (2021).[28] Rick Merritt. 2020. What Is MLOps? https://blogs.nvidia.com/blog/2020/09/03/
what-is-mlops/. NVIDIA , Sep. 03, 2020.
[29] Melanie Mitchell. 1996. An introduction to genetic algorithms.
[30] V Mnih, K Kavukcuoglu, D Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare,
Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig
Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan
Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015. Human-level
control through deep reinforcement learning. Nature 518(7540) (2015), 529–533.
[31] G. Nuti, Mahnoosh Mirghaemi, P. Treleaven, and Chaiyakorn Yingsaeree. 2011.
Algorithmic trading. Computer 44 (2011), 61–69.
[32] John Schulman, F. Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
2017. Proximal policy optimization algorithms. ArXiv abs/1707.06347 (2017).
[33] Wharton Research Data Service. 2015. Standard & poor’s compustat. Data
retrieved from Wharton Research Data Service.
[34] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George
Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-
vam, Marc Lanctot, et al .2016. Mastering the game of Go with deep neural
networks and tree search. nature 529, 7587 (2016), 484–489.
[35] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George
van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-
vam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalch-
brenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu,
Thore Graepel, and Demis Hassabis. 2016. Mastering the game of Go with deep
neural networks and tree search. Nature 529(7587) (2016), 484–489.
[36] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
et al.2017. Mastering the game of go without human knowledge. nature 550,
7676 (2017), 354–359.
[37] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-
duction . MIT press.
[38] NVIDIA DGX A100 system reference architecture. 2020. NVIDIA DGX SuperPOD:
Scalable infrastructure for AI leadership . NVIDIA Corporation.
[39] Damian A. Tamburri. 2020. Sustainable MLOps: Trends and challenges. In 2020
22nd International Symposium on Symbolic and Numeric Algorithms for Scientific
Computing (SYNASC) . 17–23.
[40] P. Treleaven, M. Galas, and V. Lalchand. 2013. Algorithmic trading review.
Commun. ACM 56 (2013), 76–85.
[41] Google Trends. 2021. What Is MLOps? https://www.google.com/trends.
[42] Jia Wu, Chen Wang, Lidong Xiong, and Hongyong Sun. 2019. Quantitative trading
on stock market based on deep reinforcement learning. In 2019 International Joint
Conference on Neural Networks (IJCNN) . 1–8.
[43] Zhuoran Xiong, Xiao-Yang Liu, Shan Zhong, Hongyang Yang, and Anwar Walid.
2018. Practical deep reinforcement learning approach for stock trading. NeurIPS
Workshop (2018).
[44] Hongyang Yang, Xiao-Yang Liu, Shan Zhong, and Anwar Walid. 2020. Deep
reinforcement learning for automated stock trading: An ensemble strategy. ACM
International Conference on AI in Finance (ICAIF) (2020).
[45] Zihao Zhang, Stefan Zohren, and Stephen Roberts. 2020. Deep reinforcement
learning for trading. The Journal of Financial Data Science 2(2) (2020), 25–40.",2021-11-07T01:03:15Z, racer hh formance scalable ep reinforcement learni quantitative nance conference july washitoiac ya  qualyi proceedis rst internatnal conference nance jack cho que e wish  sh gandhi  group nick star jonny rash i formance m  oud ops continuous  oud jacomo cor bo olifl minicolas how it mc disney analytics api rl quant vine da reinforcement stable lapse esp holt alpha marine er photo sta ke wa margimichael  iproceedis internatnal conference learni representatns lapse esp holt hurt so yer semi mu nos karesimoavod zmir nih tom ward not am dotoglad  roi tim harley articial intellence runni shane   ray ka uk co lu iproceedis internatnal conference machine learni maer vessel manuel kr ois didark our ii ke mae  quathomas kick b vla had asset  racer ar  ashley hl antofaff imaximiaernest us adam lee assi kane vis to rene track pra full hari wal  jesse leg kl imo alex nic hmatias pl appears alec bradford  sc humasalmosid or yu hu articial intellence wu stable c yao lia di ki xu injury lia ar   kelly out t additsley linemapubhi co inc mark la no vic rackrishna mac hari b morgasecuritimay beer krier conometrics matmatical methods prograi journal iyi li yichuali yuac zh a ya  optimistic adaptive workshapplicatns infrastruure multi agent learni eric lia richard lia robert ishihara phip merit roy fox joseph gonzalez kegoldberg stic ray lib ar  jack lia vior make vi uk hand a nez tackli fox iconf robot learni co l li ap jonathahunt rlitz el  ess rez yu val as sawie stra continuous co paul terek alma ma rut   domaiandrew tam buroud uti  ya  ze chu li hao rawa jia hao c elegant scalable elastic ep reinforcement learni library nance foundatelegant  ya  ze chu li zhu raya jia hao c hao rawa near valid lia guo michael jordaelegant  racer scalable ep reinforcement learni worksheur  ya  ho ya ya iacria   ki ya b christina dawa ep reinforcement learni worksheur  ya  ho ya ya jie chagao christina dawa ep internatnal conference nance vior make vi uk ukasz wa wr  nia yuso guo miclle lu ki er storey mtackli did hoe ller ia red iarthur all shire akur hand isaac gym hh   rick merit what is ops sep melanie mitcll anih ka uk co lu sandrew rush joel ve ness marc belle mare alex grmartired mler s d je land george str ovsk s petersocharlbale amir dik loais antoog lou leki harsh akumar awie stra shane  m is has sab is humanature nut mah not sh mir gh emi tre lee articial intellence ya boryi free algorithmic uter  sc huma pra full hari wal alec bradford leg kl imo proximal ar  sharoreseardata service standard data sharoreseardata service did saja hu chris additarthur guez laurent  re george vadrisc juliascar it wise loais antoog lou ved anne ers l marc la to masteri go did saja hu chris additarthur guez laurent  re george drisc juliascar it wise loais antoog lou ved anne ers l marc la to sanrs die le madominic grew  ham nal kal a su tsk etimot l li ap maleine ea ray ka uk co lu tre gra ep el m is has sab is masteri go nature did sjuliascar it wise karesimoaloais antoog lou aja hu arthur guez thomas hurt lucas baker maw articial intellence adriaboltomasteri richard suoandrew bar to reinforcement asu scalable coratdomaitam burjust articial intellence table ops trends iinternatnal symposium symbolic numeric algorithms scientic uti tre lee gala lal hand algorithmic com mu trends what is ops jia wu cwa li do so ho yo suquantitative iinternatnal joint conference neural networks zhu raso  ya  shaho ho ya ya near valid praical eur workshho ya ya  ya  shaho near valid ep ainternatnal conference nance zi hao  stefaoh resteproberts ep t journal nancial data science
paper_qf_30.pdf,1,Qlib: An AI-oriented Quantitative Investment Platform,"  Quantitative investment aims to maximize the return and minimize the risk in
a sequential trading period over a set of financial instruments. Recently,
inspired by rapid development and great potential of AI technologies in
generating remarkable innovation in quantitative investment, there has been
increasing adoption of AI-driven workflow for quantitative research and
practical investment. In the meantime of enriching the quantitative investment
methodology, AI technologies have raised new challenges to the quantitative
investment system. Particularly, the new learning paradigms for quantitative
investment call for an infrastructure upgrade to accommodate the renovated
workflow; moreover, the data-driven nature of AI technologies indeed indicates
a requirement of the infrastructure with more powerful performance;
additionally, there exist some unique challenges for applying AI technologies
to solve different tasks in the financial scenarios. To address these
challenges and bridge the gap between AI technologies and quantitative
investment, we design and develop Qlib that aims to realize the potential,
empower the research, and create the value of AI technologies in quantitative
investment.
","Qlib : An AI-oriented Quantitative Investment Platform
Xiao Yang ,Weiqing Liu ,Dong Zhou ,Jiang Bian and Tie-Yan Liu
Microsoft Research
fXiao.Yang, Weiqing.Liu, Zhou.Dong, Jiang.Bian, Tie-Yan.Liu g@microsoft.com
Abstract
Quantitative investment aims to maximize the re-
turn and minimize the risk in a sequential trad-
ing period over a set of ﬁnancial instruments. Re-
cently, inspired by rapid development and great po-
tential of AI technologies in generating remark-
able innovation in quantitative investment, there
has been increasing adoption of AI-driven work-
ﬂow for quantitative research and practical invest-
ment. In the meantime of enriching the quan-
titative investment methodology, AI technologies
have raised new challenges to the quantitative in-
vestment system. Particularly, the new learning
paradigms for quantitative investment call for an in-
frastructure upgrade to accommodate the renovated
workﬂow; moreover, the data-driven nature of AI
technologies indeed indicates a requirement of the
infrastructure with more powerful performance; ad-
ditionally, there exist some unique challenges for
applying AI technologies to solve different tasks
in the ﬁnancial scenarios. To address these chal-
lenges and bridge the gap between AI technologies
and quantitative investment, we design and develop
Qlib that aims to realize the potential, empower the
research, and create the value of AI technologies in
quantitative investment.
1 Introduction
Quantitative investment, one of the hottest research ﬁelds,
has been attracting numerous brilliant minds from both the
academia and ﬁnancial industry. In the last decades, with
continuous efforts in optimizing the quantitative methodol-
ogy, the whole community of professional investors has sum-
marized a well-established yet imperfect quantitative research
workﬂow. Recently, emerging AI technologies start a new
trend in this research ﬁeld. With increasing attention to ex-
ploring AI’s great potential in quantitative investment, AI
technologies have been widely adopted in the practical in-
vestment by quantitative researchers.
While AI technologies have been enriching the quantita-
tive investment methodology, they also put forward new chal-
lenges to the quantitative investment system from multipleperspectives. First, the technological revolution in the quan-
titative investment workﬂow, caused by the ﬂexibility of AI
technologies, tends to require new supportive infrastructure.
For example, while the traditional quantitative investment
usually splits the whole workﬂow into a couple of sub-tasks,
including stock trend prediction, portfolio optimization, etc.,
AI technologies make it possible to establish an end-to-end
solution that generates the ﬁnal portfolio directly. To support
such end-to-end solution, it is necessary to upgrade the cur-
rent infrastructure due to its data-driven nature.
Meanwhile, the AI technologies have to deal with the
unique problems in some new scenarios, which require both
plenty of domain knowledge in ﬁnance and rich experience in
data science. Applying the solutions to quantitative research
tasks without any domain adaptation rarely works. Such a
circumstance leads to urgent demands for a platform to ac-
commodate such a modern quantitative research workﬂow in
the age of AI and provide guidance for the application of AI
technologies in the ﬁnancial scenario.
Therefore, we propose a new AI-oriented Quantitative In-
vestment Platform called Qlib1. It aims to assist the research
efforts of exploring the great potential of AI technologies
in quantitative investment as well as empower quantitative
researchers to create more signiﬁcant values on AI-driven
quantitative investment. Speciﬁcally, the AI-oriented frame-
work of Qlib is designed to accommodate the AI-based solu-
tions. Moreover, it provides high-performance infrastructure
dedicated for quantitative investment scenario, which makes
many AI research topics possible. In addition, a batch of tools
designed for machine learning in the quantitative investment
scenario is integrated with Qlib to beneﬁt users in making
fully utilization of AI technologies.
At last, we demonstrate some use cases and evaluate the
performance of the infrastructure of Qlib by comparing sev-
eral solutions for a typical task in quantitative investment.
The results show the infrastructure of Qlib dedicated to quan-
titative investment outperforms most of existing solutions on
this task.
2 Background and Related Works
In this section, we will ﬁrst demonstrate the major practical
problems of a modern quantitative researcher when applying
1The code is available at https://github.com/microsoft/qlibarXiv:2009.11189v1  [q-fin.GN]  22 Sep 2020",2020-09-22T12:57:10Z,lib aquantitative investment platform  ya i ki  do hou lia bitie ya msoft resear ya i ki  hou do lia bitie ya abstra quantitative re iparticularly to lib introduquantitative irecently with w rst for to meanw applyi sutrefore quantitative iplatform lib it spec lib oilib at lib t lib background related works it  sep
paper_qf_30.pdf,2,Qlib: An AI-oriented Quantitative Investment Platform,"  Quantitative investment aims to maximize the return and minimize the risk in
a sequential trading period over a set of financial instruments. Recently,
inspired by rapid development and great potential of AI technologies in
generating remarkable innovation in quantitative investment, there has been
increasing adoption of AI-driven workflow for quantitative research and
practical investment. In the meantime of enriching the quantitative investment
methodology, AI technologies have raised new challenges to the quantitative
investment system. Particularly, the new learning paradigms for quantitative
investment call for an infrastructure upgrade to accommodate the renovated
workflow; moreover, the data-driven nature of AI technologies indeed indicates
a requirement of the infrastructure with more powerful performance;
additionally, there exist some unique challenges for applying AI technologies
to solve different tasks in the financial scenarios. To address these
challenges and bridge the gap between AI technologies and quantitative
investment, we design and develop Qlib that aims to realize the potential,
empower the research, and create the value of AI technologies in quantitative
investment.
","of AI technologies in quantitative investment, which moti-
vates the birth of Qlib. After that, we will brieﬂy introduce
the related work.
2.1 Practical Problems
Quantitative research workﬂow revolution
In the traditional investment research workﬂow, researchers
often develop trading signals by linear models [Petkova,
2006 ]or manually designed rules [Murphy, 1999 ]based on
several factors(factors are similar to features in machine
learning) and basic ﬁnancial data. And then, a trading strat-
egy(typically Barra [Sheikh, 1996 ]) is followed to generate
the target portfolio. At last, researchers evaluate the trading
signal and portfolio by a back-testing function.
With the rise of AI technologies, it launches a technologi-
cal revolution of traditional quantitative investment. The tra-
ditional quantitative research workﬂow is too primitive to ac-
commodate such ﬂexible technologies. In order to show the
difference more intuitively, we’ll demonstrate a typical mod-
ern research workﬂow based on AI technologies. It starts with
a dataset with lots of features(typically more than hundreds
of dimensions). Manually designing such amount of features
takes lots of time. It is common to leverage machine learning
algorithms to generate such features automatically [Potvin et
al., 2004; Neely et al. , 1997; Allen and Karjalainen, 1999;
Kakushadze, 2016 ]. Generating data [Feng et al. , 2019 ]
is another option for constructing a dataset. Based on di-
verse datasets, researchers have provided hundreds of ma-
chine learning methods to mine trading signals [Sezer et al. ,
2019 ]. Researchers could generate the target portfolio based
on such trading signals. But such a workﬂow is not the
only choice. Instead of dividing a task into several stages,
RL(reinforcement learning) provides an end-to-end solution
from the data to the ﬁnal trading actions directly [Deng et
al., 2016 ]. RL optimizes the trading strategy by interacting
with the environment, which is a trading simulator in the ﬁ-
nancial scenario. RL needs a responsive simulator instead of
a back-testing function in the traditional research workﬂow.
Moreover, most of the AI algorithms have complicated hy-
perparameters, which need to be tuned carefully.
AI technologies are so ﬂexible and already beyond the
scope of existing tools designed for traditional methodolo-
gies. Building a research workﬂow based on AI technologies
from scratch takes much time.
High performance requirements for infrastructure
With the emerging of AI technologies, the requirements for
infrastructure have changed. Such a data-driven method
could leverage a huge amount of data. The amount of data
could reach the order of TB magnitude in the scenario of
high-frequency trading. Besides, it is very common to de-
rive thousands of new features (e.g., Alpha101 [Kakushadze,
2016 ]) from the basic price and volume data, which con-
sist of only ﬁve dimensions in total. Some researchers
even try to create new factors or features by searching ex-
pressions [Allen and Karjalainen, 1999; Neely et al. , 1997;
Potvin et al. , 2004 ]. Such heavy work of data processing
overburdens the researchers and even make some researchtopics impossible. Such circumstances put forward more
stringent performance requirements for the infrastructure.
Obstacles to apply machine learning solutions
The ﬁnancial data and task have their uniqueness and chal-
lenges. Applying the machine learning solutions to quantita-
tive research tasks without any adaptation rarely works. Due
to the extremely low SNR(Signal to Noise Ratio) in ﬁnancial
data, it is very hard to build a successful data-driven strategy
in ﬁnancial markets. Most machine learning algorithms are
data-driven and have to deal with such difﬁculties. Without
carefully handling the details, machine learning models can
hardly achieve satisfying performance. Even a minor mis-
take can make the model over-ﬁt the noise rather than learn
effective patterns. Rightly handling the details requires a lot
of domain knowledge of the ﬁnancial industry. Moreover, the
typical objectives, such as annualized return, are often not dif-
ferentiable, which makes it hard to train models directly for
machine learning methods. Deﬁning a reasonable task with
appropriate supervised targets is very important for model-
ing the ﬁnance data. Such barriers daunt quite a lot of data
scientists without much domain knowledge of the ﬁnancial
industry.
Another necessary step to build a machine learning ap-
plication is hyperparameter optimization. Different machine
learning algorithms have different hyperparameter search
spaces, each of which has multiple dimensions with different
meanings and priorities. Some of the quantitative researchers
come from the traditional ﬁnancial industry and don’t have
much knowledge about machine learning. Such huge learning
cost stops many users from giving full play to the maximum
value of machine learning.
2.2 Related Work
In the ﬁnancial industry, an investment strategy will become
less proﬁtable with more investors following it. Therefore,
the ﬁnancial practitioners, especially quantitative researchers,
are never keen to share their own algorithms and tools. OLPS
[Liet al. , 2016 ]is the ﬁrst open-source toolbox for portfo-
lio selection. It consists of a family of classical strategies
powered by machine learning algorithms as benchmarks and
toolkit to facilitate the development of new learning meth-
ods. This toolbox only supports Matlab and Octave, which is
not compatible with current scientiﬁc mainstream language
Python and thus not friendly to the modern machine learn-
ing algorithms. Its framework is quite simple, and modern
quantitative research workﬂow based on AI technologies is
much more complicated. Other quantitative tools emerge in
recent years. QuantLib [Firth, 2004 ]only focuses part of
modern quantitative research workﬂow. QUANTAXIS2fo-
cuses more on the IT infrastructure instead of the research
workﬂow. Quantopian releases a series of open-source tools
1) Alphalens: a Python Library for performance analysis of
predictive (alpha) stock factors 2) Zipline: an event-driven
system for back-testing 3) Pyfolio: a Python library for per-
formance and risk analysis of ﬁnancial portfolios. All of them
only focus on the analysis of trading signals or an investment
portfolio.
2https://github.com/QUANTAXIS/QUANTAXIS",2020-09-22T12:57:10Z,lib after praical problems quantitative ipet v murp and barra sikh at with t iit manually it proteinewly allekar al articial intellence eu sha genti fe based ser researcrs but instead e obudi hh with sut besis alpha u sha some allekar al articial intellence enewly proteisusuobstat applyi due snal noise rat most without everhtly o suanotr dferent some surelated work itrefore t it  mat lab e pythoits otr quant lib birth quato plaalpha lens pytholibrary zip line py fpythoall
paper_qf_30.pdf,3,Qlib: An AI-oriented Quantitative Investment Platform,"  Quantitative investment aims to maximize the return and minimize the risk in
a sequential trading period over a set of financial instruments. Recently,
inspired by rapid development and great potential of AI technologies in
generating remarkable innovation in quantitative investment, there has been
increasing adoption of AI-driven workflow for quantitative research and
practical investment. In the meantime of enriching the quantitative investment
methodology, AI technologies have raised new challenges to the quantitative
investment system. Particularly, the new learning paradigms for quantitative
investment call for an infrastructure upgrade to accommodate the renovated
workflow; moreover, the data-driven nature of AI technologies indeed indicates
a requirement of the infrastructure with more powerful performance;
additionally, there exist some unique challenges for applying AI technologies
to solve different tasks in the financial scenarios. To address these
challenges and bridge the gap between AI technologies and quantitative
investment, we design and develop Qlib that aims to realize the potential,
empower the research, and create the value of AI technologies in quantitative
investment.
","Overall, Qlib is the ﬁrst open-source platform that accom-
modates the workﬂow of a modern quantitative researcher
in the age of AI. It aims to empower every quantitative re-
searcher to realize the great potential of AI technologies in
quantitative investment.
3 AI-oriented Quantitative Investment
Platform
3.1 Overall Design
In the cooperation with the quantitative researcher with years
of hands-on experience in the ﬁnancial market, we’ve en-
countered all of the above problems and explored all kinds of
solutions. Motivated by current circumstances, we implement
Qlib to apply AI technologies in quantitative investment.
AI-oriented framework Qlib is designed in a modularized
way based on modern research workﬂow to provide the max-
imum ﬂexibility to accommodate AI technologies. Quan-
titative researchers could extend the modules and build a
workﬂow to try their ideas efﬁciently. In each module, Qlib
provides several default implementation choices which work
very well in practical investment. With these off-the-shelf
modules, quantitative researchers could focus on the problem
they are interested in a speciﬁc module without distracted by
other trivial details. Besides code, computation and data can
also be shared in some modules, so Qlib is designed to serve
users as a platform rather than a toolbox.
High-performance infrastructure The performance of
data processing is important to data-driven methods like AI
technologies. As an AI-oriented platform, Qlib provides a
high-performance data infrastructure. Qlib provides a time-
series ﬂat-ﬁle database3. Such a database is dedicated to sci-
entiﬁc computing on ﬁnance data. It greatly outperforms cur-
rent popular storage solutions like general-purpose databases
and time-series databases on some typical data processing
tasks in quantitative investment research. Furthermore, the
database provides an expression engine, which could acceler-
ate the implementation and computation of factors/features,
which make research topics that rely on expressions compu-
tation possible.
Guidance for machine learning Qlib has been integrated
with some typical datasets for quantitative investment, on
which typical machine learning algorithms could success-
fully learn patterns with generalization ability. Qlib pro-
vides some basic guidance for machine learning users and
integrates some reasonable tasks which consist of reasonable
feature space and target label. Some typical hyperparameter
optimization tools are provided. With guidance and reason-
able settings, machine learning models could learn patterns
with better generalization ability instead of just over-ﬁtting
the noise.
3.2 AI-oriented Framework
Figure 1 shows the overall framework of Qlib. This frame-
work aims to 1) accommodate the modern AI technology, 2)
3https://en.wikipedia.org/wiki/Flat-ﬁle database
Data ServerModel CreatorPortfolio Genrator
CreatorOrder Executor
Data EnhancementEnsemble
DataOrder Executor
Creator
Model Manager
ModelModelModel
Static W orkﬂow Dynamic Modeling AnalysisData Layer Interday Model Intraday
TradingInterday
Strategy
Models 
ModelModelModelEnsemble CreatorPortfolio GeneratorHighly user-
customizable
Executed Result
Forecasting/AlphaAlpha
AnalyserPortfolio
Analyser
Alpha Analysis
Report
return riskPortfolio Analysis
Report
return riskPortfolio &  Orders
Execution
Analyser
Execution
Analysis
ReportNormal Module Data Flow
FeedbackData ServerModel CreatorPortfolio Genrator
CreatorOrder Executor
Data EnhancementEnsemble
DataOrder Executor
Creator
Model Manager
ModelModelModel
Static W orkﬂow Dynamic Modeling AnalysisData Layer Interday Model Intraday
TradingInterday
Strategy
Models 
ModelModelModelEnsemble CreatorPortfolio GeneratorHighly user-
customizable
Executed Result
Forecasting/AlphaAlpha
AnalyserPortfolio
Analyser
Alpha Analysis
Report
return riskPortfolio Analysis
Report
return riskPortfolio &  Orders
Execution
Analyser
Execution
Analysis
ReportNormal Module Data Flow
FeedbackFigure 1: modules and a typical workﬂow built with Qlib
help the quantitative researchers build a whole research work-
ﬂow with minimal efforts 3) and leave them the maximal ﬂex-
ibility to explore problems they are interested without getting
distracted by other parts.
Such a target leads to a modularized design from the per-
spective of system design. The system is split into several
individual modules based on the modern practical research
workﬂow. Most of the quantitative investment research direc-
tions, no matter traditional or AI-based, could be regarded as
implementations of one or multiple modules’ interfaces. Qlib
provides several typical implementations that work well in
practical investment for users in each module . Moreover, the
modules provide the ﬂexibility for researchers to override ex-
isting methods to explore new ideas. With such a framework,
researchers could try new ideas and test the overall perfor-
mance with other modules with minimal cost.
The modules of Qlib are listed in Figure 1 and connected
in a typical workﬂow. Each module corresponds to a typi-
cal sub-task in quantitative investment. A implementation in
the module can be regarded as a solution for this task. We’ll
introduce each module and give some related examples of ex-
isting quantitative research to show how Qlib accommodate
them.
It starts with the Data Server module in the bottom left
corner, which provides a data engine to query and process
raw data. With retrieved data, researcher could build his
own dataset in the Data Enhancement module . Researchers
have tried a lot solutions to build better datasets by exploring
and constructing effective factors/features [Potvin et al. , 2004;
Neely et al. , 1997; Allen and Karjalainen, 1999; Kakushadze,
2016 ]. Generating datasets for training [Feng et al. , 2019 ]is
another research direction to provide datasets solution. The
Model Creater module learns models based on datasets. In
recent years, numerous researchers have explored all kinds of
models to mine trading signals from ﬁnancial dataset [Sezer
et al. , 2019 ]. Moreover, meta-learning [Vilalta and Drissi,
2002 ]that tries to learn to learn provides a new learning
paradigm for the Model Creator module. Given plenty of
methods to model the ﬁnancial data in a modern research
workﬂow, the model management system has become a nec-",2020-09-22T12:57:10Z,ovll lib it quantitative investment platform ovll simotivated lib lib quailib with besis lib hh t as lib lib suit furtr guidance lib lib some with framework  lib  flat data sermeator tfgerat or eator orr executor data enhancement ensemble data orr executor eator mmanager mmmstatic dynamic moli analysis data layer inter day mintra day tradi inter day strategy mols mmmensemble eator tfgentor hhly executed result forecasti alpha alpha analyse tfanalyse alpha analysis ret tfanalysis ret tforrs executanalyse executanalysis ret normal module data flow feedback data sermeator tfgerat or eator orr executor data enhancement ensemble data orr executor eator mmanager mmmstatic dynamic moli analysis data layer inter day mintra day tradi inter day strategy mols mmmensemble eator tfgentor hhly executed result forecasti alpha alpha analyse tfanalyse alpha analysis ret tfanalysis ret tforrs executanalyse executanalysis ret normal module data flow feedback  lib sut most lib owith t lib  ea lib it data serwith data enhancement researcrs proteinewly allekar al articial intellence eu sha genti fe t meate iser ov alta dr is si meator given
paper_qf_30.pdf,4,Qlib: An AI-oriented Quantitative Investment Platform,"  Quantitative investment aims to maximize the return and minimize the risk in
a sequential trading period over a set of financial instruments. Recently,
inspired by rapid development and great potential of AI technologies in
generating remarkable innovation in quantitative investment, there has been
increasing adoption of AI-driven workflow for quantitative research and
practical investment. In the meantime of enriching the quantitative investment
methodology, AI technologies have raised new challenges to the quantitative
investment system. Particularly, the new learning paradigms for quantitative
investment call for an infrastructure upgrade to accommodate the renovated
workflow; moreover, the data-driven nature of AI technologies indeed indicates
a requirement of the infrastructure with more powerful performance;
additionally, there exist some unique challenges for applying AI technologies
to solve different tasks in the financial scenarios. To address these
challenges and bridge the gap between AI technologies and quantitative
investment, we design and develop Qlib that aims to realize the potential,
empower the research, and create the value of AI technologies in quantitative
investment.
","essary part of the workﬂow. The Model Manager module
is designed to handle such problems for modern quantita-
tive researchers. With diverse models, ensemble learning is
quite an effective way to enhance the performance and ro-
bustness of machine learning models, and it is frequently
used in the ﬁnancial area [Qiuet al. , 2014; Yang et al. , 2017;
Zhao et al. , 2017 ]. It is supported by Model Ensemble mod-
ule.Portfolio Generator module aims to generate a port-
folio from trading signals output by models, which is known
as portfolio management [Qian et al. , 2007 ]. Barra [Sheikh,
1996 ]provides the most popular solution for this task. With
the target portfolio, we provide a high-ﬁdelity trading simu-
lator, Orders Executor module , to examine the performance
of a strategy and Analyser modules to automatically analyze
the trading signals, portfolio and execution results. The Order
Executor module is designed as a responsive simulator rather
than a back-testing function, which could provide the infras-
tructure for some learning paradigm(e.g., RL) that requires
feedback of the environment produced by the Analyser mod-
ules.
The data in quantitative investment are in time-series for-
mat and updated by time. The size of in-sample dataset in-
creases by time. A typical practice to leverage the new data is
to update our models regularly [Wang et al. , 2019b ]. Besides
better utilization of increasing in-sample data, dynamically
updating models [Yang et al. , 2019 ]and trading strategies
[Wang et al. , 2019a ]will improve the performance further
due to dynamic nature of the stock market [Adam et al. , 2016 ].
Therefore, it is obviously not the optimal solution to use a
set of static model and trading strategies in Static Workﬂow .
Dynamic updating of models and strategies is a important re-
search direction in quantitative investment. The modules in
theDynamic Modeling provide interfaces and infrastructure
to accommodate such solutions.
3.3 High Performance Infrastructure
Financial data
We’ll summarise the data requirements in quantitative re-
search in this section. In quantitative research, the most
frequently-used format of data follow such format
BasicData T=fxi;t;ag;i2Inst;t2Time;a2Attr
wherexi;t;a is the value of basic type(e.g. ﬂoat, int), Inst
denotes the ﬁnancial instruments set(e.g. stock, option, etc.),
Time denotes the timestampes set(e.g. trading days of stock
market),Attr denotes the possible attributes set of an instru-
ment(e.g. open price, volume, market value), Tdenote the
latest timestamp of the data(e.g. the latest trading date). xi;t;a
denotes the value of attribute aof instrument iat timet.
Besides, instruments pools are necessary information to
specify a set of ﬁnancial instruments which change over time
PoolT=fpooltg;t2Time;pool tInst
S&P 500 Index4is a typical example of Pool .
Data update is an essential feature. The existing historical
data will not change over time. Only the append operation of
4https://en.wikipedia.org/wiki/S%26P 500Indexnew data is necessary. The formalized update operation is
BasicData T=OldBasicData T[fxi;t;a newg
BasicData T+1=BasicData T[fxi;T+1;ag
PoolT+1=PoolT[fpoolt+1g
User queries can be formalized as
DataQuery =fxi;t;ajit2poolt;poolt2Poolquery
a2Attrquery;timestartttimeendg
which represents data query of some attributes of instruments
in a speciﬁc time range in a speciﬁc pool.
Such requirements are quite simple. Many off-the-shelf
open-source solutions support such operations. We classify
them into three categories and list the popular implementa-
tions in each category.
General-purpose database: MySQL [MySQL, 2001 ],
MongoDB [Chodorow, 2013 ]
Time-series database: InﬂuxDB [Naqvi et al. , 2017 ]
Data ﬁle for scientiﬁc computing: Data organized
by numpy [Oliphant, 2006 ]array or pandas [McKinney,
2011 ]dataframe
The general-purpose database supports data with diverse
formats and structures. Besides, it provides lots of sophis-
ticated mechanisms, such as indexing, transaction, entity-
relationship model, etc. Most of them add heavy dependen-
cies and unnecessary complexity to a speciﬁc task rather than
solving the key problems in a speciﬁc scenario. The time-
series database optimizes the data structures and queries for
time-series data. But they are still not designed for quanti-
tative research, where the data are usually in compact array-
based format for scientiﬁc computation to take advantage of
hardware acceleration. It will save a great amount of time if
the data keep the compact array-based format from the disk
to the end of clients without format transformation. How-
ever, both general-purpose and time-series database store and
transfer the data in a different format for the general purpose,
which is inefﬁcient for scientiﬁc computation.
Due to the inefﬁciency of databases, array-based data gain
popularity in the scientiﬁc community. Numpy array and pan-
das dataframe are the mainstream implementations in scien-
tiﬁc computation, which are often stored as HDF5or pickle6
on the disk. Data in such formats have light dependencies
and are very efﬁcient for scientiﬁc computing. However, such
data are stored in a single ﬁle and hard to update or query.
After an investigation of above storage solutions, we ﬁnd
none could ﬁt the quantitative research scenario very well. It
is necessary to design a customized solution for quantitative
research.
File storage design
Figure 2 demonstrates the ﬁle storage design. As shown in the
left part of the ﬁgure, Qlib organize ﬁles in a tree structure.
Data are separated into folders and ﬁles according to different
5https://en.wikipedia.org/wiki/Hierarchical Data Format
6https://docs.python.org/3/library/pickle.html",2020-09-22T12:57:10Z,t mmanager with qi et ya hao it mensemble tfgentor iabarra sikh with orrs executor analyse t orr executor analyse t t wa besis ya wa adam trefore static work dynamic t dynamic moli hh formance infrastruure nancial  ibasic data ist time at tr ist time at tr note besis potime ist inx ata t only inx new t basic data old basic data basic data basic data popouser data query poquery at tr query sumany  genl my my mono cho do row time ina qv data data elephant mc kidney t besis most t but it how due num py data after it le  as lib data hirchical data format
paper_qf_30.pdf,5,Qlib: An AI-oriented Quantitative Investment Platform,"  Quantitative investment aims to maximize the return and minimize the risk in
a sequential trading period over a set of financial instruments. Recently,
inspired by rapid development and great potential of AI technologies in
generating remarkable innovation in quantitative investment, there has been
increasing adoption of AI-driven workflow for quantitative research and
practical investment. In the meantime of enriching the quantitative investment
methodology, AI technologies have raised new challenges to the quantitative
investment system. Particularly, the new learning paradigms for quantitative
investment call for an infrastructure upgrade to accommodate the renovated
workflow; moreover, the data-driven nature of AI technologies indeed indicates
a requirement of the infrastructure with more powerful performance;
additionally, there exist some unique challenges for applying AI technologies
to solve different tasks in the financial scenarios. To address these
challenges and bridge the gap between AI technologies and quantitative
investment, we design and develop Qlib that aims to realize the potential,
empower the research, and create the value of AI technologies in quantitative
investment.
","daily
features
GOOGL
open.bin
calendar .txtfor a speciﬁc frequency
attributes
instrument
instruments pools
sp500.txt
shared timeline
Reference 
GOOGL	2004-08-19
MSFT		1986-03-13
AAPL		1980-12-12
AMZN		1997-05-15
...size = (N + 1) * 4 Bytes
Timestamp 1 Timestamp 2 Timestamp T
Fixed-width binary datavalue1 value 2 value N Start timestamp Index
TimeFigure 2: The description of the ﬂat-ﬁle database; the left part is the
structure of ﬁles; the right part is the content of ﬁles
frequencies, instruments and attributes. All the values of at-
tributes are stored in binary data in a compact ﬁxed-width for-
mat so that indexing by bytes becomes possible. The shared
timeline is stored separately in a ﬁle named ”calendar.txt”.
The data ﬁle of attribute values sets its ﬁrst 4 bytes to the in-
dex value of the timeline to indicates the start timestamp of
the series of data. With the start time index, Qlib could align
all the values on the time dimension.
The data are stored in a compact format, which is efﬁ-
cient to be combined into arrays for scientiﬁc computation.
While it achieves high performance like array-based data in
scientiﬁc computation, it meets data update requirements in
the quantitative investment scenario. All data are arranged
in the order of time. New data could be updated by ap-
pending, which is quite efﬁcient. Adding and removing at-
tributes or instruments are quite straightforward and efﬁcient,
because they are stored in separate ﬁles. Such a design is ex-
tremely light-weighted. Without the overheads of databases,
Qlib achieves high performance.
Expression Engine
It is quite a common task to develop new factors/features
based on basic data. Such a task takes a large proportion of
the time of many quantitative researchers. Both Implement
such factors by code, and the computation process is time-
consuming. Therefore, Qlib provides an expression engine to
minimize the effort of such tasks.
Actually, the nature of factors/features is a function that
transforms the basic data into the target values. The func-
tion could break down into a combination of a series of ex-
pressions. The expression engine is designed based on this
idea. With this expression engine, quantitative researchers
could implement new factors/features by writing expressions
instead of complicated code. For example, The Bollinger
Band technical indicator [Bollinger, 2002 ]is a widely used
technical factor and its upper bounds can be implemented by
just a simple expression ”(MEAN( $close, N)+2*STD( $close,
N)-$close)/MEAN( $close, N)” with the expression engine.
Such an implementation is simple, readable, reusable and
maintainable. Users can easily build a dataset with just a se-
ries of simple expressions. Searching expressions to construct
effective trading signals is a typical research topic, which has
been explored by many researchers [Allen and Karjalainen,
1999; Neely et al. , 1997; Potvin et al. , 2004 ]. An expressionengine is an essential tool for such a research topic.
Cache system
Stocks
Attr and F(Attr)
time GOOGLMSFTStocks
Attrclose.bin
open.bintime1.Orgianl Binary Data
Stocks
F(Attr)high/close
open/closetime2.ExpressionCache
Update
Expression
Cache3.DatasetCache
Cache for Saving  Calculation T ime Cache For Saving Combination T ime
GOOGLMSFTUpdate
Dataset
CacheStocks
Attr and F(Attr)
time GOOGLMSFTStocks
Attrclose.bin
open.bintime1.Orgianl Binary Data
Stocks
F(Attr)high/close
open/closetime2.ExpressionCache
Update
Expression
Cache3.DatasetCache
Cache for Saving  Calculation T ime Cache For Saving Combination T ime
GOOGLMSFTUpdate
Dataset
Cache
Figure 3: The disk cache system of Qlib; expression cache for saving
time of expression computation; dataset cache for saving time of
data combination
To avoid replicated computation, Qlib has a built-in cache
system. It consists of memory cache and disk cache.
In-memory cache When Qlib computes factors/features
with its expression engine, it parses the expression into a syn-
tax tree. All computed results of nodes will be stored in an
LRU(Least Recently Used) cache in memory. The replicated
computation of same (sub-)expressions can be saved.
Disk cache A typical workﬂow of data processing in quan-
titative investment can be divided into three steps: fetching
original data, computing expressions and combining data into
arrays for scientiﬁc computation. Computing expressions and
combining data are very time-consuming. It could save much
time if we can cache the shared intermediate data. In prac-
tical data processing tasks, many intermediate results can be
shared. For example, the same expression computation can
be shared by different data processing tasks. Therefore Qlib
designed a 2-level disk cache mechanism. The cache sys-
tem is shown in Figure 3. The left part is the original data
we described in Section 3.3. The ﬁrst level is expression
cache, which will save all the computed expressions to the
disk cache. The data structure of the expression cache is
the same as the original data. With the expression cache,
the same expression will be computed only once. After the
expression cache is dataset cache, which stores the combined
data to save the combination time. The cache data of both lev-
els are arranged by time and indexable on the time dimension,
so the disk cache can be shared even when the query time
changes. Moreover, Qlib support data update by appending
new data thanks to the data arrangement by time. The main-
tenance of the data is much easier with such a mechanism.
3.4 Guidance for Machine Learning
As we discussed in Section 2, guidance for machine learning
algorithms is very important. Qlib provides typical datasets
for machine learning algorithms. Some typical task settings
can be found in Qlib , such as data pre-processing, learn-
ing targets, etc. Researchers don’t have to explore everything
from scratch. Such guidances provide lots of domain knowl-
edge for researchers to start their journey in this research area.
For most machine learning algorithms, hyperparameter op-
timization is a necessary step to achieve better generaliza-
tion. Although it is important, it takes a lot of effort and is",2020-09-22T12:57:10Z,reference byttimestamp timestamp timestamp xed start inx time  t all t t with lib t w all new addi suwithout lib esseine it suboth implement trefore lib aually t t with for t bollier band bollier suusers searchi allekar al articial intellence enewly proteiacac stocks at tr at tr stocks at tr ose or gaibinary data stocks at tr esscac update esscac data set cac cac si calculatcac for si combinatupdate data set cac stocks at tr at tr stocks at tr ose or gaibinary data stocks at tr esscac update esscac data set cac cac si calculatcac for si combinatupdate data set cac  t lib to lib it iwlib all least recently used t disk uti it ifor trefore lib t  t set t with after t olib t guidance machine learni as selib some lib researcrs sufor although
paper_qf_30.pdf,6,Qlib: An AI-oriented Quantitative Investment Platform,"  Quantitative investment aims to maximize the return and minimize the risk in
a sequential trading period over a set of financial instruments. Recently,
inspired by rapid development and great potential of AI technologies in
generating remarkable innovation in quantitative investment, there has been
increasing adoption of AI-driven workflow for quantitative research and
practical investment. In the meantime of enriching the quantitative investment
methodology, AI technologies have raised new challenges to the quantitative
investment system. Particularly, the new learning paradigms for quantitative
investment call for an infrastructure upgrade to accommodate the renovated
workflow; moreover, the data-driven nature of AI technologies indeed indicates
a requirement of the infrastructure with more powerful performance;
additionally, there exist some unique challenges for applying AI technologies
to solve different tasks in the financial scenarios. To address these
challenges and bridge the gap between AI technologies and quantitative
investment, we design and develop Qlib that aims to realize the potential,
empower the research, and create the value of AI technologies in quantitative
investment.
","HDF5 MySQL MongoDB InﬂuxDB Qlib -E -D Qlib +E -D Qlib +E +D
Storage(MB) 287 1,332 911 394 303 802 1,000
Load Data(s) 0.800.22 182.54.2 70.34.9 186.51.5 0.950.05 4.90.07 7.40.3
Compute Expr.(s) 179.84.4 137.77.6 35.32.3 -
Convert Index(s) - 3.60.1 -
Filter by Pool(s) 3.390.24 -
Combine data(s) 1.190.30 -
Total (1CPU) (s) 184.43.7 365.37.5 253.66.7 368.23.6 147.08.8 47.61.0 7.40.3
Total(64CPUs) (s) - 8.80.6 4.20.2 -
Table 1: Performance comparison of different storage solutions
quite repetitive. Therefore, Qlib provides a Hyperparame-
ters Tuning Engine(HTE) to make such a task easier. HTE
provides an interface to deﬁne a hyperparameter search space
and then search the best hyperparameters automatically.
In a typical ﬁnancial task of modeling time-series data, the
new data comes in sequence by time. To leverage the new
data, models have to be re-trained on new data periodically.
The new best hyperparameters change but are often close to
previous best hyperparameters. HTE provides a mechanism
dedicated to hyperparameter optimization on ﬁnancial tasks.
It generates a new distribution for hyperparameters search
space for better a chance to reach the best point with fewer
trials. The distribution for searching can be formalized as
pnew(x) =pprior(x)'prev;2(x)
Expprior['prev;2(x)]
wherepprior is the original hyperparameters search space;
'prev;2(x)N(prev;2);prev is the best hyperparam-
eter in last model training. The domain of hyperparameter
search space remains the same, but the probability density
aroundprev increases.
4 Use Case & Performance Evaluation
4.1 Use Case
Qlib provide a Conﬁg-Driven Pipeline Engine(CDPE) to
help researchers build the whole research workﬂow show in
Figure 1 easier. The user could deﬁne a workﬂow with just
a simple conﬁg ﬁle like List ??(some trivial details are re-
placed by ”...”). Such an interface is not mandatory, and we
leave the maximal ﬂexibility to users to build a quantitative
research workﬂow by code like building blocks.
4.2 Performance Evaluation
The performance of data processing is important to data-
driven methods like AI technologies. As an AI-oriented plat-
form, Qlib provides a solution for data storage and data pro-
cessing. To demonstrate the performance of Qlib, We com-
pare Qlib with several other solutions discussed in Section
3.3, which includes HDF5, MySQL, MongoDB, InﬂuxDb and
Qlib. The Qlib +E -D indicates Qlib with expression cache
enabled and dataset cache disabled, and so forth.
Figure 4: A Conﬁguration example of CDPE
The task for the solutions is to create a dataset from the
basic OHLCV7daily data of a stock market, which involves
data query and processing. The ﬁnal dataset consists of 14
factors/features derived from OHLCV data(e.g. ”Std($close,
5)/$close”). The time of the data ranges from 1/1/2007 to
1/1/2020. The stock pool consists of 800 stocks each day,
which changes daily.
Besides the comparison of the total time of each solution,
we break down the task into following steps for more details.
Load Data Load the OHCLV data or cache into RAM
as the array-based format for scientiﬁc computation.
Compute Expr. Compute the derived factors/features.
Convert Index It only applies to Qlib. Because Qlib
doesn’t store the indices(i.e., timestamp, stock id) in the
original data, it has to set up data indices.
Filter data Filter the stock data by a speciﬁc pool. For
example, SP500 involves more than 1 thousand stock in
total, but it only includes 500 stocks daily. The data not
included in SP500 on a speciﬁc day should be ﬁltered
out, though it has ever been in SP500. It is impossible to
ﬁlter out data when loading data, because some derived
features rely on historical OHLCV data.
Combine data Concatenate all the data of different
stocks into a single piece of array-based data
As we can seen in Table 1. Qlib’s compact storage achieves
similar size and loading speed as the dedicated scientiﬁc
7The open, high, low, close price and trading volume of a stock",2020-09-22T12:57:10Z,my mono ilib lib lib storage load data ute  convert inx lter pocombine total total us table formance trefore lib   me tuni eine ito t it t ex t use case formance evaluatuse case lib codrivepipeline eine  t t suformance evaluatt as lib to lib  lib semy mono idb lib t lib lib  cot t std t t besis load data load ute  ute convert inx it lib because lib lter lter for t it combine concatenate as table lib t
paper_qf_30.pdf,7,Qlib: An AI-oriented Quantitative Investment Platform,"  Quantitative investment aims to maximize the return and minimize the risk in
a sequential trading period over a set of financial instruments. Recently,
inspired by rapid development and great potential of AI technologies in
generating remarkable innovation in quantitative investment, there has been
increasing adoption of AI-driven workflow for quantitative research and
practical investment. In the meantime of enriching the quantitative investment
methodology, AI technologies have raised new challenges to the quantitative
investment system. Particularly, the new learning paradigms for quantitative
investment call for an infrastructure upgrade to accommodate the renovated
workflow; moreover, the data-driven nature of AI technologies indeed indicates
a requirement of the infrastructure with more powerful performance;
additionally, there exist some unique challenges for applying AI technologies
to solve different tasks in the financial scenarios. To address these
challenges and bridge the gap between AI technologies and quantitative
investment, we design and develop Qlib that aims to realize the potential,
empower the research, and create the value of AI technologies in quantitative
investment.
","HDF5 data ﬁle. The databases take too much time on loading
data. After looking into the underlying implementation, we
ﬁnd that data go through too many layers of interfaces and
unnecessary format transformations in both general-purpose
database and time-series database solution. Such overheads
greatly slow down the data loading process. Due to the mem-
ory cache of Qlib, Qlib -E -D saves about 24% of the time of
Compute Expr. Moreover, Qlib provides expression cache
and dataset cache mechanism. With expression cache en-
abled in Qlib +E -D, 80.4% of the time for Compute Expr.
is saved if no expression cache is missed. Combining the fac-
tors/features into one piece of array-based data for each stock
accounts for the major time consuming of Qlib +E -D, which
is included in the Compute Expr. step. Besides the computa-
tion cost, the most time-consuming step is data combination.
The dataset cache is designed to reduce such overheads. As
shown in the column Qlib +E +D, the time cost is further re-
duced.
Moreover, Qlib can leverage multiple CPU cores to accel-
erate computation. As we can see in the last line of Tabel 1,
the time cost is signiﬁcantly reduced for Qlib with multiple
CPUs. Qlib +E +D can’t be accelerated further due to it just
reads the existing cache and almost computes nothing.
4.3 More about Qlib
Qlib an opensource platform in continuous development.
More detailed documentations can be found in its github
repository8. A lot of features(e.g. data service with client-
server architecture, analysis system, automatic deployment
on the cloud) not introduced in detail in this paper could be
found in the online repository. Your contributions are wel-
comed.
5 Conclusion
In this paper, we present practical problems of modern quan-
titative researchers in the age of AI. Based on these practical
problems, we design and implement Qlib that aims to em-
power every quantitative researcher to realize the great po-
tential of AI-technologies in quantitative investment.
References
[Adam et al. , 2016 ]Klaus Adam, Albert Marcet, and
Juan Pablo Nicolini. Stock market volatility and learning ,
2016.
[Allen and Karjalainen, 1999 ]Franklin Allen and Risto Kar-
jalainen. Using genetic algorithms to ﬁnd technical trad-
ing rules. Journal of ﬁnancial Economics , 51(2):245–271,
1999.
[Bollinger, 2002 ]John Bollinger. Bollinger on Bollinger
bands . McGraw Hill Professional, 2002.
[Chodorow, 2013 ]Kristina Chodorow. MongoDB: the
deﬁnitive guide: powerful and scalable data storage . ”
O’Reilly Media, Inc.”, 2013.
8https://github.com/microsoft/qlib/[Deng et al. , 2016 ]Yue Deng, Feng Bao, Youyong Kong,
Zhiquan Ren, and Qionghai Dai. Deep direct reinforce-
ment learning for ﬁnancial signal representation and trad-
ing. IEEE transactions on neural networks and learning
systems , 28(3):653–664, 2016.
[Feng et al. , 2019 ]Fuli Feng, Huimin Chen, Xiangnan He,
Ji Ding, Maosong Sun, and Tat-Seng Chua. Enhancing
stock movement prediction with adversarial training. In
Proceedings of the 28th International Joint Conference
on Artiﬁcial Intelligence , pages 5843–5849. AAAI Press,
2019.
[Firth, 2004 ]N Firth. Why use quantlib. Paper available
at: http://www. quantlib. co. uk/publications/quantlib. pdf ,
2004.
[Kakushadze, 2016 ]Zura Kakushadze. 101 formulaic al-
phas. Wilmott , 2016(84):72–81, 2016.
[Liet al. , 2016 ]Bin Li, Doyen Sahoo, and Steven CH Hoi.
Olps: a toolbox for on-line portfolio selection. The Journal
of Machine Learning Research , 17(1):1242–1246, 2016.
[McKinney, 2011 ]Wes McKinney. pandas: a foundational
python library for data analysis and statistics. Python for
High Performance and Scientiﬁc Computing , 14, 2011.
[Murphy, 1999 ]John J Murphy. Technical analysis of the ﬁ-
nancial markets: A comprehensive guide to trading meth-
ods and applications . Penguin, 1999.
[MySQL, 2001 ]AB MySQL. Mysql, 2001.
[Naqvi et al. , 2017 ]Syeda Noor Zehra Naqvi, Soﬁa Yfanti-
dou, and Esteban Zim ´anyi. Time series databases and in-
ﬂuxdb. Studienarbeit, Universit ´e Libre de Bruxelles , 2017.
[Neely et al. , 1997 ]Christopher Neely, Paul Weller, and Rob
Dittmar. Is technical analysis in the foreign exchange mar-
ket proﬁtable? a genetic programming approach. Jour-
nal of ﬁnancial and Quantitative Analysis , 32(4):405–426,
1997.
[Oliphant, 2006 ]Travis E Oliphant. A guide to NumPy , vol-
ume 1. Trelgol Publishing USA, 2006.
[Petkova, 2006 ]Ralitsa Petkova. Do the fama–french factors
proxy for innovations in predictive variables? The Journal
of Finance , 61(2):581–612, 2006.
[Potvin et al. , 2004 ]Jean-Yves Potvin, Patrick Soriano, and
Maxime Vall ´ee. Generating trading rules on the stock mar-
kets with genetic programming. Computers & Operations
Research , 31(7):1033–1047, 2004.
[Qian et al. , 2007 ]Edward E Qian, Ronald H Hua, and
Eric H Sorensen. Quantitative equity portfolio manage-
ment: modern techniques and applications . CRC Press,
2007.
[Qiuet al. , 2014 ]Xueheng Qiu, Le Zhang, Ye Ren, Pon-
nuthurai N Suganthan, and Gehan Amaratunga. Ensemble
deep learning for regression and time series forecasting.
In2014 IEEE symposium on computational intelligence in
ensemble learning (CIEL) , pages 1–6. IEEE, 2014.",2020-09-22T12:57:10Z,t after sudue lib lib ute  olib with lib ute  combini lib ute  besis t as lib olib as abel lib us lib  lib lib  your conusibased lib referencadam ass adam albert marc et juapablo nico istock allekar al articial intellence efranklialleis to kar usi journal economics bollier  bollier bollier bollier mc raw hl professnal cho do row istina cho do row mono really media inc e yue e fe bao you yo  zhi quareqi o articial intellence articial intellence ep fe ful fe hui miclia na ji di mao so sutat so cha enhanci iproceedis internatnal joint conference art intellence  birth birth w pa u sha  u sha pot t bili dozeyahoo stevehoi lps t journal machine learni researmc kidney s mc kidney pythohh formance cie nti uti murp  murp technical peuimy my myself na qv d poor ze  na qv so anti stefazim time studialbeit  libre bruxellnewly  newly paul seller rob it mar is jour quantitative analysis elephant tris elephant num py tr el gpubhi pet v ral its pet v do t journal nance protei yproteipatrick soprano maxima all genti uters oatns researiaedward iaronald hua eric so rensequantitative  qi et due e qi le  ye reposugar thage hamara tu ensemble in
paper_qf_30.pdf,8,Qlib: An AI-oriented Quantitative Investment Platform,"  Quantitative investment aims to maximize the return and minimize the risk in
a sequential trading period over a set of financial instruments. Recently,
inspired by rapid development and great potential of AI technologies in
generating remarkable innovation in quantitative investment, there has been
increasing adoption of AI-driven workflow for quantitative research and
practical investment. In the meantime of enriching the quantitative investment
methodology, AI technologies have raised new challenges to the quantitative
investment system. Particularly, the new learning paradigms for quantitative
investment call for an infrastructure upgrade to accommodate the renovated
workflow; moreover, the data-driven nature of AI technologies indeed indicates
a requirement of the infrastructure with more powerful performance;
additionally, there exist some unique challenges for applying AI technologies
to solve different tasks in the financial scenarios. To address these
challenges and bridge the gap between AI technologies and quantitative
investment, we design and develop Qlib that aims to realize the potential,
empower the research, and create the value of AI technologies in quantitative
investment.
","[Sezer et al. , 2019 ]Omer Berat Sezer, Mehmet Ugur
Gudelek, and Ahmet Murat Ozbayoglu. Financial time se-
ries forecasting with deep learning: A systematic literature
review: 2005-2019. arXiv preprint arXiv:1911.13288 ,
2019.
[Sheikh, 1996 ]Aamir Sheikh. Barra’s risk models. Barra
Research Insights , pages 1–24, 1996.
[Vilalta and Drissi, 2002 ]Ricardo Vilalta and Youssef
Drissi. A perspective view and survey of meta-learning.
Artiﬁcial intelligence review , 18(2):77–95, 2002.
[Wang et al. , 2019a ]Lewen Wang, Weiqing Liu, Xiao Yang,
and Jiang Bian. Conservative or aggressive? conﬁdence-
aware dynamic portfolio construction. In 2019 IEEE
Global Conference on Signal and Information Processing
(GlobalSIP) , pages 1–5. IEEE, 2019.
[Wang et al. , 2019b ]Shouxiang Wang, Xuan Wang,
Shaomin Wang, and Dan Wang. Bi-directional long
short-term memory method based on attention mechanism
and rolling update for short-term load forecasting. Inter-
national Journal of Electrical Power & Energy Systems ,
109:470–479, 2019.
[Yang et al. , 2017 ]Bing Yang, Zi-Jia Gong, and Wenqi
Yang. Stock market index prediction using deep neural
network ensemble. In 2017 36th Chinese Control Confer-
ence (CCC) , pages 3882–3887. IEEE, 2017.
[Yang et al. , 2019 ]Xiao Yang, Weiqing Liu, Lewen Wang,
Cheng Qu, and Jiang Bian. A divide-and-conquer frame-
work for attention-based combination of multiple invest-
ment strategies. In 2019 IEEE Global Conference on Sig-
nal and Information Processing (GlobalSIP) , pages 1–5.
IEEE, 2019.
[Zhao et al. , 2017 ]Yang Zhao, Jianping Li, and Lean Yu. A
deep learning ensemble approach for crude oil price fore-
casting. Energy Economics , 66:9–16, 2017.",2020-09-22T12:57:10Z,ser obe rat ser meet gur gu  lek ahmed l oz bay og lu nancial   sikh amir sikh barra barra researinshts v alta dr is si ricardo v alta yourself dr is si art wa le wa i ki   ya lia biconservative lobal conference snal informatprocessi global wa show lia wa juawa sha miwa dawa bi inter journal elerical  energy tems ya bi ya zi jia go qi ya stock ichinese contrconfer ya  ya i ki  le wa c qu lia bilobal conference s informatprocessi global hao ya hao japi li leayu energy economics
paper_qf_31.pdf,1,Correlation structure of extreme stock returns,"  It is commonly believed that the correlations between stock returns increase
in high volatility periods. We investigate how much of these correlations can
be explained within a simple non-Gaussian one-factor description with time
independent correlations. Using surrogate data with the true market return as
the dominant factor, we show that most of these correlations, measured by a
variety of different indicators, can be accounted for. In particular, this
one-factor model can explain the level and asymmetry of empirical exceedance
correlations. However, more subtle effects require an extension of the one
factor model, where the variance and skewness of the residuals also depend on
the market return.
","arXiv:cond-mat/0006034v2  [cond-mat.dis-nn]  22 Jan 2001Correlation structure of extreme stock returns
Pierre Cizeau†, Marc Potters†and Jean-Philippe Bouchaud∗,†
†Science & Finance
The Research Division of Capital Fund Management
109–111 rue Victor-Hugo, 92532 Levallois cedex, France
http://www.science-ﬁnance.fr
∗Service de Physique de l’ ´Etat Condens´ e,
Centre d’´ etudes de Saclay,
Orme des Merisiers, 91191 Gif-sur-Yvette cedex, FRANCE
First version: June 2, 2000
This version
October 23, 2018
Abstract
It is commonly believed that the correlations between stock returns in-
crease in high volatility periods. We investigate how much o f these correla-
tions can beexplained within a simple non-Gaussian one-fac tor description
withtime independent correlations. Using surrogate data with the true
market return as the dominant factor, we show that most of the se correla-
tions, measured by a variety of diﬀerent indicators, can be ac counted for.
In particular, this one-factor model can explain the level a nd asymmetry
of empirical exceedance correlations. However, more subtl e eﬀects require
an extension of the one factor model, where the variance and s kewness of
the residuals also depend on the market return.
1 Introduction
Understanding the relationship between the statistics of individual stock returns
and that of the corresponding index is a major issue in several ﬁnan ce problems
such asriskmanagement [1] ormarket micro-structure modeling. I t isalsocrucial
for building optimized portfolios containing both index and stocks der ivatives
[2, 3]. Although the index return is the (weighted) sum of stock retu rns, it
actually displays very diﬀerent statistical properties from what wo uld result if
the stock returns were independent. In particular, the cumulant s (that is, the
volatility, the skewness and the kurtosis) of the index distribution, which should
1",2000-06-02T12:03:25Z, jacorrelatpierre size au marc ters  phippe  chascience nance t researdiviscapital fund management vior hugo levallois france service psique etat cons centre ay or me merit tier g veed rst june  ober abstra it  usi iintroduunrstandi although in
paper_qf_31.pdf,2,Correlation structure of extreme stock returns,"  It is commonly believed that the correlations between stock returns increase
in high volatility periods. We investigate how much of these correlations can
be explained within a simple non-Gaussian one-factor description with time
independent correlations. Using surrogate data with the true market return as
the dominant factor, we show that most of these correlations, measured by a
variety of different indicators, can be accounted for. In particular, this
one-factor model can explain the level and asymmetry of empirical exceedance
correlations. However, more subtle effects require an extension of the one
factor model, where the variance and skewness of the residuals also depend on
the market return.
","be suppressed by a power of the number of stocks Nfor independent returns,
are still very large, even for N= 500. The negative skewness of the index, in
particular, is actually larger than for individual stocks, and reﬂect s a speciﬁc
leverage eﬀect [4].
It is a common belief that cross-correlations between stocks actu allyﬂuctuate
in time, and increase substantially in a period of high market volatility. T his has
beendiscussed inmanypapers–see forexample [5], withmorerecent discussions,
includingnewindicators, in[6,7,8,9]. Furthermore, thisincreaseist houghttobe
larger for large downward moves than for large upward moves. The dynamics of
these correlations themselves, and their asymmetry, should be es timated, leading
to rather complex models [7, 10, 11, 12]. The view of ‘moving’ correlat ions has a
direct consequence for risk management: the risk for a given port folio is seen as
resulting from both volatility ﬂuctuations and correlation ﬂuctuatio ns.
An alternative point of view is provided by factor models with a ﬁxed co rrela-
tion structure. The simplest version contains a unique factor – the market itself.
In this case, the time ﬂuctuations of the measured cross-correla tions between
stocks is, as we show below, directly related to the ﬂuctuations of t he market
volatility. The notion of “correlation” risk therefore reduces to ma rket volatility
risk, which considerably simpliﬁes the problem. In this paper, we want to address
to what extent a non-Gaussian one-factor model is able to capture the essential
features of stocks cross-correlations, in particular in extreme m arket conditions.
Our conclusion is that most of the extreme risk correlations, measu red by diﬀer-
ent indicators, are actually captured by this simple ﬁxed-correlation model. This
model is able to reproduce quantitatively the observed exceedance correlations [6]
without invoking the idea of ‘regime switching’ recently advocated in t his context
in [7, 8].
However, a more detailed analysis shows that a reﬁned model is need ed to ac-
countforthedependenceoftheconditionalvolatilityandskewnes softheresiduals
on the market return.
2 A non-Gaussian one-factor model
We want to compare empirical measures of correlation with the pred iction of a
ﬁxed-correlation model. However, for generic non-Gaussian probability distribu-
tions of returns, there is no unique way of building a multivariate proc ess. A
natural choice is to assume that the return of every stock is the s um of random
independent (non-Gaussian) factors. While a multivariate Gaussian process can
always be decomposed into independent factors, this is not true fo r generic non-
Gaussian distributions. The existence of such a decomposition is thu s part of the
deﬁnition of our model.
2",2000-06-02T12:03:25Z,for t it furtr t t at it iour   w t
paper_qf_31.pdf,3,Correlation structure of extreme stock returns,"  It is commonly believed that the correlations between stock returns increase
in high volatility periods. We investigate how much of these correlations can
be explained within a simple non-Gaussian one-factor description with time
independent correlations. Using surrogate data with the true market return as
the dominant factor, we show that most of these correlations, measured by a
variety of different indicators, can be accounted for. In particular, this
one-factor model can explain the level and asymmetry of empirical exceedance
correlations. However, more subtle effects require an extension of the one
factor model, where the variance and skewness of the residuals also depend on
the market return.
","The model: We will call marketthe dominant factor in this decomposition
and write:
ri(t) =βirm(t)+ǫi(t). (1)
The daily return is deﬁned as ri(t) =Si(t)/Si(t−1)−1, where Si(t) is the
value of the stock ion dayt. The return is thus decomposed into a market part
rm(t) and a residual part ǫi(t). In a generic factor model, the residuals ǫi(t) are
combinations of all the factors except the market and are theref ore independent
of it. The one-factor model corresponds to the simple case where the ǫi(t) are
also independent of one another.
The market is deﬁned as a weighted sum of the returns of all stocks . The
weights can be those of a market index such as the S&P 500. These c ould also be
the components of the eigenvector with the largest eigenvalue of t he stocks cross-
correlation matrix [13]. We have chosen to work simply with uniform weig hts,
leading to the following deﬁnition:
rm=1
NN/summationdisplay
i=1ri. (2)
Had we chosen another weighting scheme for the deﬁnition of the ma rket, the
theoreticalresultsbelowwouldstillholdexactlyprovidedthatwere placeaverages
over all stocks by the corresponding weighted averages. On our d ata set, the
diﬀerent weighted averages give essentially the same results. The c oeﬃcients βi’s
are then given by:
βi=/angbracketleftrirm/angbracketright−/angbracketleftri/angbracketright/angbracketleftrm/angbracketright
/angbracketleftr2
m/angbracketright−/angbracketleftrm/angbracketright2, (3)
where the brackets refer to time averages. This model is meaningf ul in the case
whereβiis constant or slowly varying in time. Eq. (2) immediately implies
(1/N)/summationtextN
i=1βi= 1.
An important qualitative assumption of this model is that although th e mar-
ket is built from the ﬂuctuations of the stocks, it is a more fundamen tal quantity
than the stocks themselves. Hence, one cannot expect to explain the statistical
properties of the market from those of the stocks within this mode l.
Real data and surrogate data: The data set we considered is composed of
the daily returns of 450 U.S. equities among the most liquid ones from 1 993 up
to 1999. In order to test the validity of a one-factor model, we also generated
surrogate data compatible with this model. Very importantly, the on e-factor
model we consider is not based on Gaussian distributions, but rathe r on fat-
tailed distributions that match the empirical observations for both the market
and the stocks daily returns [14].
The procedure we used to generate the surrogate data is the follo wing:
3",2000-06-02T12:03:25Z,t  t si si si t it t t tse  had ot  eq ance real t ivery t
paper_qf_31.pdf,4,Correlation structure of extreme stock returns,"  It is commonly believed that the correlations between stock returns increase
in high volatility periods. We investigate how much of these correlations can
be explained within a simple non-Gaussian one-factor description with time
independent correlations. Using surrogate data with the true market return as
the dominant factor, we show that most of these correlations, measured by a
variety of different indicators, can be accounted for. In particular, this
one-factor model can explain the level and asymmetry of empirical exceedance
correlations. However, more subtle effects require an extension of the one
factor model, where the variance and skewness of the residuals also depend on
the market return.
","•Compute the βi’s using Eq. (3) over the whole time period [15]. These βi’s
arefound to berather narrowly distributed around1, with (1 /N)/summationtextN
i=1β2
i=
1.05.
•Compute the variance of the residuals σ2
ǫi=σ2
i−β2
iσ2
m. On the dataset we
usedσmwas 0.91% (per day) whereas the rms σǫiwas 1.66%.
•Generate the residual ǫi(t) =σǫiui(t), where the ui(t) are independent
randomvariablesofunit variancewith aleptokurtic (fattailed) distr ibution
— we have chosen here a Student distribution with an exponent µ= 4:
P(u) =3
(2+u2)5/2, (4)
which is known to represent adequately the empirical data [14].
•Compute the surrogate return as rsurr
i(t) =βirm(t) +ǫi(t), where rm(t) is
thetruemarket return at day t.
Therefore, withinthismethod, boththeempiricalandsurrogater eturnsarebased
on the very same realization of the market statistics. This allows us t o compare
meaningfully the results of the surrogate model with real data, wit hout further
averaging. It also short-cuts the precise parameterization of th e distribution of
market returns, in particular its correct negative skewness, whic h turns out to be
crucial.
3 Conditioning on large returns
Conditioning on absolute market return: We have ﬁrst studied a measure
of correlations between stocks conditioned on an extreme market return. It is
indeed commonly believed that cross-correlations between stocks increase in such
“high-volatility” periods. A natural measure is given by the following c oeﬃcient:
ρ>(λ) =1
N2/summationtext
i,j(/angbracketleftrirj/angbracketright>λ−/angbracketleftri/angbracketright>λ/angbracketleftrj/angbracketright>λ)
1
N/summationtext
i(/angbracketleftr2
i/angbracketright>λ−/angbracketleftri/angbracketright2
>λ), (5)
where the subscript > λindicates that the averaging is restricted to market
returnsrmin absolute value larger than λ. Forλ= 0 the conditioning disap-
pears. Note that the quantity ρ>is the average covariance divided by the average
variance, and therefore diﬀers from the average correlation coe ﬃcient. We have
studied the latter quantity, andthe following conclusions remain valid in this case
also.
In a ﬁrst approximation, the distribution of individual stocks retur ns can be
taken to be symmetrical, leading /angbracketleftri/angbracketright>λ≃0. The above equation can therefore
be transformed into:
ρ>(λ)≃σ2
m(λ)
1
N/summationtextN
i=1σ2
i(λ), (6)
4",2000-06-02T12:03:25Z,ute eq tse ute ogente stunt ute trefore  it conditni conditni  it for note  it
paper_qf_31.pdf,5,Correlation structure of extreme stock returns,"  It is commonly believed that the correlations between stock returns increase
in high volatility periods. We investigate how much of these correlations can
be explained within a simple non-Gaussian one-factor description with time
independent correlations. Using surrogate data with the true market return as
the dominant factor, we show that most of these correlations, measured by a
variety of different indicators, can be accounted for. In particular, this
one-factor model can explain the level and asymmetry of empirical exceedance
correlations. However, more subtle effects require an extension of the one
factor model, where the variance and skewness of the residuals also depend on
the market return.
","0 1 2 3 40.20.40.60.81
PSfrag replacements
λ(%)ρ>(λ)Empirical data
One factor model
Figure 1: Correlation measure ρ>(λ) conditional to the absolute market return
to be larger than λ, both for the empirical data and the one-factor model. Note
that both show a similar apparent increase of correlations with λ. This eﬀect is
actually overestimated by the one-factor model with ﬁxed residua l volatilities. λ
is in percents.
whereσ2
m(λ) is the market volatility conditioned to market returns in absolute
value larger than λ, andσ2
i(λ) =/angbracketleftr2
i/angbracketright>λ−/angbracketleftri/angbracketright2
>λ. In the context of a one-factor
model, we therefore obtain:
ρ>(λ) =σ2
m(λ)/parenleftBig
1
N/summationtextN
i=1β2
i/parenrightBig
σ2m(λ)+1
N/summationtextN
i=1σ2ǫi. (7)
The residual volatilities σ2
ǫiare independent of rmand therefore of λwhereas
σ2
m(λ) is obviously an increasing function of λ. Hence the coeﬃcient ρ>(λ) is an
increasing function of λ. The one-factor model therefore predicts an increase of
thecorrelations(asmeasuredby ρ>(λ))inhighvolatilityperiods. Thisconclusion
is quite general, it holds in particular for any factor model, even with G aussian
statistics. Therefore, the very fact of conditioning the correlat ion on large market
returns leads to an increase of the measured correlation. A similar d iscussion in
the context of Gaussian models can be found in [6].
More precisely, we can now compare the coeﬃcient ρ>(λ) measured empiri-
callytooneobtainedwithintheone-factormodeldeﬁnedabove. Th isispresented
in Fig. 1. Interestingly, the surrogate and empirical correlations a re similar, dis-
playing qualitatively the same increase of the cross-correlation whe n conditioned
to large market returns. This shows that a one-factor model doe s indeed account
quantitatively for the apparent increase of cross-correlations in high volatility
5",2000-06-02T12:03:25Z,flag emical one  correlatnote  ib b t nce t  conustrefore  th  interesti 
paper_qf_31.pdf,6,Correlation structure of extreme stock returns,"  It is commonly believed that the correlations between stock returns increase
in high volatility periods. We investigate how much of these correlations can
be explained within a simple non-Gaussian one-factor description with time
independent correlations. Using surrogate data with the true market return as
the dominant factor, we show that most of these correlations, measured by a
variety of different indicators, can be accounted for. In particular, this
one-factor model can explain the level and asymmetry of empirical exceedance
correlations. However, more subtle effects require an extension of the one
factor model, where the variance and skewness of the residuals also depend on
the market return.
","0.50.60.70.80.91
-8 -6 -4 -2 0 2 4 6PSfrag replacements
rm(%)P(sign(ri) = sign( rm))
Empirical data
One factor model
Figure 2: Conditional probability that a stock has the same sign as th e market
return as a function of the market return; rmis in percent. Each cross represents
the empirical probability using 4% of the days centered around a give n market
return. The dotted line is the prediction of the non-Gaussian one-f actor model.
periods.
The one-factor model actually even overestimates the correlations for large
λ. This overestimation can be understood qualitatively as a result of a positive
correlation between the amplitude of the market return |rm|and the residual
volatilities σǫi, which we discuss in more details in Section 4 below (see in par-
ticular Fig. 5). For large values of λ,σǫiis found to be larger than its average
value. FromEq. (7), thislowersthecorrelation ρ>(λ)ascomparedtothesimplest
one-factor model where the volatility ﬂuctuations of the residuals are neglected.
Conditional fraction of positive/negative returns: Another quantity of
interest is the fraction of stocks returns having the same sign as t he market
return, as a function of the market return itself. The empirical re sults are shown
on Fig. 2. We observe that for the largest returns, 90% of the sto cks have the
same return sign as that of the market. Therefore, the sign of th e market appears
to have a very strong inﬂuence on the sign of individual stock retur ns.
This fraction can be calculated exactly within the one-factor model. Focusing
on positive market return (the case of negative returns can be tr eated similarly),
a stock return riis positive whenever ǫi>−βirm. Therefore the average fraction
f(t) of stocks having a positive return for a given market return rm(t) is
f(t) =1
NN/summationdisplay
i=1P</parenleftBiggβirm(t)
σǫi/parenrightBigg
, (8)
6",2000-06-02T12:03:25Z,flag emical one  conditnal eat t  se for from eq conditnal anotr t   trefore  focusi trefore b b
paper_qf_31.pdf,7,Correlation structure of extreme stock returns,"  It is commonly believed that the correlations between stock returns increase
in high volatility periods. We investigate how much of these correlations can
be explained within a simple non-Gaussian one-factor description with time
independent correlations. Using surrogate data with the true market return as
the dominant factor, we show that most of these correlations, measured by a
variety of different indicators, can be accounted for. In particular, this
one-factor model can explain the level and asymmetry of empirical exceedance
correlations. However, more subtle effects require an extension of the one
factor model, where the variance and skewness of the residuals also depend on
the market return.
","whereP<is the cumulative normalized distribution of the residual (chosen her e
to be a Student distribution with a exponent µ= 4).f(t) is also plotted on Fig. 2
and ﬁts well the empirical results. The theoretical estimate slightly overestimates
the fraction f(t) for positive market returns. As explained above, the correlation s
between σǫiand|rm|do lower f(t) as needed for the positive side. However, the
corresponding fraction for the negative side would then be undere stimated.
Conditioning on large individual stock returns – quantile c orrelations
and exceedance correlations: Sincethevolatilityoftheresidualsistwotimes
larger than the volatility of the market, the conditioning by extreme market
events does not necessarily select extreme individual stock moves . The quantities
studied in the previous section, namely return correlations and sign correlations,
are therefore more related to the central part of the stocks dis tribution rather
than to their extreme tails. We now study more speciﬁcally how extre me stock
returns are correlated between themselves. A ﬁrst possibility is to studyquantile
correlations , that we deﬁne as:
ρ(q) =1
N2/summationtext
i,j(/angbracketleftrirj/angbracketrightq−/angbracketleftri/angbracketrightq/angbracketleftrj/angbracketrightq)
1
N/summationtext
i/parenleftBig
/angbracketleftr2
i/angbracketrightq−/angbracketleftri/angbracketright2
q/parenrightBig, (9)
where the subscript qindicates that we only retain in the average days such that
both|ri|and|rj|take their q−quantile value, within a certain tolerance level
(this tolerance is taken to be 4% of the total interval for each qua ntile). In the
limitq→1, this selects extremes days for both stocks iandjsimultaneously.
The empirical results for ρ(q) are compared with the prediction of the one-factor
model in Fig. 3. The agreement is again very good, though the one-f actor model
still slightly overestimates the true correlations in the extremes.
Another interesting quantity that has been much studied in the eco nometric
literature recently, is the so-called exceedance correlation funct ion, introduced in
[6]. One ﬁrst deﬁnes normalized centered returns ˜ riwith zero mean and unit
variance. The positive exceedance correlation between iandjis deﬁned as:
ρ+
ij(θ) =/angbracketleft˜ri˜rj/angbracketright>θ−/angbracketleft˜ri/angbracketright>θ/angbracketleft˜rj/angbracketright>θ/radicalBig
(/angbracketleft˜r2
i/angbracketright>θ−/angbracketleft˜ri/angbracketright2
>θ)(/angbracketleft˜r2
j/angbracketright>θ−/angbracketleft˜rj/angbracketright2
>θ), (10)
where the subscript > θmeans that both normalized returns are larger than θ.
Largeθ’s correspond to extreme correlations. The negative exceedance correla-
tionρ−
ij(−θ) is deﬁned similarly, the conditioning being now on returns smaller
than−θ. Fig. 4 shows the exceedance correlation function, averaged ove r the
pairsiandj, both for real data and for the surrogate one-factor model da ta. As
in previous papers, we have shown ρ+
ij(θ) for positive θandρ−
ij(−θ) for negative
θ. As in previous studies [6, 7, 8], we ﬁnd that ρ±(±θ)growswith|θ|and is larger
for large negative moves than for large positive moves. This is in stro ng contrast
7",2000-06-02T12:03:25Z,stunt  t as conditni since t volatity of t residual is two timt  b b it  t anotr one t b large t  as as 
paper_qf_31.pdf,8,Correlation structure of extreme stock returns,"  It is commonly believed that the correlations between stock returns increase
in high volatility periods. We investigate how much of these correlations can
be explained within a simple non-Gaussian one-factor description with time
independent correlations. Using surrogate data with the true market return as
the dominant factor, we show that most of these correlations, measured by a
variety of different indicators, can be accounted for. In particular, this
one-factor model can explain the level and asymmetry of empirical exceedance
correlations. However, more subtle effects require an extension of the one
factor model, where the variance and skewness of the residuals also depend on
the market return.
","00.10.20.30.40.50.60.7
00.10.20.30.40.50.60.70.80.91PSfrag replacements
qρ(q)Empirical data
Surrogate data
Figure 3: Correlation between stocks for joint extreme moves, ρ(q), as a function
of the quantile value q, both for real data and the surrogate one-factor model.
with the prediction of a Gaussian model, which gives a symmetric tent- shaped
graph that goes to zero for large |θ|. Note however that previous studies have
focused on ﬁxed pairs of assets iandj(for example a few pairs of international
markets). The result of Fig. 4 is interesting since it reveals a system atic eﬀect
over all pairs of a pool of 450 stocks.
Several models have been considered to explain the observed resu lts [7, 8].
Simple GARCH or Jump models cannot account for the shape of the ex ceedance
correlations. Qualitatively similar graphs can however be reproduce d within a
rather sophisticated ‘regime switching’ model, where the two asset s switch be-
tween a positive, low volatility trend with small cross-correlations an d a negative,
high volatility trend with large cross-correlations. Note that by con struction, this
‘regime switching’ model induces a strong skew in the ‘index’ (i.e. the a verage
between the two assets). Fig. 4 however clearly shows that a ﬁxedcorrelation
non Gaussian one-factor model is enough to explain quantitatively t he level and
asymmetry of the exceedance correlation function. In particular , the asymmetry
is induced by the large negative skewness in the distribution of index r eturns,
and the growth of the exceedance correlation with |θ|is related to distribution
tails fatter than exponential (in our case, these tails are indeed po wer-laws).
4 Conditional statistics of the residuals
We conclude from the above results that the observed ﬂuctuation s of the stock
cross-correlations are mainly a consequence of the volatility ﬂuctu ations and
8",2000-06-02T12:03:25Z,flag emical surrogate  correlatnote t  sevl simple jump qualitative note  iconditnal 
paper_qf_31.pdf,9,Correlation structure of extreme stock returns,"  It is commonly believed that the correlations between stock returns increase
in high volatility periods. We investigate how much of these correlations can
be explained within a simple non-Gaussian one-factor description with time
independent correlations. Using surrogate data with the true market return as
the dominant factor, we show that most of these correlations, measured by a
variety of different indicators, can be accounted for. In particular, this
one-factor model can explain the level and asymmetry of empirical exceedance
correlations. However, more subtle effects require an extension of the one
factor model, where the variance and skewness of the residuals also depend on
the market return.
","−2 −1 0 1 200.20.40.60.81
PSfrag replacements
θρ±(±θ)Empirical data
Surrogate data
Figure 4: Average exceedance correlation functions between sto cks as a function
of the level parameter θ, both for real data and the surrogate one-factor model.
We have shown ρ+
ij(θ) for positive θandρ−
ij(−θ) for negative θ. Note that this
quantity growswith|θ|and is strongly asymmetric.
skewness of the market return, and that a non Gaussian one-fac tor model does
reproduce satisfactorily most of the observed eﬀects. However , some small sys-
tematic discrepancies appear, and call for an extension of the one -factor model.
The most obvious eﬀect not captured bya one-factormodel isthe recently discov-
ered ‘ensemble’ skewness in the daily distribution of stock returns, as discussed
by Lillo and Mantegna [16]. More precisely, they have shown that the h istogram
of all the stocks returns for a given day displays on average a positive skewness
when the market return is positive, and a negative skewness when t he market
return is negative. The amplitude of this skewness furthermore gr ows with the
absolute value of the market return. Note that this skewness is notrelated to the
possible non zero skewness of individual stocks that has been rece ntly discussed
in several papers in relation with extended CAPM models [12, 17].
Clearly, this‘ensemble’ skewness thatdepends onthemarket retu rncannotbe
explained by theabove one-factor model where the residuals have a timeindepen-
dent zero skewness. The one-factor model is certainly an oversim pliﬁcation of the
reality: although the market captures the largest part of the cor relation between
stocks, industrial sectors are also important, as can be seen fro m a diagonaliza-
tion of the correlation matrix [18]. Large moves of the market can be dominated
by extreme moves of a single sector, while the other sectors are re latively unaf-
fected. This eﬀect does induce some skewness in the ﬁxed-day hist ogram of stock
returns distribution.
9",2000-06-02T12:03:25Z,flag emical surrogate  ge  note t lle antenna  t note  t large 
paper_qf_31.pdf,10,Correlation structure of extreme stock returns,"  It is commonly believed that the correlations between stock returns increase
in high volatility periods. We investigate how much of these correlations can
be explained within a simple non-Gaussian one-factor description with time
independent correlations. Using surrogate data with the true market return as
the dominant factor, we show that most of these correlations, measured by a
variety of different indicators, can be accounted for. In particular, this
one-factor model can explain the level and asymmetry of empirical exceedance
correlations. However, more subtle effects require an extension of the one
factor model, where the variance and skewness of the residuals also depend on
the market return.
","0.511.522.53
00.511.522.533.544.55PSfrag replacements
|rm|(%)Σ (%)Empirical data
Linear ﬁt: ρ= 0.54
Figure5: Dailyresidual ‘volatility’ Σ (Eq. (11))averaged over the diﬀ erent stocks
foragivendayasafunctionofthemarketreturnforthesameday . Σisinpercent.
A way to account for this eﬀect is to allow the distribution of the resid ualǫi(t)
to depend on the market return rm(t). In order to test this idea, we have studied
directly some moments of the distribution of the residuals for a given day as a
function of the market return that particular day. We have studie d the following
quantities:
Σ = [|ǫi−[ǫi]|], (11)
S=[ǫi]−Med(ǫi)
Σ, (12)
K=[ǫ2
i]−[ǫi]2
Σ2, (13)
where the square brackets [ ...] means that we average over the diﬀerent stocks
for a given day and Med selects the median value of ǫi. These three quantities
should be thought as robustalternatives to the standard variance, skewness and
kurtosis, which are based on higher moments of the distribution.
The quantity Σ measures the ‘volatility’ of the residuals and is shown in Fig. 5
as a function of |rm(t)|. A linear regression is also shown for comparison. It is
clear that there is a positive correlation between the market volatilit y and the
volatility of the residuals, not captured by the simplest one-factor model. As
explained above, this eﬀect actually allows one to account quantitat ively for the
systematic overestimation of the observed correlations.
In order to conﬁrm the skewness eﬀect of Lillo and Mantegna, we ha ve then
studied the quantity S. This quantity is positive if the distribution is positively
skewed. Fig. 6 shows a scatter plot of Sas a function of rm[19]. Again these
10",2000-06-02T12:03:25Z,flag emical linear  articial intellence ly residual eq i med med tse t  it as ille antenna   as ag articial intellence
paper_qf_31.pdf,11,Correlation structure of extreme stock returns,"  It is commonly believed that the correlations between stock returns increase
in high volatility periods. We investigate how much of these correlations can
be explained within a simple non-Gaussian one-factor description with time
independent correlations. Using surrogate data with the true market return as
the dominant factor, we show that most of these correlations, measured by a
variety of different indicators, can be accounted for. In particular, this
one-factor model can explain the level and asymmetry of empirical exceedance
correlations. However, more subtle effects require an extension of the one
factor model, where the variance and skewness of the residuals also depend on
the market return.
","-0.4-0.3-0.2-0.100.10.20.30.4
-8-6-4-2 0246PSfrag replacements
rm(%)SEmpirical data
Linear ﬁt: ρ= 0.44
Figure6: Dailyresidual ‘skewness’ Saveraged over thediﬀerent stocks foragiven
day as a function of the market return for the same day. Note tha t the skewness
is computed using low moments of the distribution to reduce the meas urement
noise and does not correspond to the usual deﬁnition (see Eq. (12 )).
two quantities are positively correlated, as emphasized by Lillo and Ma ntegna
(although their analysis is diﬀerent from ours).
Therefore, both the volatility and the skew of the residuals are quit e strongly
correlated with the market return. One could wonder if higher mome nts of the
distribution are also sensitive to the value of rm. We have therefore studied the
quantity Kas one possible reﬁned measure of the shape of the distribution of
residuals. This is shown in Fig. 7 and reveals a much weaker dependenc e than
the previous two quantities.
5 Conclusion
We have thus shown that the apparent increase of correlation bet ween stock
returns in extreme conditions can be satisfactorily explained within a staticone-
factor model which accounts for fat-tail eﬀects. In this model, c onditioning on
a high observed volatility naturally leads to an increase of the appare nt cor-
relations. The much discussed exceedance correlations can also be reproduced
quantitatively and reﬂects both the non-Gaussian nature of the ﬂ uctuations and
the negative skewness of the index, and notthe fact that correlations themselves
are time dependent.
This one-factor model is however only an approximation to the true correla-
tions, and more subtle eﬀects (such as the Lillo-Mantegna ‘ensemble ’ skewness)
11",2000-06-02T12:03:25Z,flag emical linear  articial intellence ly residual ged note eq lle ma trefore one  as   conus it  lle antenna
paper_qf_31.pdf,12,Correlation structure of extreme stock returns,"  It is commonly believed that the correlations between stock returns increase
in high volatility periods. We investigate how much of these correlations can
be explained within a simple non-Gaussian one-factor description with time
independent correlations. Using surrogate data with the true market return as
the dominant factor, we show that most of these correlations, measured by a
variety of different indicators, can be accounted for. In particular, this
one-factor model can explain the level and asymmetry of empirical exceedance
correlations. However, more subtle effects require an extension of the one
factor model, where the variance and skewness of the residuals also depend on
the market return.
","11.522.53
00.511.522.533.54PSfrag replacements
|rm|(%)KEmpirical data
Linear ﬁt: ρ=−0.13
Figure 7: Daily residual ‘kurtosis’ averaged over the diﬀerent stoc ks for a given
day as a function of the market return for the same day. Again, th e kurtosis
is computed using low moments of the distribution to reduce the meas urement
noise and does not correspond to the usual deﬁnition (see Eq. (13 )).
require an extension of the one factor model, where the variance a nd skewness of
the residuals themselves depend on the market return.
Acknowledgments: We wish to thank M. Meyer and J. Miller for many useful
discussions.
References
[1] E.J. Elton and M.J. Gruber, Modern Portfolio Theory and Investment Anal-
ysis, Wiley, (1995).
[2] see e.g.: J.C. Hull Futures, Options and Other Derivatives , Prentice Hall
(2000).
[3] N. Taleb, Dynamical Hedging , Wiley, (1998).
[4] J.P. Bouchaud, M. Potters, A. Matacz, The leverage eﬀect in ﬁnancial mar-
kets: retarded volatility and market panic , e-print cond-mat/0101120.
[5] W.L. Lin, R.F. Engle, T. Ito, Do bulls and bears move across borders? In-
ternational transmission of stock returns and volatility , inThe review of
Financial Studies ,7, 507 (1994); C.B. Erb, C.R. Harvey, T.E. Viskanta,
12",2000-06-02T12:03:25Z,flag emical linear  articial intellence ly ag articial intellence eq ackledgment  meyer mler referenceltoruler mortftory investment anal rey hull futuroptns otr rivativpraice hall tale dynamical edgi rey  chaters mata cz t lieagle to do it nancial studier harvey vis want
paper_qf_31.pdf,13,Correlation structure of extreme stock returns,"  It is commonly believed that the correlations between stock returns increase
in high volatility periods. We investigate how much of these correlations can
be explained within a simple non-Gaussian one-factor description with time
independent correlations. Using surrogate data with the true market return as
the dominant factor, we show that most of these correlations, measured by a
variety of different indicators, can be accounted for. In particular, this
one-factor model can explain the level and asymmetry of empirical exceedance
correlations. However, more subtle effects require an extension of the one
factor model, where the variance and skewness of the residuals also depend on
the market return.
","Forecasting International correlations ,Financial Analysts Journal ,50, 32
(1994); B. Solnik, C. Boucrelle, Y. Le Fur, International Market Correla-
tions and Volatility ,Financial Analysts Journal ,52, 17 (1996).
[6] F. Longin, B. Solnik, Correlation structure of international equity markets
during extremely volatile periods , working paper (1999).
[7] A. Ang, G. Bekaert, International Asset Allocation with time varying cor-
relations, NBER working paper (1999); A. Ang, G. Bekaert, International
Asset Allocation with Regime Shifts , working paper (2000).
[8] A.Ang, J.Chen, Asymmetric correlations of Equity Portfolio , working paper
(2000).
[9] This has also been argued recently by S. Drozdz, F. Gr¨ ummer, F . Ruf and
J. Speth, Dynamics of competition between collectivity and noise in t he stock
market, e-print cond-mat/9911168.
[10] Y. Baba, R. Engle, D. Kraft, K. F. Kroner, Multivariate Simultaneous Gen-
eralized ARCH , Discussion paper 89-57, University of California, San Diego.
[11] J.Y. Campbell, A.W. Lo, A.C. McKinley, The Econometrics of Financial
Markets, Princeton University Press (1997), and references therein.
[12] G. Bekaert, G. Wu, Asymmetric volatility and Risk in Equity markets , The
Review of Financial Studies 13, 1 (2000).
[13] H. Markowitz, Portfolio Selection: Eﬃcient Diversiﬁcation of Investmen ts
Wiley, (1959); see also: J.P. Bouchaud and M. Potters, Th´ eorie des risques
ﬁnanciers , Al´ ea-Saclay, Eyrolles, (1997) (in French), Theory of Finan-
cial Risks , Cambridge University Press (2000); L. Laloux, P. Cizeau, J.P.
Bouchaud and M. Potters, Phys. Rev. Lett. 83, 1467 (1999).
[14] see e.g.: C.W.J. Granger, Z.X. Ding, Stylized facts on the temporal distribu-
tional properties of daily data from speculative markets , Working Paper 94-
19, University of California, San Diego (1994); D.M. Guillaume et al., “From
the bird’s eye to the microscope,” Finance and Stochastics 1, 2 (1997); V.
Plerou, P. Gopikrishnan, L.A. Amaral, M. Meyer, H.E. Stanley, Phys. Rev.
E606519 (1999).
[15] We do not think that computing βiandσǫi“in sample” has any consequence
on the presented results.
[16] F. Lillo and R.N. Mantegna, Symmetry alteration of ensemble return
distribution in crash and rally days of ﬁnancial markets , e-print cond-
mat/0002438 (2000).
13",2000-06-02T12:03:25Z,forecasti internatnal nancial analysts journal s t elle le fur internatnal market core la volatity nancial analysts journal lo is correlata be ka ert internatnal asset allocata be ka ert internatnal asset allocatregime shts a casyetric equity tf dr oz dz gr ru pe th dynamics baba eagle draft krone multivariate simultaneous gediscuss calornia sadiego campbell lo mc kled t econometrics nancial markets princeto  be ka ert wu asyetric risk equity t review nancial studimarwitz tfseledivers invest merey  chaters th al ay ey roll frentory arisks cambridge   halo ux size au  chaters ph ys rev   di stylized worki pa  calornia sadiego gulaume from nance stochastic le rou go pi krishna moral meyer stanley ph ys rev  lle antenna syetry
paper_qf_31.pdf,14,Correlation structure of extreme stock returns,"  It is commonly believed that the correlations between stock returns increase
in high volatility periods. We investigate how much of these correlations can
be explained within a simple non-Gaussian one-factor description with time
independent correlations. Using surrogate data with the true market return as
the dominant factor, we show that most of these correlations, measured by a
variety of different indicators, can be accounted for. In particular, this
one-factor model can explain the level and asymmetry of empirical exceedance
correlations. However, more subtle effects require an extension of the one
factor model, where the variance and skewness of the residuals also depend on
the market return.
","[17] C. Harvey, A. Siddique, Conditional Skewness in Asset Pricing Tests , Jour-
nal of Finance LV, 1263 (2000).
[18] For a related discussion, see: R.N. Mantegna, H.E. Stanley, An introduction
to Econophysics , Cambridge University Press (1999), Chapter 13.
[19] Due to the discreteness of quoted prices, a substantial frac tion of the stocks
closes at the same price two days in a row, producing a zero return. This
large number of exactly zero returns often makes the median retu rn to be
zero which in turnsinduces anuninteresting systematic eﬀect inthe quantity
S. To get rid of this eﬀect we have added to each return of random nu mber
uniformly distributed between -0.25% and 0.25%.
14",2000-06-02T12:03:25Z,harvey si que conditnal skew ness asset prici tests jour nance for antenna stanley aeco no psics cambridge   chapter due  to
paper_qf_32.pdf,1,Economy of scales in R&D with block-busters,"  Are large scale research programs that include many projects more productive
than smaller ones with fewer projects? This problem of economy of scale is
particularly relevant for understanding recent mergers in particular in the
pharmaceutical industry. We present a quantitative theory based on the
characterization of distributions of discounted sales S resulting from new
products. Assuming that these complementary cumulative distributions have fat
tails with approximate power law structure S^{-a}, we demonstrate that economy
of scales are automatically realized when the exponent a is less than one.
Empirical evidence suggests that the exponent a is approximately equal to 2/3
for the pharmaceutical industry.
","arXiv:cond-mat/0001434   31 Jan 2000ECONOMY OF SCALES IN R&D WITH BLOCK-BUSTERS
D. Sornette1,2
Abstract : Are large scale research programs that include many projects more productive than
smaller ones with fewer projects? This problem of economy of scale is relevant for understanding
recent mergers in particular in the pharmaceutical industry. We present a quantitative theory based
on the characterization of distributions of discounted sales S resulting from new drugs. Assuming
that these complementary cumulative distributions have fat tails with approximate power law
structure S−α, we demonstrate that economy of scales are automatically realized when α<1.
Empirical evidence suggests that α ≅ 2/3 for the pharmaceutical industry.
Journal of Economic Literature Classification Code:
C44 - Statistical Decision Theory; Operations Research
L22 - Firm Organization and Market Structure: Markets vs. Hierarchies; Vertical Integration
Keywords: Pareto law, Levy law, power law, economy of scale, central limit theorem, block-
buster. Lévy laws
1 Institute of Geophysics and Planetary Physics and Department of Earth and Space Sciences,
UCLA, Los Angeles, CA 90095-1567
tel: (310) 825 28 63  Fax: (310) 206 3051      email: sornette@moho.ess.ucla.edu
2LPMC, CNRS and Université de Nice-Sophia Antipolis, B.P. 71, Parc Valrose
06108 Nice Cedex 2, France",2000-01-31T03:48:42Z, jacorvee abstra are   assumi emical journal economic re assicatco statistical cistory oatns researrm organizatmarket struure markets hirchivertical integratkeywords party levy institute geopsical planetary psics partment earth space scienclos aelfax  nice sophia anti po parc val rose nice ce france
paper_qf_32.pdf,2,Economy of scales in R&D with block-busters,"  Are large scale research programs that include many projects more productive
than smaller ones with fewer projects? This problem of economy of scale is
particularly relevant for understanding recent mergers in particular in the
pharmaceutical industry. We present a quantitative theory based on the
characterization of distributions of discounted sales S resulting from new
products. Assuming that these complementary cumulative distributions have fat
tails with approximate power law structure S^{-a}, we demonstrate that economy
of scales are automatically realized when the exponent a is less than one.
Empirical evidence suggests that the exponent a is approximately equal to 2/3
for the pharmaceutical industry.
","1-Introduction
Some industries, such as the pharmaceutical and movie industries, are characterized by the
occurrence of “block-busters”, i.e. remarkably successful products with exceptional sales much
larger than the average. How much exceptional are these block-busters is an important question for
firm strategy and the policy of economy of scales (Sornette and Zajdenweber,  1999).
A specific quantification of this observation has been proposed by using the Pareto or power law
distribution. Scherer (1965) found that the distribution of sales per drug in the pharmaceutical
industry is consistent with a power law distribution as defined in equation (1.1) with exponent α
smaller than 0.5. Grabowski and Vernon (1990, 1994) construct a discounted present value per
new chemical entity (NCE) and divide the drugs in decile in descending order, leading to a value
distribution compatible with a power law distribution of the tail with exponent α approximately
equal to 2/3 (Sornette, unpublished). Such a small exponent implies that both the variance and the
mean are mathematically infinite and standard economic reasoning and techniques can not apply.
Sornette and Zajdenweber (1999) have shown that the distribution of incomes per movie is stable
over twenty years and is compatible with a power law distribution with exponent α approximately
equal to 1.5. Scherer (1998) revisited a similar problem and investigated empirically the distribution
of profits from technological innovations, with the aim to test whether it conforms most closely to
the Paretian (power law), log-normal or some other distribution. Scherer (1998) looked at the data
in a different way by subtracting the estimated production and marketing costs from sales revenues
to obtain a Marshall quasi-rents to R&D investment. He then finds that the distribution of quasi-
rents from recent marketed pharmaceutical entities is not a power law but has a thinner tail. This is
demonstrated by the downward curvature in a log-log plot (logarithm of the ordinate as a function
of the logarithm of the abscissa) This result seems to contradict the older analysis [Scherer, 1965].
However, the procedure of subtracting production and marketing costs introduce a strong bias for
intermediate and small sales that are not taken into account.
Indeed, let us call S the cumulative discounted sale from a pharmaceutical innovation. Let us
assume that the distribution of sales can be parameterized by the power law
P(S) dS = α  (Smin)α  S-(1+α) dS where Smin ≤ S . (1.1)",2000-01-31T03:48:42Z,introdusome how corvee za ber party sc rer grabow vernocorvee sucorvee za ber sc rer are thasc rer marshall    sc rer ined  sm ism in
paper_qf_32.pdf,3,Economy of scales in R&D with block-busters,"  Are large scale research programs that include many projects more productive
than smaller ones with fewer projects? This problem of economy of scale is
particularly relevant for understanding recent mergers in particular in the
pharmaceutical industry. We present a quantitative theory based on the
characterization of distributions of discounted sales S resulting from new
products. Assuming that these complementary cumulative distributions have fat
tails with approximate power law structure S^{-a}, we demonstrate that economy
of scales are automatically realized when the exponent a is less than one.
Empirical evidence suggests that the exponent a is approximately equal to 2/3
for the pharmaceutical industry.
","P(S) dS is the probability for an innovation to produce sales between S and S+dS. We assume a
minimum value Smin such that the distribution (1) vanished below Smin. The distribution (1) is
normalized. The exponent α controls the frequency of large sales: the smaller α is, the more
probable are large sales S relative to smaller ones. Suppose that we look at the distribution of s=S-
C, where C is assumed to be the fixed production and marketing costs subtracted to the sales
leading to the quasi-rents s. In log-log scales, the distribution of s given by
ln[P(s)] = constant - (1+ α) ln[s+C] . (1.2)
For s>>C, ln[s+C] is indistinguishable from ln[s] and (2) gives a straight line in the log-log plots
qualifying a power law. However, when s is not much larger than C, the term ln[s+C] saturates,
i.e. instead of - ln[s] growing without bounds, - ln[s+C] saturated to the value - ln[C]. In the log-
log representation used for instance by Scherer (1998), this leads to a progressive bent towards the
horizontal, which can be misinterpreted as a departure from power law statistics for S. Scherer
(1998) analysis thus does not invalidate the power law hypothesis for the distribution of sales.
In the analysis that follows, we start from the hypothesis that the distribution of sales is a power
law with exponent α less than one. This case requires a novel and special approach that we develop
here. Our main result is that the typical return per product is an increasing function of the total size
of the portfolio of products. Our analysis provides an exact benchmark to gauge empirical
evidence.
Power law distributions (1.1) are sometimes called ``fractal’’ or self-similar (Dubrulle et al., 1997).
A power law distribution characterizes the absence of any characteristic size: independently  of the
value of S, the number of realizations larger than λS is a constant factor λ-α  times the number of
realizations larger than S.  Suppose S=$109 which occurs with frequency f. Then the frequency of
sales equal to $ 2 109 is 2-α  f = 0.63 f for α=2/3!  Take now, S=$1010 which occurs with
frequency f’=10-α f = 0.22 f for α=2/3. Then the frequency of $ 2 1010 is 2-α  f’ = 0.63 f’, i.e. is
exactly the same ratio. For any other class of distributions, this ratio of frequencies will depend,
not only on the ratio of the values but, also on the absolute value of S. Consider for instance an
exponential distribution exp[-S/S0] with characteristic value S0=$108. Sales equal to or larger than
S=7S0 occur with a frequency approximately equal to 10-3. Sales of 2S then occur with frequency
10-6 giving a ratio of frequency(2S)/frequency(S)=10-6/10-3=10-3. This ratio is not constant but",2000-01-31T03:48:42Z, sm ism it t suppose ifor isc rer sc rer i our our  dub rul le suppose ttake tfor consir salsal
paper_qf_32.pdf,4,Economy of scales in R&D with block-busters,"  Are large scale research programs that include many projects more productive
than smaller ones with fewer projects? This problem of economy of scale is
particularly relevant for understanding recent mergers in particular in the
pharmaceutical industry. We present a quantitative theory based on the
characterization of distributions of discounted sales S resulting from new
products. Assuming that these complementary cumulative distributions have fat
tails with approximate power law structure S^{-a}, we demonstrate that economy
of scales are automatically realized when the exponent a is less than one.
Empirical evidence suggests that the exponent a is approximately equal to 2/3
for the pharmaceutical industry.
","decreases fast: for instance frequency(4S)/frequency(2S)=10-12/10-6=10-6, illustrating the absence of
self-similarity in this exponential case.
2-Theoretical analysis of the portfolio problem
2-1 The α<1 power law paradox
Consider a company developing N innovations per year. To simplify the analysis, we discount
the total future sales to the value at the inception of the product. The total sale over one year of
the company is the sum of its N sales for its N active products for that year:
W(N) = S1 + S2 + ... + SN  . (2.1)
Before deriving the rigorous analysis in section 2.2, let us provide an heuristic but nevertheless
accurate derivation (up to precise numerical pre-factors). For large N, W is approximately
W(N) ≅ N EN[S]   , (2.2)
where EN[S] is the expectation of the sale size S per product conditioned on the realization of N
innovations. The need to impose this condition stems from the fact that the unconditioned
expectation is formally infinite for α<1 as can be seen by a direct estimation of the integral ∫ S P(S)
dS from Smin to infinity. This divergence reflects the fact that the distribution (1.1) decays so slowly
to zero for large S that arbitrarily large values occur with sufficient frequency to draw the
expectation to infinity. Of course, this mathematical divergence is not a problem in practice because
one always observes a finite sample N with a maximum sale Smax. The correct expression of the
expectation is thus such that it must be performed over all possible values S smaller or equal to
Smax(N)
EN[S]  ≅ ∫SminSmax(N) S P(S) dS (2.3)
where the maximum sale Smax(N) is a function of the size N of the portfolio of innovations. It is
given by",2000-01-31T03:48:42Z,toical t consir to t before for t sm i of max t max sm imax max it
paper_qf_32.pdf,5,Economy of scales in R&D with block-busters,"  Are large scale research programs that include many projects more productive
than smaller ones with fewer projects? This problem of economy of scale is
particularly relevant for understanding recent mergers in particular in the
pharmaceutical industry. We present a quantitative theory based on the
characterization of distributions of discounted sales S resulting from new
products. Assuming that these complementary cumulative distributions have fat
tails with approximate power law structure S^{-a}, we demonstrate that economy
of scales are automatically realized when the exponent a is less than one.
Empirical evidence suggests that the exponent a is approximately equal to 2/3
for the pharmaceutical industry.
","N ∫Smax(N)∞  P(S) dS  ≅    1.  (2.4)
The integral in eq.(2.4) is the probability that one sale is larger than or equal to Smax(N). This
probability multiplied by N gives the number of sales larger or equal to Smax(N). Equating this
number with 1 imposes that there is typically only one sale larger than or equal to Smax(N). This
thus defines the larger sale. Solving (2.4), we get
Smax(N) ≅ Smin   N1/α   ,  (2.5)
showing that, as the size N of the portfolio increases, the largest possible sale increases faster than
N. Inserting (2.5) into (2.3), we get
EN[S]  ≅ [α/(1−α)]  Smin  N(1/α −1)  , (2.6)
where we have neglected 1 compared to N(1/α −1). This is warranted for α<1 for which 1/α  - 1 > 0,
which ensures that N(1/α −1)  grows with N. Inserting this result (2.6) into the expression (2.2) of the
yearly sale W gives
W(N) ≅ α/(1−α)  Smin  N1/α     for α ≤ 1 . (2.7)
Note that Smax(N)/W(N) is found independent of N: the single maximum sale is a finite fraction of
the total cumulative sale! This approximate calculation predicts Smax(N)/W(N) = (1−α)/α = 1/2 for
α=2/3 while the exact result is 1−α=1/3 for α=2/3 (Feller, 1971). The main point here is not to be
quantitatively precise but to capture correctly the growth of the total sale W(N) faster than N as N
grows.
Indeed, the main message of this calculation is that, for α<1, W(N) increases with the size N of the
portfolio faster than N. The function W(N) has thus the convexity property",2000-01-31T03:48:42Z,max t max  max equati max  solvi max sm iinserti sm i inserti sm inote max  max seller t ined t
paper_qf_32.pdf,6,Economy of scales in R&D with block-busters,"  Are large scale research programs that include many projects more productive
than smaller ones with fewer projects? This problem of economy of scale is
particularly relevant for understanding recent mergers in particular in the
pharmaceutical industry. We present a quantitative theory based on the
characterization of distributions of discounted sales S resulting from new
products. Assuming that these complementary cumulative distributions have fat
tails with approximate power law structure S^{-a}, we demonstrate that economy
of scales are automatically realized when the exponent a is less than one.
Empirical evidence suggests that the exponent a is approximately equal to 2/3
for the pharmaceutical industry.
","W(NA + NB) > W(NA) + W(NB)  , (2.8)
where the equality is recovered for α≥ 1. 
This result leads to a paradox. Indeed, consider two companies A and B developing respectively
NA and NB innovations per year and their hypothetical merger C=A+B with NA + NB innovations
per year. The yearly sales for each company is
WA = S1 + S2 + ... + SNA (2.9)
WB = SNA+1 + SNA+2 + ... + SNA+NB            (2.10)
It is clear that the total sale of C is simply the sum of the sales of A and the sales of B:
WA + WB  = S1 + S2 + ... + SNA + SNA+1 + SNA+2 + ... + SNA+NB . (2.11)
There thus seems to be little advantage in the merger. However, equation (2.11) is valid for each
realization, i.e. year after year. The result (2.8) derives from the fact that we have calculated the
typical, in other words, most probable finite size expectations. Thus, the most probable (in a
statistical sense) total sale of C is larger than the sum of the most probable total sales of A and of B!
In intuitive terms, the mechanism is the following. The total sale W(N) is controlled by the largest
sale Smax(N) as well as the few largest ones. This is true for portfolios A, B and C. Now, the
largest sale Smax(NA + NB) is equal to either the largest sale of A or the largest sale of B. Let us
assume that it is the former. Then in this statistical realization, the portfolio A had a lucky year
while portfolio B had a bad year. It may face difficulties or even fill up bankruptcy. For the merged
portfolio, the risks are smaller as the most probable sale is larger than the some of the two in the
most probable sense. In other words, the most probable largest and total sales are not additive for
power law distributions with exponent α < 1 This is the essence of the inequality (2.8) that we now
put on a rigorous basis.
2-2 Rigorous analysis of the distributions of aggregate sales
For S distributed according to the distribution (1.1), the distribution of W(N) converges for large N",2000-01-31T03:48:42Z, ined t it tre t  it max   max  tit for i rorous for
paper_qf_32.pdf,7,Economy of scales in R&D with block-busters,"  Are large scale research programs that include many projects more productive
than smaller ones with fewer projects? This problem of economy of scale is
particularly relevant for understanding recent mergers in particular in the
pharmaceutical industry. We present a quantitative theory based on the
characterization of distributions of discounted sales S resulting from new
products. Assuming that these complementary cumulative distributions have fat
tails with approximate power law structure S^{-a}, we demonstrate that economy
of scales are automatically realized when the exponent a is less than one.
Empirical evidence suggests that the exponent a is approximately equal to 2/3
for the pharmaceutical industry.
","to the fully asymmetric (W>0) stable Lévy law Lα(W(N)) with exponent α (Gnedenko and
Kolmogorov, 1954). The Lévy law has the same power law structure as (1.1) for W large ut
presents a smoother behavior for small values. For α<1, as for the power law (1.1), its mean and
variances are both mathematically infinite. The characteristic scale of the fluctuations is given by a
scale parameter C defined by the asymptotic expression of Lα(W(N)) for large W:
LαN (W)  =  C(N) W-(1+α)     for W>>1 . (2.12)
There are no analytic expression of stable Lévy law except for a few special values of the exponent
α. The Lévy law for α=1 is the Cauchy (or Lorentz) law. Lévy laws are characterized by their
characteristic function which, for α<1, reads
Lα(k) = e -a |k|α , (2.13)
where a is a constant proportional to the scale parameter C (Gnedenko and Kolmogorov, 1954).
The distribution Lα(W(N)) can be expressed in a way that makes explicit the dependence on N:
LαN (W) dW =  N-1/α Lα1(W/N1/α) dW  . (2.14)
This exact relationship (2.14) means that the distribution of N variables is identical to the Lévy
distribution of one variable when W(N) is re-scaled by N1/α  and when the distribution LαN(W(N) is
multiplied by N1/α. The expression (2.14) expresses the mathematical property of stability under
convolution, that actually defines the Lévy laws.
We can thus write the distribution of yearly sales of the three portfolios A, B and C as follows:
PA(WA) dWA =  NA-1/α  Lα1(WA/NA1/α) dWA  , (2.15)
PB(WB) dWB  =  NB-1/α   Lα1(WB/NB1/α) dWB  , (2.16)",2000-01-31T03:48:42Z,ge lmogorov t for t tre t catc lorenzo ge lmogorov t  t 
paper_qf_32.pdf,8,Economy of scales in R&D with block-busters,"  Are large scale research programs that include many projects more productive
than smaller ones with fewer projects? This problem of economy of scale is
particularly relevant for understanding recent mergers in particular in the
pharmaceutical industry. We present a quantitative theory based on the
characterization of distributions of discounted sales S resulting from new
products. Assuming that these complementary cumulative distributions have fat
tails with approximate power law structure S^{-a}, we demonstrate that economy
of scales are automatically realized when the exponent a is less than one.
Empirical evidence suggests that the exponent a is approximately equal to 2/3
for the pharmaceutical industry.
","PC(WC) dWC =  (NA + NB)-1/α Lα1(WC/ (NA + NB)1/α) dWC  . (2.17)
These expressions formalize rigorously the heuristic derivation leading to (2.8).
To see this, let us consider the following problem, which could be called the bankruptcy problem
or the minimum return problem. Let us assume that a portfolio is sustainable if its income W(N) is
larger than a fixed factor f times its size N. The factor f embodies the marginal costs associated to
each R&D innovation. The total cost per year is thus proportional to the number of innovations. It
can also take into account the cost of all the researches that did not lead to any innovation, if the
number of successful innovations are proportional to the number of attempts. The important point
is that we can quantify the benefits of the merging of the two portfolios A and B by comparing the
aggregate costs to the aggregate earnings.
The probability that the portfolio A remains viable is equal to the probability that the total sales WA
be larger than the threshold f NA:
pA =  ∫fNA+∞
    dWA PA(WA) = ∫ x(NA) +∞
    dx Lα1(x) . (2.18)
The second equality in (2.18) is obtained by posing x=WA/NA1/α and the lower bound is
x(NA) = f / NA(1/α)-1  . (2.19)
The same result holds for portfolio B (resp. C) by replacing NA by NB (resp. NA + NB). Note that
the only dependence of pA in NA appears in the lower bound (2.19) of the integral. For α < 1, the
larger NA is, the smaller is the lower bound (2.19) and the larger is the probability of getting an
earning larger than f NA. We thus obtain our fundamental result:
 if   α < 1,  then pC    >   pA  and   pC   > pB       for any f>0 . (2.20)
The probability to obtain an earning per innovation larger than any threshold increases with the total
size. If an industry is characterized by very fat tail distributions of sales per product, economy of
scale is obtained by growing or merging. From expression (2.7) and inequality (2.8) as well as",2000-01-31T03:48:42Z,tse to  t t it t t t t note for  t  from
paper_qf_32.pdf,9,Economy of scales in R&D with block-busters,"  Are large scale research programs that include many projects more productive
than smaller ones with fewer projects? This problem of economy of scale is
particularly relevant for understanding recent mergers in particular in the
pharmaceutical industry. We present a quantitative theory based on the
characterization of distributions of discounted sales S resulting from new
products. Assuming that these complementary cumulative distributions have fat
tails with approximate power law structure S^{-a}, we demonstrate that economy
of scales are automatically realized when the exponent a is less than one.
Empirical evidence suggests that the exponent a is approximately equal to 2/3
for the pharmaceutical industry.
","from the expressions (2.18) and (2.19), we see that the crucial ingredient is the ratio of the typical
(most probable) sale proportional to N1/α  over the size N of the portfolio. Still another way to
stress the importance of this ratio is to look at the typical total return per product
R = (sales/investment)   ∼   N1/α-1  .  (2.21)
The larger is the portfolio, the larger is the return per innovation.
It is important to stress that these results emphasizing the comparison between the typical (most
probable) largest sale per product proportional to N1/α over the size N of the portfolio holds only for
α ≤ 1. For an exponent α > 1, the integral defining the expectation converges when the upper
bound is put to infinity. The expectation is thus well-defined and the total sale W(N) is simply
proportional to N.
3-Concluding remarks
Economy of scale and increasing return per innovation has been shown to hold under the simple
hypothesis that the distribution of sales per product is a power law with tail exponent α less than
one. Its underlying hypotheses are somewhat restrictive: we have neglected any dependence in the
research and development of new product as a function of size. Our derivation does not account
either for the relevant controlling economic factors, such as level of investment, research
organization, spillover, etc. We believe however that this rigorous result provides a useful
benchmark as it delineates a simple and well-posed example where economy of scale can be
demonstrated unambiguously.
Ackowledgements: I am grateful to W. Comenor, V. Pisarenko and A. Ratcliff for stimulating exchanges.",2000-01-31T03:48:42Z,stl t it for t conudi economy its our  ck owl edge ment come nor visa re rat f
paper_qf_32.pdf,10,Economy of scales in R&D with block-busters,"  Are large scale research programs that include many projects more productive
than smaller ones with fewer projects? This problem of economy of scale is
particularly relevant for understanding recent mergers in particular in the
pharmaceutical industry. We present a quantitative theory based on the
characterization of distributions of discounted sales S resulting from new
products. Assuming that these complementary cumulative distributions have fat
tails with approximate power law structure S^{-a}, we demonstrate that economy
of scales are automatically realized when the exponent a is less than one.
Empirical evidence suggests that the exponent a is approximately equal to 2/3
for the pharmaceutical industry.
","References  :
 Dubrulle, B., F. Graner and D. Sornette, eds. (1997) Scale invariance and beyond (EDP
Sciences and Springer, Berlin).
Feller,  W (1971) An introduction to probability theory and ts applications (John Wiley and sons,
New York, vol. II).
Gnedenko, B.V.  and Kolmogorov, A.N. (1954) Limit distributions for sum of independent
random variables (Addison Wesley, Reading MA).
 Scherer, F.M.  (1965) Firm size, market structure, opportunity and the output of patented
inventions, American Economic Review 55, 1097-1123.
Scherer, F.M. (1998) The size distribution of profits from innovation, Annale d’Economie et de
Statistique 49/50, 495-516.
Sornette, D. and D. Zajdenweber (1999) The economic return of research: the Pareto law and its
implications,  European Physical Journal B, 8 (4), 653-664.",2000-01-31T03:48:42Z,referencdub rul le gra corvee scale scienc berliseller a rey new york ge lmogorov limit additsley readi sc rer rm economic review sc rer t anna le eco no mie status que corvee za ber t party apsical journal
paper_qf_33.pdf,1,Model Risk Analysis via Investment Structuring,"  ""What are the origins of risks?"" and ""How material are they?"" -- these are
the two most fundamental questions of any risk analysis. Quantitative
Structuring -- a technology for building financial products -- provides
economically meaningful answers for both of these questions. It does so by
considering risk as an investment opportunity. The structure of the investment
reveals the precise sources of risk and its expected performance measures
materiality. We demonstrate these capabilities of Quantitative Structuring
using a concrete practical example -- model risk in options on vol-targeted
indices.
","arXiv:1507.07216v2  [q-fin.GN]  28 Jul 2015Model Risk Analysis via Investment Structuring
Andrei N. Soklakov∗
25 July 2015
“What are the origins of risks?” and “How material are they?” – these are the
two most fundamental questions of any risk analysis. Quanti tative Structuring –
a technology for building ﬁnancial products – provides econ omically meaningful
answers for both of these questions. It does so by considerin g risk as an investment
opportunity. The structure of the investment reveals the pr ecise sources of risk and
its expected performancemeasuresmateriality. We demonst rate thesecapabilities of
Quantitative Structuring using a concrete practical examp le – model risk in options
on vol-targeted indices.
1 Introduction
Success ofindividual businesses aswell asentire industries isultimat ely determined bythe
success of their products. Motivated by this observation and by t he clear present need to
improvetheﬁnancialindustry, weembarkedonaprojectofQuant itativeStructuring[1,2].
AlthoughQuantitativeStructuring isﬁrmlyfocusedonproductdes ign, itstechniques turn
outtobeuseful beyond their originalpurpose. Here, forinstanc e, we describe applications
to model risk.
What could product design possibly tell us about model risk? Try to m arket a skew or a
volatilityproduct(investment orhedging)withoutmentioningeither skeworvolatilityand
the deep connection between product design and modeling becomes obvious. Modeling
concepts deeply permeate product design. It should therefore c ome as no surprise that
progress in structuring reﬂects back on modeling.
Model risk has many distinct ﬂavors: model choice, calibration qualit y, implementation –
to name just a few. Although our methodologies are very general, w e decided to introduce
them by working through a concrete practical example. The releva nt general concepts are
introducedalongthewayasneeded. Thespeciﬁc exampleweselecte dforthispresentation
is model choice for options on vol-targeted indices. Following the exa mple, we show how
the same methods are used to handle other ﬂavors of model risk.
∗Head of Equities Model Risk and Analytics, Deutsche Bank.
The views expressed herein should not be considered as investment advice or promotion. They represent
personal research of the author and do not necessarily reﬂect t he view of his employers, or their associates
or aﬃliates. Andrei.Soklakov@(db.com, gmail.com).
1",2015-07-26T16:19:17Z, jul mrisk analysis investment struuri andrew ola v july what how quant struuri it t  quantitative struuri introdusuccess motivated quant struuri although quantitative struuri re what try moli it malthough t t spec followi ad entitimrisk analytics utsc bank t ty andrew ola v
paper_qf_33.pdf,2,Model Risk Analysis via Investment Structuring,"  ""What are the origins of risks?"" and ""How material are they?"" -- these are
the two most fundamental questions of any risk analysis. Quantitative
Structuring -- a technology for building financial products -- provides
economically meaningful answers for both of these questions. It does so by
considering risk as an investment opportunity. The structure of the investment
reveals the precise sources of risk and its expected performance measures
materiality. We demonstrate these capabilities of Quantitative Structuring
using a concrete practical example -- model risk in options on vol-targeted
indices.
","2 Example: Options on vol-targeted indices
2.1 Rationale
Volatility is both a measure and a source of ﬁnancial risks. Naturally, many investors are
attracted to ideas which aim for stable, predicable exposure to vola tility. One such idea
is volatility targeting, also known as volatility control.
LetStbe the value of some index at time t. A vol-targeted version of this index, Xt, is
deﬁned by the initial condition X0=S0and an iterative relationship such as
Xi+1−Xi
Xi=σ
σi·Si+1−Si
Si, (1)
whereσiis the volatility of the original index Siat timeiandσis the volatility target,
i.e. the desired volatility of the new index Xt.
Legal practicalities demand an indisputably clear procedure for com putingσi. The usual
estimator of realized variance over the window of nmost recent business days is a simple
natural choice:
σi=/radicaltp/radicalvertex/radicalvertex/radicalbtA
ni/summationdisplay
k=i−nln2Sk
Sk−1, (2)
whereAis the annualization factor (the total number of business days per year).
As a compromise between the accuracy and the time-lag of the estim ator,nis usually
chosen to be around 40 business days. With such a small data set, it is very tempting
to experiment with eﬃciency. Diﬀerent ad-hoc schemes combining diﬀ erent estimators
have been proposed. This has lead to a number of practical variatio ns of the vol-targeting
scheme. There are of course no clear winners among such variation s, so for the purpose
of our demonstrations we chose to focuse on the core idea of vol-t argeting as captured by
Eqs. (1) and (2) with n= 40.
Imagine now a structuring department promoting a new investment strategy which they
packaged for marketing purposes as an index. The volatility informa tion on such an index
is patchy due to its bespoke nature, so volatility targeting is used. C lients like the idea
but ask for some additional capital protection. In other words, t he clients want options
onXt.
2.2 Model choice
What model shall we use for pricing vanillas on Xt? Practitioners familiar with vol-
targeting would easily recognize the proposal to use a constant vo latility (Black-Scholes)
model with the spot X0=S0and the volatility equal to the target value σ. Let us
imagine that we just encountered this proposal for the ﬁrst time. We need to respond to
the proposal but also form an independent data-driven opinion on t he minimal modeling
requirements for such trades.
For the sake of clarity let us choose Stto be a real index. Let us take Euro Stoxx 50 as
of 28 January 2011 – the same market data as we used for other co mpletely unrelated
2",2015-07-26T16:19:17Z, optns ratnale volatity naturally one  st be xt xi xi xi si si si si at xt legal t sk sk is as with di  tre imagine t ixt mwhat xt praitners  schools   for st to  euro to xx uary
paper_qf_33.pdf,3,Model Risk Analysis via Investment Structuring,"  ""What are the origins of risks?"" and ""How material are they?"" -- these are
the two most fundamental questions of any risk analysis. Quantitative
Structuring -- a technology for building financial products -- provides
economically meaningful answers for both of these questions. It does so by
considering risk as an investment opportunity. The structure of the investment
reveals the precise sources of risk and its expected performance measures
materiality. We demonstrate these capabilities of Quantitative Structuring
using a concrete practical example -- model risk in options on vol-targeted
indices.
","illustrations of Quantitative Structuring. Furthermore, let us set the target σ= 10%.
This is a typical target volatility level which is, as usual, signiﬁcantly low er than the
native volatility of the underlying index.
We consider a broad market of alternative bookings. To this end we im plemented the
volatility targeting scheme (1)-(2) in Monte-Carlo (100000 paths) using the following
diﬀusions for St.
•BS(target)
10%– this is just the simplest benchmark model assuming Stfollows lognormal
process with constant volatility σ= 10%. Should Stindeed follow this process,
volatility-targeting would be redundant.
•BS(ATMF)
23.95%– same as above except the volatility is set to the 6 months ATM Forwa rd
value which happened to be 23.95%.
•Local Vol – the standard local volatility model [3, 4] calibrated to t he actual market
of Euro Stoxx vanillas.
•SLVρ
η/κ– this is a stochastic local volatility model. We defer all relevant descr iptions
of this model to Appendix 4.1. Here we mention that this is a popular ge neraliza-
tion of the local volatility model which introduces an extra stochast ic factor for
the volatility. Parameters ηandκ, known as the vol-of-vol and mean-reversion,
determine the evolution of this extra factor and ρstands for its correlation with the
equity.
Why would we consider SLVρ
η/κ? Looking at Eq. (2) we note that options on Xt
might be viewed as derivatives on volatility of St. Furthermore, looking at Eq. (1)
we see that thevol-spot correlationmay also play a role. SLVρ
η/κisa standard model
which captures such eﬀects (vol-of-vol and forward skew).
We use several instances of this model, starting with SLV−70%
5/10and SLV−70%
2.25/10. The
ﬁrst of these instances is a very stressed example. It can happen when the model
is forced to match a really stressed and illiquid cliquet market (when th e prices
are heavily inﬂuenced by bid-oﬀer spreads and cannot really be expla ined by the
diﬀusions of Appendix 4.1). Most practitioners would say that such p arameter
values are far too extreme and the model is breaking. SLV−70%
2.25/10provides a bit more
realistic scenario. We also consider parameters from [5] to gain furt her insight into
the stability of our conclusions regarding model choice.
2.3 Model risk analysis
Good booking proposals faithfully reﬂect the market view on the tra de and introduce no
further views of their own. When that is not the case, diﬀerent boo king proposals can be
traded either against each other or against the market to create investment opportunities.
Above we introduced a ﬂat-vol proposal for booking options on vo l-targeted indices and
listed a range of possible market alternatives. Imagine now an invest or who believes in
this booking proposal and would like to use it as a sole basis for his inves tment strategy.
What would this investment strategy look like and how would it perform ?
3",2015-07-26T16:19:17Z,quantitative struuri furtr   to   st st follows should st ined for wa local veuro to xx   re meters w looki eq xt st furtr eq  t it  most  mgood wabove imagine what
paper_qf_33.pdf,4,Model Risk Analysis via Investment Structuring,"  ""What are the origins of risks?"" and ""How material are they?"" -- these are
the two most fundamental questions of any risk analysis. Quantitative
Structuring -- a technology for building financial products -- provides
economically meaningful answers for both of these questions. It does so by
considering risk as an investment opportunity. The structure of the investment
reveals the precise sources of risk and its expected performance measures
materiality. We demonstrate these capabilities of Quantitative Structuring
using a concrete practical example -- model risk in options on vol-targeted
indices.
","QuantitativeStructuringprovides uswiththeanswers. Letusarr iveatthemstep-by-step.
The ﬁrst thing we have to do is to identify the underlying variable for t he investment. For
instance, if our investor believes that the booking proposal is espe cially great for vanillas
on the value of the vol-targeted index in 6 months, then the variable in question is X6m.
Now that we have a variable, we can extract the knowledge about th is variable as seen by
the investor and by the market. Mathematically this means extract ing investor-believed
and market-implied probability distributions. This is done as follows.
For a Monte-Carlo booking the investor-believed probability distribu tion forX6mis just
a histogram of the simulated values X6m. Let us imagine that the histogram has N
buckets and let b1,...,b Nbe the realized frequencies which deﬁne the investor-believed
distribution over these buckets.
The market-implied distribution for our chosen variable X6mis, again, just a histogram
m1,...,m Nobtained by grouping thevalues of X6mover thesame set of buckets, only this
time we compute the values by implementing faithfully the vol-targetin g mechanism (1)-
(2) and by using market dynamics.
Giveninvestor-believed andmarket-implieddistributions {bi}N
i=1and{mi}N
i=1wecanshow
(see Appendix 4.2) that the maximum expected rate of return is ach ieved by a payoﬀ
function which is proportional to
fi=bi/mi, (3)
and the expected rate of return from this investment is
ER =N/summationdisplay
i=1bilnfi+RFR−CR, (4)
whereRFRis the risk-free rate and CRis the rate charged by the market makers as
commission (we encounter CRas trading costs).
Theaboveequationstellusthemaximumexpectedperformance(4 )andthecorresponding
investment structure (3). Below we show how this is used to unders tand the materiality
and the speciﬁc origins of model risk.
The risky part of ER, lets call it MRPfor model risk premium1, is given by the relative
entropy
MRP =N/summationdisplay
i=1bilnfi=N/summationdisplay
i=1bilnbi
mi. (5)
This fact, which hasthe same mathematical rootsasKelly’s game-th eoretic interpretation
of entropy [6], is interesting to us because of useful fundamental properties of entropy. In
particular, we have Gibbs’ inequality: MRP≥0withMRP = 0 if and only if the distri-
butions{bi}and{mi}are identical. Gibbs’ inequality guarantees that MRPis capturing
every diﬀerence between the distributions – from Monte-Carlo nois e to conceptual dis-
agreements such as presence or absence of skew. Nothing is going to escape our attention.
1In general, this quantity should really be called Investment Risk Prem ium. In earlier presentations
on model risk we used to call it Model Risk Return (e.g. the original ve rsion arXiv:1304.7533v1 of [7]).
4",2015-07-26T16:19:17Z,quantitative struuri provis  us arr t for  matmatically  for    be t out articial intellence ned giveinvestor  is is ras t above equatns tells t maximum eeed formance below t for  kelly ibbs gibbs is   nothi iinvestment risk prem imrisk ur
paper_qf_33.pdf,5,Model Risk Analysis via Investment Structuring,"  ""What are the origins of risks?"" and ""How material are they?"" -- these are
the two most fundamental questions of any risk analysis. Quantitative
Structuring -- a technology for building financial products -- provides
economically meaningful answers for both of these questions. It does so by
considering risk as an investment opportunity. The structure of the investment
reveals the precise sources of risk and its expected performance measures
materiality. We demonstrate these capabilities of Quantitative Structuring
using a concrete practical example -- model risk in options on vol-targeted
indices.
","The economic implications of Eq. (4) are just as fundamental. We not e that the above
investment would not make any economic sense if ER<RFR. Indeed, the growth-
optimizing investment strategy (3) is very risky. No sane person wo uld choose to go for
such a risky investment if its expected rate of return was below risk -free. We derive that
for the model risk to be economically material we must have at least
MRP>CR. (6)
Note that the above materiality criterion does not depend on the de tails behind the
concept of risk-free rate – it drops out as long as we use it in a consis tent way.
With a moment of thought we also discover that the requirement (6) is very lenient. MRP
is the only positive contribution which might push ERaboveRFR. Consequently, for the
investment to be successful, MRPhas to cover more than the trading costs (CR), it has
to cover all costs of running the business – salaries, rent, etc.
The overall costs of running a business are speciﬁc to each busines s. At the same time
these costs are very intuitive – most business people can judge if r% per annum amounts
to a good investment for them or is simply not worth their eﬀorts (se e Appendix 4.2). For
the purposes of our illustrations, we set a Spartan threshold of 1% per annum. No equity
derivative business would plan to survive on a smaller risky return on t heir operating
capital.MRPbelow that threshold would indicate economically immaterial model risk .
Let us now come back to our example of vanilla options on a vol-target ed index. So far
we translated the proposed booking into the investor-believed dist ribution and used a
set of alternative models to give us the range of market-implied distr ibutions. Our next
step is to compute MRP and judge the materiality of the correspond ing model risk. Bold
numbers in Fig. 1A are the annualized values of MRP. We see that the m odel risk of
using the proposed naive booking is immaterial (0.44% p.a.) only in the ca se when the
underlying Stfollows BS(target)
10%, i.e. when the vol-targeting is completely redundant. Even
whenStfollowsaBlack-Scholesdynamicswithsomereasonablevolatilityvaluew hichdoes
not coincide with the target level we have material model risk (4.92% p.a.). The value
of MRP goes up to 6.06% p.a. for the local volatility and even further t o 8.31% p.a. for
SLV−70%
5/10. This means that the original proposal of using ﬂat volatility model is severely
deﬁcient. Even if we discard the results for the extremely stresse d markets (represented
by SLV−70%
5/10), it is clear that, at the very least, we should use the local volatility m odel.
Without getting entangled in formalities, let us introduce a very prac tical concept of a
“goodmodeling choice”. Intuitively, this is given by the simplest model with the property
that reasonable generalizations of it do not reveal material increa se in model risk.
LookingthroughthetableofFig.1A,weseethat,intermsoftheto talMRP, thediﬀerence
between Local Vol and SLV−70%
2.25/10is not material (6.06% vs 6.15%). This could be an
indication that the local volatility is a good modeling choice. To conﬁrm, we need to
double-check that these modeling approaches are not just equidis tant from the naive ﬂat-
vol booking but are indeed close to each other. Upgrading the book ing proposal to use
5",2015-07-26T16:19:17Z,t eq  ined no  note with above consequently has t at  for spartak no below  so our bold   st follows evest follows  schools dynamics with some reasonable volatity value t  evewithout intuitively looki through t table of  local v to upgradi
paper_qf_33.pdf,6,Model Risk Analysis via Investment Structuring,"  ""What are the origins of risks?"" and ""How material are they?"" -- these are
the two most fundamental questions of any risk analysis. Quantitative
Structuring -- a technology for building financial products -- provides
economically meaningful answers for both of these questions. It does so by
considering risk as an investment opportunity. The structure of the investment
reveals the precise sources of risk and its expected performance measures
materiality. We demonstrate these capabilities of Quantitative Structuring
using a concrete practical example -- model risk in options on vol-targeted
indices.
","1Model choice in options on vol-targeted indices: ﬂat volatility booking proposal
A. Model Risk (% per annum)
Market Booking, b= BS(target)
10%
m b b∗b∗∗
SLV−70%
5/108.31 8.26 3.75
SLV−70%
2.25/106.15 3.70 1.93
Local Vol 6.06 2.56 1.53
BS(ATMF)
23.95%4.92 2.26 0.04
BS(target)
10%0.44 0.46 0.02B.f∗∗=b∗∗/m– SLV markets
X6m
0.7 0 .85 1 1 .15 1 .30.511.5 SLV−70%
5/10
SLV−70%
2.25/10
Local VolC.f∗∗– Black-Scholes markets
X6m
0.7 0 .85 1 1 .15 1 .30.511.5
BS(ATMF)
23.95%
BS(target)
10%
Note: We use real market data for STOXX50E as of 28/Jan/2011. By X6Mwe denote the value of the 10%
vol-targeted version of the index in 6 months. The table on Figure A pres ents quantitative breakdown of
model risk from using a ﬂat-volatility booking. On Figures B and C we lo ok at the residual risks which are
not explained by Figure A. Here we learn how to use payoﬀ functions to de termine otherwise unknown sources
of risk: risk reversal structures on Figure B point to skew risk whi le Figure C displays a classic signature of
Monte-Carlo noise. For detailed explanations of all graphs see the main tex t.
2Model choice stability: Local Vol booking of options on vol-targeted indices
ρ 0 −0.7
κη0 0 .5 1 0 0 .5 1
0.50.02 0.07 0.76 0.02 0.04 0.86
1.00.02 0.05 0.55 0.02 0.05 0.51
2.00.02 0.03 0.31 0.02 0.02 0.21
4.00.02 0.02 0.10 0.02 0.02 0.06Note: Here we consider local volatility bookings of options
onX6m. The values of model risk premium are computed
relative to a more general stochastic local volatility model
SLVρ
η/κand reported as % p.a. Vol-of-vol and mean rever-
sion parameters ( ηandκ) are taken from [5]. Two levels
of spot-vol correlations are examined: zero (as in [5]) and
a more realistic -70%. The levels of model risk found in
this experiment are clearly immaterial (all below 1% p.a.).
local volatility and keeping SLV−70%
2.25/10as the market we compute MRP 0.25% p.a. The
diﬀerence between these two models is clearly immaterial.2
Theability ofstochasticlocalvolatility modelstocapturereal marke ts isatopicinitsown
right which lies outside the scope of this demonstration. Having said t hat, our discussion
of a good modeling choice does need some reasonable set of SLV para meters (in addition
to the single cliquet-inspired instance SLV−70%
2.25/10considered above). We decided to use
Ref. [5] as an independent source of SLV parameters. In [5] the au thors demonstrated the
capabilities of SLV model in relation to volatility derivatives. This is a goo d alternative
to what we considered so far. As a further enhancement, we also e xamine non-zero
correlation ( ρ=−70%) alongside the zero correlation used in [5]. Results are presente d
on Fig. 2. We see that a lot of reasonable deviations from local volatilit y booking do not
lead to material model risk.2In the context of the above experiments, Local Volatility
model appears to be a good modeling choice. Stochastic local volatilit y may be required
2Please note that we do not discuss introduction of caps (explicit or im plicit) on the leverage ratio
σ/σi. Depending on the contract, such caps can lead to material import ance of stochastic volatility
models.
6",2015-07-26T16:19:17Z,mmrisk market booki local vlocal v schools note  jaby  t  os  re     for mlocal vnote re t vtwo t t t abity hi  ref i as results   ilocal volatity stochastic please pendi
paper_qf_33.pdf,7,Model Risk Analysis via Investment Structuring,"  ""What are the origins of risks?"" and ""How material are they?"" -- these are
the two most fundamental questions of any risk analysis. Quantitative
Structuring -- a technology for building financial products -- provides
economically meaningful answers for both of these questions. It does so by
considering risk as an investment opportunity. The structure of the investment
reveals the precise sources of risk and its expected performance measures
materiality. We demonstrate these capabilities of Quantitative Structuring
using a concrete practical example -- model risk in options on vol-targeted
indices.
","only if you believe in the really extreme market dynamics (as captured by SLV−70%
5/10).
Although it would be prudent to keep reserves against extreme mar kets, the usage of
models with such unrealistic dynamics for hedging purposes should be avoided.
As we already mentioned, any risk analysis revolves around two fund amental questions –
regarding the origins and the materiality of risk. So far in our example we investigated
the question of overall materiality. We found that the naive booking proposal comes with
model risk of at least 6% per annum – a material ﬁgure. We now want t o determine
the origins of this risk. In other words, let us investigate what exac tly is wrong with the
proposed booking.
LetMandBbe two sets of numbers. To make it more speciﬁc, let us think of thes e as
the actual Monte-Carlo samples which we used to build the distributio ns{mi}and{bi}
in the above experiments. We found that the quality of the numbers Bis not good and
we would like to make them statistically closer to M. The obvious ﬁrst thing to do is
to correct the mean, i.e. to add a constant correction, Average( M)-Average( B), to every
number in B. Let us call this mean-corrected set B∗. Similarly, we can correct both the
mean and the variance – we would just need two constants: one for the multiplicative
and the other for the additive correction. This standard exercise produces a new set B∗∗
which now has the same mean and variance as M. We can go on matching moments in
this way, but let us stop here and let us build the distributions {b∗
i}and{b∗∗
i}fromB∗
andB∗∗in the exact same way as we built the original {bi}fromB.
Let us now have a closer look at the Local Vol market. Replacing {bi}with{b∗
i}and then
with{b∗∗
i}we see a drop in model risk premium from 6.06% to 2.56% and then to 1.53 %
(per annum) – see Fig. 1A. This means that over a half of model risk c omes from the
failure of the booking proposal to get the correct forward. Half o f the remaining model
risk comes from the erroneous assumption that the target varian ce is achieved perfectly.
To understand the remaining 1.53% we use Eq. (3) and plot f∗∗
i=b∗∗
i/mi(see the green
line on Fig. 1B). We immediately recognize the risk-reversal shape of a skew product [7].
The vol-targeting scheme may reduce the skew but, clearly, does n ot eliminate it. We
identify the failure of the booking proposal to capture skew as a ma jor contribution to
the residual MRP of 1.53%.
It is instructive to ponder a little bit on the insight we just gained rega rding the speciﬁc
example of options on vol-targeted indices. Before doing the analys is, how many of us
would conﬁdently predict that the problems with the forward would d ominate all other
limitations combined? It might appear obvious to us now but, without t he beneﬁt of
hindsight, how many practitioners would even mention forward as a p otential problem?
How many practitioners, when asked about top three limitations, wo uld mention skew as
the least important? Even in this very simple example we gained signiﬁca nt insight.
Asafurther illustrationofEq. (3)asanexplanatorytoolfortheo riginsofrisk, itishelpful
to contrast Fig. 1B, which we just discussed, with the exact same a nalysis performed
for Black-Scholes markets (see Fig. 1C). Such markets have no sk ew and, consequently,
7",2015-07-26T16:19:17Z,although as so   i and be to    is t ge ge  simarly    local vreplaci   half to eq   t  it before it how eveas furtr eq   schools  such
paper_qf_33.pdf,8,Model Risk Analysis via Investment Structuring,"  ""What are the origins of risks?"" and ""How material are they?"" -- these are
the two most fundamental questions of any risk analysis. Quantitative
Structuring -- a technology for building financial products -- provides
economically meaningful answers for both of these questions. It does so by
considering risk as an investment opportunity. The structure of the investment
reveals the precise sources of risk and its expected performance measures
materiality. We demonstrate these capabilities of Quantitative Structuring
using a concrete practical example -- model risk in options on vol-targeted
indices.
","Fig. 1C provides a very diﬀerent picture. We see a near-constant b ond-like payoﬀ for a
wide range of values around ATM with some peaks in the far wings. This is a classic
signature of Monte-Carlo noise. It is explained by good convergenc e across most of the
grid of equally spaced buckets with poor sampling in the furthest buc kets.
3 Summary and Discussion
Investments combine and contrast two kinds of knowledge: marke t-implied and investor-
believed. Without any loss of generality, we can view both kinds of kno wledge as models.
Model risk assessment and investment design merge into a single ﬁeld .
Using the speciﬁc example of options on vol-targeted indices, we sho wed how the methods
of Quantitative Structuring allow us to measure and dissect model r isk. It is important
to emphasize that the techniques we used are completely independe nt of the example.
Equations (3) and (5) together with the generic moment-matching techniques are all we
used to understand the origins and materiality of the risks.
What would be diﬀerent if, instead of model choice, we decided to inve stigate, say, the
quality of calibrations? Take, for instance, the SLV model and its ab ility to ﬁt the vanilla
markets. We would use terminal stock value as the variable, imply {mi}and{bi}from
the market and the calibrated SLV respectively, and perform the a nalysis in the exact
same manner as above. Similarly, we could have given an illustration of im plementation
risk – using two diﬀerent implementations of the same model to compu te{mi}and{bi}.
Diﬀerent ﬂavors of model risk, diﬀerent context (such as ﬁnancia l products) – all of that
would be reﬂected in the constructions of the distributions {mi}and{bi}.
In the following we further clarify the methodology by summarizing it a s an easily-
productionizable algorithm
1.List all relevant variables
This is the opportunity to specify the type of products which are co vered by the
investigation. For example, vanilla equity options depend on the term inal equity
values,ST. Asians and lookbacks depend on/summationtext
t<TStand max t<TStrespectively.
Most auto-callables are derivatives of the pair ( Sτ,τ) whereτis the autocall time.
2.Deﬁne buckets on the variables
This is the opportunity to capture the granularity of the relevant m arkets. Here we
note the number of liquid strikes, barrier levels, etc. which might be t raded at any
time. For example, the above analysis of options on vol-targeted ind ex usedN= 20
equally spaced buckets covering the entire Monte-Carlo range of s imulated X6m. If
instead we looked at vanillas on liquid equity indices, typical values for Nwould be
in hundreds.
3.Imply probability distributions
This can be done either directly from the Monte-Carlo engine as illustr ated above or
indirectly – via pricing of digitals or vanillas. The latter method is importa nt when
using PDE-based pricers especially when working with early-exercise derivatives [7].
8",2015-07-26T16:19:17Z,     it suary discussinvestments without musi quantitative struuri it equatns what take  simarly di it  for  stand st respeively most   re for    would imply    t
paper_qf_33.pdf,9,Model Risk Analysis via Investment Structuring,"  ""What are the origins of risks?"" and ""How material are they?"" -- these are
the two most fundamental questions of any risk analysis. Quantitative
Structuring -- a technology for building financial products -- provides
economically meaningful answers for both of these questions. It does so by
considering risk as an investment opportunity. The structure of the investment
reveals the precise sources of risk and its expected performance measures
materiality. We demonstrate these capabilities of Quantitative Structuring
using a concrete practical example -- model risk in options on vol-targeted
indices.
","4.Make a judgement on materiality
Compute MRP and make a judgement whether, in your business area , this is a
material rate of return.
5.Locate Model Risk
Dissect and analyze model risk using the combination of the moment m atching
technique and the visual display of the payoﬀ (as demonstrated in t his paper).
As a ﬁnal remark, it is worth noting that despite the clear ﬁnancial c ontext of this paper,
the above techniques can be considered alongside general techniq ues of statistics. Indeed,
wecouldhavejustaseasilychosen tolookatriskfactorswhich, inte rmsoftheirmodeling,
are not ﬁnancial: life expectancies, rates of traﬃc accidents, wea ther readings, etc. It
would be very exciting to see ﬁnancial intuition, such as recognizing t he shape of payoﬀ
functions, being used outside of ﬁnance. This could be a chance for our industry to
contribute back to the wider scientiﬁc community.
4 Appendix
4.1 Stochastic local volatility model
The stochastic local volatility model used in this paper, SLVρ
η/κ, is very similar to the one
discussed in [5]. The model is deﬁned by two correlated diﬀusions
dS=µSdt+σ(S,t)Z(t)SdWS, (7)
dlnZ= (−κlnZ−η2/2)dt+ηdWZ, (8)
dWSdWZ=ρdt. (9)
Without the stochastic multiplier Zthis would be just the standard local volatility model.
Additional parameters include vol-of-vol η, mean-reversion κ, and the spot-vol correlation
ρ. Thelocalvolatilityfunction, σ(S,t),isbuiltasdescribedin[5]tomatchvanillamarkets.
4.2 Investment performance
Each and every investment idea is naturally followed by the question o f its performance.
In Quantitative Structuring every investment is built as a solution to an optimization
problem which captures the goals of the client. In such a setting eve ry investment comes
with its own natural performance measure – the objective of the o ptimization.
Imagine, for example, a market of options on some underlying x. Without any practical
loss of generality, we can assume that xtakes discrete values and that the market consists
of binary securities each paying 1 for some value of xand zero otherwise. In real applica-
tions (such as SPX options markets) diﬀerent values of xstand for price intervals for the
underlying (such as S&P 500 index) and the number of diﬀerent value s ofxis determined
by how many such intervals are resolved by the market, i.e. by the nu mber of liquidly
traded strikes.
9",2015-07-26T16:19:17Z,make ute locate mrisk dissent as ined it   stochastic t t dt sd sd without  additnal t local volatity funinvestment eaiquantitative struuri iimagine without in
paper_qf_33.pdf,10,Model Risk Analysis via Investment Structuring,"  ""What are the origins of risks?"" and ""How material are they?"" -- these are
the two most fundamental questions of any risk analysis. Quantitative
Structuring -- a technology for building financial products -- provides
economically meaningful answers for both of these questions. It does so by
considering risk as an investment opportunity. The structure of the investment
reveals the precise sources of risk and its expected performance measures
materiality. We demonstrate these capabilities of Quantitative Structuring
using a concrete practical example -- model risk in options on vol-targeted
indices.
","A growth-optimizing investor into the above market is deﬁned as som eone who seeks a
product with a payoﬀ function gxwhich maximizes the expected rate of return
ER =/summationdisplay
xbxlngxsubject to the budget constraint/summationdisplay
xgxpx= 1.(10)
The meaning of quantities pxandbxis given by the above optimization: bxis the investor-
believed probability that the market variable will ﬁx at the value x, andpxis a market
price for the security which pays 1 if that happens and zero otherw ise. From the point of
view of a growth-optimizing investor, ER is clearly the best and the mo st natural measure
of investment performance.
In the case of more general investors, this is of course not true. It turns out, however,
that the solution to (10) is a very convenient tool for understand ing general investors [2].
Despite the very well known limitations, growth rate remains an intuit ive and frequently
used measure of performance. Indeed, it is probably the only numb er which is guaranteed
to feature in performance reports on all scales: from marketing b rochures of tiny start-ups
to expert projections of the World Economy. This is due to a very sim ple practical fact
– most people who ever had a business, or even just a bank account , can tell whether r%
per annum is a good or a bad return for them. Furthermore, people have strong feelings
on the strategy of growth optimization [2], and this adds further de pth to their individual
judgements including informed demand for further research.
In summary, performance of every investment should ideally be mea sured relative to its
goals. Although theoretically pleasing, this is often diﬃcult in practice . Quantitative
Structuring suggests a growth-optimizing strategy as a useful p ractical benchmark.
Solving the optimization (10), we compute
gx=bx/px. (11)
Intuitively, the prices {px}must have something to do with the probabilities of x. They
are, however, actual market prices and would not necessarily sum up to one. There are
two competing reasons for that. First, there is time value of money which normally has
the eﬀect of reducing (discounting) the numerical values of prices . Second, every business
has to make a living, so market makers charge commission on top of th eoretical values.
In summary, we write/summationdisplay
xpx= DF·CF, (12)
where we introduced the discount and the commission factors DF an d CF to capture the
two competing eﬀects. We introduce the market-implied probability d istribution mxby
normalizing the prices
mxdef=px
DF·CF. (13)
Notethat,giventheprices {px},themarket-implieddistribution mxisalwayswell-deﬁned.
The particular numerical split of the normalization factor into DF and CF reﬂects the
particular funding arrangements secured by the market makers. We deﬁne
RFRdef=−lnDF,CRdef= lnCF, (14)
10",2015-07-26T16:19:17Z,t from iit spite ined world economy  furtr ialthough quantitative struuri solvi intuitively ty tre rst second i note that t  f f
paper_qf_33.pdf,11,Model Risk Analysis via Investment Structuring,"  ""What are the origins of risks?"" and ""How material are they?"" -- these are
the two most fundamental questions of any risk analysis. Quantitative
Structuring -- a technology for building financial products -- provides
economically meaningful answers for both of these questions. It does so by
considering risk as an investment opportunity. The structure of the investment
reveals the precise sources of risk and its expected performance measures
materiality. We demonstrate these capabilities of Quantitative Structuring
using a concrete practical example -- model risk in options on vol-targeted
indices.
","where, to give some names, we chose RFR and CR as abbreviations of “risk-free rate”
and “commission rate” respectively.
Putting everything together we compute the structure of the gr owth-optimal investment
gx∝fxdef=bx/mx (15)
and its performance
ER =/summationdisplay
xbxlnfx+RFR−CR. (16)
The value of ER can be computed for any investment. By construct ion, the growth-
optimalinvestment hasthehighestpossiblevalueofER.Additionalr isk-aversion[8]willof
course reduce ER and the amount of reduction would quantify the p rice of risk-aversion in
terms of expected return. Similarly, we can quantify the price of an y other modiﬁcation to
theinvestment –fromﬁnishing touchesandad-hocadjustmentst ochangesininvestments’
most fundamental assumptions. Model risk analysis considered in t his paper is a modest
example of the latter. For a completely diﬀerent example, the intere sted readers are
referred to [9].
References
[1] Soklakov, A., “Bayesian lessons for payout structuring”, RISK , Sept. (2011), 115-119.
arXiv:1106.2882.
[2] Soklakov, A., “Why quantitative structuring?”, July (2015). ar Xiv:1507.07219.
[3] Dupire, B., “Pricing with a smile”, Risk, January (1994), 18-20.
[4] Derman, E. and Kani, I, “Riding on a smile”, Risk, February (1994) , 32-39.
[5] Ren, Y., Madan, D., Qian, M. Q., “Calibrating and pricing with embedde d local
volatility models”, RISK, September (2007).
[6] Kelly J L Jr, “A New Interpretation of Information Rate”, Bell Sy stem Technical
Journal, 917-26 (1956).
[7] Soklakov, A., “Deriving Derivatives”, April (2013). arXiv:1304.75 33.
[8] Soklakov, A., “Elasticity theory of structuring”, April (2013). arXiv:1304.7535.
[9] Soklakov, A., “Quantitative Structuring vs the Equity Premium Pu zzle”, July (2015).
arXiv:1507.07214.
11",2015-07-26T16:19:17Z,pui t by additnal simarly mfor referencola v bayesiasept  ola v w july  dup ire prici risk uary r maani ridi risk ruary remadmaiacelebrati september kelly jr new interpatinformatrate bell sy technical journal ola v rivi rivativapr  ola v elasticity apr  ola v quantitative struuri equity premium pu july 
paper_qf_34.pdf,1,Why Quantitative Structuring?,"  Quality-designed consumer products are easy to recognize. Wouldn't it be
great if the quality of financial products became just as apparent? This paper
is addressed to financial practitioners. It provides an informal introduction
to Quantitative Structuring -- a technology of manufacturing quality financial
products (information derivatives). The presentation is arranged in three
parts: the main text assumes no prior knowledge of the topic; important
detailed discussions are arranged as a set of appendices; finally, a list of
references provides further details including applications beyond product
design: from model risk to economics and statistics.
","arXiv:1507.07219v4  [q-fin.GN]  7 Sep 2020Why Quantitative Structuring?∗
Andrei N. Soklakov†
Quality-designed consumer products are easy to recognize. Wouldn’t it be
great if the quality of ﬁnancial products became just as appa rent?
This paper is addressed to ﬁnancial practitioners. It provi des an informal
introduction to Quantitative Structuring – a technology of manufacturing
quality ﬁnancial products (information derivatives).
The presentation is arranged in three parts: the main text as sumes no prior
knowledge of the topic; important detailed discussions are arranged as a set
of appendices; ﬁnally, a list of references provides furthe r details including
applications beyond product design: from model risk to econ omics and
statistics.
∗Original version 25 July 2015, this presentation 7 September 2020.
†Head of Strategic Development, Asia-Paciﬁc Equities, Deutsche Ba nk.
The views expressed herein should not be considered as investment advice or promotion. They represent
personal research of the author and do not necessarily reﬂect t he view of his employers, or their associates
or aﬃliates. Andrei.Soklakov@(db.com, gmail.com).
1",2015-07-26T16:34:36Z, sep w quantitative struuri andrew ola v quality would it quantitative struuri t orinal july september ad strategic velopment asia pac entitiutsc ba t ty andrew ola v
paper_qf_34.pdf,2,Why Quantitative Structuring?,"  Quality-designed consumer products are easy to recognize. Wouldn't it be
great if the quality of financial products became just as apparent? This paper
is addressed to financial practitioners. It provides an informal introduction
to Quantitative Structuring -- a technology of manufacturing quality financial
products (information derivatives). The presentation is arranged in three
parts: the main text assumes no prior knowledge of the topic; important
detailed discussions are arranged as a set of appendices; finally, a list of
references provides further details including applications beyond product
design: from model risk to economics and statistics.
","1 Motivation
We believe that products are very important for any business. Pro ducts pay our salaries
and deﬁne our relationships with clients. A business without a produc t is a business in
trouble. Successful products open many doors – anything from b usiness expansion to the
support of charitable causes becomes possible. Products are quit e simply the ultimate
source of our risks and rewards.
We believe that every eﬀort must be made to improve ﬁnancial produ cts. Almost every
other industry has seen dramatic improvements in product design. The ﬁnancial industry
cannot aﬀord staying behind.
We believe that the future of ﬁnance lies inadopting a more scientiﬁc a pproach to product
design. We note that the numerous successful applications of scie nce in other industries
became possible only because science itself reached the necessary level of clarity. To
achieve similar results in ﬁnance, we should be prepared to upgrade t he scientiﬁc standing
of ﬁnance.
We expect many challenges along the way. It will help us to remember t hat there were
tough times in the history of every single branch of science. During s uch times problems
appeartobespecial, evenbeyondtherealmoflogic. Moraldilemmas, regulatorypressures
and the need to persuade the public dominate the agenda. Somehow , these are also
the times when progress is made. Right now is probably the best time e ver to discuss
improvements in ﬁnancial products.
2 Guiding principle
Quality research is hard. To achieve it we have to ﬁght our own preju dice and numerous
subconscious biases. This is what science is really all about. Science is not conﬁned to
any particular subject, it happens whenever there is a genuine and honest attempt to
understand something.
Wewanttounderstandhowtomake“good”ﬁnancialproducts. Cle arly, noteveryproduct
is “good”. There must be properties, fundamental laws if you will, wh ich “good” ﬁnancial
products must satisfy. But how do we look for these fundamental laws? How do we even
knowifwehavefoundone? Thisisanextremely hardquestion. Wenee dsomeinspiration,
and there is no better place for ﬁnding that than in the real succes s stories of science.
Let us pick some well-understood scientiﬁc theories which are as diﬀe rent from each other
as possible. Newtonian Mechanics and Darwinian Evolution are good ex amples. What
do these theories have in common? What makes them useful? Why do w e teach these
theories to our children despite well known factual contradictions ?
Both theories contain a powerful observation, a paradigm which gr eatly simpliﬁes and
facilitates understanding. For example, the famous Newton’s law, F=ma, is just a
deﬁnition of a quantity which Newton decided to call “force”. Mathe matically speaking,
it is a triviality. So, where is the breakthrough, where is the insight? – we might ask. The
insight lies in the fact that thinking in terms of forces greatly simpliﬁes our understanding
of many physical phenomena. So much so, that the greatest achie vements of Newton are
2",2015-07-26T16:34:36Z,motivat pro successful produs  almost t   to  it  moral deas somehow rht guidi quality to  science  want to unrstand how to make e tre but how  is aextremely  nee  neoniameics darwiniaevolutwhat what w both for neoneoma t so t so neon
paper_qf_34.pdf,3,Why Quantitative Structuring?,"  Quality-designed consumer products are easy to recognize. Wouldn't it be
great if the quality of financial products became just as apparent? This paper
is addressed to financial practitioners. It provides an informal introduction
to Quantitative Structuring -- a technology of manufacturing quality financial
products (information derivatives). The presentation is arranged in three
parts: the main text assumes no prior knowledge of the topic; important
detailed discussions are arranged as a set of appendices; finally, a list of
references provides further details including applications beyond product
design: from model risk to economics and statistics.
","now accessible to school children. Similarly, Darwin’s observation of n atural selection
gives us a thinking paradigm, a concept. This concept is not even qua ntitative, but it
makes the living world much easier to understand.
So, this is what we need to ﬁnd – a technical concept which makes ﬁnancial products easier
to understand .
[ If we had to jump ahead and reveal our candidate for this magical concept we would
say that good ﬁnancial products are always based on research, a nd the optimal products
are most easily understood via likelihood functions describing this research (Sec. 4). ]
3 Financial products
What exactly are ﬁnancial products? Browsing through termshee ts we quickly discover
that each and every product is really just a function, F(x), which states how beneﬁts
(normally cashﬂows) depend on the underlying variables, x(which may include time,
market prices, credit ratings, weather readings or actions of oth er people – anything that
is relevant for a given product). In the following we refer to F(x) as a payoﬀ function.
Now we know what our theory has to produce – payoﬀ functions.
The landscape of all ﬁnancial products is huge. We need a good place to start the
exploration. In Appendix 6.1 we consider all possibilities and conclude t hat investment
products provide a very good starting point.
3.1 Investment products
Investment structuring is an old problem. Even Modern Portfolio Th eory is now over 60
yearsold. Nevertheless, thequalityofinvestment productsstillh asroomforimprovement.
Analysis which leads to this conclusion is presented in Appendix 6.2. Her e we focus on
the constructive elements and ask ourselves what minimal feature s we want to see in a
good investment product.
Wedemandthateach investment product hasawell-deﬁned purpos e, accuratelyexpresses
clients’ views and has logical integrity. These three requirements a re not independent.
Let us examine them as we would examine the facets of a crystal whe n looking for the
most promising direction of study.
①Purpose
Each investment product must have a goal. Mathematically, this is fo rmulated as an
optimization problem. Although routinely violated in practice (see App endix 6.2), this
requirement is very well established in ﬁnance and economics. The be st known exam-
ple is probably the Markowitz optimal portfolio which is constructed a s a solution to a
particular mean-variance optimization problem. Expected utility the ory provides a more
general framework for rational investors. Even clients who choo se to depart fromrational-
ity have somemeans of describing their goals– theoptimizationsetup of prospect theory1.
1The relationship between our approach and Behavioral Finance is ou tlined in Appendix 6.4.
3",2015-07-26T16:34:36Z,simarly darwi so  sec nancial what browsi i t  i investment investment evemortfth nevertless analysis  r  mand that eatse  purpose eamatmatically although app t marwitz eeed evet behral nance 
paper_qf_34.pdf,4,Why Quantitative Structuring?,"  Quality-designed consumer products are easy to recognize. Wouldn't it be
great if the quality of financial products became just as apparent? This paper
is addressed to financial practitioners. It provides an informal introduction
to Quantitative Structuring -- a technology of manufacturing quality financial
products (information derivatives). The presentation is arranged in three
parts: the main text assumes no prior knowledge of the topic; important
detailed discussions are arranged as a set of appendices; finally, a list of
references provides further details including applications beyond product
design: from model risk to economics and statistics.
","②Accurately expressed views
Investors may agree with the market, but most often they do not . Investors search for
undervalued or overvalued opportunities and bring this new informa tion to the market. It
is very important for both the investor and wider society that the r esults of their research
are expressed accurately. This means that investment products must be able to reﬂect
subtle diﬀerences in investors’ views. Investors should be able to c ombine views and
there should be no implicit extrapolation of views beyond the researc hed scope. Further
clariﬁcations of these requirements can be found in Appendix 6.2.
③Logical integrity
Information processing is a big part of investment activity. Inform ation processing obeys
laws commonly known as logic. Even before we explore what the logical integrity of an
investment looks like we know that such integrity must be important.
The ﬁrst of the above requirements has been extensively explored within the ﬁeld of
economics. The secondrequirement isobvious inthatit shouldfollowf romanyreasonable
approach to investment. The third requirement is relatively new and it requires major
clariﬁcation. In the next section we explain what we mean by logical int egrity via its
implementation. Because logic is the backbone of any branch of scien ce, we hope that
this relatively new line of enquiry will give us a glimpse of our ultimate drea m – ﬁnance
as a scientiﬁc discipline.
4 Logic, likelihoods and information derivatives
Letxbe some underlying variable (as introduced above in section 3). In th e presence of
uncertainty, our knowledge about xis described by a probability distribution p(x). Upon
learning new data, d, knowledge p(x) should be updated to p(x|d). The logic behind this
update is well known in probability theory as the Bayes’ theorem [1]. T his reads
p(x|d) =Ld(x)p(x), (1)
whereLd(x) is called the likelihood function . As we can see from the above equation
the likelihood function encapsulates everything we need to know in or der to update our
knowledge about xon the account of learning d.
Imagine an investor for whom we can reconstruct the entire logical path. This means that
we know the starting distribution p(x) assumed by the investor and all the subsequent
learning steps of the form (1) which lead to the ﬁnal knowledge
p(x|d1,d2,...,dn) =Ldn(x)·····L d2(x)Ld1(x)p(x). (2)
It is important that the starting distribution, p(x), also known as the prior, is built by
the investor using publicly available (market) information and that th e datad1,d2,...,dn
(which may include assumptions as well as established facts) come in a ddition to this
prior knowledge. In [2] we proposed to call such investors logical.
4",2015-07-26T16:34:36Z,accurately investors investors it  investors furtr  logical informatiform evet t t ibecause logic  be iupot basld ld as imagine  ld ld it in
paper_qf_34.pdf,5,Why Quantitative Structuring?,"  Quality-designed consumer products are easy to recognize. Wouldn't it be
great if the quality of financial products became just as apparent? This paper
is addressed to financial practitioners. It provides an informal introduction
to Quantitative Structuring -- a technology of manufacturing quality financial
products (information derivatives). The presentation is arranged in three
parts: the main text assumes no prior knowledge of the topic; important
detailed discussions are arranged as a set of appendices; finally, a list of
references provides further details including applications beyond product
design: from model risk to economics and statistics.
","The concept of a likelihood function is key indescribing logical investor s. Earlier we noted
that ﬁnancial products are also deﬁned by functions – the payoﬀ f unctions. We see that
within any sensible investment theory the two concepts – the likelihoo d and the payoﬀ
functions – must be connected.
Historically, we have not had much science about payoﬀ functions. T ypically, the struc-
ture of ﬁnancial derivatives has been either postulated or assume d as given – suggested
spontaneously by wise markets. By contrast, the concept of a like lihood function is ex-
tremely well known across all kinds of scientiﬁc applications: probab ility and statistics,
computer science and artiﬁcial intelligence, physics and engineering , biology and medicine
– to name just a few.
It should come as no surprise that thinking in terms of likelihood funct ions greatly sim-
pliﬁes the understanding and structuring of investment products . Mathematically, this
manifests in the form of very simple intuitive equations for payoﬀ fun ctions (see Ap-
pendix 6.3).
Thinking in terms of likelihoods makes our products explicitly dependen t on all the in-
formation included in the relevant learning step. For this reason we p ropose to call the
resulting products information derivatives .2
5 Results so far
5.1 Theory
The main technical result is a pair of very simple equations (see Appen dix 6.3). These
equations allow us to design products which satisfy all of the require ments discussed in
this paper. In particular, going through the key requirements fro m section 3.1, we ﬁnd:
①: Each of our products has a well-deﬁned goal. If required, this goa l can be written
as an optimization problem in the notation of the expected utility theo ry.
For the ﬁrst time, we can design a ﬁnancial derivative and claim it to be “the best
product” not just because it is a good marketing move but because it is true (in a
very well-deﬁned mathematical and economic sense).
②: All of our products accurately express clients’ views. The views o f the client (or
their research advisor) are included in a transparent way as inputs of our equations.
The equations are very simple so they can also be used in reverse: to show the views
expressed by the clients’ current positions; or the views that the client would be
expressing if they went for any particular product [4]. This suppor ts a meaningful
and constructive conversation with the client making sure that the y do end up
expressing the views that they actually have.
③: The logical integrity of new products is ensured by the connection of the payoﬀ
functions with the likelihood functions. This connection is woven in the derivation
oftheequations [2,4]andisthereasonfortheir simplicity andintuit ive convenience.
2This generalizes our earlier deﬁnition proposed in [3].
5",2015-07-26T16:34:36Z,t earlier  historically by it matmatically ap thinki for results tory t ppetse iea for all t t  t  
paper_qf_34.pdf,6,Why Quantitative Structuring?,"  Quality-designed consumer products are easy to recognize. Wouldn't it be
great if the quality of financial products became just as apparent? This paper
is addressed to financial practitioners. It provides an informal introduction
to Quantitative Structuring -- a technology of manufacturing quality financial
products (information derivatives). The presentation is arranged in three
parts: the main text assumes no prior knowledge of the topic; important
detailed discussions are arranged as a set of appendices; finally, a list of
references provides further details including applications beyond product
design: from model risk to economics and statistics.
","5.2 Practical and scientiﬁc considerations
With all their shortcomings, ﬁnancial products of the past have giv en us a tremendous
amount of experience. We respect this knowledge and want to reta in as much of it as
possible.3To this end we require our theory to have the ability to analyze tradit ional
products even though it may expose them as deﬁcient in some way. I n [5] we check this
requirement by looking at some traditional products. We start with vanilla instruments
(spot, vol, skew products) moving into more exotic path-depende nt derivatives. We also
reproduce key design ideas ofbespoke equity indices anddiscuss th e early exercise feature.
As expected, we ﬁnd that traditional products use hidden assump tions which often make
some approximate sense but in general may not be easy to justify.
In the same paper [5] we noticed that product and model design for m related disciplines.
Indeed, modeling deﬁciencies can be exploited as investment opport unities. We develop
this idea further in [6] where we present an economically meaningful a pproach to model
risk assessment.
In Ref [2] we focused our attention on risk aversion. This very impor tant topic arises in
practice every time we try to be conservative or stay on the safer side of some investment
strategy. Using our equations we found that the standard ad-ho c methods of engineering
risk aversion (such as expressing a more conservative view or modif ying payoﬀs) do not
normally achieve their goal. Instead, investments can very easily tu rn into a gamble.
Being conservative turns out to be a delicate task and our equation s provide the tools for
accomplishing this task.
In Ref [7] we touch on the subject of long-term investments. We ch allenge our theory
by considering the equity premium puzzle which has been defying mains tream economics
for the last 30 years. In the long run, riskier assets (such as equit ies) tend to produce
higher returns than the less risky ones (such as bonds). What is pu zzling is the numerical
magnitude of the eﬀect: the standard (consumption-based) eco nomic theory predicts
a much smaller eﬀect (an order of magnitude smaller) than observed in the market.
Resolving the puzzle is considered important because ofits relevanc e tosocially important
long-term investments such as pensions. With just a few lines of mat hematics our theory
predicts a correct ballpark ﬁgure for the equity premium. In the co ntext of the scientiﬁc
method this result is especially important because it tests the validity of our approach
against real data.
Ironically, innovation often struggles to explain the beneﬁts it bring s. This can be a major
practical challenge, even when the target audience are experts. Take, for example, the
light bulb. Today it is used as an icon symbolizing all ideas and innovation. In 1878
a British Parliament Committee described Edison’s light bulb as “good enough for our
Transatlantic friends ... but unworthy of the attention of p ractical or scientiﬁc men” . In
1911, speaking in his role as Professor of Strategy (at Ecole Super ieure de Guerre) the
famous French general, military strategist and later World War I Allied Commander-in-
Chief Ferdinand Foch described airplanes as “interesting toys ... of no military value” .
In 1943, Thomas Watson, chairman of IBM, talking about the comme rcial future of com-
puting estimated: “I think there is a world market for maybe ﬁve computers” .
3Insciencethisapproachisknownasthe correspondence principle . Itensuresaccumulationofscientiﬁc
knowledge. Every new theory is required to retain all of the useful (valid) insights from its predecessors.
6",2015-07-26T16:34:36Z,praical with  to   as iined  iref  usi instead bei iref  iwhat resolvi with iironically  take today ibritish parliament coiee editor transatlantic iprofessor strategy ecole su guerre frenworld war allied coanr chief ferdinand oithomas  iscience  approais kas t it ensuraccumulatof cie nti every
paper_qf_34.pdf,7,Why Quantitative Structuring?,"  Quality-designed consumer products are easy to recognize. Wouldn't it be
great if the quality of financial products became just as apparent? This paper
is addressed to financial practitioners. It provides an informal introduction
to Quantitative Structuring -- a technology of manufacturing quality financial
products (information derivatives). The presentation is arranged in three
parts: the main text assumes no prior knowledge of the topic; important
detailed discussions are arranged as a set of appendices; finally, a list of
references provides further details including applications beyond product
design: from model risk to economics and statistics.
","Prejudice against innovation is clearly a strong and important pheno menon which poses a
major practical problem. Within the scientiﬁc approach there is only one thing we can do
about that. Confronted by misunderstanding or prejudice we sho uld research its origins.
In doing so we might ﬁnd a way of addressing cognitive diﬃculties, impro ve our products
or even discover a very diﬀerent set of needs.
Thinking in terms of likelihood functions, we see that ﬁnancial produc ts are intimately
connected to probabilities. Perhaps some of our diﬃculties mirror th e general lack of
intuition regarding probabilistic and statistical concepts.
Indeed, we show [8] that understanding ﬁnancial performance of informationderivatives is
very helpful for understanding disagreement between probability distributions. In statis-
tics such disagreements are quantiﬁed by highly abstract (axiomat ically motivated) mea-
sures called divergencies. Our ﬁnancial intuition thus helps statistic al understanding.
We further show how understanding statistical disagreements, in turn, creates trading
opportunities [8]. Indeed, imagine a couple of people who disagree on p robabilities for
somefutureevent. Thisisenoughforthemtotrade. Indeed, bec ausetheybelievediﬀerent
probability distributions they would arrive at diﬀerent expectations . As a result, one can
design a zero-sum trade which, in terms of expectations, is likely to lo ok attractive to
both people. In eﬀect, we now know how to structure not just indiv idual products but
entire markets (where investors with diﬀerent views make a market for each other, see
Ref. [8] and Appendix V of Ref. [7]).
Having committed to the scientiﬁc path in understanding ﬁnancial pr oducts we cannot
escape the fact that ﬁnancial planning is a physical process which h appens inside the
human brain. In Section 4.3 of [8] we reconciled our structuring equa tions with the results
of independent neurophysiological experiments. This, of course, is only the beginning. As
experimental studies of the brain intensify, we expect a lot more da ta to become available.
Ourtheorywillevolveaccordingly. Weimagineafutureinwhichreallysu ccessful ﬁnancial
products are built as reﬂections or extensions of popular cognitive strategies implemented
by the brain.
6 Appendix: FAQs and discussions
6.1 Why so much focus on investments?
Let us ask ourselves a more fundamental question: Why do we have customers? People
come to us with diﬀerent stories but all of them point to three funda mental reasons:
investment, raising capital and hedging. On top of these we must re member requirements
from wider society, which can be summarized as providing eﬃcient alloc ation of resources
wherever needed.
Thinking through the above reasons and requirements, we see tha t investment products
are taking a lead role. Indeed, investment and raising capital are re ally two sides of the
same coin: a customer who is trying to raise capital is looking for inves tors and must
therefore present them with an investable opportunity.
7",2015-07-26T16:34:36Z,prejudice withiconfronted ithinki haps ined iour  ined  is enough for tm to tra ined as iref  ref hi ise as our tory wl evolve accordy  imagine future iwhireally su  qs w  w people othinki ined
paper_qf_34.pdf,8,Why Quantitative Structuring?,"  Quality-designed consumer products are easy to recognize. Wouldn't it be
great if the quality of financial products became just as apparent? This paper
is addressed to financial practitioners. It provides an informal introduction
to Quantitative Structuring -- a technology of manufacturing quality financial
products (information derivatives). The presentation is arranged in three
parts: the main text assumes no prior knowledge of the topic; important
detailed discussions are arranged as a set of appendices; finally, a list of
references provides further details including applications beyond product
design: from model risk to economics and statistics.
","Even when we discuss solutions to social issues we talk about investm ents: in schools,
in hospitals, in local communities. The beneﬁts of such investments m ay be diﬃcult to
capture mathematically, but there is no doubt – eﬃcient allocation of resources is an
investment-type problem.
Worried about older people? Well, pensions are examples of long-term investments. In
fact, any kind of serious economic planning needs good quality invest ment behavior, for
it is the prime supplier of accurate market information.
Right now our trust in investment products is at an all time low. Regula tion is pouring
over investment banks like concrete over leaking nuclear reactors . At the same time we
understand that outlawing investment products is not an option: in addition to all of the
above reasons, this will encourage speculation inproducts which ha ve never been designed
as investment vehicles: debt insurance and other hedging instrume nts. Improving the
design of investment products appears to be an urgent priority.
6.2 Legacy investment products – what are the issues?
Most people would agree that our investment banks needs serious im provement. However,
it is always a good exercise to identify issues as precisely as possible. I n this section we
summarize our argument for reform of investment products.
We start by looking at the most basic promise of each and every inves tment product – the
promise to express accurately investors’ views relative to the mar ket. Let us examine the
quality of this promise. The following points summarize three types of problems which
we regard as critical.
View diﬀerentiation
Consider two investors with views similar in direction but very diﬀerent in strength.
Imagine a market of options with a well pronounced skew. One invest or believes that the
skew should be a bit less pronounced than the market-implied while the other thinks that
there should be no skew at all. The diﬀerence in views is signiﬁcant, but because both
investors agree on the direction, currently they would be oﬀered t he same set of products.
View integration
Consider an investor with a view on both the volatility and the skew. Th e investor
believes that the skew should disappear and the volatility will realize 5 v ol points below
the current market expectation. How should the investor allocate their money? Should
they put most of it on the volatility or on the skew? Are we even sure t hat the investor is
better oﬀ with two separate trades (one on the vol and the other on the skew), or is there
a structured product which is better optimized to express the com bined view? Structurers
are currently poorly equipped to tackle these types of questions.
The above two problems are already pretty bad. We cannot diﬀeren tiate or combine
customer views! An equivalent situation in mathematics or science wo uld be like having
a number theory which does not tell us how to add numbers or a phys ical theory which
cannot capture the relative strength of forces. Surely, the situ ation cannot get any worse
than this. The following observation says that it actually does.
8",2015-07-26T16:34:36Z,evet worried ll irht re gul at improvi legacy most   t view consir imagine one t view consir th how should are struure rs t  asurely t
paper_qf_34.pdf,9,Why Quantitative Structuring?,"  Quality-designed consumer products are easy to recognize. Wouldn't it be
great if the quality of financial products became just as apparent? This paper
is addressed to financial practitioners. It provides an informal introduction
to Quantitative Structuring -- a technology of manufacturing quality financial
products (information derivatives). The presentation is arranged in three
parts: the main text assumes no prior knowledge of the topic; important
detailed discussions are arranged as a set of appendices; finally, a list of
references provides further details including applications beyond product
design: from model risk to economics and statistics.
","View extrapolation
Investment products often extrapolate customer views beyond their original research con-
text. Even if the expressed view makes sense in a limited region (e.g. n ear ATM), the
extrapolated view often has no bounds. Indeed, quite a few invest ment products (includ-
ingstandardizedderivatives) oﬀertheoreticallyunlimitedgainsorca nleadtotheoretically
unlimited losses – all mostly in the circumstances which are very diﬃcult to research (no
market information, low-probability scenarios, etc.). Such extrap olation of views can lead
to systemic accumulation of completely unnecessary risks which are often in the tails.
It would beuseful if in additionto the above, we could also give speciﬁc examples pointing
totheﬂawsofreal-lifeproducts. Thepresentations byMerton[9] andDupire[10]aregood
independent examples of such analysis for simple and complex derivat ives respectively.
However, as soon as we start thinking about such examples we quick ly realize that no
ﬁnitenumber ofthemisever suﬃcient –justlikedebunking oneperpe tualmotionmachine
after another does not stop all attempts to invent new ones. How could we show that
investment products contain more than a few rotten apples, that huge classes of them
are fundamentally ﬂawed? There is only one way of doing that – we nee d a method, an
ability to create any number of realistic examples on demand. Here is o ne such method...
The ﬁrst thing we need to acknowledge is that any investment is an op timization. This is
in fact true in a much wider range of situations – pretty much anythin g that we “want”
anything that hasa “goal”can beviewed as anoptimization. The langu ageof maximizing
returns, minimizing risks and constraining losses is the language of op timization which
investors use naturally to describe their goals.
To make it as fair as possible, let us use some examples from readers’ personal experience.
So please sit back, relax, and remember as many investment produc ts as you can. Barrier
options, lookback structures, mountain ranges, cliquets – the wh ole lot. Take as much
time as you like. And as you are recalling them, choose you favorite pr oduct. Make sure
you are really comfortable with your choice. Imagine all of the featu res of this product.
Remember all the clever ideas behind these individual features. Now , could you please
explain: what optimization problem does it solve? I don’t know which pro duct you have
chosen but I am pretty sure that you are now struggling with this qu estion. I hear some
people trying to talk about bears and bulls, but let us remain focused : What optimization
problem does it solve? Animal spirits are not helping. The more you thin k the more you
realize that, strictly speaking, the product was never designed to solve any optimization.
In fact, nobody even bothered to state the problem mathematica lly.
This could not possibly be good – neither for the client nor for wider so ciety. There are
of course exceptions to the above argument, but we all know that the vast majority of
investment products would not stand up to this very simple examinat ion.
The good news is that all of the speciﬁc issues discussed in this sectio n can be resolved
by Quantitative Structuring (see the main text).
6.3 Structuring equations
In Ref. [2] we discovered that for a large class of logical investors, the logical path (2) can
be viewed as a simple two-step learning process: one step incorpora ting market research
9",2015-07-26T16:34:36Z,view investment eveined suit t presentatns mortodup ire how tre re t  t to so barrier take and make imagine remember  what animal t i tre t quantitative struuri struuri iref
paper_qf_34.pdf,10,Why Quantitative Structuring?,"  Quality-designed consumer products are easy to recognize. Wouldn't it be
great if the quality of financial products became just as apparent? This paper
is addressed to financial practitioners. It provides an informal introduction
to Quantitative Structuring -- a technology of manufacturing quality financial
products (information derivatives). The presentation is arranged in three
parts: the main text assumes no prior knowledge of the topic; important
detailed discussions are arranged as a set of appendices; finally, a list of
references provides further details including applications beyond product
design: from model risk to economics and statistics.
","and the other learning investors’ private information (risk aversio n). The two learning
steps gave us two equations which, by way of introduction, we summ arize here as
b=f m (3)
dlnF
dlnf=1
R. (4)
One way of explaining these equations is to note that they are obeye d by a payoﬀ function
F(x) which solves the following optimization problem
max
F/integraldisplay
b(x)U(F(x))dxsubject to budget constraint/integraldisplay
F(x)m(x)dx= 1.(5)
Risk aversion coeﬃcient Ris connected to the utility Uthroughthe standard Arrow-Pratt
formula: R=−FU′′
FF/U′
F. The economic meaning of the market-implied and investor-
believed distributions m(x) andb(x) is explained by the above optimization problem.
The connection between equations (3) and (4) to the optimization ( 5) shows that all
our investment products have a well-deﬁned economic goal which is c onsistent with the
expected utility theory. The ﬁrst requirement of section 3.1 is satis ﬁed.
Thereisofcoursemoretoourequationsthantheirconnectiontoo ptimizationproblem(5).
Coming back to the structure of logical paths (2), we note that eq uivalent conclusions
can be reached via many diﬀerent logical paths. Consequently, the re are many equivalent
forms our equations can take. These forms diﬀer in the intermediat e payoﬀ functions,
such as the function f(x) which is typically found from Eq. (3) and then used to obtain
the ﬁnal payoﬀ function Fby integrating Eq. (4).
The intermediate function f(x) in our equations has the meaning of the optimal payoﬀ
function of the growth-optimizing investor – the investor which aims for the greatest
expected rate of return. For such investor R= 1, Eq. (4) becomes redundant and we are
only left with Eq. (3).
Looking closely at Eq. (3) we see that it has an easily-recognizable Ba yesian structure (1)
with the payoﬀ fplaying the role of the likelihood function [4]. For the growth-optimizing
investor the connection between payoﬀ and likelihood functions tak es its simplest form –
they simply coincide. Thinking in terms of likelihood functions is inmany wa ys equivalent
to thinking in terms of growth-optimizing investors.
First introduced by Bernoulli over 270 years ago, the concept of a growth-optimizing
investor is widely researched and used. Economists use it every time they mention log
utility, the hedge fund community know it under the name of Kelly stra tegy. Promoted
by some and criticized by others, growth-optimizing strategy left v ery few researchers
without a strong opinion on it. This includes legendary fund managers as well as Nobel
prize economists. Paul Samuelson left us with a unique illustration of h ow far people are
prepared to go to specify their own position relative to the growth- optimizing strategy.
Samuelson decided to address his audience with a paper which was pain stakingly edited
to consist of exclusively monosyllabic words [11]. Our morally fragile indu stry ﬁnally had
a leader which was prepared to insult his own audience on an almost poe tic level (not to
mention the epic sacriﬁce of clarity in such a presentation).
With the huge beneﬁt of hindsight and with deepest respect to our p ioneers (including
theirmistakes), wenotethattheheateddebatesofthepastmiss edaveryimportantpoint.
10",2015-07-26T16:34:36Z,t one risk is through t arrow pra t t t tre is of course  to our equatns thatir connetoo comi consequently tse eq by eq t for eq eq looki eq ba for thinki rst bernoulli economists kelly promoted  nobel paul samuelsosamuelsoour with
paper_qf_34.pdf,11,Why Quantitative Structuring?,"  Quality-designed consumer products are easy to recognize. Wouldn't it be
great if the quality of financial products became just as apparent? This paper
is addressed to financial practitioners. It provides an informal introduction
to Quantitative Structuring -- a technology of manufacturing quality financial
products (information derivatives). The presentation is arranged in three
parts: the main text assumes no prior knowledge of the topic; important
detailed discussions are arranged as a set of appendices; finally, a list of
references provides further details including applications beyond product
design: from model risk to economics and statistics.
","The readiness with which people position themselves relative to the gr owth-optimizing
investor and the strength of their opinions makes the growth-opt imizing investor a very
goodbenchmarkofinvestmentbehavior–averyconvenient interm ediatestep. Likelihood-
based thinking exposes this fact, beneﬁts from it and provides str iking conceptual and
technical simplicity (just think of understanding and solving Eqs. (3 ) and (4)).
6.4 Behavioral Finance
One of the frequently asked questions on Quantitative Structurin g is how our approach
(which is rational by construction) can coexist with the ﬁndings of B ehavioral Finance.
This is a very interesting question. In fact this is much more than jus t a question – this
is an opening to a very important debate about how we deal with the im perfections of
human judgement. On the one hand we must be aware and respectf ul of such limitations
(they are real measurable psychological phenomena), and on the other hand we must help
people to overcome their limitations.
The best way of clarifying our current position in this debate is to give an example.
Imagine a questionnaire onan established technical subject. To be speciﬁc, let us consider
a test in geometry. Imagine that a wide representative set of peop le participated in this
test. What results are we going to see and what conclusions should w e make from that?
Pretty clearly, we are going to see people making mistakes. There will be patterns in our
data – some mistakes will be more frequent than others. Some of th e mistakes may be
very robust and even traceable to various behavioral and cognitiv e tendencies (just think
of optical illusions). The knowledge of this can be very important: hu mans constantly
interpret geometrical conﬁgurations (such as judging distances ) in the context of their
behavior (such as driving a car).
So what shall we do about this rich database of mistakes? Shall we ele vate them to
the rank of fundamental laws of nature and replace mathematical fact with “behavioral
geometry”? No. Of course not. What we should do is build tools that h elp people to
overcome their limitations (e.g. chevrons on the motorways that he lp to judge distances,
pocket calculators to assist with mental arithmetic, etc.).
In the above example, we refused to override the axioms of geomet ry on the account of
human error, but at the same time we acknowledged the importance of cognitive eﬀects.
There is no contradiction there. Similarly, we should not contrast Be havioral Finance
with the need to facilitate rational behavior among people. The two g o hand in hand.
Both approaches are based on healthy scepticism about the eﬃcien cy of the markets.
In Quantitative Structuring we no longer trust the markets to com e up (spontaneously)
with sensible ﬁnancial derivatives – we decide to build tools to help us wit h our cognitive
limitations.
11",2015-07-26T16:34:36Z,t likelihood behral nance one quantitative stru ur inance  iot imagine to imagine what prey tre some t so shall no of what itre simarly be nance t both iquantitative struuri
paper_qf_34.pdf,12,Why Quantitative Structuring?,"  Quality-designed consumer products are easy to recognize. Wouldn't it be
great if the quality of financial products became just as apparent? This paper
is addressed to financial practitioners. It provides an informal introduction
to Quantitative Structuring -- a technology of manufacturing quality financial
products (information derivatives). The presentation is arranged in three
parts: the main text assumes no prior knowledge of the topic; important
detailed discussions are arranged as a set of appendices; finally, a list of
references provides further details including applications beyond product
design: from model risk to economics and statistics.
","References
[1] Jaynes, E. T., “Probability Theory, The Logic of Science” (2003) .
[2] Soklakov, A., “Elasticity theory of structuring”, Risk, Decembe r (2016), 81-86.
arXiv:1304.7535.
[3] Soklakov, A., “Information derivatives”, Risk, April (2008), 90 -94.
[4] Soklakov, A., “Bayesian lessons for payout structuring”, Risk, Sept. (2011), 115-119.
arXiv:1106.2882.
[5] Soklakov, A., “Deriving Derivatives”, Risk, July (2016), 78-83. a rXiv:1304.7533.
[6] Soklakov, A., “Model Risk Analysis via Investment Structuring”, July (2015).
arXiv:1507.07216.
[7] Soklakov, A., “Onetradeatatime–unraveling theequitypremiump uzzle”, published
as supplementary materials with “Economics of disagreement – ﬁnan cial intuition for
the R´ enyi divergence”, Entropy 22(8), 860 (2020).
[8] Soklakov, A., “Economics of disagreement – ﬁnancial intuition for the R´ enyi diver-
gence”, Entropy 22(8), 860 (2020).
[9] Merton, R., “ObservationsontheScienceofFinanceinthePract iceofFinance”March
(2009), available online http://videolectures.net/mitworld mertonoots/
[10] Dupire, B., Quant Congress USA (2011). See also a review by Car ver, L., “Quant
Congress USA: BMW options ’a recipe for disaster’ ”, Risk, July (201 1).
[11] Samuelson, P., “Why we should not make mean log of wealth big thou gh years to
act are long”, Journal of Banking and Finance 3(1979), 305-307.
12",2015-07-26T16:34:36Z,reference probabity tory t logic science ola v elasticity risk c em be  ola v informatrisk apr ola v bayesiarisk sept  ola v rivi rivativrisk july  ola v mrisk analysis investment struuri july  ola v one tra at time economics entropy ola v economics entropy mortoobservatns ot science of nance it pr a nance mardup ire quant coress  car quant coress risk july samuelsow journal banki nance
paper_qf_35.pdf,1,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","Introduction to Solving Quant Finance Problems with
Time-Stepped FBSDE and Deep Learning
Bernhard Hientzsch
11/25/2019
Abstract
In this introductory paper, we discuss how quantitative nance problems under
some common risk factor dynamics for some common instruments and approaches
can be formulated as time-continuous or time-discrete forward-backward stochastic
dierential equations (FBSDE) nal-value or control problems, how these nal value
problems can be turned into control problems, how time-continuous problems can be
turned into time-discrete problems, and how the forward and backward stochastic dif-
ferential equations (SDE) can be time-stepped. We obtain both forward and backward
time-stepped time-discrete stochastic control problems (where forward and backward
indicate in which direction the YSDE is time-stepped) that we will solve with opti-
mization approaches using deep neural networks for the controls and stochastic gradi-
ent and other deep learning methods for the actual optimization/learning. We close
with examples for the forward and backward methods for an European option pricing
problem. Several methods and approaches are new.
1 Introduction
This paper is structured as follows: in section 2, we quickly discuss the general risk factor
dynamics used. In section 3, we describe the prototypical instruments and instrument
features treated: Europeans, Barriers, and Bermudans/Exercise opportunities. Section 4
states what we are interested in computing. Section 5 shows how one can obtain (continuous
time) FBSDE formulations and FBSDE nal value problems from a variety of sources and
approaches. Section 6 shows how one can obtain (continuous time) FBSDE stochastic
control formulations. Section 7 shows how one can obtain (discrete time) FBSDE stochastic
control formulations for the introduced nancial instruments. Section 8 describes how these
discrete time FBSDE stochastic control problems can be solved by deep neural networks
and deep learning. Section 9 presents the application of the forward and the backward
methods to European option pricing problem for a one-dimensional example which allows
good visualization and understanding. Section 10 concludes.
Corporate Model Risk, Wells Fargo Bank, bernhard.hientzsch@wellsfargo.com
1arXiv:1911.12231v1  [q-fin.CP]  27 Nov 2019",2019-11-27T15:45:06Z,introdusolvi quant nance problems time stepped ep learni bernard his nez sabstra i  asevl introdu iabarriers bermuda ns exercise seseseseseseasecorate mrisk lls cargo bank  nov
paper_qf_35.pdf,2,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","2 Risk Factor Dynamics
The vector of risk factors Xtunder consideration is assumed to follow the SDE:
dXt=(t;Xt)dt+N(t;Xt)dWt (1)
The operator connected to the risk factor dynamics is:
Ltu(t;x) :=1
2Tr",2019-11-27T15:45:06Z,risk faor dynamics t xt unr xt xt xt  t lt tr
paper_qf_35.pdf,3,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","3.2 Simple Barriers
Barrier options are options that will pay only if some barrier level or region is touched
(knocked-in) or not touched (knock-out), or change nal payout upon touching a barrier.
The simplest barrier options are those with a single barrier at some constant level which
is active for the entire life of the instrument.
Once again, two standard examples: A standard up-and-out barrier call option with
upper barrier Bwill pay the nal call payo unless the underlier Sof the option was
observed at a level SBduring the life of the option and otherwise will pay nothing. A
standard up-and-in barrier call option with upper barrier Bwill pay the nal call payo
only if the underlier Sof the option was observed at a level SBduring the life of the
option and otherwise will pay nothing.
In terms of simulation or simulation-like approaches, one follows the risk factor simula-
tion until maturity or barrier breach (whatever comes rst) and then uses the nal value or
the value of the knocked-in instrument on/in the barrier (or zero, if there is no knock-in).
3.2.1 Special case: Treatment as European
We will explain the case of an upper barrier at a constant level B: Call PBreach( XT;X0)
the probability that the barrier was breached given initial and nal value P(XtBj0<
t < T;X 0;XT)) and call the payout when triggered gB(XT) and when not triggered
gNB(XT). If the nal value of Xis not beyond the barrier, the nal value of the in-
strument is either gB(XT) with probability PBreach( XT;X0) orgNB(XT) with probability
(1",2019-11-27T15:45:06Z,simple barriers barrier t once wl of  wl of  ispecial treatment a call breaxt bj  is breach
paper_qf_35.pdf,4,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","worthgE(XtE)) or continuing to hold on to the instrument with the given nal payment
g(X(TP)).
Exercise opportunities are often handled by either computing an expected holding
value/continuation value hv(XtE) and exercising when the exercise value is larger than
the holding value ( gE(XtE)>hv(XtE)) or by dening an exercise strategy es(XtE) that is
only true/one when the instrument should be exercised and false/zero otherwise. Given
some holding value function or exercise strategy function, an exercise opportunity can be
directly simulated and pricing happens like the barrier case (where dierent actions are
taken depending on whether the barrier was hit or not).
Proceeding from the last exercise opportunity to the rst, the case of nitely many
exercise opportunities can be reduced to the case of a single exercise opportunity. Exercise
time intervals can be approximated by appropriately frequent discrete exercise times.
4 Analytics to be computed
At a minimum, we want to compute the value at initial time with given xed risk factor
values. (This corresponds to the dynamics being started at X0=x0withx0being those
xed risk factor values.) In many situations, we would like to compute the value at initial
time with risk factor values within a certain range around some given xed values (for
sensitivities and other purposes). This can be achieved by modeling X0as a random
variable with the appropriate domain, for instance.
The methods that we will present will compute simulated instrument values along
simulated paths. Forward methods will give us simulated instrument values conditional
on the shared past. Backward methods will give simulated instrument values that take
future values of the risk factors and of the instrument value into account. To convert to
instrument values conditional on the shared past, an adapted projection or approximation
needs to be computed from the simulation results of the backward methods.
In general, it would be useful to determine the instrument value at certain intermediate
times over a certain range of risk factors. This can be achieved for instance by starting the
computation at a future time t0with random Xt0with the appropriate domain of interest,
but potentially also with other approaches. For instance, this can be used to compute
holding values for exercise opportunities.
5 Obtaining time-continuous FBSDE nal value problems
for Quantitative Finance problems
A time continuous FBSDE problem has the following form:
The forward SDE (FSDE) for the dynamics:
dXt=(t;Xt)dt+N(t;Xt)dWt (4)
4",2019-11-27T15:45:06Z,xt exercise xt xt xt xt giveproceedi exercise analytics at  i t forward backward to i xt for out articial intellence ni quantitative nance t xt xt xt 
paper_qf_35.pdf,6,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","5.3 Expectations under Numeraire Measures
Assume the solution is characterized by
u(t;x)
N(t)=Eg(XT)
N(T)Xt=x
(13)
with some deterministic or stochastic dynamics for the numeraire N(t), whereEis in the
measure corresponding to the numeraire N(t). Under that measure, the relative value of
the instrument as measured in units of numeraire is a martingale. Therefore, the generator
functionfwill be zero.
The solution u(t;X) will be given as u(t;X) =N(t)YtwhereXt=XandYsolves a
FBSDE with a zero generator function.
5.4 Nonlinear PDE
One of the nonlinear Feynman-Kac theorems states (El Karoui, Peng, and Quenez [EKPQ97]
and Perkowski [Per10]) that under appropriate assumptions, the solution of
ut(t;x) +Ltu(t;x) +f(t;x;u (t;x);ru(t;x)) = 0 (14)
is given as the YsolutionYtof a BSDE
",2019-11-27T15:45:06Z,eeatns num  ire measur xt is unr trefore t wre xt and solvnonlinear one leymaka el taro ui pe que nez  ow   lt solutof
paper_qf_35.pdf,7,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","ofrl(t)Xt(this setting is called \\\\dierential rates""), then the fgenerator function in the
BSDE is as follows:
f(t;X(T);Y(t);t) =",2019-11-27T15:45:06Z,xt
paper_qf_35.pdf,8,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","6.1 From Final Value problems - Forward Approach
If theYtBSDE is treated in a forward manner, the initial value of Y,Y0is part of the
control (ifX0is a xed value), while if X0is random, Y0=Yinit(X0)) is a function to be
determined as part of the stochastic control problem. Transaction costs and similar could
be treated as part of the BSDE or as part of the running cost. The nal cost will be some
(risk) measure on how well the nal value is replicated, for instance the L2distance
fc(XT;YT) =jjYT",2019-11-27T15:45:06Z,from nal value forward approa yiit transat
paper_qf_35.pdf,9,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","7 Obtaining time-discrete FBSDE stochastic control prob-
lems
7.1 Time-discretizing time-continuous FBSDE
Applying a simple Euler-Maruyama discretization for both XtandYt, we obtain
Xti+1=Xti+(ti;Xti)ti+T(ti;Xti)Wi(25)
Yti+1=Yti",2019-11-27T15:45:06Z,out articial intellence ni time applyi ruler   xt and ti ti ti ti wi ti ti
paper_qf_35.pdf,10,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","where the drift term in the risk neutral case is
driftterm (ti;ti;Yti;ti) := (1 +r(t)ti)Yti (33)
and in the dierential rate case is (recall 0(t) =Yt",2019-11-27T15:45:06Z,ti ti
paper_qf_35.pdf,11,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","7.3.1 Europeans
Here, in the functional, there is typically no running cost, and the only term is the L2
distance or similar measure on the replication of the nal values.
After time-stepping the Ytiforward, the functional given by the nal cost term is
evaluated and gives the objective/loss function.
Even if the functional would include running costs, one would carry along the running
cost together with the XtiandYtias, sayJti.
7.3.2 Barriers
We will discuss only the case of knock-out barriers with immediate rebate on hitting the
barrier. Other barrier option types can be treated similarly.
The evolution of the Xtiis monitored and a barrier indicator Barriertiis introduced
that turns from 0 to 1 when the evolution of the Xtibreaches the barrier and otherwise
remains constant (in particular it stays at 1 even if Xshould no longer breach the barrier
at a later time). We introduce additional state variables tB
tj,XB
tj, andYB
tjthat record the
timeti, theXti, and theYtiat barrier breach or maturity (whatever comes rst) at a time
tjat or after the breach/maturity. Depending on the value of tB
tN(maturity or not), the
nal cost is the L2norm of the dierence (or other risk measure) between YB
tNand the
appropriate payo g(XtN) orgb(tB
tN;XB
tN).
This is a new method. We are currently implementing this method with promising
results and expect to publish details of the method and results soon.
7.3.3 Exercise Opportunities
First, one determines the exercise strategy or the holding value, for instance by starting
forward or backward method with random initial risk factor values at the potential exercise
time.
Given hv(XtE), at timetE, we will check gE(XtE)>hv(XtE).3If that is true, tEwill
be marked as exercise time tE,XtEas the risk factor values XEat exercise, and YtEas the
valueYEat exercise, otherwise, tEwill be maturity and XEandYEare the corresponding
values at maturity. The nal cost will be determined as
fc(XE;YE) =jjYE",2019-11-27T15:45:06Z,are after ti forward eveti and ti as ti barriers  otr t ti is barrier ti is ti breacs should  ti ti at pendi and xt   exercise optunitirst givext xt xt  wl xt as eat as eat wl and are t
paper_qf_35.pdf,12,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","In the context of FBSDE, this is a new method. We are currently implementing this
method with promising results and expect to publish details of the method and results
soon.
7.4 Backward time-stepping methods
In these methods, the risk factors Xtiare time-stepped forwards and the portfolio value
Ytiis time-stepped backwards.
7.4.1 Europeans
Here, in the functional, there is typically no running cost, and the only term is the variance
of the initial values of Y,Y0(for xedX0) or theL2distance of the Y0from a to-be-
determined function Yinit(X0), which can be evaluated once Yhas been time-stepped back
to the initial time.
Even if the functional would include running costs, one would carry along the running
cost together with the XtiandYtias, sayJti, and then add the initial term as described
above to compute the total cost/total value of the functional.
7.4.2 Barriers
We will disccuss only the case of knock-out barriers with immediate rebate on hitting the
barrier. Other barrier option types can be treated similarly.
The evolution of the Xtiis monitored and a barrier indicator InBarriertiis introduced
that will be equal to 1 only at the times when the Xtiare in the barrier region and the
barrier is active and be equal to 0 at all other times.
AfterYhas been time-stepped backward from ti+1totias in the European case, Y
will be overwritten with the value of the knocked-in rebate gB(ti;Xti) ifInBarriertiis 1 and
will be unchanged otherwise. In this way, the correct value of Yin the barrier is always
enforced.
The initial cost does not change. If there is running cost, the running cost term is reset
to zero (or the running cost corresponding to the knocked-in instrument).
This is a new method. We are currently implementing this method with promising
results and expect to publish details of the method and results soon.
7.4.3 Exercise Opportunities
AfterYhas been time-stepped backwards until tE, the holding value based exercise strategy
gE(XtE)>hv(XtE) or the directly given exercise strategy es(XtE) will be checked. If the
used strategy indicates exercise, Ywill be overwritten by the exercise value gE(XtE).
This is a new method. We are currently implementing this method with promising
results and expect to publish details of the method and results soon.
12",2019-11-27T15:45:06Z,i backward iti are ti is are yiit has eveti and ti as ti barriers  otr t ti is ibarrier ti is ti are after has ati ibarrier ti is iyit    exercise optunitiafter has xt xt xt  wl xt  
paper_qf_35.pdf,13,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","Alternatively, without determining an exercise strategy or holding value, Ycould be
overwritten with the exercise value gE(XtE) if the exercise value is larger than the backward
time-stepped portfolio value. This exercise strategy is not adapted and cannot be applied in
general unless the future is known (and will lead to noisy results with clairvoyance/foresight
bias). This approach has been used by Wang et al [WCS+18] for Bermudan swaptions in
the LMM model.
One can also determine some exercise strategy based on a mini-batch or other approx-
imation of the dynamics, within the optimization. This has been proposed and applied in
Liang, Xu, and Li [LXL19] for Bermudan and callable products.
8 Solving time-discrete FBSDE stochastic control problems
by Deep Neural Networks and Deep Learning
Given the time-discrete appropriately time-stepped FBSDE and the appropriate stochastic
control functional (written here to cover both forward and backward methods):
J(t;initial;nal) =E N",2019-11-27T15:45:06Z,alternatively could xt   wa bermuda one  lia xu li bermuda solvi ep neural networks ep learni given
paper_qf_35.pdf,14,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","x1w1
x2w2iActivation
function
youtOutput
x3w3
WeightsBias
b
Inputs
Figure 1: Example neuron in a (D)NN
8.1 Deep Neural Networks
In the literature, there are many dierent architectures given. In the context of deep BSDE
methods, Chan-Wai-Nam, Mikael, and Warin [CWNMW19] presents several choices.
One of the most straightforward settings is a fully connected feedforward deep network.
Assume we want to approximate a function from Rd1toRd2and assume we have inter-
mediate layer sizes m0=d1,m1,m2,. . . ,mL=d2. One way to describe such is to dene
them as a composition of ane transformations Al(x) and component-wise applications of
activation functions
N(x; ) =LALL",2019-11-27T15:45:06Z,aivatoutput hts bias inputs   ep neural networks iichaarticial intellence nam michael war ione  rd rd one al
paper_qf_35.pdf,15,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","xi
1
xi
2
xi
3
xi
4i
1
i
2
i
3
i
4Hidden
layer 1Hidden
layer 2Input
layerOutput
layer
Figure 2: Example DNN for the portfolio function i(Xi) for an four-dimensional case.
15",2019-11-27T15:45:06Z,hidhidinput output   xi
paper_qf_35.pdf,16,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","just subtract o the center and divide by the (half-)width (""prescaling""). If the range is not
known or previous layers or changes in the input stream are driving inputs to undesirable
ranges, one can apply batch-normalization. That means that one uses the information
from a mini-batch to normalize the input to a layer (or output) and learn the appropriate
parameters. The quantities computed from mini-batches in training are replaced by moving
averages or population averages during inference. Batch-normalization was introduced by
Ioe and Szegedy [IS15] and is described in the following:
The batch normalization transform/layer, as used in the training of the networks, has
two parameters andthat are learned during the training. ( is a non-trainable parameter
that prevents division by zero or very small numbers.) On a mini-batch of length B, it
consist of the following operations:
B=1
BBX
b=1xb (43)
2
B=1
BBX
b=1(xb",2019-11-27T15:45:06Z, that t bat size ged t on
paper_qf_35.pdf,17,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","will denote the reparametrization of the functional J(t;initial;nal;:::) with the param-
eter collection  as J(;:::).
Pure stochastic gradient descent (SGD) would be
 ",2019-11-27T15:45:06Z,pure
paper_qf_35.pdf,18,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","New optimization methods and variants and/or combinations of older optimization
methods are steadily coming out and are being published at a steady rate, such as recti-
ed Adam, Lookahead, and Ranger, which we will not discuss here. However, any such
promising algorithms are typically implemented in TensorFlow, Keras, or PyTorch and can
therefore be relatively easily applied.
9 Examples
We will present some simple example from our current work. We will pick an one-
dimensional Black-Scholes setting since for this setting we have analytical solutions and the
results can be easily visualized. (Multi-dimensional extensions have been obtained for ge-
ometric basket options and other situations but visualization would be more of a challenge
in such cases.)
We consider the one-dimensional Black-Scholes model with constant drift and short
rate of 0.06 and constant volatility of 20% (0.2). We either start the underlier at a spot of
120 or vary it uniformly between 70 and 170. We consider a combination of a long call at
120 and two short calls at 150 (which leads to delta of varying sign so that the dierential
rates model would lead to non-linear pricing). Both calls have maturity of half a year, 0.5.
We discretize time with 50 time steps.
We use mini-batches of size 512. Our networks have 4 layers of sizes 1,11,11, and 1. We
prescale with given center and width in underlier. We used ELU as an activation function
for all layers except the output layer, for which we used identity.
Learning rate is 1e-3 and we use Adam with Tensor Flow standard parameters. We
will compute loss functions for validation for randomly chosen mini-batches of the same
size. The loss function for validation is therefore computed as an MC sample (and we can
try to get an idea for the distribution by repeatedly computing it given dierent mini-
batches/samples). We run 20000 mini-batches.
We train separate networks imostly, but present one example where the inetworks
share parameters (and have an additional tinput).
9.1 Forward methods
First, some results for the variant with xed initial risk factors. Figure 3 shows that the
loss functional decays quite quickly as a function of number of mini-batches run and how
price and delta at xed initial risk factor (\\\\spot"") converge up to good accuracy.
Now for the variant with random initial risk factors: Figure 4 shows that the loss
functional decays quite quickly for this case also, that the nal payo is well replicated,
that the analytical solution is well approximated, the delta is quite well approximated as
well (considering that we only hedge at discrete times and not continuously), and that the
Ysurface and the portfolio functions surface looks well-dened and smooth.
18",2019-11-27T15:45:06Z,new adam look aad  tensor flow ker as py tors    schools multi   schools   both   our   learni adam tensor flow  t   forward rst    surface
paper_qf_35.pdf,19,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","Figure 3: Forward method with xed initial risk factor values. Upper: loss func-
tion/functional over mini-batch number. Lower-left: convergence of price. Lower-right:
convergence of delta.
19",2019-11-27T15:45:06Z, forward up lor lor
paper_qf_35.pdf,20,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","Figure 4: Forward method with random initial risk factor values. Upper-left: Loss
function/functional over mini-batch number. Upper-right: Final Payo match at 20000.
Middle-left: comparison of initial Ynetwork and analytical solution. Middle-right: com-
parison of initial portfolio delta and analytical delta. Lower-left: scatter plot of Y. Lower-
right: scatter plot of .
20",2019-11-27T15:45:06Z, forward up loss up nal pay middle network middle lor lor
paper_qf_35.pdf,21,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","Figure 5: Backward method with xed initial risk factor values. Upper: loss func-
tion/functional over mini-batch number. Lower-left: convergence of price. Lower-right:
convergence of delta.
9.2 Backward methods
First, some results for the variant with xed initial risk factors. Figure 5 shows that the loss
functional decays quite quickly as a function of number of mini-batches run and how price
and delta at xed initial risk factor (\\\\spot"") converge up to good accuracy. (In this case,
we actually trained  networks with shared parameters - which means that the network
also hastas an input.) We see that the backward method seems to converge faster than
the forward method in this case.
Now for the variant with random initial risk factors: Figure 6 shows that the loss
functional decays quite quickly for this case also, that the analytical initial solution is well
approximated and that the rolled-back Y0are concentrated around the initial solution and
21",2019-11-27T15:45:06Z, backward up lor lor backward rst  i  
paper_qf_35.pdf,22,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","network, the delta is quite well approximated as well (considering that we only hedge at
discrete times and not continuously), and that the Ysurface and the portfolio functions
surface looks well-dened and smooth.
10 Conclusion
We demonstrated how a wide variety of modeling approaches in quantitative nance for
European, Barrier, and Bermudan option pricing can be solved through deep learning op-
timization approaches for the forward-backward stochastic dierential equation (FBSDE)
formulations where the forward and the backward SDE are time-stepped to simulate path-
wise values and showed examples for European option pricing for both the forward and
backward approaches.
References
[CWNMW19] Quentin Chan-Wai-Nam, Joseph Mikael, and Xavier Warin. Machine learn-
ing for semi linear PDEs. Journal of Scientic Computing , 79(3):1667{1712,
2019. arXiv:1809.07609.
[EHJ17] Weinan E, Jiequn Han, and Arnulf Jentzen. Deep learning-based numerical
methods for high-dimensional parabolic partial dierential equations and
backward stochastic dierential equations. Communications in Mathematics
and Statistics , 5(4):349{380, 2017. arXiv:1706.04702.
[EKPQ97] Nicole El Karoui, Shige Peng, and Marie Claire Quenez. Backward stochas-
tic dierential equations in nance. Mathematical nance , 7(1):1{71, 1997.
Also available on semanticscholar.org.
[GMKS19] Lukas Gonon, Johannes Muhle-Karbe, and Xiaofei Shi. Asset pricing
with general transaction costs: Theory and numerics. arXiv preprint
arXiv:1905.05027 , 2019.
[HJE18] Jiequn Han, Arnulf Jentzen, and Weinan E. Solving high-dimensional par-
tial dierential equations using deep learning. Proceedings of the National
Academy of Sciences , 115(34):8505{8510, 2018.
[IS15] Sergey Ioe and Christian Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. arXiv preprint
arXiv:1502.03167 , 2015.
[KB14] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 , 2014.
22",2019-11-27T15:45:06Z,surface conus abarrier bermuda areferenchaarticial intellence nam joseph michael xier war imachine journal cie nti uti   iajie qu haarulf jetzeep counicatns matmatics statistics  nicole el taro ui shi ge pe   articial intellence re que nez backward matmatical also lucas go nojohannmule kar be   shi asset tory   jie qu haarulf jetze iasolvi proceedis natnal acamy sciencseri christiasize ged bataccelti   died erik ki ma jiy ba adam  
paper_qf_35.pdf,23,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","Figure 6: Backward method with random initial risk factor values. Upper: Loss func-
tion/functional over mini-batch number. Middle-left: comparison of initial Ynetwork,
rolled-back Y0, and analytical solution. Middle-right: comparison of initial portfolio delta
and analytical delta. Lower-left: scatter plot of Y. Lower-right: scatter plot of .
23",2019-11-27T15:45:06Z, backward up loss middle network middle lor lor
paper_qf_35.pdf,24,"Introduction to Solving Quant Finance Problems with Time-Stepped FBSDE
  and Deep Learning","  In this introductory paper, we discuss how quantitative finance problems
under some common risk factor dynamics for some common instruments and
approaches can be formulated as time-continuous or time-discrete
forward-backward stochastic differential equations (FBSDE) final-value or
control problems, how these final value problems can be turned into control
problems, how time-continuous problems can be turned into time-discrete
problems, and how the forward and backward stochastic differential equations
(SDE) can be time-stepped. We obtain both forward and backward time-stepped
time-discrete stochastic control problems (where forward and backward indicate
in which direction the Y SDE is time-stepped) that we will solve with
optimization approaches using deep neural networks for the controls and
stochastic gradient and other deep learning methods for the actual
optimization/learning. We close with examples for the forward and backward
methods for an European option pricing problem. Several methods and approaches
are new.
","[LXL19] Jian Liang, Zhe Xu, and Peter Li. Deep learning-based least
square forward-backward stochastic dierential equation solver for
high-dimensional derivative pricing. arXiv preprint arXiv:1907.10578 ,
2019. Also available at SSRN: https://ssrn.com/abstract=3381794 or
http://dx.doi.org/10.2139/ssrn.3381794.
[Per10] Nicolas Perkowski. Backward Stochastic Dierential Eequations: an Intro-
duction, 2010. Available on semanticscholar.org.
[WCS+18] Haojie Wang, Han Chen, Agus Sudjianto, Richard Liu, and Qi Shen. Deep
learning-based BSDE solver for LIBOR market model with application to
bermudan swaption pricing and hedging. arXiv preprint arXiv:1807.06622 ,
2018. Also available at SSRN: https://ssrn.com/abstract=3214596 or
http://dx.doi.org/10.2139/ssrn.3214596.
[YXS19] Bing Yu, Xiaojing Xing, and Agus Sudjianto. Deep-learning based numer-
ical BSDE method for Barrier options. arXiv preprint arXiv:1904.05921 ,
2019. Also available at SSRN: https://ssrn.com/abstract=3366314 or
http://dx.doi.org/10.2139/ssrn.3366314.
24",2019-11-27T15:45:06Z,jalia t xu peter li ep   also  nicolas  ow  backward stochastic di equatns intro  articial intellence  hao jie wa hacagsud jato richard  qi p   also bi yu  ji ki agsud jato ep barrier   also
paper_qf_36.pdf,1,"A note on the option price and 'Mass at zero in the uncorrelated SABR
  model and implied volatility asymptotics'","  Gulisashvili et al. [Quant. Finance, 2018, 18(10), 1753-1765] provide a
small-time asymptotics for the mass at zero under the uncorrelated
stochastic-alpha-beta-rho (SABR) model by approximating the integrated variance
with a moment-matched lognormal distribution. We improve the accuracy of the
numerical integration by using the Gauss--Hermite quadrature. We further obtain
the option price by integrating the constant elasticity of variance (CEV)
option prices in the same manner without resorting to the small-strike
volatility smile asymptotics of De Marco et al. [SIAM J. Financ. Math., 2017,
8(1), 709-737]. For the uncorrelated SABR model, the new option pricing method
is accurate and arbitrage-free across all strike prices.
","A note on the option price and `Mass at zero in the
uncorrelated SABR model and implied volatility asymptotics'
Jaehyuk Choia,, Lixin Wub
aPeking University HSBC Business School,
University Town, Nanshan, Shenzhen 518055, China
bDepartment of Mathematics, The Hong Kong University of Science and Technology,
Clear Water Bay, Kowloon, Hong Kong, China
Abstract
Gulisashvili et al. [Quant. Finance, 2018, 18(10), 1753{1765] provide a small-time asymptotics for
the mass at zero under the uncorrelated stochastic-alpha-beta-rho (SABR) model by approximating the
integrated variance with a moment-matched lognormal distribution. We improve the accuracy of the
numerical integration by using the Gauss{Hermite quadrature. We further obtain the option price by
integrating the constant elasticity of variance (CEV) option prices in the same manner without resorting
to the small-strike volatility smile asymptotics of De Marco et al. [SIAM J. Financ. Math., 2017, 8(1),
709{737]. For the uncorrelated SABR model, the new option pricing method is accurate and arbitrage-
free across all strike prices.
Keywords: Stochastic volatility, SABR model, CEV model, Gauss{Hermite quadrature
1. Introduction
The stochastic-alpha-beta-rho (SABR) model proposed by Hagan et al. (2002) is one of the most popular
stochastic volatility models adopted in the nancial industry thanks to the availability of an approximate
equivalent Black{Scholes (BS) volatility formula (hereafter, the HKLW formula). Despite its enormous
successes, the model still poses challenges for enhancements. See Antonov et al. (2019) for an extensive
literature review.
The processes for the price and volatility under the SABR model are respectively given by
dXt=YtX
tdWt(X0=x0) anddYt=YtdZt(Y0=y0); (1)
whereis the volatility of volatility, is the elasticity parameter, and WtandZtare the (possibly
Corresponding author Tel: +86-755-2603-0568, Address: Rm 755, Peking University HSBC Business School, University
Town, Nanshan, Shenzhen 518055, China
Email addresses: jaehyuk@phbs.pku.edu.cn (Jaehyuk Choi), malwu@ust.hk (Lixin Wu)
The source code used in this study can be found at https://github.com/PyFE/PyfengForPapers
Preprint submitted to arXiv March 26, 2021arXiv:2011.00557v2  [q-fin.MF]  25 Apr 2021",2020-11-01T16:49:58Z,mass jae  uk choi lix iwu peaki  business scho tow snz partment matmatics t ho   science technology ear water bay wlooho   abstra gul is ash va quant nance  gamrmit   marco anc math for keywords stochastic gamrmit introdut paga schools spite  antot xt  td  and are correspondi tel address rm peaki  business scho tow snz em articial intellence jae  uk choi lix iwu t py py fe for pas pre print  mar apr
paper_qf_36.pdf,2,"A note on the option price and 'Mass at zero in the uncorrelated SABR
  model and implied volatility asymptotics'","  Gulisashvili et al. [Quant. Finance, 2018, 18(10), 1753-1765] provide a
small-time asymptotics for the mass at zero under the uncorrelated
stochastic-alpha-beta-rho (SABR) model by approximating the integrated variance
with a moment-matched lognormal distribution. We improve the accuracy of the
numerical integration by using the Gauss--Hermite quadrature. We further obtain
the option price by integrating the constant elasticity of variance (CEV)
option prices in the same manner without resorting to the small-strike
volatility smile asymptotics of De Marco et al. [SIAM J. Financ. Math., 2017,
8(1), 709-737]. For the uncorrelated SABR model, the new option pricing method
is accurate and arbitrage-free across all strike prices.
","correlated) standard Brownian motions. We will denote the time-to-maturity of the option by Tand the
strike price by K. We also denote = 1",2020-11-01T16:49:58Z,brownish  and 
paper_qf_36.pdf,4,"A note on the option price and 'Mass at zero in the uncorrelated SABR
  model and implied volatility asymptotics'","  Gulisashvili et al. [Quant. Finance, 2018, 18(10), 1753-1765] provide a
small-time asymptotics for the mass at zero under the uncorrelated
stochastic-alpha-beta-rho (SABR) model by approximating the integrated variance
with a moment-matched lognormal distribution. We improve the accuracy of the
numerical integration by using the Gauss--Hermite quadrature. We further obtain
the option price by integrating the constant elasticity of variance (CEV)
option prices in the same manner without resorting to the small-strike
volatility smile asymptotics of De Marco et al. [SIAM J. Financ. Math., 2017,
8(1), 709-737]. For the uncorrelated SABR model, the new option pricing method
is accurate and arbitrage-free across all strike prices.
","Figure 1: The mass at zero, msabr, as a function of time to maturity, T. We test (x0;y0;) = (0:03;0:6;1), and= 0:5
(left) and= 0:3 (right) with n= 10 Gauss{Hermite quadrature points.
0.00 0.05 0.10 0.15 0.20 0.25 0.300.00.10.20.30.40.50.6LN-GHQ
MC0.00 0.05 0.10 0.15 0.20 0.25 0.30
T (time-to-expiry)0.00.10.20.30.40.50.6mSABR=P(XT=0)
LN-GHQ
MC
0.0 0.1 0.2 0.3 0.4 0.50.00.20.40.60.81.0LN-GHQ
MC0.0 0.1 0.2 0.3 0.4 0.5
T (time-to-expiry)0.00.20.40.60.81.0mSABR=P(XT=0)
LN-GHQ
MC
contrasts to Figure 2 of Gulisashvili et al. (2018) where the `LN approx' label shows signicant deviation
from the `Exact' label. Our method also preserves the monotonicity of the mass at zero as Tincreases.
Consequently, we do not see the need for the `small-time' and `hybrid' approaches introduced in Gulisas-
hvili et al. (2018). The convergence of the GHQ is known to be very fast. We use merely n= 10 GHQ
points for Figure 1. The errors from the converged values, evaluated with n= 100, are in the orders of
10",2020-11-01T16:49:58Z, t  gamrmit  gul is ash va exa our ineasconsequently gul is as t   t
paper_qf_36.pdf,5,"A note on the option price and 'Mass at zero in the uncorrelated SABR
  model and implied volatility asymptotics'","  Gulisashvili et al. [Quant. Finance, 2018, 18(10), 1753-1765] provide a
small-time asymptotics for the mass at zero under the uncorrelated
stochastic-alpha-beta-rho (SABR) model by approximating the integrated variance
with a moment-matched lognormal distribution. We improve the accuracy of the
numerical integration by using the Gauss--Hermite quadrature. We further obtain
the option price by integrating the constant elasticity of variance (CEV)
option prices in the same manner without resorting to the small-strike
volatility smile asymptotics of De Marco et al. [SIAM J. Financ. Math., 2017,
8(1), 709-737]. For the uncorrelated SABR model, the new option pricing method
is accurate and arbitrage-free across all strike prices.
","Figure 2: The Black-Scholes volatility smile as a function of log strike price for two parameter sets: ( x0;y0;;;T ) =
(0:5;0:5;0:4;0:5;2) (left) and ( x0;y0;;;T ) = (0:05;0:4;0:6;0:3;1) (right). The mass at zero, estimated by the method of
this study, are msabr= 0:1657 and 0:7624, respectively.
 0.0 0.5 1.06080100120140160180200Implied BS volatility (%)
LN-GHQ
HKLW
DMHJ
Exact2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0
log10(K/x0)6080100120140160180200Implied BS volatility (%)
LN-GHQ
HKLW
DMHJ
Exact
 0.0 0.5 1.0150200250300350400450Implied BS volatility (%)
LN-GHQ
HKLW
DMHJ
Exact2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0
log10(K/x0)150200250300350400450Implied BS volatility (%)
LN-GHQ
HKLW
DMHJ
Exact
Figure 2 demonstrates the advantages of the direct pricing method. We examine two parameter
sets tested by prior studies, von Sydow et al. (2019) and Cai et al. (2017). The exact options prices
are available from the references. We also compare the HKLW formula (labeled as HKLW) as it is the
industry standard.7The lognormal approximation with GHQ (labeled as LN-GHQ) shows an excellent
agreement with the exact implied volatilities near the money. In the second parameter set (right), the
HKLW formula signicantly deviates from the exact value. In the low-strike region, the volatility smile
from our method is also consistent with the DMHJ formula, with estimated mass at zero, msabr= 0:1657
(left) and 0:7624 (right), respectively. These are very close from the values from the Monte-Carlo method,
msabr= 0:1634 (left) and 0 :7758 (right) respectively. Yet, not surprisingly, the DMHJ formula diverges
near the money ( K=x0). Therefore, our new pricing method is superior to the existing method across
all strike range.
Funding
Jaehyuk Choi gratefully acknowledges the nancial support of the 2019 Bridge Trust Asset Management
Research Fund. Lixin Wu was supported by Hong Kong RGC Grant #16306717.
7For other advanced volatility approximation methods, see Choi and Wu (2019).
5",2020-11-01T16:49:58Z, t  schools t implied exa implied exa implied exa implied exa   show articial intellence t  t iitse   yet trefore fundi jae  uk choi bridge trust asset management researfund lix iwu ho  grant for choi wu
paper_qf_36.pdf,6,"A note on the option price and 'Mass at zero in the uncorrelated SABR
  model and implied volatility asymptotics'","  Gulisashvili et al. [Quant. Finance, 2018, 18(10), 1753-1765] provide a
small-time asymptotics for the mass at zero under the uncorrelated
stochastic-alpha-beta-rho (SABR) model by approximating the integrated variance
with a moment-matched lognormal distribution. We improve the accuracy of the
numerical integration by using the Gauss--Hermite quadrature. We further obtain
the option price by integrating the constant elasticity of variance (CEV)
option prices in the same manner without resorting to the small-strike
volatility smile asymptotics of De Marco et al. [SIAM J. Financ. Math., 2017,
8(1), 709-737]. For the uncorrelated SABR model, the new option pricing method
is accurate and arbitrage-free across all strike prices.
","References
Abramowitz, M., Stegun, I.A. (Eds.), 1972. Handbook of Mathematical Functions with Formulas,
Graphs, and Mathematical Tables. New York. URL: https://www.math.hkbu.edu.hk/support/
aands/toc.htm .
Antonov, A., Konikov, M., Spector, M., 2015. The free boundary SABR: Natural extension to negative
rates. Risk September, 1{6.
Antonov, A., Konikov, M., Spector, M., 2019. Modern SABR Analytics. SpringerBriefs in Quantitative
Finance, Cham. doi: 10.1007/978-3-030-10656-0 .
Cai, N., Song, Y., Chen, N., 2017. Exact Simulation of the SABR Model. Operations Research 65,
931{951. doi: 10.1287/opre.2017.1617 .
Choi, J., Liu, C., Seo, B.K., 2019. Hyperbolic normal stochastic volatility model. Journal of Futures
Markets 39, 186{204. doi: 10.1002/fut.21967 .
Choi, J., Wu, L., 2019. The equivalent constant-elasticity-of-variance (CEV) volatility of the stochastic-
alpha-beta-rho (SABR) model. arXiv:1911.13123 [q-n] URL: http://arxiv.org/abs/1911.13123 ,
arXiv:1911.13123 .
De Marco, S., Hillairet, C., Jacquier, A., 2017. Shapes of Implied Volatility with Positive Mass at Zero.
SIAM Journal on Financial Mathematics 8, 709{737. doi: 10.1137/14098065X .
Gulisashvili, A., Horvath, B., Jacquier, A., 2018. Mass at zero in the uncorrelated SABR model and
implied volatility asymptotics. Quantitative Finance 18, 1753{1765. doi: 10.1080/14697688.2018.
1432883 .
Hagan, P.S., Kumar, D., Lesniewski, A.S., Woodward, D.E., 2002. Managing Smile Risk. Wilmott
September, 84{108.
Islah, O., 2009. Solving SABR in Exact Form and Unifying it with LIBOR Market Model. SSRN
Electronic Journal doi: 10.2139/ssrn.1489428 .
Matsumoto, H., Yor, M., 2005. Exponential functionals of Brownian motion, I: Probability laws at xed
time. Probability Surveys 2, 312{347. doi: 10.1214/154957805100000159 .
von Sydow, L., Milovanovi c, S., Larsson, E., In't Hout, K., Wiktorsson, M., Oosterlee, C.W.,
Shcherbakov, V., Wyns, M., Leitao, A., Jain, S., Haentjens, T., Wald en, J., 2019. BENCHOP -
SLV: The BENCHmarking project in Option Pricing - Stochastic and Local Volatility problems. In-
ternational Journal of Computer Mathematics 96, 1910{1923. doi: 10.1080/00207160.2018.1544368 .
6",2020-11-01T16:49:58Z,referenc with ste gueds handbook matmatical funns s graphs matmatical tablnew york antov seor t natural risk september antov seor moranalytics  briefs quantitative nance chaarticial intellence so cexa simulatmoatns researchoi  seo bolic journal futurmarkets choi wu t    marco hl articial intellence et jac er shapimplied volatity positive mass zero journal nancial matmatics gul is ash va horvath jac er mass quantitative nance pagakumar lnie  woodland managi sme risk pot september is lah solvi exa form unyi market meleronic journal matsumoto or eonential brownish probabity probabity surveys show maovi larsoiout wi or somost er  shc rb v wy ns lei tao articial intellence ha ent lens wall t marki optprici stochastic local volatity ijournal uter matmatics
paper_qf_37.pdf,1,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","arXiv:2102.08338v1  [q-fin.CP]  16 Feb 2021Multilayer heat equations: application to ﬁnance
Andrey Itkin1Alexander Lipton2and Dmitry Muravey3
1Tandon School of Engineering, New York University, New York , USA
2The Jerusalem School of Business Administration, The Hebre w University of Jerusalem, Jerusalem, Israel;
Connection Science and Engineering, Massachusetts Instit ute of Technology, Cambridge, MA, USA
3Moscow State University, Moscow, Russia
February 17, 2021
In this paper, we develop a Multilayer (ML) method for solving one-factor parabolic
equations. Our approach provides a powerful alternative to the well-known ﬁnite
diﬀerence and Monte Carlo methods. We discuss various advan tages of this approach,
which judiciously combines semi-analytical and numerical techniques and provides a fast
and accurate way of ﬁnding solutions to the corresponding eq uations. To introduce
the core of the method, we consider multilayer heat equation s, known in physics for a
relatively long time but never used when solving ﬁnancial pr oblems. Thus, we expand
the analytic machinery of quantitative ﬁnance by augmentin g it with the ML method.
We demonstrate how one can solve various problems of mathema tical ﬁnance by using
our approach. Speciﬁcally, we develop eﬃcient algorithms f or pricing barrier options
for time-dependent one-factor short-rate models, such as B lack-Karasinski and Verhulst.
Besides, we show how to solve the well-known Dupire equation quickly and accurately.
Numerical examples conﬁrm that our approach is considerabl y more eﬃcient for solving
the corresponding partial diﬀerential equations than the c onventional ﬁnite diﬀerence
method by being much faster and more accurate than the known a lternatives.
Introduction
The problem of solving partial diﬀerential equations (PDEs ) with moving boundaries appears naturally
in various areas of science and technology. As mentioned in ( Kartashov, 2001), such problems have been
known in physics for a long time. They arise in several ﬁelds, such as (a) nuclear power engineering and
safety of nuclear reactors; (b) combustion in solid-propel lant rocket engines; (c) laser action on solids;
(d) the theory of phase transitions (the Stefan problem and t he Verigin problem); (e) the processes of
sublimation in freezing and melting; (f) in the kinetic theo ry of crystal growth; etc., see (Kartashov,
1999) and references therein. Analytical solutions to thes e problems often require rather sophisticated
methods., which were actively developed by the Russian math ematical school in the 20th century starting
from A.V. Luikov, and then by B.Ya. Lyubov, E.M. Kartashov, a nd many others.
1",2021-02-16T18:22:12Z,  multiplayer andrew it kialexanr tediary  very and oschoeineeri new york  new york t jerusalem schobusiness administratt  bre  jerusalem jerusalem israel connescience eineeri massachusist it technology cambridge mow state  mow  ruary imultiplayer our    to   spec kara i hul st besis dup ire numerical introdut as kart ash ov ty stefave r ikart ash ov analytical lui v ya ly ub ov kart ash ov
paper_qf_37.pdf,2,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
As applied to mathematical ﬁnance, one of these methods - the method of heat potentials (HP) -
was actively utilized by A. Lipton and his co-authors to solv e various mathematical ﬁnance problems,
see (Lipton, 2001; Lipton and de Prado, 2020) and references therein. A complementary method of a
generalized integral transform (GIT) is developed in (Carr and Itkin, 2021; Itkin and Muravey, 2020a;
Carr et al., 2020) to price barrier and American options in th e semi-closed form. These authors studied
the time-dependent Ornstein-Uhlenbeck (OU), Hull-White, CIR, and CEV models. An extension of the
method of heat potentials for the Bessel process called the m ethod of Bessel potentials is developed by
(Carr et al., 2020), who also describe a general scheme of how to construct the potential method for any
linear diﬀerential operator with time-independent coeﬃci ents. Finally, they also extended the method of
generalized integral transform to the Bessel process. In al l cases, a semi-analytical (or semi-closed form)
solution means that ﬁrst, one needs to solve a linear Volterr a equation of the second kind. Then the
option price is represented as a one-dimensional integral.
(Carr and Itkin, 2021; Itkin and Muravey, 2020a; Carr et al., 2020) show that the new method is
computationally more eﬃcient than the existing ones, such a s the backward and forward ﬁnite diﬀerence
methods while providing better accuracy and stability. Als o, the heat potential and GIT methods do
not duplicate but rather complement each other. The former p rovides very accurate results for short
maturities, and the latter for long maturities.
Even though many new problems have been solved in the above-c ited papers, some of the ﬁnancial
models are hard to solve by using these methods directly. For instance, this is the case for the Black-
Karasinski model, popular among practitioners. Another pr oblem is the calibration of the local (or
implied) volatility surface in various one-factor models. Almost all popular analytic and semi-analytical
methods approach the solution of this problem by doing it ter m-by-term, which, doubtless, produces
computational errors. For more details, see (Itkin, 2020) a nd references therein.
In this paper, we attack this class of problems (some of them u nsolved in the semi-analytical form)
by using another method, which we call the method of multilay er (ML) heat equation. An alternative
approach is given in (Dias), where an innovative technique o f recursive images is presented to obtain
solutions to the transient diﬀusion equation in a N-layered material based on the superposition of Green
functions for a semi-inﬁnite material. The solution is init ially built for a single layer over a substrate
by constructing a sequential sum of reﬂected image function s. These functions are chosen to satisfy in
sequence the boundary conditions, ﬁrst at the front interfa ce, then at the back interface, then again at
the front interface, and so on until the added functions’ mag nitude becomes negligible.
Based on this so-called ""1-layer"" algorithm, the author als o constructs a ""2-layer"" algorithm by sequen-
tial application of the ""1-layer"" algorithm ﬁrst to layer 1, then to layer 2, then again to layer 1, and so on.
The sequential application of the N−1 algorithm naturally leads to the N-layer algorithm. This scheme
works for the ﬁrst and second kind boundary conditions but do es not apply to the case where there is a
contact resistance between layers or the convective heat tr ansfer at the end interfaces.
Note that this algorithm as applied to the local volatility c alibration problem is similar to the approach
used in (Lipton and Sepp, 2011; Itkin and Lipton, 2018; Carr a nd Itkin, 2020, 2019).
Since the ML method splits the whole (possibly inﬁnite) doma in in the space variable into a sequence
of sub-domains, one could extend it naturally to solving par abolic equations with coeﬃcients being
functions of time tand location x. At every sub-interval, the corresponding parabolic opera tor could be
either approximated by the operator with the space-homogen eous coeﬃcients or, possibly, reduced to the
heat equation by a series of transformations. After either a pproximation or reduction, the ML method
can be applied.
Moreover, the method could be extended further to deal with n on-linear volatility, drift, and killing
term. Again, piecewise approximations of these terms lead t o the parabolic equations at every sub-
interval that could be transformed to the heat equation. The n, the application of the ML method solves
the problem.
Page 2 of 36",2021-02-16T18:22:12Z,multiplayer as teteterad carr it kiit ki very carr tse or nste ihlebeck hull white avessel vessel carr nally vessel ivolt err tcarr it kiit ki very carr als t evefor  kara i anotr almost for it kiiadias greet tse based t  note tesep it kitecarr it kisince at after oag articial intellence t page
paper_qf_37.pdf,3,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
The main idea of this paper is to combine the ML method with the method of heat potentials1and
the GIT method. Since both provide a semi-analytical soluti on for sub-interval problems, a combination
of these solutions within the ML heat equation method result s in the problem’s full solution, expressed
explicitly via one-dimensional integrals. At each layer, t hese integrals depend either on the yet unknown
potential density (in the HP method) or on the solution gradi ent at the layer’s boundaries (the GIT
method). These unknown functions solve the interconnected systems of the integral Volterra equations
of the second kind derived in the paper. Once this solution is found (either numerically or, sometimes,
analytically), the whole problem is solved. Note that one ca n transform the system of integral equations
to linear equations on a time-space grid, which is lower band ed (in our case, block lower triangular).
Therefore, the corresponding system can be solved with comp lexityO(M2N) whereNis the number of
layers, and Mis the number of time steps, see (Itkin and Muravey, 2020a) in more detail.
We also propose a particular construction of the layers’ int ernal boundaries, which allows the repre-
sentation of every integral in the Volterra equation as conv olution. Applying the Laplace transform, we
obtain a system of linear equations with a block-tridiagona l matrix (it contains four blocks). This system
can be eﬃciently solved numerically (with complexity O(N)). In some cases, it can be solved analytically.
After this system’s solution is found, we use the Gaver-Steh fest method to compute the inverse Laplace
transform, also with linear complexity in the number of laye rsN. This algorithm solves the system of
the Volterra equations and thus the whole problem.
We illustrate these novel ideas by representing several sig niﬁcant ﬁnancial problems in the form
suitable for solving them by the ML method. These problems in clude pricing barrier options in the time-
dependent Black-Karasinski and modiﬁed Black-Karasinski (Verhulst) models, see (Itkin et al., 2020), as
well as the solution of the Dupire equation. We also provide s everal numerical examples to demonstrate
our method’s high speed and accuracy compared with standard ﬁnite-diﬀerence (FD) methods.
To the best of our knowledge, all the paper results are new and contribute to the existing ﬁnancial
and physics literature. It is interesting to note that our me thod is capable of solving similar problems
that appear in medicine and biology in addition to ﬁnance. Fo r instance, our technique is well-suited
for studying (a) the growth of diﬀusive brain tumors, which c onsiders the brain tissue’s heterogeneity,
(Asvestas et al., 2014); (b) the transdermal drug release fr om an iontophoretic system, (Pontrelli et al.,
2016); (c) and many other similar problems. It is imperative to emphasize that our method allows solving
the ML problems with time-dependent boundaries and time- an d space-dependent diﬀusion coeﬃcients.
In contrast, the method of (Carr and March, 2018) and all othe r known approaches operate only with
constant boundaries (possibly with time-dependent bounda ry conditions) and spatially piecewise constant
diﬀusion coeﬃcients. Moreover, their setting corresponds to one of our numerical examples in Section 5.
Since in (Carr and March, 2018) the solution is obtained by us ing spectral (eigenvector) series, while we
apply the Laplace transform method, our approach is about 10 00 times faster.
The rest of the paper is organized as follows. In Section 1 we c onstruct the solution of the ML heat
equation by using the method of heat potentials. In Section 2 , we solve this equation by using the GIT
method. In Section 3.1.1 we describe the pricing of barrier o ptions in the time-dependent Black-Karasinski
(BK) model and also in our modiﬁcation of this model, which wa s introduced in (Itkin et al., 2020) and is
called the Verhulst model. In particular, we demonstrate ho w to reduce the pricing PDEs for both models
to the ML heat equation. Also, in Section 3.1.1 we provide a ge neralization of this approach for some
other models. In Section 3.2 we apply the results of Section 2 .3 to investigate the case of space-dependent
volatilityσ(x) in conjunction with solving the Dupire equation. Section 4 is dedicated to the solution
of the Volterra equations. In particular, we describe a cons truction of the internal boundaries, which
allows a transformation of the Volterra equations of the sec ond kind to Abel equations. We solve the
latter equations via the Laplace transform. Section 5 descr ibes some numerical experiments with the ML
method. The ﬁnal section concludes.
1More general potential methods, e.g., the method of Bessel p otentials, can also be used in such a scheme.
Page 3 of 36",2021-02-16T18:22:12Z,multiplayer t since at tse volterra once note trefore is mis it ki very  volterra applyi place  iafter ge teh place  volterra  tse  kara i  kara i hul st it kidup ire  to it fo as vests pont re lli it icarr marosesince carr marplace t iseiseise kara i it kihul st ialso seisesedup ire sevolterra ivolterra abel  place set  vessel page
paper_qf_37.pdf,4,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
1 Solving the ML heat equation via the HP method
Let us consider the following initial-boundary problem
Lu(τ,x) = 0, (x,τ)∈Ω :/bracketleftig
y−(τ),y+(τ)/bracketrightig
×R+, (1)
u(0,x) =f(x), y−(0)<x<y+(0),
u(τ,y−(τ)) =χ−(τ), u (τ,y+(τ)) =χ+(τ).
Here the operator Lis a partial diﬀerential operator of the parabolic type
L=−∂
∂τ+∂
∂x/parenleftbigg
σ2(τ,x)∂
∂x/parenrightbigg
+µ(τ,x)∂
∂x+ν(τ,x), (2)
σ(τ,x),µ(τ,x),ν(τ,x) are some known functions, Ω is the spatial-temporal domain with curvilinear tem-
poral boundaries, and χ−(τ),χ+(τ) are known functions of time (the boundary conditions).
Similar to (Itkin and Muravey, 2020a), we represent the solu tion in the form
u(x,τ) =q(x,τ) +/integraldisplayy+(0)
y−(0)f(ξ)G(x,ξ,τ )dξ, (3)
where G(x,ξ,τ ) is Green’s function of the problem. Then the function q(x,τ) solves a problem similar to
Eq. (1) but with the homogeneous initial condition
Lq(τ,x) = 0, (x,τ)∈Ω :/bracketleftig
y−(τ),y+(τ)/bracketrightig
×R+, (4)
q(0,x) = 0, y−(0)<x<y+(0),
q(τ,y−(τ)) =χ−(τ)−/integraldisplayy+(0)
y−(0)f(ξ)G(y−(τ),ξ,τ)dξ=φ−(τ),
q(τ,y+(τ)) =χ+(τ)−/integraldisplayy+(0)
y−(0)f(ξ)G(y+(τ),ξ,τ)dξ=φ+(τ).
If the Green function G(x,ξ,τ ) is known, the problem in Eq. (4) can be solved via the HP metho d,
(Itkin and Muravey, 2020b). Otherwise, one can apply the ML m ethod as this is described below.
To use the ML method, suppose the domain Ω could be split into Nlayers: Ω =/uniontextN
i=1Ωi, where each
layer is a curvilinear strip
Ωi= [yi(τ),yi+1(τ)]×R+, y i(τ)<y i+1(τ),∀τ >0,∀i= 1,...,N, (5)
y1(τ) =y−(τ), y N+1(τ) =y+(τ).
Let us seek for the solution of the problem Eq. (4) in the form
u(τ,x) =N/summationdisplay
i=1ui(τ,x)1x−yi(τ)1yi+1(τ)−x, 1x=/braceleftigg
1, x ≥0
0, x< 0,(6)
and request that both u(τ,x) and its ﬂux are continuous functions of x2. Using these conditions at
every boundary yi(τ), i= 2,...,N together with the boundary conditions yields the following system of
equations
ui(τ,yi+1(τ)) =ui+1(τ,yi+1(τ)), (7)
2These conditions are natural in physics if by u(t, x) we assume, e.g., the temperature and interpret σ2∂xu(t, x) as the
heat ﬂux. Therefore, it is standard to require continuity of the heat ﬂux rather than the ﬁrst derivative ∂xu(t, x), (Lienhard
IV and Lienhard V, 2019)
Page 4 of 36",2021-02-16T18:22:12Z,multiplayer solvi  lu re  simar it ki very greeteq  greeeq it ki very otrwise to layers  eq usi tse trefore liehard liehard page
paper_qf_37.pdf,5,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
σ2
i(τ,yi+1(τ))∂ui
∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=yi+1(τ)=σ2
i+1(τ,yi+1(τ))∂ui+1
∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=yi+1(τ), i= 1,...,N −1,
u1(τ,y−(τ)) =χ−(τ), u N+1(τ,y+(τ)) =χ+(τ).
The ﬁrst condition means a continuity of the function uat every boundary yi, i= 1,...,N −1. The
second condition is a continuity of the heat ﬂux at the same bo undary. The last line follows from the
boundary conditions in Eq. (4).
Also, let us deﬁne the operator Lfor the whole domain Ω as follows
L=N/summationdisplay
i=1Li1x−yi(τ)1yi+1(τ)−x, (8)
where
Li=−∂
∂τ+∂
∂x/parenleftbigg
σ2
i(τ,x)∂
∂x/parenrightbigg
+µi(τ,x)∂
∂x+νi(τ,x), (9)
andLiui= 0.
The idea of the ML method is to assume that Green’s function Gi(x,τ|ξ,s) associated with the
operator Lican be obtained in the closed form. For an arbitrary dependen ciesσi(τ,x),µi(τ,x),νi(τ,x)
this is not the case, but for various speciﬁc forms of these fu nctions this can be done. For instance,
whenµ(τ,x) =ν(τ,x) = 0 and σ(τ,x) =σ(τ) orσ(τ,x) =σ(x), etc., (Polyanin, 2002). Otherwise,
the functions σi(τ,x),µi(τ,x),νi(τ,x) can be approximated at every layer, e.g., by piecewise cons tant or
linear function in xand an arbitrary function of τ, or by piecewise constant functions in τand piecewise
linear functions in x, etc. These approximations make the ML method somewhat simi lar to the FD
method, however, with some critical distinctions, see Sect ion 6.
It is important to mention, that the operator Liin Eq. (9), while natural for physics where a divergent
form of the parabolic equation (e.g., the heat equation) is c ommonly accepted, is just rarely used in
mathematical ﬁnance. Instead, in ﬁnance it is natural to con sider a non-divergent (non-conservative)
form, which for the heat equation reads
Li=−∂
∂τ+σ2
i(τ,x)∂2
∂x2. (10)
Obviously, when σi=σi(τ),∀i, i.e.σi(τ,x) is a straight line at given τ, both operators in Eq. (9) and
Eq. (10) coincide. However, if one solves Eq. (10) by the ML me thod, it can be unclear what continuity
condition should be used. As shown in (Lejay, 2006), for the d ivergent heat/diﬀusion equation with
drift this condition remains the same, i.e. this is a continu ity of ﬂux over the boundary. Obviously, a
non-divergent heat equation can be represented in this form , i.e. the divergent diﬀusion part plus drift.
Therefore, the continuity condition is still represented b y the equality of ﬂuxes over the boundary, but
the equation now includes an extra drift term.
As applied to the ML method, this can be seen as follows. Suppo se we apply the ML method to some
parabolic equation, and approximate all coeﬃcients in the d rift and killing terms by piecewise constant
function at every interval. Then, at the i-th interval this equation reads
∂ui(τ,x)
∂τ=σ2(τ,x)∂2ui(τ,x)
∂x2+αi∂ui(τ,x)
∂x+βiui(τ,x), (11)
whereαi=const, β i=const, i = 1,N. By transforming it to a divergent form we obtain
∂ui(τ,x)
∂t=∂
∂x/parenleftbigg
σ2(τ,x)∂ui(τ,x)
∂x/parenrightbigg
+ [αi−2σ(τ,x)σx(τ,x)]∂ui(τ,x)
∂x+βiui(τ,x). (12)
Page 5 of 36",2021-02-16T18:22:12Z,multiplayer t t t eq also for li li  t greegi li cafor for poly aiotrwise tse se it li ieq instead li obvusly eq eq eq as le jay obvusly trefore as supp tby page
paper_qf_37.pdf,6,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
Hence, again, the continuity condition for this equation is given in Eq. (7). Further, Eq. (12) by a
series of transformations can be reduced to a non-divergent heat equation in Eq. (10). Accordingly, these
transformations should be applied to Eq. (7) as well to obtai n the correct continuity conditions.
When the external boundaries are constant, i.e. y−(t) =χ−(t) =const, y+(t) =χ+(t) =const
one may use an alternative where a non-divergent heat equati on can be reduced to a divergent one by a
change of variables x/ma√sto→y=g(x), whereg(x) is some function which depends on σ2(x). In more detail
this is shown in Appendix A. Accordingly, the operator Eq. (1 0) transforms to
Li=−∂
∂τ+∂
∂y/parenleftbigg
Ξ2
i(τ,y)∂
∂y/parenrightbigg
, (13)
where Ξ2(τ,y) is a new diﬀusion coeﬃcient, which can be expressed via σ(τ,x), again see Appendix A.
In what follows, we provide our analysis for Eq. (9); Eq. (10) can be analyzed similarly, as explained
above. For simplicity and without loss of generality, we giv e an exposition of the HP method assuming
µi(τ,x) =νi(τ,x) = 0, and either σi(τ,x) =σi(τ), orσi(τ,x) is a piecewise constant function of xfor
every layer. In this case each equation Liui= 0 by some change of variables τ/ma√sto→¯τ,x/ma√sto→¯xcan be
transformed to the heat equation Eq. (63) with σ2
i(¯τ,¯x) = 13, (Polyanin, 2002), and the corresponding
Green function G(¯x,ξ,¯τ) reads
G(¯x,ξ,¯τ) =1
2√
π¯τe−(¯x−ξ)2
4¯τ. (14)
Also, these transformations modify the boundary y(τ)/ma√sto→¯y(¯τ). Some examples of such transforma-
tions are presented in Section 3. In Appendix B we also provid e some recipes on how to proceed if one
needs to generalize this approach by considering a general c aseσ=σ(τ,x).
To the end of this section, for easiness of reading let us drop the bar over new variables. Now, following
the general idea of the method of heat potentials for pricing double barrier options, (Itkin and Muravey,
2020a; Carr et al., 2020), we represent each function qi(τ,x) as
qi(τi,x) =/integraldisplayτi
0/braceleftigg
Ψi(k)∂G(x,ξ,τ i−k)
∂ξ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
ξ=yi(k)+ Φ i(k)∂G(x,ξ,τ i−k)
∂ξ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
ξ=yi+1(k)/bracerightigg
dk. (15)
In Eq. (15) the second integral is a sum of two single layer pot entials with the potential densities Ψ i(τ) and
Φi(τ). By writing Eq. (15) we take into account that according to E q. (45) and Eq. (72), the transformed
timeτmight diﬀer for each interval, therefore, the notation τiis used. However, e.g., for the problem
described in Section 3.1.1 all new times are equal, i.e. τi=τ, i= 1,...,N + 1.
Since the domain Ω consists of Nlayers, there are 2 Nunknown density functions Ψ i(τi),Φi(τi), i=
1,...,N . To determine them one need to plug the representation of qi(τi,x) in Eq. (15) into Eq. (7), and
then solve thus obtained system of the integral Volterra equ ations of the second kind.
However, it is known, (Tikhonov and Samarskii, 1963) that th e integral in Eq. (15) for x=yi(τi) and
x=yi+1(τi) is discontinuous, but with the ﬁnite value at x=yi(τi) +ε,∀i= 1,...,N + 1 whenε→0.
Then, as shown in (Itkin and Muravey, 2020b), Eq. (15) should be represented in the form
qi(τ,yi(τ)) =1
2σ2
i(yi(τ))Ψi(τ) (16)
+/integraldisplayτ
0/braceleftigg
Ψi(k)∂G(yi(τ),ξ,τ −k)
∂ξ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
ξ=yi(k)+ Φ i(k)∂G(yi(τ),ξ,τ −k)
∂ξ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
ξ=yi+1(k)/bracerightigg
dk, τ =τi,
3Of course, there exist other possible representations of σiwhich give rise to the heat equation, or e.g., to the Bessel
equation, Carr et al. (2020).
Page 6 of 36",2021-02-16T18:22:12Z,multiplayer nce eq furtr eq eq accordy eq wi accordy eq li  ieq eq for i eq poly areealso some sei to  it ki very carr ieq by eq eq sesince layers unkto eq eq volterra thoov sama  eq tit ki very eq of vessel carr page
paper_qf_37.pdf,7,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
qi(τ,yi+1(τ)) = −1
2σ2
i(yi+1(τ))Φi(τ)
+/integraldisplayτ
0/braceleftigg
Ψi(k)∂G(yi+1(τ),ξ,τ −k)
∂ξ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
ξ=yi(k)+ Φ i(k)∂G(yi+1(τ),ξ,τ −k)
∂ξ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
ξ=yi+1(k)/bracerightigg
dk. τ =τi+1.
The gradients of qi(τ,x) for the heat equation with σi=σ=const have been derived ﬁrst in (Lipton
and Kaushansky, 2020a,b), and later in (Itkin and Muravey, 2 020b) by using a diﬀerent method4. The
result read
∂qi(τ,x)
∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=yi(τ)=−Ψi(τ)
2σ3/parenleftbigg1√πτ+y′
i(τ)
σ/parenrightbigg
+/integraldisplayτ
0Ψi(k)e−(yi(τ)−yi(k))2
4σ2(τ−k)−Ψi(τ)
4σ3/radicalbig
π(τ−k)3dk (17)
−/integraldisplayτ
0Ψi(k)(yi(τ)−yi(k))2e−(yi(τ)−yi(k))2
4σ2(τ−k)
8σ5/radicalbig
π(τ−k)5dk−/integraldisplayτ
0Φi(k)∂2G(x,ξ,σ2(τ−k))
∂ξ∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleξ=yi+1(k)
x=yi(τ)dk, τ =τi,
∂qi(τ,x)
∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=yi+1(τ)=−/integraldisplayτ
0Ψi(k)∂2G(x,ξ,σ2(τ−k))
∂ξ∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleξ=yi(k)
x=yi+1(τ)dk−Φi(τ)
2σ3/parenleftigg
1√πτ−y′
i+1(τ)
σ/parenrightigg
+/integraldisplayτ
0Φi(k)e−(yi+1(τ)−yi+1(k))2
4σ2(τ−k) −Φi(τ)
4σ3/radicalbig
π(τ−k)3dk−/integraldisplayτ
0Φi(k)(yi+1(τ)−yi+1(k))2e−(yi+1(τ)−yi+1(k))2
4σ2(τ−k)
8σ5/radicalbig
π(τ−k)5dk, τ =τi+1,
whereG(x,ξ,τ ) is given in Eq. (14).
2 Solving the ML heat equation via the GIT method
2.1 Background
An alternative method to construct the ML problem solution i s generalized integral transform (GIT). The
GIT method is used in physics, (Kartashov, 1999, 2001), but w as unknown in ﬁnance until its ﬁrst use in
(Carr and Itkin, 2021). The previously known solution to the heat equation, using the GIT method, was
obtained only for the domain S∈[0,y(t)]. For other domains, the solution was unknown even in physi cs.
(Itkin and Muravey, 2020a) were the ﬁrst to construct the GIT solution for the domain S∈[y(t),∞).
The latter technique was extended further for the CIR and CEV models, (Carr et al., 2020), the Black-
Karasinski model, (Itkin et al., 2020), and ﬁnally for doubl e barrier options in (Itkin and Muravey, 2020b).
The latter problem deals with the spatial domain determined by two moving in time boundaries, and
boundary conditions, which are arbitrary functions of time .
The GIT and HP methods are similar but have an essential diﬀer ence. In the HP method, the solution
is represented in the form of heat potential with the unknown potential density function Ψ( τ) which solves
the corresponding Volterra equation of the second kind, see Section 1. In the GIT method, similar to,
e.g., the Fourier method, we start with applying some integr al transform to the PDE under consideration.
The transform has to be such that the transformed equation wi thx/ma√sto→pis solvable analytically in time.
The second step is to construct an inverse transform, which c ould be computed analytically using the
complex analysis. If this is possible, then the solution can be represented as an explicit integral of some
kernel multiplied by the unknown function Ω( τ). Hence, this looks pretty similar to the HP method.
However, the function Ω( τ) has now a transparent meaning - this is the gradient of the so lution at the
moving boundary. It turns out that this gradient also solves a Volterra equation of the second kind. As
4These results can be naturally generalized for the case σ=σ(x).
Page 7 of 36",2021-02-16T18:22:12Z,multiplayer t tekau sha it ki very t eq solvi background at kart ash ov carr it kit for it ki very t carr  kara i it kiit ki very t t ivolterra seicourier t t  nce it volterra as tse page
paper_qf_37.pdf,8,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
mentioned, explicit construction of such forward and inver se transforms is performed in (Carr and Itkin,
2021; Itkin and Muravey, 2020a; Carr et al., 2020; Itkin et al ., 2020; Itkin and Muravey, 2020b) for various
models and spatial domains. Also, the authors show that the p erformance of both methods is the same.
Both HP and GIT methods are faster than the ﬁnite-diﬀerence a pproach and provide higher accuracy.
As mentioned in (Itkin and Muravey, 2020b), it is not unreaso nable to ask why we need two methods -
the HP and GIT, which are used to solve the same problem and dem onstrate the same performance. The
answer is interesting. As shown in (Carr et al., 2020), the GI T method produces very accurate results at
high strikes and maturities (i.e., when the option price is r elatively small), in contrast to the HP method,
which struggles under these circumstances. One can verify t his fact by looking at the exponents under
the GIT solution integral proportional to the time τ. Contrary, when the price is higher (short maturities,
low strikes), the GIT method is slightly less accurate than t he HP method since the exponents in the HP
solution integral are inversely proportional toτ.
Thus, the GIT and HP methods complement each other for the CIR and CEV models. For other
models reducible to the heat equation, the same conclusion h olds; see (Itkin and Muravey, 2020a). This
statement is true because GIT integrals contain the diﬀeren ce of two exponents, which becomes small at
largeτ. On the contrary, the HP exponent tends to one at large τ. Therefore, the convergence properties
of the two methods are diﬀerent at large τ, so they complement each other.
This situation is well known for the heat equation with const ant coeﬃcients, (Lipton, 2001). There
exist two representations of the solution: one - obtained by using the method of images, and the other
one - by the Fourier series. Although both solutions are equa l when considered as inﬁnite series, their
convergence properties are diﬀerent.
2.2 Solution of the heat equation
To apply the GIT method to the solution of Eq. (1), we can use th e results obtained in (Itkin and Muravey,
2020a). There it is assumed that Liis the heat equation operator
Li=−∂
∂τ+∂2
∂x2, (18)
and then the solution of Eq. (1) can be represented in the form
ui(τ,x) =∞/summationdisplay
n=−∞/braceleftigg/integraldisplayyi+1(0)
yi(0)ui(0,ξ)Υn,i(x,τ|ξ,0)dξ+/integraldisplayτ
0/bracketleftig
Ωi(s) +χ+
i(s)y′
i+1(s)/bracketrightig
Υn,i(x,τ|yi+1(s),s)ds,
+/integraldisplayτ
0/bracketleftig
Θi(s)−χ−
i(s)y′
i(s)/bracketrightig
Υn,i(x,τ|yi(s),s)ds (19)
+/integraldisplayτ
0χ−
i(s)Λn,i(x,τ|yi(s),s)−χ+
i(s)Λn,i(x,τ|yi+1(s),s)ds/bracerightigg
,
Υn,i(x,τ|ξ,s) =1
2/radicalbig
π(τ−s)/bracketleftigg
e−(2nli(τ)+x−ξ)2
4(τ−s)−e−(2nli(τ)+x+ξ−2yi(τ))2
4(τ−s)/bracketrightigg
,
Λn,i(x,τ|ξ,s) =x−ξ+ 2nli(τ)
4/radicalbig
π(τ−s)3e−(2nli(τ)+x−ξ)2
4(τ−s)+x+ξ−2yi(τ) + 2nli(τ)
4/radicalbig
π(τ−s)3e−(2nli(τ)+x+ξ−2yi(τ))2
4(τ−s).
Hereχ−
i(τ),χ+
i(τ) are the boundary conditions at the left and right boundarie s of thei-th interval,
and
li(τ) =yi+1(τ)−yi(τ), τ =τi, (20)
Page 8 of 36",2021-02-16T18:22:12Z,multiplayer carr it kiit ki very carr it kiit ki very also both as it ki very t as carr one contrary  for it ki very  otrefore  tetre courier although solutto eq it ki very tre li is li eq re page
paper_qf_37.pdf,9,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
Ωi(τ) =−∂ui(τ,x)
∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=yi(τ)Θi(τ) =∂ui(τ,x)
∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=yi+1(τ).
The functions Ω( τ),Θ(τ) for the heat equation in Eq. (18) can be found by solving a sys tem of the
Volterra equations of the second kind. As applied to our prob lem for the i-th interval with i∈[1,N], it
reads, (Itkin and Muravey, 2020b)
−Ωi(τ) =/integraldisplayyi+1(0)
yi(0)u(0,ξ)υ−
i(τ|ξ,0)dξ (21)
−χ−
i(τ)√πτ+/integraldisplayτ
0χ−
i(s)−χ−
i(τ)
2/radicalbig
π(τ−s)3ds+/integraldisplayτ
0/bracketleftig
χ−
i(s)d/parenleftig
η−
i(τ|yi(s),s)/parenrightig
−χ+
i(s)d/parenleftig
η−
i(τ|yi+1(s),s)/parenrightig/bracketrightig
−/integraldisplayτ
0Ωi(s)yi(τ)−yi(s)
2/radicalbig
π(τ−s)3e−(yi(τ)−yi(s))2
4(τ−s)ds+/integraldisplayτ
0/bracketleftig
Θi(s)υ−
i(τ|yi+1(s),s) + Ω i(s)υ−
0,i(τ|s)/bracketrightig
ds
Θi(τ) =/integraldisplayyi+1(0)
yi(0)u(0,ξ)υ+
i(τ|ξ,0)dξ
+χ+
i(τ)√πτ−/integraldisplayτ
0χ+
i(s)−χ+
i(τ)
2/radicalbig
π(τ−s)3ds+/integraldisplayτ
0/bracketleftig
χ−
i(s)d/parenleftig
η+
i(τ|yi(s),s)/parenrightig
−χ+
i(s)d/parenleftig
η+
i(τ|yi+1(s),s)/parenrightig/bracketrightig
−/integraldisplayτ
0Θi(s)yi+1(τ)−yi+1(s)
2/radicalbig
π(τ−s)3e−(yi+1(τ)−yi+1(s))2
4(τ−s)ds+/integraldisplayτ
0/bracketleftig
Θi(s)υ+
0,i(τ|s) + Ω i(s)υ+
i(τ|yi(s),s)/bracketrightig
ds.
Here the following notation is used
η−
i(τ|ξ,s) =−δξ,yi(s)/radicalbig
π(τ−s)+1/radicalbig
π(τ−s)∞/summationdisplay
n=−∞e−(yi(τ)−ξ+2nli(τ))2
4(τ−s), (22)
η+
i(τ|ξ,s) =−δξ,yi+1(s)/radicalbig
π(τ−s)+1/radicalbig
π(τ−s)∞/summationdisplay
n=−∞e−(yi(τ)−ξ+(2n+1)li(τ))2
4(τ−s),
υ−
i(τ|ξ,s) =−∞/summationdisplay
n=−∞yi(τ)−ξ+ 2nli(τ)
2/radicalbig
π(τ−s)3e−(yi(τ)−ξ+2nli(τ))2
4(τ−s),
υ+
i(τ|ξ,s) =−∞/summationdisplay
n=−∞yi(τ)−ξ+ (2n+ 1)li(τ)
2/radicalbig
π(τ−s)3e−(yi(τ)−ξ+(2n+1)li(τ))2
4(τ−s),
υ−
0,i(τ|s) =υ−
i(τ|yi(s),s) +yi(τ)−yi(s)
2/radicalbig
π(τ−s)3e−(yi(τ)−yi(s))2
4(τ−s),
υ+
0,i(τ|s) =υ+
i(τ|yi+1(s),s) +yi+1(τ)−yi+1(s)
2/radicalbig
π(τ−s)3e−(yi+1(τ)−yi+1(s))2
4(τ−s),
whereδξ,xis the Kronecker symbol. It is worth emphasizing that all sum mands in Eq. (21) are regular.
The integrands in the ﬁrst two lines have weak (integrable) s ingularities, while other summands are
regular.
At the boundaries of the domain where the solution of our prob lem is deﬁned, we have
χ−
1=χ−, χ−
N=χ+, (23)
whereNis the number of intervals.
In (Itkin and Muravey, 2020b), an alternative system of the V olterra equations of the second kind is
also obtained, which has the form of Eq. (21), but with a diﬀer ent deﬁnition of the coeﬃcients. We have
η−
i(τ|ξ,s) =−δξ,yi(s)/radicalbig
π(τ−s)+1
li(τ)θ3(φi(ξ,yi(τ)),ωi), (24)
Page 9 of 36",2021-02-16T18:22:12Z,multiplayer t eq volterra as it ki very re krone cker it eq t at is iit ki very eq  page
paper_qf_37.pdf,10,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
η+
i(τ|ξ,s) =−δξ,yi+1(s)/radicalbig
π(τ−s)+1
l(τ)θ3(φi(ξ+li(τ),yi(τ)),ωi),
υ−
i(τ|ξ,s) =−π
2l2
i(τ)θ′
3(φi(ξ,yi(τ)),ωi),
υ+
i(τ|ξ,s) =−π
2l2
i(τ)θ′
3(φi(ξ+li(τ),yi(τ)),ωi).
Hereθ3(z,ω) is the Jacobi theta function of the third kind, (Mumford et a l., 1983), which is deﬁned as
follows:
θ3(z,ω) = 1 + 2∞/summationdisplay
n=1ωn2cos (2nz), (25)
Also, in Eq. (24) the following notation is used
ωi=e−π2(τ−s)
l2
i(τ), φ i(x,ξ) =π(x−ξ)
2li(τ), (26)
∂θ3(z,ω)
∂z=θ′
3(z,ω) =−4∞/summationdisplay
n=1nωn2sin (2nz).
Formulas Eq. (24) and Eq. (22) are complementary. Since the e xponents in the deﬁnition of the theta
functions in Eq. (24) are proportional to the diﬀerence τ−s, the Fourier series Eq. (24) converge fast if
τ−sis large. Contrary, the exponents in Eq. (22) are inversely p roportional to τ−s. Therefore, the
series Eq. (22) converge fast if τ−sis small.
2.3 Solution of Eq. (10) when σis piecewise constant
Here we assume that σi(x) =σi, i= 1,...,N , i.e. the volatility is a piecewise constant function of x.
For instance, this is true for the problem described in Secti on 3.2. As shown there, the pricing PDE can
be transformed to Eq. (9) instead of Eq. (71). According to th e transformation in Eq. (72) the clock will
run diﬀerently at each ML interval, which is inconvenient. T herefore, instead of a change of temporal
variable, below we use a transformation of the spatial varia blex. This transformation allows using the
same time at each ML interval. To achieve this, we change the d eﬁnition of τ(t) in Eq. (72) to
τ=1
2/integraldisplayT
0e−2/integraltexts
0[r(s)−q(s)]dkds, (27)
wherer(t),q(t) for the problem considered in Section 3.2 are the determini stic interest rate and continuous
dividends. This converts the problem in Section 3.2 and the P DE Eq. (70) to
∂U(τ,x)
∂τ=σ2(x)∂2U(τ,x)
∂x2, (28)
U(0,x) =U0(x) = (x−S)+,
U(τ,0) = 0, U (τ,x)/vextendsingle/vextendsingle/vextendsingle
x→∞=x−e−/integraltextT
0(r(s)−q(s))dsS.
And, according to Eq. (69)
σ2(T,K) =vi(T), K ∈[Ki,Ki+1]. (29)
The boundary and initial conditions in Eq. (28) are the direc t translation of those conditions in Eq. (66)
and Eq. (67).
Page 10 of 36",2021-02-16T18:22:12Z,multiplayer re jacob mum ford also eq s eq eq since eq courier eq contrary eq trefore eq soluteq re for se as eq eq accordi eq  to eq se seeq and eq ki ki t eq eq eq page
paper_qf_37.pdf,11,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
Again, as shown in Appendix A, the problem in Eq. (28) can be tr ansformed to
∂U(τ,ˆx)
∂τ=∂
∂ˆx/parenleftbigg
Ξ2(ˆx)∂U(τ,ˆx)
∂ˆx/parenrightbigg
, ˆx= ˆx(x), (30)
U(0,ˆx) =U0(ˆx) = (x(ˆx)−S)+,
U(τ,ˆx(0)) = 0, U (τ,x(ˆx))/vextendsingle/vextendsingle/vextendsingle
x(ˆx)→∞=x(ˆx)−e−/integraltextT
0(r(s)−q(s))dsS.
Also, since based on Eq. (72)
U(τ,x(ˆx)) =P(T,K)e−/integraltextT
0q(s)ds,
the continuity of the solution and its ﬂux at all internal bou ndaries can be expressed as
χ+
i(τ) =χ−
i+1(τ). (31)
Ξ2
i+1Ωi+1(τ) =−Ξ2
iΘi(τ).
To use the results of the previous Section, we proceed by appl ying the following transformation to
Eq. (19)
¯x= Ξ iˆx, ¯y(τ) = Ξ iy(τ), ¯ξ= Ξ iξ, ¯l(τ)≡Ξil(τ). (32)
Note, that the last equality in Eq. (32) is actually the deﬁni tion of ¯l(τ). Another complication which
comes due to this transformation is that in new variables ¯ xthe layers stop to be continuous. In other
words, the upper boundary of the i-th layer ¯y+
i(τ) and the lower boundary of the ( i+ 1)-th layer ¯ y−
i+1(τ)
are now not equal. Therefore, in what follows to avoid any con fusion we will explicitly use this notation,
i.e. the left and right boundaries of the i-th layer are denoted as y−
i(τ) andy−
i+1(τ).
Also, per these transformations, we have
ui(0,¯ξ) =ui(0,Ξi,ξ), u i(0,Ξi,ξ)dξ=1
Ξiui(0,¯ξ)d¯ξ, (33)
/integraldisplayyi+1(0)
yi(0)u(0,ξ)υ−
i(τ|ξ,0)dξ=1
Ξi/integraldisplay¯yi+1(0)
¯yi(0)u(0,¯ξ)¯υ−
i(τ|¯ξ,0)d¯ξ,
¯υ−
i(τ|¯ξ,0) = −∞/summationdisplay
n=−∞¯y−
i(τ)−¯ξ+ 2n¯li(τ)
2Ξi/radicalbig
π(τ−s)3e−(¯y−
i(τ)−¯ξ+2n¯li(τ))2
4Ξ2
i(τ−s).
It is easy to check that this transformation leaves φi(ˆx,ξ)) and∂ˆx(φi(ˆx,ξ)) invariant, but with the
new deﬁnition
¯ωi=e−π2Ξ2
i(τ−s)
¯l2
i(τ).
Finally, let us redeﬁne the partial derivatives
¯Ωi=−∂u(τ,¯x)
∂¯x/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
¯x=¯y−
i(τ), ¯Θi=∂u(τ,¯x)
∂¯x/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
¯x=¯y+
i(τ)(34)
instead of their deﬁnitions in Eq. (20), i.e. Ω i= Ξ i¯Ωi,Θi= Ξ i¯Θi. Then the continuity conditions in
Eq. (83) change to
¯χ+
i(τ) = ¯χ−
i+1(τ), (35)
Ξ3
i+1¯Ωi+1(τ) =−Ξ3
i¯Θi(τ).
Page 11 of 36",2021-02-16T18:22:12Z,multiplayer ag articial intellence  eq also eq to seeq note eq anotr itrefore also it nally eq teq page
paper_qf_37.pdf,12,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
To simplify notation, we omit bars from all new variables ass uming this doesn’t bring any confusion.
Then Eq. (19) transforms to
ui(τ,x) =∞/summationdisplay
n=−∞/braceleftigg
1
Ξi/integraldisplayyi+1(0)
yi(0)ui(0,ξ)Υn,i(x,τ|ξ,0)dξ (36)
+/integraldisplayτ
0/bracketleftbigg1
ΞiΩi(s) +1
Ξi+1χ+
i(s)y+′
i(s)/bracketrightbigg
Υn,i(x,τ|y+
i(s),s)ds,
+1
Ξi/integraldisplayτ
0/bracketleftig
Θi(s)−χ−
i(s)y−′
i(s)/bracketrightig
Υn,i(x,τ|y−
i(s),s)ds
+/integraldisplayτ
0χ−
i(s)Λn,i(x,τ|y−
i(s),s)−χ+
i(s)Λn,i(x,τ|y+
i(s),s)ds/bracerightigg
,
Υn,i(x,τ|ξ,s) =1
2/radicalbig
π(τ−s)
e−(2nli(τ)+x−ξ)2
4Ξ2
i(τ−s)−e−(2nli(τ)+x+ξ−2y−
i(τ))2
4Ξ2
i(τ−s)
,
Λn,i(x,τ|ξ,s) =x−ξ+ 2nli(τ)
4Ξi/radicalbig
π(τ−s)3e−(2nli(τ)+x−ξ)2
4Ξ2
i(τ−s)+x+ξ−2y−
i(τ) + 2nli(τ)
4Ξi/radicalbig
π(τ−s)3e−(2nli(τ)+x+ξ−2y−
i(τ))2
4Ξ2
i(τ−s).
By analogy, the modiﬁed Volterra equations can be obtained f rom Eq. (21).
In Eq. (21) the unknown variables are [ χ−
1(τ),χ+
1(τ),Ω1(τ),Θ1(τ),...,χ−
N(τ),χ+
N(τ),ΩN(τ),ΘN(τ)],
so that there are 4 Nunknowns in total. The boundary conditions in Eq. (23) and th e continuity conditions
in Eq. (83) reduce the number of unknown variables to 2 N−2, becauseχ+
i(τ),Θi(τ) can be expressed via
χ−
i(τ),Ωi(τ) and substituted into Eq. (21). Thus, the GIT method provide s a signiﬁcant simpliﬁcation
of the system of Volterra equations as compared with the HP me thod.
3 Application to ﬁnance
In this section, we consider several models that are frequen tly used in mathematical ﬁnance. We provide
a short description of each model and demonstrate how to redu ce the corresponding pricing problem to
the form suitable for solving it by the ML method.
3.1 One-factor short-rate models
As the ﬁrst example, we consider one-factor short interest r ate (IR) models. Although these models
were developed a long time ago, they are still essential and w idely used by practitioners. While one
can price zero-coupon bonds (ZCB) and European options on th e ZCB and swaptions for many of them
analytically, this is not true for exotic options. For insta nce, pricing of barrier options when the barriers
are time-dependent and could pay time-dependent rebates ha s to be done numerically. The same is true
for American options.
However, as mentioned in the Introduction, one can ﬁnd the so lution to these problems semi-analytically
using the HP and GIT methods for some one-factor models, incl uding the time-dependent OU (Vasicek)
model in (Carr and Itkin, 2021; Lipton and Kaushansky, 2020a ,b), for the Hull-White model in (Itkin
and Muravey, 2020a), for the CEV and CIR models in (Carr et al. , 2020), and then in a general form
for any model that can be reduced to the heat equation - in (Itk in and Muravey, 2020b). In other words,
solving these problems doesn’t require the ML method. There fore, below we consider some other models
for which the barrier pricing problems cannot be directly so lved by the HP or GIT methods but can be
solved by using the ML method.
Page 12 of 36",2021-02-16T18:22:12Z,multiplayer to teq by volterra eq ieq unkns t eq eq eq  volterra applicati one as although w afor t introduvas ice carr it kitekau sha hull white it ki very carr it  very itre page
paper_qf_37.pdf,13,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
3.1.1 Pricing zero-coupon bonds and barrier options for the Black-Karasinski and similar mod-
els
The Black-Karasinski (BK) model was introduced in (Black an d Karasinski, 1991), see also (Brigo and
Mercurio, 2006) for a more detailed discussion. The BK is a on e-factor short interest rate model of the
form
dzt=k(t)[θ(t)−zt]dt+σ(t)dWt, r ∈R, t≥0, (37)
rt=s(t) +Rezt, r (t= 0) =r0.
Hereκ(t)>0 is the constant speed of mean-reversion, θ(t) is the mean-reversion level, σ(t) is the volatility,
Ris some constant with the same dimensionality as rt, eg., it can be 1/(1 year). This model is similar to
the Hull-White model but preserves the positivity of rtby exponentiating the Ornstein-Uhlenbeck (OU)
random variable zt. Because of that, usually, practitioners add a determinist ic function (shift) s(t) to the
deﬁnition of rtto address possible negative rates and be more ﬂexible when c alibrating the term-structure
of the interest rates.
By Itô’s lemma the short rate ¯ rt= (rt−s(t))/Rin the BK model solves the following stochastic
diﬀerential equation (SDE)
d¯rt= [kθ(t) +1
2σ(t)2−klog ¯rt]¯rtdt+σ(t)¯rtdWt. (38)
This SDE can be explicitly integrated. Let 0 ≤s≤t≤T, Thenrtcan be represented as, (Brigo and
Mercurio, 2006)
¯rt= exp/bracketleftbigg
e−k(t−s)log ¯rs+k/integraldisplayt
se−k(t−u)θ(u)du+/integraldisplayt
sσ(u)e−k(t−u)dW(u)/bracketrightbigg
, (39)
and thus, conditionally on ﬁltration Fsis lognormally distributed and always positive.
However, in the BK model, the price P(t,T) of a (ZCB) with the maturity Tis not known in closed
form since this model is not aﬃne. Multiple good approximati ons have been developed in the literature
using asymptotic expansions of various ﬂavors, see, e.g., ( Antonov and Spector, 2011; Capriotti and
Stehlikova, 2014; Horvath et al., 2017), and also survey in ( Turfus, 2020).
Despite this lack of tractability, the BK model is widely use d by practitioners for modeling interest
rates and credit and is also known in commodities as the Schwa rtz one-factor model. The BK model is
attractive because it is relatively simple, guarantees non -negativity of the prices (which could be a bad
feature in the current environment). It could also be calibr ated to the given term-structure of interest
rates and the prices or implied volatilities of caps, ﬂoors, or European swaptions since the mean-reversion
level and volatility are functions of time. However, for exo tic options, e.g., highly liquid barrier options,
these prices are not known yet in closed form. Therefore, var ious numerical methods are used to obtain
them.
Here we describe how one can reduce the pricing problem for th e ZCB to the ML heat equation. Since
this problem is deﬁned at a semi-inﬁnite domain, the corresp onding ML heat equation is also deﬁned
at a semi-inﬁnite interval. Thus, the number of layers could be inﬁnite. Therefore, truncation of the
semi-inﬁnite interval to a ﬁnite is needed. Of course, the im pact of the remainder should be assessed
appropriately.
Along the BK model lines, consider a model where the dynamics of the underlying stochastic variable
ztis the OU process deﬁned in Eq. (37). We assume that the intere st ratertis some deterministic
function of zt
rt=s(t) +f(t,zt), z 0= 0. (40)
In particular, according to Eq. (37) for the BK model we have f(t,zt) =Rezt, and soR=r0−s0.
Page 13 of 36",2021-02-16T18:22:12Z,multiplayer prici  kara i t  kara i  kara i br mercury t  rez re is  hull white or nste ihlebeck because by it i   t not cabr mercury fs is tis multiple antoseor caps o teh li v horvath turf us spite sha t it atrefore re since  trefore of alo eq  ieq rez page
paper_qf_37.pdf,14,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
In terms of z, the corresponding PDE for the ZCB price F(t,r) in Eq. (49) and for the option price
C(t,r) in Eq. (52) reads
0 =∂V
∂t+1
2σ2(t)∂2V
∂z2+κ(t)[θ(t)−z]∂V
∂z−[s(t) +f(t,z)]V, (41)
whereV=V(t,z) is eitherF(t,z) orC(t,z). This equation should be solved subject to the terminal
and boundary conditions. For the ZCB price they are given in E q. (50) and Eq. (51), and for the
Down-and-out barrier Call option price - in Eq. (53) and in Eq . (55), Eq. (56). Note, that solving
Eq. (41) for F(t,z) assumes that z∈/parenleftbigf−1(t,−∞),f−1(t,∞)/parenrightbig, while forC(t,z) the domain of deﬁnition
isz∈[Lz(t),f−1(t,∞))., whereLz(t) =f−1(t,L(t)), andf−1(t,r) is the inverse function.
To apply the ML method to Eq. (41), for instance, when solving the barrier option pricing problem,
we truncate the interval [ L(t),∞) from above at z=zmaxto make it [ Lz(t),zmax]. The reason this
is possible lies in the fact that when zincreases, the ZCB price tends to zero based on the boundary
condition. Therefore, the Call option price in this limit va nishes as well. Thus, the contribution of the
region [zmax,f−1(t,∞)) to the Call option price becomes negligible5.
Now we split the interval [ Lz(t),zmax] intoN > 0 sub-intervals, and at every interval [ zi,zi+1], i=
1,...,N assume that f(t,z) =ai(t) +bi(t)z). Accordingly, at every interval i, i= 1,...,N the PDE
Eq. (41) takes the form
0 =∂V
∂t+1
2σ2(t)∂2V
∂z2+κ(t)[θ(t)−z]∂V
∂z−[s(t) +ai(t) +bi(t)z]V. (42)
This PDE can be transformed to the heat equation
∂U
∂τ=∂2U
∂x2, (43)
by the change of variables, (Polyanin, 2002; Itkin and Murav ey, 2020a; Lipton and Kaushansky, 2020b)
V(t,z) = exp[αi(t)z+βi(t)]U(τ,x), τ =φ(t), x =zψ(t) +̺(t), (44)
where
ψ(t) =C1exp/parenleftbigg/integraldisplayt
Sκ(q)dq/parenrightbigg
, φ (t) =1
2/integraldisplayS
tσ2(q)ψ2(q)dq+C2, (45)
αi(t) =ψ(t)/integraldisplayt
Sbi(q)
ψ(q)dq+C3ψ(t), ̺ i(t) =−/integraldisplayt
S/bracketleftig
κ(q)θ(q) +σ2(q)αi(q)/bracketrightig
ψ(q)dq+C5,
βi(t) =−1
2/integraldisplayt
Sαi(q)/bracketleftig
2κ(q)θ(q) +σ2(q)αi(q)/bracketrightig
dq+/integraldisplayt
S[s(q) +ai(q)]dq+C4,
whereC1,...,C 5are some constants. In our case we can choose C1= 1,C2=C3=C4=C5= 0.
One of the advantages of such an approach is that the new time τdoesn’t depend on the speciﬁc
intervali, i.e. the time τruns in sync for all intervals [ zi,zi+1], i= 1,...,N .
Thus, the main trick here is in using the approximation f(t,z) =ai(t) +bi(t)z). This approximation
provides the second order of accuracy in the length of the int erval (similar to the ﬁnite-diﬀerence method
of the second order), and allows reduction of the PDE at each i nterval to the heat equation (while the
original PDE doesn’t hold this property).
At the end of this Section, note that Eq. (42), if used for pric ing ZCB, doesn’t need the boundary
condition at the left boundary z→ −∞ , as this is discussed in (Itkin and Muravey, 2020a) with a
5For some choices of the functions f(t, z) the value f−1(t,∞) could be ﬁnite which eliminates the need for truncation.
Page 14 of 36",2021-02-16T18:22:12Z,multiplayer ieq eq  for eq dowcall eq eq eq note eq lz lz to eq lz t trefore call  call  lz accordy eq  poly aiit ki tekau sha bi ione   at seeq it ki very for page
paper_qf_37.pdf,15,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
reference to Fichera theory, (Oleinik and Radkevich, 1973) . However, the price of the ZCB at some ﬁxed
left boundary zmin, i.e.V(t,zmin) can be found having in mind that the transformed PDE in Eq. (4 2) is
aﬃne, which yields
V(z,t,S ) =A(t,S)eB(t,S)Rez. (46)
With allowance for the terminal condition in Eq. (50), the so lution reads, (Itkin and Muravey, 2020a)
B(t,S) =e/integraltextt
0κ(m)dm/integraldisplayt
Sbi(m)e−/integraltextm
0κ(q)dqdm, (47)
A(t,S) = exp/bracketleftbigg/integraldisplayt
S/parenleftbigg
ai(m) +s(m)−1
2B(m,S)/parenleftig
2θ(m)κ(m) +B(m,S)σ2(m)/parenrightig/parenrightbigg
dm/bracketrightbigg
.
It can be seen that B(t,S)<0 ift<S . Therefore, F(r,t,S )→0 whenz→ ∞ .
3.1.2 The modiﬁed BK (Verhulst) model
Since the BK model is not fully tractable, in (Itkin et al., 20 20) we introduced a slightly modiﬁed version
of the model as follows
dzt=k(t)[¯θ(t)−ezt]dt+σ(t)dWt, (48)
rt=s(t) +Rezt, z 0= 0, R =r0−s(0).
It can be seen, that at small t|zt| ≪1, and so choosing ¯θ(t) = 1 +θ(t) replicates the BK model in the
linear approximation on zt. Similarly, the choice ¯θ(t) =eθ(t)replicates the BK model at ztclose the
mean-reversion level θ(t). Thus, this model acquires the properties of the BK model wh ile is a bit more
tractable as this will be seen below.
It is worth noting that if by using Itô’s lemma we re-write Eq. (48) for the stochastic variable rt, the
resulting dynamics can be recognized as the stochastic Verh ulst or stochastic logistic model, which are
well-known in the population dynamics and epidemiology; se e, eg., (Verhulst, 1838; Bacaer, 2011; Giet
et al., 2015) and references therein. For more information, see (Itkin et al., 2020).
By the Itô’s lemma and the Feynman–Kac formula any contingen t claim written on the rtas the
underlying (for instance, price F(¯r,t,S ) of a Zero-coupon bond (ZCB) with maturity S) solves the
following partial diﬀerential equation
0 =∂F
∂t+1
2σ2(t)¯r2∂2F
∂¯r2+κ(t)¯r[Θ(t)−¯r]∂F
∂¯r−(s(t) +R¯r)F, (49)
¯rt=rt−s(t)
r0−s(0)=ezt, Θ(t) =¯θ(t) +1
2σ2(t).
This equation should be solved subject to the terminal condi tion
F(¯r,S,S ) = 1, (50)
and the boundary condition
F(¯r,t,S )/vextendsingle/vextendsingle/vextendsingle
¯r→∞= 0, (51)
see, eg., (Andersen and Piterbarg, 2010).
In the sequel we will also consider a Down-and-Out barrier Ca ll option written on the ZCB. It is
known, (Andersen and Piterbarg, 2010), that under a risk-ne utral measure the option price C(t,¯r) solves
the same PDE as in Eq. (49),
0 =∂C
∂t+1
2σ2(t)¯r2∂2C
∂¯r2+κ(t)¯r[Θ(t)−¯r]∂C
∂¯r−(s(t) +R¯r)C. (52)
Page 15 of 36",2021-02-16T18:22:12Z,multiplayer c r ole iik rake rieq rez with eq it ki very bi it trefore t hul st since it ki rez it simarly  it it eq ve rh hul st ba car gie for it kiby it leymaka zero  anrsosite rb arg idowout ca it anrsosite rb arg eq page
paper_qf_37.pdf,16,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
The terminal condition at the option maturity T≤Sfor this PDE reads
C(T,¯r) = (F(¯r,T,S )−K)+, (53)
whereKis the option strike.
By a standard contract, the lower barrier LF(t) (which we assume to be time dependent as well) is
set on the ZCB price, and not on the underlying interest rate ¯ r. This means that it can be written in
the form
C(t,¯r) = 0 ifF(¯r,t,S ) =LF(t). (54)
This condition can be translated into the ¯ rdomain by solving the equation
F(¯r,t,S ) =LF(t),
with respect to ¯ r. Denoting the solution of this equation as L(t) we ﬁnd that Eq. (54) in the ¯ rdomain
reads
C(t,L(t)) = 0. (55)
The second boundary can be naturally set at ¯ r→ ∞ . As at ¯r→ ∞ the ZCB price tends to zero, the Call
option price also vanishes in this limit. This yields
C(t,¯r)/vextendsingle/vextendsingle/vextendsingle
¯r→∞= 0. (56)
Accordingly, Eq. (52) has to be solve at ¯ r∈[L(t),∞).
3.1.3 Pricing barrier options in the Verhulst model
As we have already mentioned, in contrast to other similar on e-factor models like the time-dependent
Ornstein-Uhlenbeck, Hull-White, CIR and CEV models which h ave been considered in (Carr and Itkin,
2021; Itkin and Muravey, 2020a; Carr et al., 2020), the solut ion of the pricing problem for the BK model
is not known in closed form6. Therefore, we propose an approximation that gives rise to a semi-analytical
solution for the barrier Call option price. This approximat ion is inspired by the ML heat equations which
are discussed in Section .
Since our problem in Eq. (52) is deﬁned at the semi-inﬁnite do main ¯r∈[L(t),∞), using the ML
approximation is time-consuming, as we need to split this se mi-inﬁnite interval into a ﬁxed number of
sub-intervals. Therefore, it is feasible ﬁrst to make a chan ge of variables
C(t,¯r) =V(t,x)e/integraltextt
0s(k)dk, x =a(t)
¯r, a (t) =e/integraltextt
0(κ(m)Θ(m)−σ2(m))dm, (57)
so the problem to solve in new variables reads
0 =∂V
∂t+1
2σ2(t)x2∂2V
∂x2+a(t)κ(t)∂V
∂x−Ra(t)V
x. (58)
Thus, now our problem is deﬁned at a ﬁxed domain x∈[0,1/L(t)], where the upper boundary is time
dependent. Accordingly, in the new variables the Down-and- Out barrier Call option becomes the Up-
and-Out barrier Call.
Doing the second change of the dependent variable
V(t,x) =u(t,x)ed(t)/x, d (t) =Re−/integraltextt
0σ2(m)dm/integraldisplayt
0e/integraltexty
0κ(m)Θ(m)dmdy (59)
6Some approaches for doing that are discussed in (Itkin et al. , 2020).
Page 16 of 36",2021-02-16T18:22:12Z,multiplayer t for is by   noti eq t as call  accordy eq prici hul st as or nste ihlebeck hull white carr it kiit ki very carr trefore call  sesince eq trefore ra  accordy dowout call up out call doi re some it kipage
paper_qf_37.pdf,17,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
yields the equation
0 =∂u
∂t+1
2σ2(t)x2∂2u
∂x2+g(t)∂u
∂x−f(t)u
x2, (60)
f(t) =1
2d(t)/parenleftig
2a(t)κ(t)−d(t)σ2(t)/parenrightig
, g (t) =a(t)κ(t)−d(t)σ2(t).
Accordingly, in the new variables the initial and boundary c onditions read
u(T,x) = exp/parenleftigg
−d(T)
x−/integraldisplayT
0s(k)dk/parenrightigg
(F(x,T,S )−K)+, (61)
u(t,0) = 0, u (t,y(t)) = 0, y(t) =a(t)/L(t).
The problem in Eq. (60), Eq. (61) cannot be solved in closed fo rm. Therefore, we proceed by borrowing
the idea from the ML approach in physics which is described in Sections 1,2. This approach implies
that the interval x∈[0,y(t)] we approximate the function ζ(x) =x2by using a piecewise constant
approximation. In more detail, we split the interval [0 ,y(t)] intoN > 0 sub-intervals, and at every
interval [xi,xi+1], i= 1,...,N assume that x2≈νi(t)7. For instance, one can choose the middle value
of the function ζ(x) at each sub- , so
νi(t) =y2(t)(i+ 1/2)2
N2.
With allowance for this approximation, at every i-th interval Eq. (60) takes the form
0 =∂u
∂t+1
2σ2(t)νi(t)∂2u
∂x2+g(t)∂u
∂x−f(t)
νi(t)u. (62)
The Eq. (62) can be transformed to the heat equation
∂U
∂τ=∂2U
∂ς2, (63)
using the transformation, (Polyanin, 2002)
u(t,x) =U(τ,ς) exp/parenleftbigg
−/integraldisplayt
0f(t)
νi(t)/parenrightbigg
, l =x−/integraldisplayt
0g(k)dk, τ =1
2/integraldisplayT
tσ2(k)νi(k)dk. (64)
Note, that under this approximation the new time τalso becomes a function of the interval i.
3.2 Local volatility and Dupire’s equation
Calibration of the local volatility model (constructed by u sing a one-factor Geometric Brownian motion
process) to a given set of option prices is a classical proble m of mathematical ﬁnance. It was considered in
multiple papers, and various solutions were proposed; see, e.g., a survey in (Itkin, 2020; Itkin and Lipton,
2018) and references therein. In particular, in (Itkin and L ipton, 2018) an analytical approach to solving
the calibration problem is developed. This approach extend s the method in (Lipton and Sepp, 2011)
by replacing a piecewise constant local variance construct ion with a piecewise linear one and allowing
non-zero interest rates and dividend yields. This approach remains analytically tractable as it combines
the Laplace transform in time with an analytical solution of the resulting spatial equations in terms of
Kummer’s degenerate hypergeometric functions.
7Since the upper boundary of the whole interval y(t) is the function of time, we need to put νi=νi(t)
Page 17 of 36",2021-02-16T18:22:12Z,multiplayer accordy t eq eq trefore sens  ifor with eq t eq poly ainote local dup ire calibratgeometric brownish it it kiit kiteiit ki tesep  place suer since page
paper_qf_37.pdf,18,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
A similar problem could be formulated not just for the Black- Scholes model but also for other models.
For instance, in (Carr and Itkin, 2020, 2019) two extensions of the Local Variance Gamma model proposed
initially in (Carr and Nadtochiy, 2017) were developed. The ﬁrst new model (ELVG) considers a Gamma
time-changed arithmetic Brownian motion with drift and the local variance to be a piec ewise linear
function of the strike. The second model (GLVG) is a geometric version of the ELVG with drift. It
also treats various cases by introducing three piecewise linear models: the local variance as a function of
strike, the local variance as a function of log-strike, and t he local volatility as a function of strike (so, the
local variance is a piecewise quadratic function of the strike). For all these extensions, the autho rs derive
an ordinary diﬀerential equation for the option price, whic h plays the role of Dupire’s equation for the
standard local volatility model. Moreover, it can be solved in closed form.
In (Itkin and Lipton, 2018; Carr and Itkin, 2020, 2019) all mo dels were calibrated to the market
quotes term-by-term. Therefore, various types of no-arbit rage interpolation were proposed to guarantee
no-arbitrage while keeping the model analytically tractab le on the other hand; further details are given
in (Itkin, 2020).
Two advantages of the semi-analytical approach, which are e ssential for calibration the model, should
be emphasized. First, the option prices can be found analyti cally in a semi closed form. Here ""semi""
means that the analytic solution requires an additional inv erse transform to be applied to get the ﬁnal
prices; see (Itkin and Lipton, 2018). However, in (Carr and I tkin, 2020, 2019) since the Dupire-like
equation is an ODE and not a PDE, this step is eliminated. Neve rtheless, all these models are calibrated
term-by-term.
This idea can be extended by constructing a semi-analytical solution of the ML heat equation, which
is analytic in time. Thus, the term-by-term calibration cou ld be eliminated, and the quotes for all strikes
and maturities can be used simultaneously. Therefore, this approach allows a further acceleration of the
calibration process.
For brevity, let us consider European options, for instance , a Put option on a stock. It is well-known
that the price P(T,K) of the option written on the underlying stock price Stas a function of the option
maturityTand strikeKsolves Dupire’s equation, (Dupire, 1994)
∂P
∂T=1
2σ2(T,K)∂2P
∂K2−(r−q)K∂P
∂K−q(T)P. (65)
withσ=σ(t,S) being the local volatility, and q(t) is the dividend yield. This PDE should be solved
subject to the initial condition at T= 0
P(0,K) = (K−S)+, (66)
and natural boundary conditions for the put option price tha t read, (Hull, 2011)
P(T,K) = 0, K →0,
P(T,K) =D(t,T)(K−S)≈D(t,T)K, K → ∞,(67)
where the discount factor D(t,T) is deﬁned as
D(t,T) = exp/parenleftigg
−/integraldisplayT
tr(k)dk/parenrightigg
. (68)
To proceed further, we use the idea in (Lipton and Sepp, 2011; Itkin and Lipton, 2018) and approx-
imate the local variance using some piecewise approximatio n in the strike space. However, in contrast
to (Lipton and Sepp, 2011; Itkin and Lipton, 2018) we make thi s approximation a function of time. Fur-
ther, suppose that for each trading maturity Tj, j∈[1,M] the market quotes are provided at a set
Page 18 of 36",2021-02-16T18:22:12Z,multiplayer  schools for carr it kilocal variance gaa carr nad sochi t gaa brownish t it for dup ire oiit kitecarr it kitrefore it kitwo rst re it kitecarr dup ire ne  trefore for aput it st as and solvdup ire dup ire  hull to tesep it kitetesep it kitefur tj page
paper_qf_37.pdf,19,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
of strikesKi,j, i= 1,...,n jwhere the strikes are assumed to be sorted in the increasing o rder. Let us
construct a ﬁnite grid in the strike space G(K) :K∈[min(Ki,j),max(Ki,j)], j= 1,...,M, i = 1,...,n j,
by splitting the whole interval K∈[min(Ki,j),max(Ki,j)] intonsub-intervals. At every interval
[Ki,Ki+1], i= 1,...,N , we approximate the local variance function σ2(T,K) by a piecewise constant
function in Kas follows:
σ2(T,K) =vi(T), K ∈[Ki,Ki+1]. (69)
This approximation is not continuous, so the local variance σ2(T,K) experience a ﬁnite jump at every
pointKi. However, it is continuous in the maturity T.
Accordingly, at every interval [ Ki,Ki+1] Eq. (65),V(T,K) takes the form
∂Pi
∂T=1
2vi(T)∂2Pi
∂K2−[r(T)−q(T)]∂Pi
∂K−q(T)Pi. (70)
This equation can be transformed to the heat equation
∂Ui
∂τ=∂2Ui
∂x2, (71)
by a change of variables
Pi(T,K) =Ui(τ,x)e−/integraltextT
0(q(s)ds, x =e−/integraltextT
0(r(s)−q(s)dsK, τ =1
2/integraldisplayT
0vi(s)e−2/integraltexts
0(r(s)−q(s))dkds. (72)
It is important to note that the new time τruns diﬀerently at every interval K∈[Ki,Ki+1] as it depends
on the local variance value vi(s) at this interval.
4 Solution of the Volterra equations
An eﬃcient solution of the derived systems of Volterra equat ions is a problem that requires some attention
and extended description. Therefore, it will be published e lsewhere. Instead, here we show that a
particular choice of the internal boundaries can reduce thi s problem to a linear system with a tridiagonal
matrix allowing the inverse Laplace transform. In this sect ion, we explain this approach in detail.
We start with Eq. (21). Using the deﬁnitions of η±,υ±,υ±
0in Eq. (22), we observe that under all
integrals in Eq. (21) the functions fi(s),Θi(s),Ωi(s) are functions of s, while functions η±,υ±,υ±
0are
functions of t−sandyi(t)−yi(s) andzi(t)−zi(s). Recall that functions y1(t) =χ−, yN(t) =χ+deﬁne
the external boundaries of the computational domain, while functionsyi(t), i= 2,...,N −1 deﬁne the
boundaries of the internal layers (the internal boundaries ).
Since the internal boundaries are artiﬁcial, we can constru ct them as we wish. For instance, we can use
polynomial functions, such as yi(s) =ais2+bis+ci, whereai,bi,ciare some constants. Then yi(t)−yi(s)
can also be represented as a certain function g(t−s). Indeed
yi(t)−yi(s) =−ai(t−s)2+ (bi+ 2ait)(t−s). (73)
A similar representation can be obtained for yi+1(s)−yi(s)
yi+1(s)−yi(s) = (ai+1−ai)s2+ (bi+1−bi)s+ (ci+1−ci) =A(t−s)2+B(t−s) +C, (74)
A=ai+1−ai, B = 2At+bi+1−bi, C =ci−ci+1+t[bi−bi+1+ (ai−ai+1)t].
Same can be done for a polynomial of any degree.
All coeﬃcients ai,bi,cican be precomputed given the external boundaries. An exampl e of this con-
struction is given in Fig. 1.
Page 19 of 36",2021-02-16T18:22:12Z,multiplayer ki  ki ki ki ki at ki ki as ki ki  ki accordy ki ki eq pi pi pi pi  ui ui pi ui it ki ki solutvolterra avolterra trefore instead place i eq usi eq eq recall since for tined at same all a page
paper_qf_37.pdf,20,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
•yN(t)
y0(t)yi(t)yi+1(t)
yi−1(t)...
......
...
Figure 1: Internal layers constructed for the given external boundar iesy0(t)andyN(t), and
the number of layers N, by using 3 points for each boundary yi(t)and polynomial curves.
In more detail, suppose that for the given functions y1(t) =χ−(t), yN+1=χ+(t), we want to have
Nlayers. We use Nuniform nodes to split the interval [ χ−(0),χ+(0)] intoNsubintervals. We do the
same for the interval [ χ−(t),χ+(t)]. If the boundaries χ−(t),χ+(t) are smooth enough, we can connect
pointsyi(0),yi(t), i= 2,...,N by straight lines in such a way that all boundaries don’t cros s each other.
Suppose this is not possible because the external boundarie s are too convex or concave. In that case,
we can ﬁnd some s=τwhere the distance between the external boundaries is minim al and put N
nodes there. Then, we can connect all points yi(0),yi(τ),yi(t) by parabolas, and again check that all the
boundaries don’t cross each other. We can continue this proc ess by using polynomials of a higher degree
to provide the ﬁnal representation of the boundaries.
Thus, we can ﬁnd a polynomial of the necessary degree to guara ntee that all boundaries don’t cross
each other unless the external curves have a very peculiar sh ape, which does not happen in the context of
ﬁnancial applications. Otherwise, we need to use a general a pproach to the computation of the integrals
in Eq. (21), which will be published elsewhere.
Provided that all the boundaries are constructed in such a wa y, one can observe that all integrals in
Eq. (21) are convolutions. Therefore, we can apply the Lapla ce transform L(f|s,λ)
L(f|s,λ) =/integraldisplay∞
0e−λsf(s)ds (75)
to both parts of each equation in Eq. (21). Taking into accoun t that
L(f∗g) =L(f)L(g),L(f′) =λL(f)−f(0), (76)
and obtain from Eq. (21) a linear system of equations for L(χ+
i),L(χ−
i),L(Ωi),L(Θi). Using the conditions
at the internal boundaries (such as in Eq. (35)), this system can be reduced to a linear system for only
Page 20 of 36",2021-02-16T18:22:12Z,multiplayer  internal ilayers  unorm sub intervals   suppose it  otrwise eq provid eq trefore lap la eq taki eq usi eq page
paper_qf_37.pdf,21,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
L(χ−
i),L(Ωi). One can check that the resulting system is block-diagonal , with all blocks being tridiagonal
matrices. Once this system is solved, the functions χ−
i(t),Ωi(t) is found by using the inverse Laplace
transform.
Consider the heat equation in a strip with a piecewise consta nt thermal conductivity coeﬃcient to
illustrate our approach.
4.1 The heat equation in a strip
Consider the following problem for the heat equation with a p iecewise constant thermal conductivity
coeﬃcient (the ML problem):
∂
∂x/parenleftbigg
σ2(x)∂U
∂x/parenrightbigg
=∂U
∂t, (x,t)∈[y0,yN]×R+ (77)
U(t,y0) = 0, U (t,yN) = 0.
U(0,x) =δ(x−x0).
Here the thermal conductivity coeﬃcient σ(x) is a piecewise constant function of x, which changes
from layer to layer
σ(x) =N/summationdisplay
i=11(yi−1<x≤yi)σi, (78)
y0<y 1<y 2<...<y i<...<y N.
andδ(x) denotes the Dirac delta function. As before yiare the boundaries of the layers in xspace.
Without loss of generality, we assume that x0∈[yj−1,yj),1<j <N .
Due to the initial condition in Eq. (77) the solution of this p roblem is Green’s function for Eq. (77).
We represent the solution U(t,x) in the form
U(t,x) =N/summationdisplay
i=11(yi−1<x≤yi)[Ui(t,x) +Hi(t,x)], (79)
where the functions Ui(t,x) andHi(t,x) solve the following problems
∂
∂x/parenleftbigg
σ2
i∂Ui
∂x/parenrightbigg
=∂Ui
∂t, (x,t)∈(yi−1,yi]×R+, (80)
limx→yi−1Ui(t,x) =χ−
i(t), U i(t,yi) =χ+
i(t),
U(0,x) = 0,
and
∂
∂x/parenleftbigg
σ2
i∂Hi
∂x/parenrightbigg
=∂Hi
∂t, (x,t)∈(yi−1,yi]×R+, (81)
limx→yi−1Hi(t,x) = 0, H i(t,yi) = 0,
H(0,x) =δ(x−x0).
A well-known physical argument shows that the solution and i ts ﬂux must be continuous at the layers’
boundaries. The ﬁrst condition yields
U1(t,y0) = 0, (82)
Page 21 of 36",2021-02-16T18:22:12Z,multiplayer one once place consir t consir re dfrac as without due eq greeeq  ui hi ui hi ui ui ui hi hi hi t page
paper_qf_37.pdf,22,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
limx→yiUi(t,x) =Ui+1(t,yi), i= 1...N−1,
limx→yNUN(t,x) = 0.
According to (Itkin and Muravey, 2020b), the function H(t,x)/ne}ationslash= 0 only at that interval which contains
the pointx0, i.e. [yj−1,yj). Therefore, the ﬂux continuity conditions could be writte n as
limx→yiσ2
i∂Ui
∂x(t,yi) =σ2
i+1∂Ui+1
∂x(t,yi), i /ne}ationslash=j−1,j, (83)
limx→yj−1σ2
j−1∂Uj−1
∂x(t,yj−1) =σ2
j∂Uj
∂x(t,yj−1) +σ2
j∂Hj
∂x(t,yj−1),
limx→yj/bracketleftbigg
σ2
j∂Uj
∂x(t,yj) +σ2
j∂Hj
∂x(t,yj)/bracketrightbigg
=σ2
j+1∂Uj+1
∂x(t,yj).
It follows from Eq. (82) that
χ−
1(t) =χ+
N(t) = 0, χ+
i(t) =χ−
i+1(t), i= 1,...,N −1. (84)
To simplify the notation let us introduce new functions fi(t), such as
χ+
i(t) =χ−
i+1(t) =fi(t), i= 0,...,N, (85)
so obviously f0(τ) =fN+1(τ) = 0. Using Eq. (21) (or Eq. 3.33 in (Itkin and Muravey, 2020b) ), one can
get an explicit representation for the derivatives of U(t,x) at each interval
∂Ui
∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=yi=fi(t)
σi√
πt−/integraldisplayt
0fi(s)−fi(t)
2σi/radicalig
π(t−s)3ds+/integraldisplayt
0/bracketleftig
fi−1(s)λ+
i(t|yi−1,s)−fi(s)λ+
0,i(t|yi,s)/bracketrightig
ds, (86)
∂Ui+1
∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=yi=−fi(t)
σi+1√
πt+/integraldisplayt
0fi(s)−fi(t)
2σi+1/radicalbig
π(t−s)3ds+/integraldisplayt
0/bracketleftig
fi(s)λ−
0,i+1(t|yi,s)−fi+1(s)λ−
i+1(t|yi+1,s)/bracketrightig
ds.
Here
λ−
i(t|ξ,s) =∞/summationdisplay
n=−∞e−(yi−ξ+2nli)2
4σ2
i(t−s)
2σ3
i/radicalbig
π(t−s)3/bracketleftigg
1−(yi−ξ+ 2nli)2
2σ2
i(t−s)/bracketrightigg
=−π2
4l3
iθ′′
3/bracketleftbiggπ(yi−ξ)
2li,qi(s)/bracketrightbigg
, (87)
λ+
i(τ|ξ,s) =∞/summationdisplay
n=−∞e−(yi−ξ+(2n+1)li)2
4σ2
i(t−s)
2σ3
i/radicalbig
π(t−s)3/bracketleftigg
1−(yi−ξ+ (2n+ 1)li)2
2σ2
i(t−s)/bracketrightigg
=−π2
4l3
iθ′′
3/bracketleftbiggπ(yi+li−ξ)
2li,qi(s)/bracketrightbigg
,
λ−
0,i(t|ξ,s) =λ−
i(t|ξ,s)−e−(yi−1−ξ)2
4σ2
i(t−s)
2σ2
i/radicalbig
π(t−s)3/bracketleftigg
1−yi−1−ξ
2σ2
i(t−s)/bracketrightigg
,
λ+
0,i(t|ξ,s) =λ+
i(t|ξ,s)−e−(yi−ξ+li)2
4σ2
i(t−s)
2σ2
i/radicalbig
π(t−s)3/bracketleftigg
1−(yi−ξ+li)2
2σ2
i(t−s)/bracketrightigg
.
qi(s) =e−π2σ2
i(t−s)/l2
i.
Hereθi(z,p), i= 2,3 are the Jacobi theta functions of the second and third kind, (Mumford et al., 1983),
andθ′′
3(z,p) is the second derivative of θ3(z,p) on the ﬁrst argument.
Page 22 of 36",2021-02-16T18:22:12Z,multiplayer ui ui accordi it ki very trefore ui ui   hj  hj  it eq to usi eq eq it ki very ui ui re re jacob mum ford page
paper_qf_37.pdf,23,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
Substituting ξ=yi−1,yi,yi+1into Eq. (87) we obtain
λ−
i+1(t|yi+1,s) =−π2
4l3
i+1θ′′
3(0,qi+1(s)), (88)
λ+
i(τ|yi−1,s) =−π2
4l3
iθ′′
3(π,qi(s)),
λ−
0,i+1(t|yi,s) =∞/summationdisplay
n=−∞
n/negationslash=0e−(2nli+1)2
4σ2
i+1(t−s)
2σ3
i+1/radicalbig
π(t−s)3/bracketleftigg
1−(2nli+1)2
2σ2
i+1(t−s)/bracketrightigg
,
λ+
0,i(t|yi,s) =−π2
4l3
iθ′′
3(π,qi(s))−e−l2
i
4σ2
i(t−s)
2σ2
i/radicalbig
π(t−s)3/bracketleftigg
1−l2
i
2σ2
i(t−s)/bracketrightigg
.
Since ats→twe haveq(s)→1, all RHSs in Eq. (88) are regular in this limit and vanish. Th e
latter is due to the fact that lim q→1θ′′
3(0,q) =θ′′
3(π,q) = 0. Therefore, all integral kernels in Eq. (86)
are regular. We also assume that functions fi(s) are smooth enough, so that in the limit all the integrals
vanish.
Applying integration by parts to Eq. (86), we get the followi ng simpliﬁed system
∂Ui
∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=yi−1=−/integraldisplayt
0/bracketleftig
ηeven
i(t,s)d(fi−1(s))−ηodd
i(t,s)d(fi(s))/bracketrightig
(89)
−fi−1(0)
σi√
πt−fi−1(0)ηeven
i(t,0) +fi(0)ηodd
i(t,0),
∂Ui
∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=yi=−/integraldisplayt
0/bracketleftig
ηodd
i(t,s)d(fi−1(s))−ηeven
i(t,s)d(fi(s))/bracketrightig
+fi(0)
σi√
πt+fi(0)ηeven
i(t,0)−fi−1(0)ηodd
i(t,0),
where
ηeven
i(t−s) =1
σi/radicalbig
π(t−s)∞/summationdisplay
n=−∞e−(2nli)2
4σ2
i(t−s), ηodd
i(t−s) =1
σi/radicalbig
π(t−s)∞/summationdisplay
n=−∞e−((2n+1)li)2
4σ2
i(t−s). (90)
Indeed, since η−
i(t|yi−1,t) = 0, η−
i(t|yi,t) = 0, fi−1(0) = 0, fi(0) = 0 we have
−fi−1(t)
σi√
πt+/integraldisplayt
0fi−1(s)−fi−1(t)
2σi/radicalbig
π(t−s)3+/integraldisplayt
0/bracketleftig
fi−1(s)d/parenleftig
η−
i(t|yi−1,s)/parenrightig
−fi(s)d/parenleftig
η−
i(t|yi,s)/parenrightig/bracketrightig
(91)
=−fi−1(t)
σi√
πt−fi−1(t)
σi/radicalbig
π(t−s)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingles=t
s=0+fi−1(s)
σi/radicalbig
π(t−s)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingles=t
s=0−/integraldisplayt
01
σi/radicalbig
π(t−s)d(fi−1(s)) +
+fi−1(s)η−
i(t|yi−1,s)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingles=t
s=0−fi(s)η−
i(t|yi,s)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingles=t
s=0−/integraldisplayt
0/bracketleftig
η−
i(t|yi−1,s)d(fi−1(s))−η−
i(t|yi,s)d(fi(s))/bracketrightig
=−/integraldisplayt
0/bracketleftigg/parenleftigg
η−
i(t|yi−1,s) +1
σi/radicalbig
π(t−s)/parenrightigg
d(fi−1(s))−η−
i(t|yi,s)d(fi(s))/bracketrightigg
=−/integraldisplayt
0/bracketleftig
ηeven
i(t,s)d(fi−1(s))−ηodd
i(t,s)d(fi(s))/bracketrightig
.
Page 23 of 36",2021-02-16T18:22:12Z,multiplayer substituti eq since eq th trefore eq  applyi eq ui ui ined page
paper_qf_37.pdf,24,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
In turn, as shown in (Itkin and Muravey, 2020b), Eq. 3.31, the functionHj(t,x) can be represented
as follows
Hj(t,x) =1
2σj√
πt∞/summationdisplay
n=−∞
e−(2nlj+x−x0)2
4σ2
jt−e−(2nlj+x+x0−2yj−1)2
4σ2
jt
. (92)
Hence, the gradients at the boundaries are
∂Hj(t,x)
∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=yj−1≡υ−(t|x0,0) (93)
=∂
∂x/braceleftigg
1
2ljθ3/bracketleftigg
π(x−x0)
2lj,qj(0)/bracketrightigg
−1
2ljθ3/bracketleftigg
π(x+x0−2yj−1)
2lj,qj(0)/bracketrightigg/bracerightigg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=yj−1
=1
4l2
j/braceleftigg
θ′
3/bracketleftigg
π(yj−1−x0)
2lj,qj(0)/bracketrightigg
−θ′
3/bracketleftigg
π(x0−yj−1)
2lj,qj(0)/bracketrightigg/bracerightigg
,
∂Hj(t,x)
∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=yj=1
4l2
j/braceleftigg
θ′
3/bracketleftigg
π(yj−x0)
2lj,qj(0)/bracketrightigg
−θ′
3/bracketleftigg
π(x0−yj)
2lj,qj(0)/bracketrightigg/bracerightigg
.
Thus, we obtain the following system of Volterra equations
/integraldisplayt
0/bracketleftigg
−σ2
iηodd
i(t−s)d(fi−1(s)) +/parenleftig
σ2
iηeven
i(t−s) +σ2
i+1ηeven
i+1(t−s)/parenrightig
d(fi(s)) (94)
−σ2
i+1ηodd
i+1(t−s)d(fi+1(s))/bracketrightigg
=hi(t),
hi(t) = 0,i/ne}ationslash=j−1,j, h j−1(t) =σ2
jυ−(t|x0,0), h j(t) =−σ2
jυ+(t|x0,0).
Since the kernels depend only on t−sone can rewrite the above equations as a convolution
/parenleftig
−σ2
iηodd
i(·)∗f′
i−1(·) +/bracketleftig
σ2
iηeven
i(·) +σ2
i+1ηeven
i+1(·)/bracketrightig
∗f′
i(·)−σ2
i+1ηodd
i+1(·)∗f′
i+1(·)/parenrightig
(t) =hi(t). (95)
4.2 The Laplace transform
Applying the Laplace transform to Eq. (95), we get
√
λ/parenleftig
−σ2
iL(ηodd
i)L(fi−1) +/parenleftig
σ2
iL(ηeven
i) +σ2
i+1L(ηeven
i+1)/parenrightig
L(fi)−σ2
i+1L(ηodd
i+1)L(fi+1)/parenrightig
=L(hi)√
λ,(96)
or, in the matrix form
Mg=σ2
j√
λ/bracketleftig
L(υ−(t|x0,0))1j−1−L(υ+(t|x0,0))1j/bracketrightig
. (97)
Here 1jdenotes the indicator vector, i.e.
1j=
0,0,..0/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
j−1,1,0,...0
⊤
, (98)
the vector gis the column vector
g= (Lf1,...,LfN−1)⊤, (99)
Page 24 of 36",2021-02-16T18:22:12Z,multiplayer iit ki very eq hj hj nce hj hj  volterra since t place applyi place eq mg re lf lf page
paper_qf_37.pdf,25,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
and the matrix Mis a symmetric tridiagonal matrix
M=
D1−β1
−β1D2−β2
−β2......
...... −βN−2
−βN−2DN−1
, (100)
Coeﬃcients of the matrix Mhave the form
βi=√
λσ2
i+1L(ηodd
i+1), D i=√
λ/bracketleftig
σ2
iL(ηeven
i) +σ2
i+1L(ηeven
i+1)/bracketrightig
, (101)
and can be found explicitly, see Appendix C
L(ηeven
i) =1
σi√
λcoth/parenleftigg√
λli
σi/parenrightigg
, L(ηodd
i) =1
σi√
λ1
sinh/parenleftig√
λli
σi/parenrightig, (102)
L(υ−(t|x0,0)) =1
σ2
jsinh/parenleftbigg
(yj+1−x0)√
λ
σj/parenrightbigg
sinh/parenleftbigg
lj√
λ
σj/parenrightbigg,L(υ+(t|x0,0)) = −1
σ2
jsinh/parenleftbigg
(x0−yj)√
λ
σj/parenrightbigg
sinh/parenleftbigg
lj√
λ
σj/parenrightbigg.
Finally, introducing the notation
ωi=li
σi, γ 1=yj+1−x0
lj, γ 2=x0−yj
lj, (103)
the system Eq. (97) can be represented as
Mg=1√
λ
sinh/parenleftig
γ1ωj√
λ/parenrightig
sinh/parenleftig
ωj√
λ/parenrightig1j−1+sinh/parenleftig
γ2ωj√
λ/parenrightig
sinh/parenleftig
ωj√
λ/parenrightig1j
, (104)
Di=σicoth/parenleftig
ωi√
λ/parenrightig
+σi+1coth/parenleftig
ωi+1√
λ/parenrightig
, β i=σi+1
sinh/parenleftig
ωi+1√
λ/parenrightig.
5 Numerical experiments
In this section, we solve the problem in Eq. (77) by using the M L method. Note that such problems
appear both in physics and in ﬁnance. A simple example of a ﬁna ncial problem is ﬁnding Green’s function
for pricing double barrier options written on the underlyin gStwith local volatility
dSt=σ(St)dWt, (105)
whereσ(S) is a piecewise constant function. Below we describe two num erical experiments.
5.1 Constant volatility σi
To start with, we assume that σi=const, i = 1,...,N , and hence, σ2
iin Eq. (77) can be pulled out of the
derivative in x. This problem has an analytic solution, see (Lipton, 2001) a nd references therein, which
Page 25 of 36",2021-02-16T18:22:12Z,multiplayer mis coe he  nally eq mg di numerical ieq note greest with st st  below constant to eq  tepage
paper_qf_37.pdf,26,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
can be represented as the Fourier series. Re-writing it by us ing the deﬁnition of Jacobi theta functions
yields
U(T,y) =1
2l/bracketleftbigg
θ3/parenleftbiggπ(y−x0)
2l,q/parenrightbigg
−θ3/parenleftbiggπ(y+x0−2y0)
2l,q/parenrightbigg/bracketrightbigg
, (106)
l=yN−y0, q =e−π2σ2
l2T.
To solve this problem, we need ﬁrst to solve the linear system in Eq. (104) numerically, and then use
the corresponding Laplace images to ﬁnd the function fi=f(yi), i= 1,...,N by applying an inverse
Laplace transform. For the latter step, we use the Gaver-Ste hfest method
f(T,y) = Λ[m]/summationdisplay
s=1St(m)
sL(fi(Λ)),Λ =log 2
T. (107)
This algorithm was widely studied (see, e.g., (Kuznetsov, 2 013) and references therein), and, provided
that the resulting function is non-oscillatory, converges very quickly. For instance, choosing m= 12 terms
in the series representing the solution is usually suﬃcient . The coeﬃcients Stscan be found explicitly in
advance.
Table 1: Parameters of the test.
y0yNσTNm
-1.0 1.0 0.5 1.0 20 16
The model parameters for this test are given in Table 1, and th e results are depicted in Fig. 2a. Here,
the left vertical axis shows the values of yi(T), and the right vertical axis shows the relative error (in
percent) of the solution compared with the analytic one.
-1 -0.5 0 0.5 1
y00.10.20.30.40.50.6Valie of the Green Function
-0.015-0.01-0.00500.0050.010.015
Rel difference, %Analytic
ILT
Diff
(a)-1 -0.5 0 0.5 1
y00.050.10.150.20.250.30.350.40.450.5Value of the Green Function
-0.08-0.06-0.04-0.0200.020.04
Rel difference, %The ML vs FD (N=41, M=40) methods
Analytic
FD
ILT
DifILT
DifFD
(b)
Figure 2: Comparison of the Analytic and ML solutions (a), and Analyti c, ML and FD
solutions (grid with 41×40nodes) (b) for σi= 0.5,T= 1. Here Analytic denotes the analytic
solution of the problem, ILT - the ML solution, FD - the FD solu tion, DiﬀILT - the relative
error of the ML solution with respect to the analytic one, Dif FD - same for the FD method.
Page 26 of 36",2021-02-16T18:22:12Z,multiplayer courier re jacob to eq place place for ge ste st  ku nets ov for t sts catable meters nm t table  re valid greefunrel analytic df value greefunrel t analytic    arisoanalytic aall ti re analytic di  page
paper_qf_37.pdf,27,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
The ML solution coincides with the analytic one with high acc uracy. The elapsed time of the experi-
ment is 8.8 ms (we run our code, written in Matlab, on a PC with t wo Quadcore CPU Intel i7-4790 3.60
GHz). The elapsed time doesn’t depend on the option maturity T, so the calculation is fast even for long
maturities. Note that the ML solution’s computation takes o nly 2.3 ms, while the remaining time is used
for computing the Gaver-Stehfest coeﬃcients (but those can be precomputed if so desired). Since the
matrix Mis tridiagonal, the ML method’s complexity is O(mN). By comparison, the complexity of the
ﬁnite diﬀerence (FD) method is O(MN),Mis the number of time steps. Obviously, for long maturities
M≫m, so the FD method is slower.
To validate this, we also implemented an FD method to solve th e same problem. The FD solver runs
on a uniform grid and is a Crank-Nicolson scheme after four st eps, while for the ﬁrst four steps it uses an
implicit Euler scheme. In other words, we start with four Ran acher steps, see (Itkin, 2017) and references
therein.
To have the same spatial approximation in y, we need to run both the ML and FD methods with
the same number of nodes. While the ML method provides an accu rate result even at N= 10, the FD
method fails and needs at least 40 nodes to converge to the sol ution. Therefore, we choose N= 41. The
same is true for the time step, so the minimal number of FD time steps in our experiment is M= 40,
while the ML method provides the solution at t=Tjust at once. The results of the comparison of both
methods with the analytic solution are given in Fig. 2b. Agai n, the left vertical axis shows the values
ofyi(T), and the right vertical axis is the relative error (in perce nt) of the solution compared with the
analytic one.
It is clear that the accuracy of the FD method is worse than tha t of the ML method. By increasing
NandM, one can improve the FD method’s accuracy, but it takes time. The elapsed time for the FD
method with N= 41,M= 40 is 41 ms, so it is about 18 times slower than the ML method. O bviously,
for longer maturities, more time-steps are necessary, so th e FD method becomes even slower.
It is also known that for small Tand volatilities, the FD method’s error increases. To illus trate this
fact, we run the same experiment, but now with T= 0.5, σ= 0.3. The results are presented in Fig. 3.
-1 -0.5 0 0.5 1
y00.20.40.60.811.2Value of the Green Function
-1-0.8-0.6-0.4-0.200.20.40.60.8
Rel difference, %The ML vs FD (N=41, M=40) methods
Analytic
FD
ILT
DifILT
DifFD
(a)-1 -0.5 0 0.5 1
y00.20.40.60.811.2Value of the Green Function
-0.35-0.3-0.25-0.2-0.15-0.1-0.0500.05
Rel difference, %The ML vs FD (N=151, M=150) methods
Analytic
FD
ILT
DifILT
DifFD
(b)
Figure 3: Comparison of the Analytic, ML and FD solutions for σi= 0.3,T= 0.5. Here
Analytic denotes the analytic solution of the problem, ILT - the ML solution, FD - the FD
solution, DiﬀILT - the relative error of the ML solution with respect to the analytic one,
DifFD - same for the FD method.
Page 27 of 36",2021-02-16T18:22:12Z,multiplayer t t mat lab quad core intel  t note ge teh  since mis by mis obvusly to t ank nico soruler irait kito w trefore t just t  articial intellence it by and t it and to t  value greefunrel t analytic   value greefunrel t analytic    arisoanalytic re analytic di  page
paper_qf_37.pdf,28,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
Fig. 3a shows that the accuracy of the ML method is still good w ith the same number of internal
layersN= 40, while the error of the FD method with N= 41,M= 40 is quite signiﬁcant. The error can
be reduced by running the FD method with N= 151,M= 150 (see Fig. 3b); however, the corresponding
elapsed time is 86 ms. Thus, for the same accuracy, the ML meth od is 37 times faster.
5.2 Piecewise constant volatility σi
Hereσ(x) is a piecewise constant function deﬁned in Eq. (78). In this case, there is no analytic solution of
the problem8, hence as a benchmark, we use an FD method, namely the same FD s olver as in the previous
experiment. However, since we solve the problem in Eq. (77), an FD scheme has to be implemented for
the conservative heat equation. While such an implementati on is possible, we prefer to rewrite Eq. (80)
and Eq. (81) in a non-divergent form. For instance,
∂Ui
∂t= Ξ2
i(x)∂2Ui
∂x2+∂Ξ2(x)
∂x∂Ui
∂x, (108)
Ξi(x) =σi[ΘH(xi+1−x)−ΘH(xi−x)],
where Θ H(x) is the Heaviside theta function, (Abramowitz and Stegun, 1 964) with Θ H(0) = 1. Accord-
ingly, on the interval x∈(yi,yi+1] we have
∂Ξ2(x)
∂x=σ2
i+1[δ(xi−x)−δ(xi+1−x)]. (109)
At the point x=xi+1this gives∂Ξ2(x)/∂x=−σ2
i+1δ(0). In turn, δ(0) can be numerically approximated
as
δ(0) =2
yN−y0, (110)
which provides the correct normalization of the Dirac delta function. Indeed, the integral over the interval
[y0,yn] of the test function equal to 1 at x=yi+1and 0 otherwise computed by using a trapezoidal rule is
equal to (yN−y0)/2. Therefore, we need to use Eq. (110) to provide the correct n umerical normalization.
Table 2: Parameters of the second experiment.
y0yNTNm M
-1.0 4.0 2.0 5016 100
In this experiment, we use parameters of the model given in Ta ble 2, and the piecewise constant
volatilityσi, which is deﬁned as follows
σi(s) =e−i/N, s ∈(yi,yi+1], i= 1,...,N. (111)
8Based on Eq. (104) it is possible to derive an explicit series representation of the solution. It will be published elsewh ere.
Page 28 of 36",2021-02-16T18:22:12Z,multiplayer  t   piecewise re eq ieq w eq eq for ui ui ui isi  with ste guaccord at idfrac ined trefore eq table meters nm ita based eq it page
paper_qf_37.pdf,29,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
-1 0 1 2 3 4
y00.050.10.150.20.250.3Value of the Green Function
-16-14-12-10-8-6-4-2024
Rel difference, %The ML vs FD (N=51, M=100) methods
FD
ILT
Dif
(a)-1 0 1 2 3 4
y00.050.10.150.20.250.3Value of the Green Function
-0.5-0.4-0.3-0.2-0.100.10.2
Rel difference, %The ML vs FD (N=201, M=100) methods
FD
ILT
Dif
(b)
Figure 4: Comparison of the ML and FD solutions for a piecewise constan tσ(x). Here ILT
denotes the ML solution, FD - the FD solution, Dif - the relati ve error of the FD solution
with respect to the ML one.
The results of the test are presented in Fig. 4a. Again, the nu mber of nodes for the ML and FD
methods is the same. The diﬀerence between the two solutions reaches 16% at the right external boundary,
4% at the left external boundary, and changes in this range in between. The elapsed time is 2.2 ms for
the ML and 50 ms for the FD methods.
To check the convergence of the solution, we rerun the calcul ation with N= 200. The results are
presented in Fig. 4b. The relative error εdrops down to be in ε∈[−0.6,0.2] percent. The elapsed time
is 5 ms for the ML and 88 ms for the FD methods. Note that in this c ase, to reduce the error, we also
need to increase the number of layers for the ML method. This e ﬀect is explained in Section 6; it is due
to the non-smoothness of σin this experiment.
The physics meaning of the obtained results is as follows. Su ppose we consider diﬀusion rather than
heat conduction. According to Eq. (111) the diﬀusion coeﬃci entσ2(x) is a decreasing function of xwhen
moving from y0toyN. Since we request continuity of the ﬂux at the internal bound aries, the gradient
of the solution increases with xwhen moving from left to right. The maximum of the solution, w hich is
located atx=x0whent= 0, travels to the right when tincreases. Recall that the solution is the Green
function of our problem. This behavior was also observed in ( Lançon et al., 2001), where the authors
studied particles trapped between two nearly parallel wall s making their conﬁnement position dependent.
They not only measured a diﬀusion coeﬃcient which depended o n the particles’ position but also reported
and explained a new eﬀect: a drift of the particles individua l positions (so change in concentration) in
the direction of the diﬀusion coeﬃcient gradient, in the abs ence of any external force or concentration
gradient.
6 Discussion
In the previous section, we have demonstrated that the ML met hod’s complexity is linear in N. The same
is true in the general case because, as mentioned at the end of Section 4, from Eq. (21) we obtain a linear
system of equations for L(χ−
i),L(Ωi) which has a block diagonal matrix with all blocks being trid iagonal
matrices. Therefore, the ML method’s complexity remains li near and approximately is O(4mN), so it
Page 29 of 36",2021-02-16T18:22:12Z,multiplayer value greefunrel t  value greefunrel t   arisore  t  ag articial intellence t t to t  t t note  set su accordi eq since t recall gree laty discussit seeq trefore page
paper_qf_37.pdf,30,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
doesn’t depend on T. Hence, if 4 mis of the order of M, the ML and the FD methods have the same
complexity. For typical values m= 12 we have 4 m= 48. Therefore, for short maturities T < 1 year,
both methods’ complexity is roughly the same. However, our m ethod has an obvious advantage for the
long maturities occurring in the Fixed Income context.
The ML method has some other advantages as well. First, the FD construction provides only the
values of the unknown function at the grid nodes in space, and at intermediate points they can be
found only by interpolation. In contrast, using the ML metho d, we obtain an analytic representation
of the solution at any x(once the values at the layers’ boundaries are found). Secon d, the Greeks, i.e.,
derivatives of the solution, can be expressed semi-analyti cally by diﬀerentiating the solution with respect
toxor some parameter of the model and performing numerical inte gration, provided that the values
at the internal boundaries are found. For the FD method, the G reeks can be found only numerically.
Moreover, to compute the Vega, a new run of the FD method is req uired, while for the ML method, all
Greeks can be calculated in one go, as described.
As far as an approximation with respect to xis concerned, the following observation holds. Using the
ML method, we obtain an analytical solution at every interva li, i= 1,...,N . However, to do this, we
need to approximate the corresponding coeﬃcient, e.g., σ(x) over layers by piecewise constant or linear
functions. For the linear approximation, the solution’s ac curacy isO((∆x)2), i.e., same as for the FD
method of the second order. Therefore, it seems that the spat ial accuracies of both ML and FD methods
are the same.
On the other hand, the error of both methods is also proportio nal to the second derivative. For the
FD method, this is the second derivative of the solution; for the ML method - the second derivative of
the coeﬃcient, e.g., σx,x(x). If the latter is smaller than the second derivative of the s olution (say, the
option Gamma), then the number of layers Ncan be decreased while providing the same accuracy. This
reduction provides an additional speedup of our method as co mpared with the FD method. This fact
is illustrated by our ﬁrst experiment where function σ(x) is smooth, so even a small number of layers
is suﬃcient to obtain a very accurate solution. In the second experiment, σ(x) jumps at the layer’s
boundaries, and, therefore, one needs to increase the numbe r of layers to provide the same accuracy.
Note that for the FD method, the diﬃculties caused by sharp gr adients can be alleviated by using
nonuniform grids where the nodes are condensed in the area wh ere gradients are high. The same approach
could be applied to the construction of internal layers in th e ML method.
Overall, we can conclude that the new ML method proposed in th is paper is signiﬁcantly faster than
the FD method, provides better accuracy, and represents the solution in a semi-analytical form. The
method’s speed is close to that for the Radial Basis Function s (RBF) approach, (Hon and Mao, 1999;
Fasshauer et al., 2004; Pettersson et al., 2008), while othe r properties listed above are superior to the
RBF.
Acknowledgments
We are grateful to Peter Carr for some fruitful discussions. Dmitry Muravey acknowledges support by
the Russian Science Foundation under the Grant number 20-68 -47030.
References
M. Abramowitz and I. Stegun. Handbook of Mathematical Functions . Dover Publications, Inc., 1964.
L.B.G. Andersen and V.V. Piterbarg. Interest Rate Modeling . Number v. 2 in Interest Rate Modeling.
Atlantic Financial Press, 2010. ISBN 9780984422111.
Page 30 of 36",2021-02-16T18:22:12Z,multiplayer nce for trefore xed income t rst isec ogreek for ovega greek as usi for trefore ofor  gaa ca  inote t ovll t radial basis funhomao as haber petersoackledgment  peter carr diary  very science foundatgrant referenc with ste guhandbook matmatical funns dopublicatns inc anrsosite rb arg interest rate moli number interest rate moli atlantic nancial  page
paper_qf_37.pdf,31,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
A. Antonov and M. Spector. General short-rate analytics. Risk, pages 66–71, 2011.
M Asvestas, A.G Sifalakis, E.P Papadopoulou, and Y.G Sarida kis. Fokas method for a multi-domain
linear reaction-diﬀusion equation with discontinuous diﬀ usivity. Journal of Physics: Conference Series ,
490(012143), 2014.
N. Bacaer. A short history of mathematical population dynamics , chapter 6, pages 35–39. Springer-Verlag,
London, 2011. ISBN 978-0-85729-114-1.
F. Black and P. Karasinski. Bond and option pricing when shor t rates are lognormal. Financial Analysts
Journal , pages 52–59, 1991.
D. Brigo and F. Mercurio. Interest Rate Models – Theory and Practice with Smile, Inﬂat ion and Credit .
Springer Verlag, 2nd edition, 2006.
L. Capriotti and B. Stehlikova. An Eﬀective Approximation f or Zero-Coupon Bonds and Arrow-Debreu
Prices in the Black-Karasinski Model. International Journal of Theoretical and Applied Finance , 17(6):
1650017, 2014.
E.J. Carr and N.G. March. Semi-analytical solution of multi layer diﬀusion problems with time-varying
boundary conditions and general interface conditions. Applied Mathematics and Computation , 333(15):
286–303, 2018.
P. Carr and A. Itkin. Geometric local variance gamma model. 2 7(2):7–30, 2019.
P. Carr and A. Itkin. An expanded local variance gamma model. 2 2020. doi: 10.1007/
s10614-020-10000-w.
P. Carr and A. Itkin. Semi-closed form solutions for barrier and American options written on a time-
dependent Ornstein Uhlenbeck process. Journal of Derivatives , Fall, 2021.
P. Carr and S. Nadtochiy. Local Variance Gamma and explicit c alibration to option prices. Mathematical
Finance , 27(1):151–193, 2017.
P. Carr, A. Itkin, and D. Muravey. Semi-closed form prices of barrier options in the time-dependent cev
and cir models. Journal of Derivatives , 28(1):26–50, 2020.
M. Craddock. Fundamental solutions, transition densities and the integration of Lie symmetries. Journal
of Diﬀerential Equations , 246:2538–2560, 2009.
C.J. Dias. A method of recursive images to solve transient he at diﬀusionin multilayer materials. 85:
1075–1083.
Bruno Dupire. Pricing with a smile. Risk, 7:18–20, 1994.
G. E. Fasshauer, A. Q. M. Khaliq, and D. A. Voss. Using meshfre e approximation for multi-asset American
option problems. J. Chinese Inst. Engrs. , 27(4):563–571, 2004.
J.S. Giet, P. Vallois, and S. Wantz-Mezieres. The logistic s de.Theory of Stochastic Processes , 20(36):
28–62, 2015.
Y. C. Hon and X. Z. Mao. A radial basis function method for solv ing options pricing model. Financial
Engineering , 8(1):31–49, 1999.
Page 31 of 36",2021-02-16T18:22:12Z,multiplayer antoseor genl risk as vests  lak is papa do poll ou arid ok as journal psics conference seriba car  verlag londo kara i bond nancial analysts journal br mercury interest rate mols tory praice sme iedit  verlag caps o teh li v aapproximatzero coupobonds arrow re pric kara i minternatnal journal toical applied nance carr marsemi applied matmatics tcarr it keometric carr it kiacarr it kisemi or nste ihlebeck journal rivativfall carr nad sochi local variance gaa matmatical nance carr it ki very semi journal rivativrad dock fundamental lie journal di equatns dias bruno dup ire prici risk as haber khaki vos usi chinese ist e rs gie val lois want mez ie rt tory stochastic processhomao nancial eineeri page
paper_qf_37.pdf,32,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
B Horvath, A. Jacquier, and C. Turfus. Analytic option price s for the black-karasinski short rate model,
2017. URL https://papers.ssrn.com/sol3/papers.cfm?abstract_id =3253833 . SSRN: 3253833.
J.C. Hull. Options, Futures, and Other Derivatives . Prentice Hall, 8rd edition, 2011.
A. Itkin. Pricing derivatives under Lévy models . Number 12 in Pseudo-Diﬀerential Operators. Birkhauser,
Basel, 1 edition, 2017.
A. Itkin. Fitting Local Volatility: Analytic and Numerical Approach es in Black-Scholes and Local Variance
Gamma Models . Number 11623. World Scientiﬁc Publishing Co. Pte. Ltd., 20 20.
A. Itkin and A. Lipton. Filling the gaps smoothly. Journal of Computational Sciences , 24:195–208, 2018.
A. Itkin and D. Muravey. Semi-closed form prices of barrier o ptions in the Hull-White model. Risk,
December 2020a.
A. Itkin and D. Muravey. Semi-analytic pricing of double bar rier options with time-dependent barriers
and rebates at hit, September 2020b. URL https://arxiv.org/abs/2009.09342 .
A. Itkin, A. Lipton, and D. Muravey. From the black-karasins ki to the verhulst model to accommodate
the unconventional fed’s policy, June 2020. URL https://arxiv.org/abs/2006.11976 .
E. M. Kartashov. Analytical methods for solution of non-sta tionary heat conductance boundary problems
in domains with moving boundaries. Izvestiya RAS, Energetika , (5):133–185, 1999.
E.M. Kartashov. Analytical Methods in the Theory of Heat Conduction in Solid s. Vysshaya Shkola,
Moscow, 2001.
A. Kuznetsov. On the convergence of the Gaver-Stehfest algo rithm. SIAM J. Numerical Analysis , 51(6):
2984–2998, 2013.
P Lançon, G Batrouni, L Lobry, and N Ostrowsky. Drift without ﬂux: Brownian walker with a space-
dependent diﬀusion coeﬃcient. Europhysics Letters (EPL) , 54(1):28–34, 2001.
A. Lejay. On the constructions of the skew brownian motion. Probability Surveys , 3:413–466, 2006.
J.H. Lienhard IV and J.H. Lienhard V. A Heat Transfer Textbook . Phlogiston Press, Cambridge, MA,
5th edition, 8 2019.
A. Lipton. Mathematical Methods For Foreign Exchange: A Financial Eng ineer’s Approach . World
Scientiﬁc, 2001.
A. Lipton and M.L. de Prado. A closed-form solution for optim al mean-reverting trading strategies. Risk,
June 2020.
A. Lipton and V. Kaushansky. On the ﬁrst hitting time density for a reducible diﬀusion process. Quan-
titative Finance, , 5, 2020a. published online.
A. Lipton and V. Kaushansky. On three important problems in m athematical ﬁnance. The Journal of
Derivatives. Special Issue , 28(2), 2020b.
A. Lipton and A. Sepp. Filling the gaps. Risk Magazine , pages 86–91, 10 2011.
D. Mumford, C. Musiliand M. Nori, E. Previato, and M. Stillma n.Tata Lectures on Theta . Progress in
Mathematics. Birkhäuser Boston, 1983. ISBN 9780817631093 .
Page 32 of 36",2021-02-16T18:22:12Z,multiplayer horvath jac er turf us analytic hull optns futurotr rivativpraice hall it kiprici number pseudo di oators bird user basel it kii local volatity analytic numerical approa schools local variance gaa mols number world cie nti pubhi co pte ltd it kitelli journal tnal sciencit ki very semi hull white risk cember it ki very semi september it kite very from june kart ash ov analytical iz vest iy ner get ika kart ash ov analytical methods tory at condusolid vy sha sh la mow ku nets ov oge teh  numerical analysis labat ro uni obey straw  drt brownish euro psics ters le jay oprobabity surveys liehard liehard at transfer textbook ph log is to cambridge tematmatical methods for foreexe nancial e approaworld cie nti terad risk june tekau sha oquanance tekau sha ot journal rivativspecial issue tesep lli risk magazine mum ford mus i and nor pre via to stl ma tata leura progress matmatics bird bostopage
paper_qf_37.pdf,33,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
O. A. Oleinik and E. V. Radkevich. Second order equations with non-negative characteristic f orm. Kluwer
Academic Publishers, 1973.
Ulrika Pettersson, Elisabeth Larsson, Gunnar Marcusson, a nd Jonas Persson. Improved radial basis
function methods for multi-dimensional option pricing. J. Comput. Appl. Math. , 222(1):82–93, 2008.
doi: 10.1016/j.cam.2007.10.038.
A.D. Polyanin. Handbook of linear partial diﬀerential equations for engin eers and scientists . Chapman
& Hall/CRC, 2002.
G. Pontrelli, M. Lauricella, J.A. Ferreira, and G. Pena. Ion tophoretic transdermal drug delivery: A
multi-layered approach. Mathematical Medicine and Biology , 00:1–18, 2016.
A.N. Tikhonov and A.A. Samarskii. Equations of mathematical physics . Pergamon Press, Oxford, 1963.
C. Turfus. Analytic swaption pricing in the black-karasins ki model, February 2020. URL https://
papers.ssrn.com/sol3/papers.cfm?abstract_id=3253866 . SSRN: 3253866.
P.F. Verhulst. Notice sur la loi que la population suit dans s on accroisseement. Correspondance mathe-
matique et physique , 10:113–121, 1838.
Appendices
A Transformation of a non-divergent heat equation to a diver gent form
Consider the PDE in Eq. (30) which is a divergent form of the he at equation
∂U(t,x)
∂t=∂
∂x/parenleftbigg
Ξ2(x)∂U(t,x)
∂x/parenrightbigg
. (A.1)
In this Section we show how to transform it to a non-divergent form as in Eq. (28) when the external
boundaries are constant, i.e. y0(t) =χ−(t) =const, y N(t) =χ+(t) =const . We start with making a
change of variables x/ma√sto→z=f(x) with
f(x) =c1+c2/integraldisplayx
01
Ξ2(k)dk, (A.2)
wherec1,c2are some constants. This transformation reduces Eq. (30) to
∂U(t,z)
∂t=σ2(z)∂2U(t,z)
∂z2, (A.3)
σ(z) =Ξ(x(z))
x′(z)=c1
Ξ(x(z)).
The Eq. (A.3) is a non-divergent form of the heat equation. Th e only thing which remains to be done
is ﬁnding the dependence x(z). Obviously, it solves the equation
z=f(x) =c2+c1/integraldisplayx
01
Ξ2(k)dk. (A.4)
Page 33 of 36",2021-02-16T18:22:12Z,multiplayer ole iik rake risecond  acamic pubrs ul rica petersoelizabeth larsogunner marcus sojonas soimproved com put ppl math poly aihandbook chapmahall pont re lli laura cell  pena matmatical medicine blogy thoov sama  equatns gamo oxford turf us analytic ruary hul st notice correspond ance appendiransformatconsir eq iseeq   eq t eq th obvusly page
paper_qf_37.pdf,34,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
Given Ξ(x), it can be solved either numerically (so this dependence ca n be precomputed), or in some
cases analytically. As an example, assume that Ξ( x) =e−ax, a=const /ne}ationslash= 0, and also let c2= 0. Then,
x=1
alog/parenleftbigg
1 +a
c1z/parenrightbigg
, (A.5)
σ2(z) =c1(c1+az).
Reverting these steps, we obtain the inverse transformatio n from a non-divergent heat equation to a
divergent one.
Also, the second continuity condition for Eq. (A.1) (an equa lity of ﬂuxes over the boundary) is given
by Eq. (7) which, by using our notation in this Section, can be re-written as
Ξ2
i(yi+1)∂Ui
∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=yi+1= Ξ2
i+1(yi+1)∂Ui+1
∂x/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
x=yi+1, i= 1,...,N −1. (A.6)
Using Eq. (A.3) this can be transformed to
∂Ui
∂z/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
z=z(yi+1)=∂Ui+1
∂z/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
z=z(yi+1), i= 1,...,N −1, (A.7)
z(yi+1) =c2+c1/integraldisplayyi+1
01
Ξ2(k)dk=c2+1
c1/integraldisplayyi+1
0σ2(k)dk.
This is the continuity condition for Eq. (A.3).
B Multilayer method for time-inhomogeneous coefﬁcients an d the do-
main
In this section we generalize the ML method to the case σ=σ(τ,x). We again consider the initial-
boundary problem Eq. (4) for the diﬀerential operator Liof the form Eq. (8) where now each operator
Lireads
Li=−∂
∂τ+σ2
i(τ,x)∂2
∂x2. (B.1)
As before, we look for the solution of the problem Eq. (4) in th e form Eq. (6) such that the conditions
Eq. (7) still hold. Our goal is to show that under certain assu mptions the problem Eq. (4) with time and
space dependent volatility σ(τ,x) can be reduced to the corresponding time-homogeneous prob lem.
Suppose that for each sub-domain Ω iwe can construct a map Mitransforming Eq. (B.1) into PDE
of the form Eq. (9). Then the solution of the transformed PDE c an be represented in the form of the
heat potential Eq. (15), and then transformed back by invert ing the map Mi. More precisely, consider a
collection of maps {M i}N
i=1acting on triplets ( τ,x,u i(τ,x))
(τ,x,u i(τ,x))Mi/ma√sto− − → (Ti(τ),Xi(τ,x),Ui(Ti,Xi)),Ti(0) = 0, (B.2)
such that the function Ui(Ti,Xi) solves the following PDE with time-independent coeﬃcient s
−∂Ui
∂Ti+A2(Xi)∂2Ui
∂X2
i= 0. (B.3)
Also, let us denote the inverse map as Υ i(τ,x), such that the following representation holds
ui(τ,x) = Υ i(τ,x)Ui(Xi(τ,x),Ti(τ)). (B.4)
Page 34 of 36",2021-02-16T18:22:12Z,multiplayer giveas treverti also eq eq seui ui usi eq ui ui  eq multiplayer i eq li of eq li reads li as eq eq eq our eq suppose mi transformi eq eq teq mi  mi ti xi ui ti xi ti ui ti xi ui ti xi ui also ui xi ti page
paper_qf_37.pdf,35,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
The map Mitransforms the sub-domain Ω ito the sub-domain Ξ i
Ξi:/bracketleftig
Y−
i(Ti),Y+
i(Ti)/bracketrightig
×R+
bounded by the curves Y−
i(Ti) and Y+
i(Ti) which are deﬁned as
Y−
i(Ti) =Xi(λi(Ti),yi(λi(Ti))),Y+
i(Ti) =Xi(λi(Ti),yi+1(λi(Ti))).
Hereλiis the inverse map T−1
i, i.e.λi(Ti) =τ(Ti) =T−1
i. Since the new time variables Tiare diﬀerent
for each layer Ξ ithe transformed boundaries are diﬀerent as well, i.e., Y+
i(Ti)/ne}ationslash=Y−
i+1(Ti+1). Also, the
initial value function f(x) is transformed to the function Fi
f(x)Mi− − → F i(Xi), Fi(Xi) =f(ηi(Xi))/Υi(ηi(Xi)),
whereηi(x) solves the equation
ηi(x) :Xi(0,ηi(x)) =x.
Since the equations Eq. (B.3) are time-homogeneous, we can r epresent their solutions in the form of
Eq. (15)
Ui(Ti,Xi) =/integraldisplayTi
0/braceleftigg
Φi(k)∂G(Xi,ξ,Ti−k)
∂ξ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
ξ=Y−
i(k)+ Ψ i(k)∂G(Xi,ξ,Ti−k)
∂ξ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
ξ=Y+
i(k)/bracerightigg
dk. (B.5)
Then making the inversion in Eq. (B.4), applying the chain ru le
∂ui
∂x=Ui(Ti(τ),Xi(τ,x))∂Υ(τ,x)
∂x+ Υ(τ,x)∂Xi(τ,x)
∂x∂Ui(Ti(τ),X)
∂X/vextendsingle/vextendsingle/vextendsingle/vextendsingle
X=Xi(τ,x),
and taking into account the discontinuity of the layer poten tials on the boundaries, we arrive at the
system of Volterra equations in Eq. (7).
The map Eq. (B.2) can be explicitly found via two diﬀerent app roaches. The ﬁrst is by application
of Lie symmetry analysis. It is well known, that if Eq. (B.3) h as six or four independent groups of
symmetries, it can be reduced to the heat or Bessel PDE, see (C raddock, 2009).
Another method is based on the theory of diﬀusion processes. Since any PDE of the form Eq. (B.1)
and Eq. (B.3) can be associated with some diﬀusion process, s ayX={Xt,t≥0}for Eq. (B.1) and
Y={Yt,t≥0}for Eq. (B.3), the map in Eq. (B.2) can be found via reduction m ethods, see (Lipton and
Kaushansky, 2020a) and references therein. The terms T(τ),Xi(τ,x) and Υ i(τ,x) are interpreted as a
scale, time and measure changes.
C Coefﬁcients of Eq. (97)
By using the deﬁnitions of coeﬃcients of Eq. (97) given in Eq. (90) and Eq. (24) and tables of Laplace
transforms we ﬁnd
L(ηeven
i) =L

∞/summationdisplay
n=−∞e−(2nli)2
4σ2
it
σi√
πt

=1
σi√
λ∞/summationdisplay
n=−∞e−√
λ|2nli|
σi=1
σi√
λ/parenleftigg
1 + 2∞/summationdisplay
n=1e−2√
λnli
σi/parenrightigg
(C.1)
Page 35 of 36",2021-02-16T18:22:12Z,multiplayer t mi transforms ti ti ti ti ti xi ti ti ti xi ti ti re ti ti since ti are ti ti also  mi xi  xi xi xi xi since eq eq ui ti xi ti xi ti xi ti teq ui ti xi xi ui ti xi volterra eq t eq t lie it eq vessel anotr since eq eq xt eq eq eq tekau sha t xi coe eq by eq eq eq place page
paper_qf_37.pdf,36,Multilayer heat equations: application to finance,"  In this paper, we develop a Multilayer (ML) method for solving one-factor
parabolic equations. Our approach provides a powerful alternative to the
well-known finite difference and Monte Carlo methods. We discuss various
advantages of this approach, which judiciously combines semi-analytical and
numerical techniques and provides a fast and accurate way of finding solutions
to the corresponding equations. To introduce the core of the method, we
consider multilayer heat equations, known in physics for a relatively long time
but never used when solving financial problems. Thus, we expand the analytic
machinery of quantitative finance by augmenting it with the ML method. We
demonstrate how one can solve various problems of mathematical finance by using
our approach. Specifically, we develop efficient algorithms for pricing barrier
options for time-dependent one-factor short-rate models, such as
Black-Karasinski and Verhulst. Besides, we show how to solve the well-known
Dupire equation quickly and accurately. Numerical examples confirm that our
approach is considerably more efficient for solving the corresponding partial
differential equations than the conventional finite difference method by being
much faster and more accurate than the known alternatives.
","Multilayer heat equations: application to ﬁnance
=1
σi√
λ
1 +2e−2√
λli
σi
1−e−2√
λli
σi
=1
σi√
λcoth/parenleftigg√
λli
σi/parenrightigg
,
L(ηodd
i) =L

∞/summationdisplay
n=−∞e−((2n+1)li)2
4σ2
it
σi√
πt

=1
σi√
λ∞/summationdisplay
n=−∞e−√
λ|(2n+1)li|
σi =2e−√
λnli
σi
σi√
λ∞/summationdisplay
n=0e−2√
λnli
σi
=1
σi√
λ2e−√
λli
σi
1−e−2√
λli
σi=1
σi√
λ1
sinh/parenleftig√
λli
σi/parenrightig,
L(υ−(t|x0,0)) = −1
σ2
j∞/summationdisplay
n=−∞(yj−x0+ 2nli)e−√
λ
σj|yj−x0+2nlj|
=−1
σ2
j∞/summationdisplay
n=1e−√
λ
σj(yj−x0)−√
λ
σj2nlj+1
σ2
j∞/summationdisplay
n=0e√
λ
σj(yj−x0)−√
λ
σj2nlj
=1
σ2
j/bracketleftigg
e√
λ
σj(yj−x0)−e−√
λ
σj(yj−x0)−2√
λ
σjlj/bracketrightigg∞/summationdisplay
n=0e−√
λ
σj2nlj
=1
σ2
je√
λ
σj(yj−x0)−e−√
λ
σj(yj−x0)−2√
λ
σjlj
1−e−2√
λ
σjlj=1
σ2
jsinh/parenleftbigg
(yj−x0+l)√
λ
σj/parenrightbigg
sinh/parenleftbigg
lj√
λ
σj/parenrightbigg =1
σ2
jsinh/parenleftbigg
(yj+1−x0)√
λ
σj/parenrightbigg
sinh/parenleftbigg
lj√
λ
σj/parenrightbigg,
L(υ+(t|x0,0)) = −1
σ2
j∞/summationdisplay
n=−∞(yj+1−x0+ 2nli)e−√
λ
σj|yj+1−x0+2nlj|
=−1
σ2
j∞/summationdisplay
n=0e−√
λ
σj(yj+1−x0)−√
λ
σj2nlj+1
σ2
j∞/summationdisplay
n=1e√
λ
σj(yj+1−x0)−√
λ
σj2nlj
=1
σ2
j/bracketleftigg
e√
λ
σj(yj+1−x0)−2√
λ
σjlj−e−√
λ
σj(yj+1−x0)/bracketrightigg∞/summationdisplay
n=0e−√
λ
σj2nlj
=1
σ2
je√
λ
σj(yj+1−x0)−e−√
λ
σj(yj+1−x0)−2√
λ
σjlj
1−e−2√
λ
σjlj=1
σ2
jsinh/parenleftbigg
(yj+1−x0−l)√
λ
σj/parenrightbigg
sinh/parenleftbigg
lj√
λ
σj/parenrightbigg =−1
σ2
jsinh/parenleftbigg
x0−yj)√
λ
σj/parenrightbigg
sinh/parenleftbigg
lj√
λ
σj/parenrightbigg.
Page 36 of 36",2021-02-16T18:22:12Z,multiplayer page
paper_qf_38.pdf,1,"Epistemic Limits of Empirical Finance: Causal Reductionism and
  Self-Reference","  The clarion call for causal reduction in the study of capital markets is
intensifying. However, in self-referencing and open systems such as capital
markets, the idea of unidirectional causation (if applicable) may be limiting
at best, and unstable or fallacious at worst. In this research, we critically
assess the use of scientific deduction and causal inference within the study of
empirical finance and econometrics. We then demonstrate the idea of competing
causal chains using a toy model adapted from ecological predator/prey
relationships. From this, we develop the alternative view that the study of
empirical finance, and the risks contained therein, may be better appreciated
once we admit that our current arsenal of quantitative finance tools may be
limited to ex post causal inference under popular assumptions. Where these
assumptions are challenged, for example in a recognizable reflexive context,
the prescription of unidirectional causation proves deeply problematic.
","Epistemic Limits of Empirical Finance: Causal Reductionism and
Self-Reference
Daniel Polakow1,2,*, Tim Gebbie3, and Emlyn Flint2,4
1Department of Statistics and Actuarial Science, University of Stellenbosch
2School of Actuarial Science, University of Cape Town
3Department of Statistical Sciences, University of Cape Town
4Peresec, Cape Town
*Corresponding author: dpolakow@sun.ac.za
March 27, 2024
‘Everything that is born is necessarily born through the action of a cause’
Timaeus, Plato c. 360 BCE
‘I can calculate the motion of heavenly bodies, but not the madness of people’
Sir Isaac Newton, 1720 CE
Abstract
The clarion call for causal reduction in the study of capital markets is intensifying. However, in self-
referencing and open systems such as capital markets, the idea of unidirectional causation (if applicable) may
be limiting at best, and unstable or fallacious at worst. In this work, we critically assess the use of scientific
deduction and causal inference within the study of empirical finance and econometrics. We then demonstrate
the idea of competing causal chains using a toy model adapted from ecological predator/prey relationships.
From this, we develop the alternative view that the study of empirical finance, and the risks contained therein,
may be better appreciated once we admit that our current arsenal of quantitative finance tools may be limited
toex post causal inference under popular assumptions. Where these assumptions are challenged, for example
in a recognizable reflexive context, the prescription of unidirectional causation proves deeply problematic.
Keywords— Causal Investing; Reflexivity; Empirical Finance; Epistemology; Risk Management; Spuriosity
1 Introduction
There is an obvious resurgence of interest in causal
inference. For good reason; causality is arguably the
holy grail of any scientific enquiry (Little, 1998) and
has been deliberated since the time of Hume (1739).
Existing coverage on causality in the philosophy liter-
ature is comprehensive and, for brevity, we will not be
reviewing this in detail here.
Rigor and a prescription of causal relevance has
also pervaded finance and economics disciplines; in-
cluding econometrics (Angrist and Pischke, 2010; Chen
and Pearl, 2013; Ahmed, 2022) and investment man-
agement (Wilcox and Gebbie, 2014; L´ opez de Prado,
2023a). Concurrently, Quantitative Finance (QF),
which generically includes the investment and finan-
cial economics disciplines, is embarking on a purge of
‘incorrect’ statistical methodologies. Such flawed sta-
tistical approaches have arguably resulted in a prolif-
eration of false claims and charlatanism (Ziliak andMcCloskey, 2008; Harvey and Liu, 2014; Bailey et al.,
2014, 2017; L´ opez de Prado, 2015, 2023a,b).
The uptake of defensible statistical methods is a pro-
gressive movement within QF that, along with more
cautious distillation of causal pathways, supports sci-
entific realism as well as positive economics. QF might
naturally benefit from such endeavors. However, while
explaining economic phenomena is the intended re-
sult1, the movement also has the potential to incor-
rectly totalize disparate phenomena. In particular,
compared with prescriptive normative economics, such
an approach can become uncoupled from market on-
tology, leading to a disconnect between the reality of
capital markets and the assumptions – and thus output
knowledge – of its reductionist studies.
In simpler terms, the assumption is that QF adheres
to scientific laws, and hence is naturally only framed as
1Sensu the positive economics of Friedman (1953) and the
apparent rise of economics as a ‘cyborg science’ (Mirowski, 2001).
1arXiv:2311.16570v4  [q-fin.GN]  26 Mar 2024",2023-11-28T07:28:08Z,epimic limits emical nance causal reduis self reference daniel poll  tim newbie em lyflint partment statistics auarial science  stellenbosschoauarial science  cape towpartment statistical scienc cape towpeers ec cape towcorrespondi mareverythi timplato sir isaac neoabstra t i from wre keywords causal investi refleity emical nance epistemology risk management spur s it introdutre for lile home existi ror agrist pi ske cpearl ahmed wsonewbie rad concurrently quantitative nance sulia mc ose key harvey  articial intellence ley rad t iisens friedmami row   mar
paper_qf_38.pdf,2,"Epistemic Limits of Empirical Finance: Causal Reductionism and
  Self-Reference","  The clarion call for causal reduction in the study of capital markets is
intensifying. However, in self-referencing and open systems such as capital
markets, the idea of unidirectional causation (if applicable) may be limiting
at best, and unstable or fallacious at worst. In this research, we critically
assess the use of scientific deduction and causal inference within the study of
empirical finance and econometrics. We then demonstrate the idea of competing
causal chains using a toy model adapted from ecological predator/prey
relationships. From this, we develop the alternative view that the study of
empirical finance, and the risks contained therein, may be better appreciated
once we admit that our current arsenal of quantitative finance tools may be
limited to ex post causal inference under popular assumptions. Where these
assumptions are challenged, for example in a recognizable reflexive context,
the prescription of unidirectional causation proves deeply problematic.
","In instances where this assumption fails and
the paradigm becomes self-imposed, scientific method-
ology is unable to uniquely discern its own limitations
or revise its usefulness. As in the proverbial Emperor’s
New Clothes, we may be left with QF dressed in sim-
ilar robes of causality. This could in part explain
why financial crises are often inherent and unavoidable
within a positive framing of QF.
Within QF and in studies of financial risk, a single
common assumption is pervasive; namely, that mar-
kets, being social systems, adhere sufficiently to epis-
temic norms. These epistemic norms carry with them
a priori the necessity of unique, well-defined causal
chains that can be meaningfully extracted from data
in a positive economics framing and a reductionist sci-
entific sense.
2 Causal chains
To understand epistemic norms in markets, first we
note that there are some important assumptions un-
derpinning economic thinking. Of course, there exist
several different economic schools of thought. The di-
chotomy between neoclassical, or orthodox, and hetero-
dox economics (arguably, everything else) is frequently
made to smooth over and sometimes smother the prag-
matic importance of these assumptions as a binary
framing that casts idealisms and normative approaches
in opposition to positivist perspectives.
While orthodox approaches idealize an economic sys-
tem in which the rationality of participants is key,
heterodox economics emphasizes inefficiencies and im-
balances generated by the system. Nonetheless, both
modern orthodox and heterodox economic approaches
aim to incorporate a range of imperfections within their
standard frameworks to render them more realistic. In
this setting, realism is defined as the ability to provide
narrative explanations that are cohesive within a given
ontology, or to make successful predictions.
At the heart of the positivist agenda, which resides
within the broader scientific realist school of thought,
is the goal of causal explanation; and unidirectional
causation is the de facto supposition. In this regard,
of contemporary relevance is the work spearheaded
by Pearl and co-workers (Pearl (1995, 2009a,b); Pearl
et al. (2016)) who have formalized empirical causal
modeling and hence laid the foundations of much of
modern artificial intelligence.
Unidirectional causality refers to Acausing B, for
example the price action in Bbeing caused by market
interest A,ceteris paribus . In terms of the mathemat-
ics of causality (Pearl, 1995), we express Bas some
function of A:
P[A=a] do[B=b]> P[A=a]. (1)
2This framing is epistemological. What is, or is not science is
itself contested because of the sociological context of the activity
(Feyerabend, 1993). The demarcation problem within QF resides
in the dynamics of the system itself - and should not be confused
with components that are driven by regulation and accounting
practices (for example).by intervention. A
diagrammatic representation of Eqn. [1] is provided in
Figure 1a.
Eqn. [1] and its associated framework accommo-
dates both neoclassical equilibria and heterodox dy-
namics, depending on the size and term of the arbi-
trageable phenomenon under consideration. Mathe-
matician Robert Buck defined ‘self-frustrating’ predic-
tions as forecasts that are initially true, but become
false on public dissemination (Buck, 1963). Moreover,
Popper (1957) in his writings on social sciences refers
to ‘self-limiting’ predictions influencing the predicted
event in a preventative sense. Markets are similarly
understood to adapt as the outputs of scrutiny are as-
similated. This understanding is accommodated within
the idealized neoclassic theory of economics.
Returning to Eqn. [1], unidirectional causality does
not permit Bto cause Aand it does not facilitate A
being limited by its subsequent impact on Bvia self-
limiting feedback. While causal graphs are typically
linear and unidirectional, self-limiting processes with
dampening feedback are commonly tolerated without
being fatal to ex post inference. Importantly, contem-
porary QF is already suitably cognizant of this ‘adap-
tation’ of the market to new information (L´ opez de
Prado, 2015; McLean and Pontiff, 2016; Falke et al.,
2022).
More generally, adaptation is a feature common to
‘reflexive’ systems; namely, systems that refer to them-
selves. While proponents of causal rigor recognize
that markets are adaptive, the fact that they are,
more broadly, self-referential is commonly overlooked.
Specifically, markets may be possessed by dynamics
that extend further than simply adaptation.
A self-referential system is reflexive in nature. This
is a surprisingly pervasive idea: the ‘Oedipus effect’ of
Popper (1957), the ‘self-fulfilling prediction’ of Buck
(1963), or the ‘back-coupling’ of Morgenstern (1972).
MacKenzie (of performativity literature renown) noted
that the study of economics does more than simply de-
scribe, but rather shapes and changes the conditions
of the economy, and society more broadly (MacKen-
zie, 2006; MacKenzie et al., 2007). It thus possesses
the ability to analyze, describe, and modify its own
structure, behavior, and properties. To say this phe-
nomenology of self-reference is well-known, recurring
and deeply studied is an understatement.3
The history and progression of economic thought
is expansive, including the phenomenon of reflexivity.
Reflexivity may pose a challenge to rational expecta-
tions theory and to the notion of an equilibrium. The
question of reflexivity has been given attention histori-
cally by many prestigious economists (including no less
than three Nobel laureates – Morgenstern, Modigliani
3Self-referential study originates from Spencer-Brown’s Laws
of Form (Spencer-Brown, 1969) and extends to the calculus de-
veloped by Varela (1975). The field finds parallels in philosophy
(radical constructivism; Von Glasersfeld (1984)), control theory
(e.g. second-order cybernetics; Von Foerster (2003)), and eco-
nomics (the Eigenform of markets; Kauffman (2009)). Daniel
Hofstadter discusses self-reference systems extensively in his pop-
ularist book (Hofstadter, 1999).
2",2023-11-28T07:28:08Z,ias emor new ots  withitse causal to of t w nonetless iat ipearl pearl pearl unidirenal causi bei ipearl bas  what   bend t eq  eq ma t robert buck buck ocmarkets  urni eq to and vi w imtantly rad mc leapontf falk  w specically  tedus cbuck morgenstermac keie mac kemac keie it to t refleity t nobel morgenstermodi lia ni self spencer browlaws form spencer broware la t volaser sf el vofo easter ei geform kauffmadaniel hofstadter hofstadter
paper_qf_38.pdf,3,"Epistemic Limits of Empirical Finance: Causal Reductionism and
  Self-Reference","  The clarion call for causal reduction in the study of capital markets is
intensifying. However, in self-referencing and open systems such as capital
markets, the idea of unidirectional causation (if applicable) may be limiting
at best, and unstable or fallacious at worst. In this research, we critically
assess the use of scientific deduction and causal inference within the study of
empirical finance and econometrics. We then demonstrate the idea of competing
causal chains using a toy model adapted from ecological predator/prey
relationships. From this, we develop the alternative view that the study of
empirical finance, and the risks contained therein, may be better appreciated
once we admit that our current arsenal of quantitative finance tools may be
limited to ex post causal inference under popular assumptions. Where these
assumptions are challenged, for example in a recognizable reflexive context,
the prescription of unidirectional causation proves deeply problematic.
","and Simon), yet it remains a topic of little serious in-
terest in the field – for many of the reasons discussed.
We do not review this literature further here but to
note the following points. At the same time as Arrow
and Debreu (1954)’s proof for the existence of a gen-
eral equilibrium was published, two influential papers
dealing with reflexivity were also published: Grunberg
and Modigliani (1954) and Simon (1954). Both papers
were a response to the unusual reflexivity predictions of
Merton (1948) - and both papers simply showed that
‘it was possible’ (emphasis ours) that reflexivity did
not matter in the accuracy of economic predictions.
The seminal work of Muth (1961) followed and
was built upon the Grunberg-Modigliani-Simon proofs.
Muth’s work has endured and remains a standard char-
acterization of rational expectations. Within it, re-
flexivity is assumed to have no impact - and this un-
derstanding was adopted as the only rational result.
Somewhat ironically though, Modigliani and Simon
were both known to have negative views on rational
expectations (see Hands (1990)).
At different times, the implications of the Grunberg-
Modigliani-Simon thinking resurfaces – for exam-
ple the multiple ‘sunspot’ equilibria in the 80’s
(Azariadis (1981); Cass and Shell (1983)). Con-
versely, while Grunberg-Modigliani-Simon has sur-
vived multiple attacks on its logic and mathematics,
some authors consider the continuous nature of the
Grunberg-Modigliani-Simon reaction functions to vio-
late social understanding and conclude that Grunberg-
Modigliani-Simon rarely ever applies (see Henshel
(1995)).
Lehmann-Waffenschmidt and Sandri (2007) char-
acterize two dynamics of reflexive systems: self-
reinforcing and destabilizing (via positive feedback
loop).4Markets are thus not limited to preventative
feedback loops; instead, such loops may also be desta-
bilizing. For example, predictions such as fundamental
market value can persist or change solely due to par-
ticipants’ beliefs. Furthermore, runs on banks, asset
bubbles, the volatility implied in option prices, booms-
and-busts and the allure of factor exposures are all
characteristic of a system possessed by self-referential
properties.5Within such systems, sufficient conviction
in an association can, in fact, result in ( i.e. cause)
that same association (Polakow, 2010). The notion of
‘spuriosity’ thus also becomes ambiguous.
Tony Lawson, the eminent heterodox economist, ex-
plains why markets ultimately cannot adhere to epis-
temic norms. Lawson (2003) defines closed systems to
refer to a situation in which a correlation (an event
regularity) occurs. In the absence of conditions sup-
porting such correlations, the system is then deemed
to be ‘open’. Markets, being a social system, are de-
4Goodhart’s law (Goodhart, 1984): any observed statistical
regularity will tend to collapse once pressure is placed upon it
for control purposes . Campbell (1979)’s law prior and the ‘Lu-
cas Critique’ (Lucas, 1976) are further examples of reflexive re-
sponses.
5Such examples may also, however, characterize systems that
are not self-referencing. Discerning a parsimonious likelihood
function is therefore not trivial.
“. . . modern mainstream economics is awash with
assumptions of perfect foresight or rational expecta-
tions, of efficient markets, and of market equilibrium,
all, at least in the manner they are typically employed,
essentially premised on the successful predictability of
future outcomes; all supposing that in an open system
actual outcomes can be effectively anticipated. Indeed
all such endeavour in effect treats open systems as if
there are actually closed” – Lawson (2003).
Markets are unequivocally an open system.6Such
a system often involves recursive or circular relation-
ships, where the system’s components interact or define
themselves in terms of other components within the
same system. Of relevance here, self-referential sys-
tems are possessed of unusual properties, highly com-
plex and challenging to understand or analyze, espe-
cially using traditional reductionist approaches and as-
sumptions of causation (Soros, 2013).
The dynamics of causal chains within self-referential
systems are often paradoxical. Umpleby (2010) notes
that in self-referencing systems we transition from a
classical paradigm of inferring A causes B, to higher-
order dynamics with very different notions of stabil-
ity, independence, structure and temporality. The
very idea of model building and usage becomes nu-
anced. The presence of reflexivity will frequently result
in circular reasoning – deemed fallacious by contem-
porary academic standards.7The problem of causal
emergence will frequently present itself in the form
where multiple competing versions co-exist and com-
pete (Hoel, 2017).
Markets comprise a system of interacting actors,
with multiple levels of causation (Wilcox and Gebbie,
2014). The debate is not whether markets, being an ob-
vious social system, are self-referential or not. Rather,
we believe the controversy revolves around two points.
Firstly, to what extent does such a self-referential sys-
tem differ ontologically from the closed system we com-
monly assume? Secondly, can such a self-referential
system produce behaviors and inefficiencies that are
persistent and non-arbitrageable?
Neoclassical economic thought, arguably the dom-
inant paradigm, is premised on the basis that imbal-
ances are only temporary and so the existence of short-
term market imperfections are tolerated in the system.8
Heterodox thinking is more permissible to tolerat-
ing longer-term disequilibrium; e.g. simple ecologi-
cal models applied to market dynamics result in en-
dogenous inefficiencies and deviations from notional
fundamental values (Galla and Farmer, 2013; Scholl
et al., 2021; Sanders et al., 2018). Bouchaud (2013)
reviews modelling efforts that include heterogeneities
6Here, a closed system means a closed system of regular
events, and implies predictability. An open system is then one
in which prediction is itself not regularly achievable.
7Reflexivity further violates the ad hominem informal fallacy
and the fallacy of accent: markets operate at both ‘observing’
and ‘participating’ levels (Umpleby, 2010).
8In this vein was Soros’ view of reflexivity rebuffed by the
economic establishment as ‘merely stating the obvious’ (Soros,
2009).
3",2023-11-28T07:28:08Z,simo at arrow re ru nberg modi lia ni simoboth mortot math ru nberg modi lia ni simomath wiomewhat modi lia ni simohands at ru nberg modi lia ni simoaria is ass sll coru nberg modi lia ni simoru nberg modi lia ni simoru nberg modi lia ni simons l leymawarschmidt and ri markets for furtr withipoll  t tony lawsolawsoimarkets good hart good hart campbell lu itique lucas sudiscardi ined lawsomarkets suof so ros t ump le by t t t ho el markets wsonewbie t ratr rstly secondly neoassical terodox gall farmer schosanrs  chare arefleity ump le by iso ros so ros
paper_qf_38.pdf,4,"Epistemic Limits of Empirical Finance: Causal Reductionism and
  Self-Reference","  The clarion call for causal reduction in the study of capital markets is
intensifying. However, in self-referencing and open systems such as capital
markets, the idea of unidirectional causation (if applicable) may be limiting
at best, and unstable or fallacious at worst. In this research, we critically
assess the use of scientific deduction and causal inference within the study of
empirical finance and econometrics. We then demonstrate the idea of competing
causal chains using a toy model adapted from ecological predator/prey
relationships. From this, we develop the alternative view that the study of
empirical finance, and the risks contained therein, may be better appreciated
once we admit that our current arsenal of quantitative finance tools may be
limited to ex post causal inference under popular assumptions. Where these
assumptions are challenged, for example in a recognizable reflexive context,
the prescription of unidirectional causation proves deeply problematic.
","as well as interactions, and proposes a potential uni-
fying framework using Random Field Ising models for
both endogenous ruptures and crises, including ‘self-
referential’ feedback loops for non-equilibrium phe-
nomenology missing in classical economic models. Hir-
shleifer et al. (2023) examine the extent to which het-
erogeneity of investment styles can co-exist in the long-
term, in contrast to the predictions wrought from tra-
ditional theory, and in response to social contagion.
The study builds on the Adaptive Markets Hypothesis
(Lo, 2004, 2017), under which market participants be-
have (mostly) rationally using a set of natural-selection
heuristics. Such heuristics can become maladaptive
during exogenous shocks.9
A B+
-
12
(a) Conventional causal graph
A B
+-3
4
(b) Alternative (dual) causal graph
Figure 1: A conventional unidirectional causal graph of
Acausing B(path 1) is shown in Fig. 1a; here with the
addition of a negative (self-limiting feedback loop, de-
noted “-”), and a positive (destabilizing feedback loop,
denoted “+”), both of the latter dependencies along
path 2. Fig. 1a (path 1 only) graphically represents
Eqn. [1]. This can be compared to the alternative
unidirectional causal graph shown in Fig. 1b (path 3
only), which represents Eqn. [2], where Bis shown
causing Aalong path 3, with the addition of negative
(self-limiting feedback loop, denoted “-”); and positive
(destabilizing feedback loop, denoted “+”), the latter
both along path 4. Feedback dynamics between these
causal chains, 1a and 1b, may be indistinguishable un-
less intervention is involved.
Yet, it appears that both schools of thought – neo-
classical and heterodox – retain the view that unidi-
rectional causality remains relevant as a formalism,
even if these linear cause-effect or circular dampening
graphs exist only as component processes enmeshed in
an ensemble of interactions of a more complex system
9The rise in popularity of dynamic stochastic general equilib-
rium (DSGE) models has been catalyzed by the need for policy
analysis and forecasting away from classical economic core frame-
works to permit for more richness and flexibility, but DSGE mod-
els have not been as successful as wished (see Stiglitz (2018)).
We further note that a return to some notional equilibrium is an
inherent feature in all DSGE models.(Hardy, 2001; Hommes, 2021).
If causality is in fact more malleable in a reflexive
system (perhaps to the point of being wholly subjec-
tive) there is a competing reality that always requires
admission as a hypothesis in any system prone to re-
flexivity, and that is: B causes A ( ceteris paribus ):10
P[B=b] do[A=a]> P[B=b] (2)
A diagrammatic representation of Eqn. [2] is provided
in Figure 1b. It is critical to realize that the feedback
dynamics embedded in Eqn. [1] will effectively be indis-
tinguishable from those of Eqn. [2] unless intervention
is invoked.11
Bouchaud explains how disappointing quantitative
success has been within the economic sciences, and mo-
tivates for a break away from classical economic tools
(Bouchaud, 2008, 2013). Lehmann-Waffenschmidt and
Sandri (2007) note that within economics, it is not
possible to draw conclusions on the logical legitimacy
of any reflexive mechanism. Shaikh (2013) noted
that even heterodox economics maintains the notion
of an equilibrium and that different economic foun-
dations are required for adequately dealing with re-
flexivity. The scientific fallibility of economics was
identified and discussed comprehensively in the early
1980s (McCloskey, 1983). Soros (2013) argued plainly
that markets did not benefit from the same epistemic
norms of science. In particular, if financial mod-
els operate within a self-referential framework they
can be non-causal, resulting in an inability to predict
(De Scheemaekere, 2009).
In summary, the presence of reflexivity poses a
clear challenge to the epistemology of QF since it is
not accommodated within a unidirectional causality
paradigm. The reason why reflexivity does so is sim-
ply stated: chains of causality are ostensibly malleable
by the actors within markets. For guidance (and illus-
tration) we turn to a well-understood set of complex
systems: ecology.12
3 Causal predation
Extending ecology metaphors within finance has prece-
dent (Scholl et al., 2021). Contemporary empirical
views provide examples of predator-prey models ex-
plaining, for example, S&P500 dispersion in terms of
contrarian and trend-following agents competing with
and predating upon each other (Lux, 2021). As a useful
metaphor, we explore the co-existing dynamics of two
competing causal chains within a similar self-referential
system, adapted from ecology (May, 1976; Sugihara
et al., 2012). This forms an illustrative toy model of
10e.g.the price action in B causes the market interest A.
11Bimpacting Avia path 2 may be indistinguishable from B
causing Avia path 3, unless by control (intervention), and then
only under certain conditions. This intervention is inevitably
top-down in nature (Craver and Bechtel, 2007; Auletta et al.,
2008).
12Ecosystems are usually thermodynamically open, but
causally closed with some regularity and predictability.
4",2023-11-28T07:28:08Z,random eld usi hi t adaptive markets sis lo suconventnal alternative  causi   eq   eq is alo feedback yet t stlitz  hardy ho eq  it eq eq  cha chaleymawarschmidt and ri sh articial intellence kh t mc ose key so ros i cinema eye re it for causal extendi schocontemary lux as may ug chara  impai via via  gre be tel all eta ecotems
paper_qf_38.pdf,5,"Epistemic Limits of Empirical Finance: Causal Reductionism and
  Self-Reference","  The clarion call for causal reduction in the study of capital markets is
intensifying. However, in self-referencing and open systems such as capital
markets, the idea of unidirectional causation (if applicable) may be limiting
at best, and unstable or fallacious at worst. In this research, we critically
assess the use of scientific deduction and causal inference within the study of
empirical finance and econometrics. We then demonstrate the idea of competing
causal chains using a toy model adapted from ecological predator/prey
relationships. From this, we develop the alternative view that the study of
empirical finance, and the risks contained therein, may be better appreciated
once we admit that our current arsenal of quantitative finance tools may be
limited to ex post causal inference under popular assumptions. Where these
assumptions are challenged, for example in a recognizable reflexive context,
the prescription of unidirectional causation proves deeply problematic.
","simple reflexive causal chains that can generate com-
plicated dynamics.13
Consider a single self-dependent and time-dependent
feature x(t) that can be well modeled by a logistics
function, here with rate of change parameter r, and
carrying capacity K
˙x(t) =rx(t)(K−x(t)). (3)
where the dot represents a time derivative. The fea-
turexcould be taken to represent the number of in-
dividuals in a population, or the log-price of an asset,
or the wealth associated with the accumulated profit-
and-loss of some trading strategy conditioned on vari-
ous information sources. This model can be randomly
perturbed, although we keep the argument determin-
istic.
Once discretized, the model demonstrates the well-
understood bifurcation phenomena shown by May
(1976). This is shown by replacing the feature deriva-
tive with the derivative of a logarithm of the feature
from Eqn. [3] and converting it into a difference equa-
tion: ln xn+1−lnxn=r(K−xn). By taking the
exponential of the log-difference equation we find the
Logistics Difference Equation (LDE)14:
xn+1=xner(K−xn). (4)
We now progress the metaphor, but for coupled fea-
tures that include an interaction term. For simplicity
we restrict ourselves to two features x(t) and y(t), with
constant causal couplings15:
˙x(t) = rx(t) (K−x(t)−ayxy(t)),
˙y(t) = ry(t) ((K−y(t)−axyx(t)). (5)
There are two coupling parameters ayxandaxy. If
the two features were correlated then this parameter
would be symmetric. The argument can be generalized
to many such features. To generate causal chains, we
discretize the coupled differential equations (Hastings,
1993; Sugihara et al., 2012) and find the causal chain
update equations:
xn+1=xnerx(Kx−xn−ay→xyn),
yn+1 =ynery(Ky−yn−ax→yxn). (6)
This model has two different growth rates, two differ-
ent carrying capacities, and two dependency variables.
With ay→x̸= 0, but ax→y= 0 this would be unidi-
rectional, from ytox. These are now situated as part
of a sequence of unidirectional causal relationships in
a chain, where each iterate xn, can be thought of as a
realization of one node, and similarly, ynfor the other,
as a realization of the graph in Fig. 1.
13The concept of ‘regularity’ can be further restricted – e.g.
Granger causality as the measure of predictability (Sugihara
et al., 2012) – this does not change our argument.
14The Logistics Map (LM) xt+1=rxt(1−xt) is the first order
approximation to the LDE using ex≈1 +x−1
2x2+. . ..
15The Lotka-Volterra (LV) model has the form, for two chains
S1andS2:˙S1=aS1−bS1S2, and ˙S2=cS2−dS1S2.We can simulate the chain of unidirectional causal
graphs and visualize key features of the resulting dy-
namics by considering iterations of the coupled LDE.
The resulting coupled bifurcation map is shown in Fig.
2. This renders our key points concrete.
We see that even if we explicitly know, with cer-
tainty, the deterministic and non-stochastic causal re-
lationship between chains xnandyn(or even if we
know the causal relationships between the underlying
causal features x(t) and y(t)) we cannot easily predict
xnat all values of r(norK) even if ynis known with
certainty itself.
Figure 2: A model of the iterative stability of sampled
sequences of unidirectional causal graphs. This models
the causal predation of the system using the Logistics
Difference Equations (LDE) from Eqn. [6]. The model
includes self-interactions and cross-interactions where
a feature xhas a unidirectional causal dependency on
another feature y, but where yis not dependent on
x. The discretized features represent realizations from
causal chains. The couplings introduce further insta-
bility and complexity. The y-axis is the feature value
(population size, strategy price, or accumulated profit-
and-loss) and is dependent on the growth factor rgiven
on x-axis, and the carrying capacity K. In this exam-
plerx=r,ry= 0.95r,Kx= 0.95,Ky= 1.00 and
ay→x=−0.1; any variety of combinations can be cho-
sen. The rounding significance is ϵ= 10−4.
As a simple representation for causal chain competi-
tion and predation, Eqn. [6] and Fig. 2 have the fur-
ther benefit of making clear the role of randomness.16
For example, either r(orK) can be changed to be
uniformly distributed, and then sampled at exponen-
tially distributed waiting times; perhaps representing
exogenous shocks from top-down causal sources from
16Difference equations can be thought of as filters, this implies
that erratic behavior due to sampling could be ameliorated using
filtering e.g.low pass filtering.
5",2023-11-28T07:28:08Z,consir t  once may  eq by logistics dference equat for tre  t to hastis ug chara ky  with tse  t  ug chara t logistics map t lot ka volterra  t      logistics dference equatns eq t t t t iky t as eq  for dference
paper_qf_38.pdf,6,"Epistemic Limits of Empirical Finance: Causal Reductionism and
  Self-Reference","  The clarion call for causal reduction in the study of capital markets is
intensifying. However, in self-referencing and open systems such as capital
markets, the idea of unidirectional causation (if applicable) may be limiting
at best, and unstable or fallacious at worst. In this research, we critically
assess the use of scientific deduction and causal inference within the study of
empirical finance and econometrics. We then demonstrate the idea of competing
causal chains using a toy model adapted from ecological predator/prey
relationships. From this, we develop the alternative view that the study of
empirical finance, and the risks contained therein, may be better appreciated
once we admit that our current arsenal of quantitative finance tools may be
limited to ex post causal inference under popular assumptions. Where these
assumptions are challenged, for example in a recognizable reflexive context,
the prescription of unidirectional causation proves deeply problematic.
",". We
can also randomly change the interaction terms, and so
on. These introductions of randomness do not improve
the situation and can undermine attempts to close the
system using additional filtering. In this framework,
it is unreasonably optimistic to assume that one can
meaningfully infer stable causal graphs uniquely from
the data.
That is not to say causal graphs are not useful.
In fact, for exploratory data analysis, or visualizing
the hypothesized system state, or rendering assump-
tions transparent, causal graphs can be informative
and should be encouraged. However, even if causal
chains are unique and can be reliably extracted, this
does not imply that the representation can faithfully
render the dynamic properties of the system itself.
In an open system, causality ultimately remains phe-
nomenological. Markets are reflexive, adaptive and
saturated with agents responding strategically to each
other. It is thus understood that coherence of percep-
tion around a common cause will often lead to forms
of tight coupling with high cohesion, and thus fragility,
being written into the financial system itself.
4 Conclusion
Research may retrospectively support unidirectional
chains of causality and their corresponding inference.
However, the conditions in which ex post understanding
can be extended to support ex ante prediction are lim-
ited. This is caused by the market’s adaptation mech-
anism combined with the understanding that chains of
causality are malleable by market actors. This neces-
sitates a distinction of practical significance between
“model estimation”, wherein the model is presumed
to be normatively true and the data representative of
the future, and “model calibration”, where one rather
seeks to facilitate reliable decision-making under un-
certainty using a model that may be misspecified or
simply incorrect.17This theme – a default necessity for
calibration despite the presence of ex post confidence –
is well-known from derivative pricing (Schoutens et al.,
2004). It is unlikely that these chains can be used
without considerable risk for prediction or control in
a forward-looking way. In financial economics, these
risks may impact studies in asset pricing (where inputs
are endogenous) more than, for example, studies within
corporate finance (where inputs are often assumed ex-
ogenous). That said, we are of the view that it does not
serve the arguments to dilute the epistemology angle
17Here language is important, particularly with respect to its
nuanced meaning in reference to its context. In much of the Fi-
nance and Econometrics literature there seems a reluctance to
engage with epistemology to the extent that it challenges nor-
mative perspectives that are tied to both incentives and the his-
torical development of ideas. For example, in some of the Econo-
metrics and Economics literature the term model ‘identification’
is often used (Kahn and Whited (2018)) to weaken the idea of
model ‘estimation’. In reality, ‘identification’ is still used to de-
note model selection within the estimation paradigm and, most
importantly, is not used in the same way that model ‘calibration’
is used in the more pragmatic QF literature.top-down, in lieu of a bottom-up argument from any
specific economic or finance model.
If reflexive, the degree to which markets behave as
a closed system is likely to be a function of only teleo-
logical conviction. This means that there may well be
nothing tangible that supervenes various model repre-
sentations. To the extent that certain QF actors dom-
inate markets, a causal chain supporting their partic-
ular narrative will become established. The risk then
exists of other actors benefiting from distinct causal
chains coming to bear. The system is therefore prone
to instability even in the absence of exogenous shocks.
In the age of hyper-connectedness, the shifting of per-
ception and sentiment has never been as fungible, or as
inexpensive.18It should be anticipated that without a
more complete theory of complexity, QF will remain
lost in a spiral of tautological self-reference.
As scientists, and as academics of QF, we naturally
aspire to high standards of critical thinking. Despite
markets exhibiting unusual epistemic dynamics, scien-
tific reductionism in QF is deployed as an inviolate pre-
scription. However, the assumption of unidirectional
causation will not always lead to the appropriate causal
inference. In contrast, within a highly reflexive system,
the pursuit of causality may, in fact, be doomed to fail-
ure.
Empirical finance may frequently find itself searching
incorrectly for answers under the only epistemological
light within which it can navigate. To the extent that
markets closely proxy closed systems, the risk of violat-
ing directional causal assumptions is arguably limited.
Yet, if markets do have the proclivity to ‘switch’ be-
tween different casual directions (as in Fig. 2), QF is
presented with an existential challenge. In such a sce-
nario of overt reflexivity, empirical finance is expected
to awaken within a deeply paradoxical associational rel-
ativity. Here, as in other contexts, realistic attempts
at hypothesizing more complex causal models will be-
come multifaceted and are less likely to give rise to
useful inference (Rasmussen et al., 2019; Saylors and
Trafimow, 2020).
None of this is novel in the real-world application
of QF i.e.in option pricing, trading and asset man-
agement. One of the early approaches to dealing
with the causation problem in factor investing – path-
dependencies and unidirectional dependencies – was to
introduce ad hoc models, such as those of Haugen and
Baker (1996) and Ferson and Harvey (1999), and relax
the linearity assumption of the pricing kernel (Wilcox
and Gebbie, 2015). This early modeling of dynamic
temporal and multi-layered dependencies may appro-
priately be couched as attempts at “machine learning”
and are not essentially inventive, nor do they progress
the central argument here. Hence, we deliberately
avoid discussion of the “data-science problem” or the
merits or fallibility of any specific algorithm. The ve-
racity of modeling tools in reflexive systems takes on
a life of their own, and invariably has less to do with
any computational tool or trick, and more to do with
18Baudrillard (1981) called this the ‘precession of simulacra’.
6",2023-11-28T07:28:08Z, tse ithat iimarkets it conusresear   shorteit ithat re i econometrics for eco no economics khawhite i  to t t iit as spite iemical to yet  ire rasmussesay lor a row none one aug ebaker fer soharvey wsonewbie  nce t baudrlard
paper_qf_38.pdf,7,"Epistemic Limits of Empirical Finance: Causal Reductionism and
  Self-Reference","  The clarion call for causal reduction in the study of capital markets is
intensifying. However, in self-referencing and open systems such as capital
markets, the idea of unidirectional causation (if applicable) may be limiting
at best, and unstable or fallacious at worst. In this research, we critically
assess the use of scientific deduction and causal inference within the study of
empirical finance and econometrics. We then demonstrate the idea of competing
causal chains using a toy model adapted from ecological predator/prey
relationships. From this, we develop the alternative view that the study of
empirical finance, and the risks contained therein, may be better appreciated
once we admit that our current arsenal of quantitative finance tools may be
limited to ex post causal inference under popular assumptions. Where these
assumptions are challenged, for example in a recognizable reflexive context,
the prescription of unidirectional causation proves deeply problematic.
","the nature of adoption and dissemination of the same.
The fundamental nature of the problem remains, irre-
spective of what tools are fashionable.
We believe causal and scientific thinking may ulti-
mately be limited within the prescription of current
QF tools for two reasons. The first presupposes that
markets are not sufficiently reflexive to matter and re-
ductionist approaches assuming unidirectional causal-
ity are valid. Within this paradigm, causal inference is
limited to ex-post , unless the phenomenon under study
is non-arbitrageable (because of the ‘tragedy of the
commons’ (Hardin, 1968)). The ability to forecast ex
ante may ultimately be a rarity, and QF should be
cognizant of overreach. Arguably, this understanding
would accomplish much of the clean-up required within
QF. The second limitation is that statistical and causal
inference will plainly be applied incorrectly where re-
flexivity is present. For these reasons, we argue that
empirical finance is not at risk of becoming a patho-
logical science ( sensu L´ opez de Prado (2015)). Rather,
empirical finance is categorically a pathological science
where it is able to act as one at all ( i.e.where not suffi-
ciently reflexive to matter), and patently not a science
elsewhere.
We close with words from one of the foremost
thinkers in causal economics, Joshua Angrist who
noted: ‘. . . Perhaps it’s worth restating an obvious
point. Empirical evidence on any given causal effect
is always local, derived from a particular time, place,
and research design. Invocation of a superficially gen-
eral structural framework does not make the under-
lying variation or setting more representative. Eco-
nomic theory often suggests general principles, but ex-
trapolation of causal effects to new settings is always
speculative. Nevertheless, anyone who makes a living
out of data analysis probably believes that heterogeneity
is limited enough that the well-understood past can be
informative about the future.’ (Angrist and Pischke,
2010). Heterogeneity is certainly limited in some eco-
nomic systems - for example where consumer prefer-
ences are simple and demand forecastable; similarly
in commodity driven economies, single-currency zones
and in traditional societies etc. Yet, the challenge re-
mains the surprising frequency with which the well-
understood past does not in fact inform the future else-
where. For example, it has been noted that the only
consistent feature of economic and quantitative finance
disciplines is the distinct inability to predict and avert
crises (Bouchaud, 2008).
The disparity between the two broad schools of eco-
nomic thought is perhaps dialectical, being two sides
of a causally confused coin. As yet, we do not have
a viable alternative paradigm that accounts for the
anomalies, and that can provide both a general and op-
erational approach to the heterodox scientific project
(Ghilarducci et al. (2023)). Such a viable alternative
is a key requirement for any Kuhnian paradigm shift.
In deeply reflexive systems, the heterodox project may
in fact lack the necessary scientific foundation - since
it is without the predictive reach of causality. This
acknowledgement is belated to the discipline, and theperils of ignoring this phenomenological reality are po-
tentially grave. Causal predation (and the conditions
of causal chain co-existence) are not satisfactorily as-
similated in current thinking. It is clear that QF (and
economics more broadly) would benefit from a better
understanding of what knowledge is likely to be im-
pacted by such reflexivity, and consequently the risks
of attaching policy intervention, regulation or invest-
ment to the same.
5 Acknowledgements
We thank Diane Wilcox and Michael Jennions for en-
gaging conversations on the topic. The views expressed
here are solely those of the authors.
References
Ahmed, J., 2022. Causal inference in econometrics: Methods and ap-
plications in finance. Adv. J. Econometrics and Finance 1(2), 20–
23. URL: https://ajeaf.com/index.php/Journal/article/view/11 .
Angrist, J.D., Pischke, J.S., 2010. The credibility revolution in em-
pirical economics: How better research design is taking the con
out of econometrics. Journal of Economic Perspectives 24(2), 3–
30. doi: 10.1257/jep.24.2.3 .
Arrow, K.J., Debreu, G., 1954. Existence of equilibrium for a
competitive economy. Econometrica 22, 265–290. URL: https:
//www.jstor.org/stable/1907353 , doi: 10.2307/1907353 .
Auletta, G., Ellis, G., Jaeger, L., 2008. Top-down causation by
information control: from a philosophical problem to a scientific
research programme. J. R. Soc. Interface 5, 1159–1172. doi: 10.
1098/rsif.2008.0018 .
Azariadis, C., 1981. Self-fulfilling prophecies. Journal of Economic
Theory 25, 380–396. doi: 10.1016/0022-0531(81)90038-7 .
Bailey, D.H., Borwein, J., L´ opez de Prado, M., Qiji, J.Z., 2014.
Pseudo-mathematics and financial charlatanism: The effects of
backtest overfitting on out-of-sample performance. Notices of
the American Mathematical Society 61, 458–471. doi: 10.1090/
noti1105 .
Bailey, D.H., Borwein, J., L´ opez de Prado, M., Qiji, J.Z., 2017. The
probability of backtest overfitting. J. Computational Finance
20(4), 1460–1559. URL: https://ssrn.com/abstract=2326253 ,
doi:10.21314/jcf.2016.322 .
Baudrillard, J., 1981. Simulacra and Simulation. The University of
Michigan Press.
Bouchaud, J.P., 2008. Economics needs a scientific revolution. Na-
ture 455, 1181. doi: 10.1038/4551181a .
Bouchaud, J.P., 2013. Crises and collective socio-economic phenom-
ena: simple models and challenges. J Stat Phys 151, 567–606.
doi:10.1007/s10955-012-0687-3 .
Buck, R.C., 1963. Reflexive predictions. Philosophy of Science 30,
359–369. doi: 10.1086/287955 .
Campbell, D.T., 1979. Assessing the impact of planned social
change. Evaluation and Program Planning 2, 67–90. URL: https:
//www.sciencedirect.com/science/article/pii/014971897990048X ,
doi:https://doi.org/10.1016/0149-7189(79)90048-X .
Cass, D., Shell, K., 1983. Do sunspots matter? Journal of Political
Economy 91, 193–227. doi: 10.1086/261139 .
Chen, B., Pearl, J., 2013. Regression and causation: A critical exam-
ination of six econometric textbooks. Real World Economics Re-
view 65, 2–20. URL: http://www.paecon.net/PAEReview/issue65/
ChenPearl65.pdf .
Craver, C., Bechtel, W., 2007. Top-down causation without top-
down causes. Biology and Philosophy 22, 547. doi: 10.1007/
s10539-006-9028-8 .
De Scheemaekere, X., 2009. The epistemology of modern finance.
Journal of Philosophical Economics 2, 99–120. URL: https://
www.proquest.com/docview/229656311 , doi: 10.46298/jpe.10578 .
Falke, A., Rej, A., Thesmar, D., 2022. When do systematic strategies
decay? Quantitative Finance 22(11), 1955–1969. doi: 10.1080/
14697688.2022.2098810 .
Ferson, W.E., Harvey, C.R., 1999. Conditioning variables and the
cross-section of stock returns. Journal of Finance 56, 1325–1360.
doi:10.1111/0022-1082.00148 .
Feyerabend, P., 1993. Against Method. Verso Book, London. 3rd.
Ed. 1975, 1988, 1993.
Friedman, M., 1953. The methodology of positive economics, in:
Friedman, M. (Ed.), Essays in Positive Economics. University of
Chicago Press, pp. 3–43.
Galla, T., Farmer, J.D., 2013. Complex dynamics in learning com-
plicated games. Proceedings of the National Academy of Sciences
110(4), 1232–1236. doi: 10.1073/pnas.1109672110 .
7",2023-11-28T07:28:08Z,t  t withihard it arguably t for rad ratr  joshua agrist haps emical invocateco nevertless agrist pi ske terogeneity yet for  chat as hi lard ucc su iai causal it ackledgement  diane wsomichael  ns t referencahmed causal methods adv econometrics nance journal agrist pi ske t how journal economic speivarrow re existence econometric all eta el jae ger tsoc interface aria is self journal economic tory articial intellence ley bor  irad qi ji pseudo t noticmatmatical society articial intellence ley bor  irad qi ji t tnal nance baudrlard simulaa simulatt  micha  chaeconomics na  chaisstat ph ys buck reflee phosop science campbell assessi evaluatprogram planni ass sll do journal political economy cpearl regressreal world economics re review cpearl gre be tel tblogy phosop  cinema eye re t journal phosophical economics falk re t mar wquantitative nance fer soharvey conditni journal nance   bend ag articial intellence nst method verso book londoed friedmat friedmaed essays positive economics  chicago  gall farmer lex proceedis natnal acamy sciences
paper_qf_38.pdf,8,"Epistemic Limits of Empirical Finance: Causal Reductionism and
  Self-Reference","  The clarion call for causal reduction in the study of capital markets is
intensifying. However, in self-referencing and open systems such as capital
markets, the idea of unidirectional causation (if applicable) may be limiting
at best, and unstable or fallacious at worst. In this research, we critically
assess the use of scientific deduction and causal inference within the study of
empirical finance and econometrics. We then demonstrate the idea of competing
causal chains using a toy model adapted from ecological predator/prey
relationships. From this, we develop the alternative view that the study of
empirical finance, and the risks contained therein, may be better appreciated
once we admit that our current arsenal of quantitative finance tools may be
limited to ex post causal inference under popular assumptions. Where these
assumptions are challenged, for example in a recognizable reflexive context,
the prescription of unidirectional causation proves deeply problematic.
","Ghilarducci, T., Knauss, Z., McGahey, R., Milberg, W., Landes, D.,
2023. The future of heterodox economics. Journal of Philosophical
Economics 16, 196–221. doi: TheFutureofHeterodoxEconomics .
Goodhart, C.A.E., 1984. Problems of Monetary Management: The
UK Experience. Macmillan Education UK, London. pp. 91–121.
doi:10.1007/978-1-349-17295-5_4 .
Grunberg, E., Modigliani, F., 1954. The predictability of social
events. Journal of Political Economy 62, 465–478. URL: https:
//www.jstor.org/stable/1827103 , doi: 10.1086/257604 .
Hands, D.W., 1990. Grunberg and Modigliani, public pre-
dictions and the new classical macroeconomics. Research
in the History of Economic Thought and Methodology 7,
207–223. URL: https://www.dwadehands.com/uploads/1/4/0/5/
140531545/hands_gru%E2%80%A6_modigliani.pdf .
Hardin, G., 1968. The tragedy of the commons. Science 162(3859),
1243–48. doi: 10.1126/science.162.3859.1243 .
Hardy, C., 2001. Self-organization, self-reference and inter-influences
in multilevel webs: Beyond causality and determinism. Journal of
Cybernetics and Human Knowing 8(3), 1–22. URL: https://www.
ingentaconnect.com/content/imp/chk/2001/00000008/00000003/95 .
Harvey, C., Liu, Y., 2014. Evaluating trading strategies. Journal of
Portfolio Management 40(5), 108–118. URL: https://ssrn.com/
abstract=2474755 , doi: 10.3905/jpm.2014.40.5.108 .
Hastings, A., 1993. Complex interations between dispersal and
dynamics: Lessons from coupled logistic equations. Ecology
74(5), 1362–1372. URL: https://www.jstor.org/stable/1940066 ,
doi:10.2307/1940066 .
Haugen, R.A., Baker, L.N., 1996. Commonality in the determinants
of expected stocks returns. Journal of Financial Economics 41,
401–439. URL: doi:10.1016/0304-405X(95)00868-F .
Henshel, R.L., 1995. The Grunberg/Modigliani and simon possibility
theorem: A social psychological critique. The Journal of Socio-
Economics 24, 501–520. doi: 10.1016/1053-5357(95)90020-9 .
Hirshleifer, D., Lo, A.W., Zhang, R., 2023. Social contagion and the
survival of diverse investment styles. Journal of Economic Dynam-
ics and Control 154. URL: https://ssrn.com/abstract=4032958 ,
doi:10.1016/j.jedc.2023.104711 .
Hoel, E.P., 2017. When the map is better than the territory. Entropy
19. doi: 10.3390/e19050188 .
Hofstadter, D., 1999. G¨ odel, Escher, Bach: An Eternal Golden Braid.
Basic Books, New York.
Hommes, C., 2021. Behavioral and experimental macroeconomics
and policy analysis: A complex systems approach. Journal of
Economic Literature 59(1), 149–219. URL: https://ssrn.com/
abstract=3288494 , doi: 10.1257/jel.20191434 .
Hume, D., 1739. A treatise of human nature. John
Noon, London. URL: https://oll.libertyfund.org/title/
bigge-a-treatise-of-human-nature .
Kahn, R., Whited, T.M., 2018. Identification is not causality, and
vice versa. The Review of Corporate Finance Studies 7, 1–
21. URL: https://academic.oup.com/rcfs/article/7/1/1/4590088 ,
doi:10.1093/rcfs/cfx020 .
Kauffman, L., 2009. Reflexivity and eigenform – the shape of pro-
cess. Constructivist Foundations 4(3), 121–137. URL: http:
//constructivist.info/4/3/121 .
Lawson, T., 2003. Soros’ theory of reflexivity: a critical comment.
Revue de philosophie ´Economique 14, 29–48. URL: https://www.
cairn.info/revue-de-philosophie-economique-2013-1-page-29.
htm, doi: 10.3917/rpec.141.0029 .
Lehmann-Waffenschmidt, M., Sandri, S., 2007. Recursivity and
self-referentiality of economic theories and their implications for
bounded rational actors. Dresden Discussion Paper Series in Eco-
nomics 03/07. URL: http://hdl.handle.net/10419/22744 .
Little, D., 1998. Microfoundations, Method and Causation: On the
Philosophy of the Social Sciences. Transaction Publishers, New
Brunswick, New Jersey.
Lo, A.W., 2004. The adaptive markets hypothesis. Journal of Port-
folio Management 30(5), 15–29. doi: 10.3905/jpm.2004.442611 .
Lo, A.W., 2017. Adaptive markets: Financial evolution
at the speed of thought. Princeton University Press,
Princeton. URL: https://press.princeton.edu/books/paperback/
9780691191362/adaptive-markets .
L´ opez de Prado, M., 2015. The Future of Empirical Finance. Journal
of Portfolio Management 41(4), 140–144. URL: https://ssrn.com/
abstract=2609734 , doi: 10.3905/jpm.2015.41.4.140 .
L´ opez de Prado, M., 2023a. Causal Factor Investing. Can factor
investing become scientific? Cambridge University Press, Cam-
bridge. URL: https://ssrn.com/abstract=4205613 , doi: 10.1017/
9781009397315 .
L´ opez de Prado, M., 2023b. The hierarchy of empirical evidence in
finance. Journal of Portfolio Management 49(9), 10–29. URL:
https://ssrn.com/abstract=4425855 , doi: 10.3905/jpm.2023.1.520 .
Lucas, R.E., 1976. Econometric policy evaluation: A critique.
Carnegie-Rochester Conference Series on Public Policy 1, 19–
46. URL: https://www.sciencedirect.com/science/article/pii/
S0167223176800036 , doi: 10.1016/S0167-2231(76)80003-6 .
Lux, T., 2021. Can heterogeneous agent models explain the alleged
mispricing of the S&P 500? Quantitative Finance 21, 1413–1433.
doi:10.1080/14697688.2021.1909744 .
MacKenzie, D., 2006. An engine, not a camera. How financial models
shape markets. Cambridge, MIT Press. doi: .
MacKenzie, D., Muniesa, F., Siu, L. (Eds.), 2007. Do economists
make markets? On the performativity of economics. Prince-
ton University Press, Princeton. doi: https://doi.org/10.1017/
S0266267109990332 .
May, R., 1976. Simple mathematical models with very complicated
dynamics. Nature 261, 459–467. doi: https://doi.org/10.1038/
261459a0 .
McCloskey, D.N., 1983. The rhetoric of economics. Journal of Eco-
nomic Literature 21(2), 481–517. URL: https://www.jstor.org/
stable/2724987 .
McLean, R., Pontiff, J., 2016. Does academic research destroy return
predictability? Journal of Finance 71(1), 5–31. URL: https:
//www.jstor.org/stable/43869094 , doi: 10.1111/jofi.12365 .
Merton, R.K., 1948. The self-fulfilling prophecy. Antioch Review
8, 193–210. URL: www.jstor.org/stable/4609267 , doi: 10.2307/
4609267 .
Mirowski, P., 2001. Machine Dreams: Economics Becomes a
Cyborg Science. Cambridge University Press. doi: 10.1017/
CBO9780511613364 .
Morgenstern, O., 1972. Descriptive, predictive and normative theory.
International Reivew for Social Sciences 25(4), 699–713. doi: 10.
1111/j.1467-6435.1972.tb01077.x .
Muth, J.F., 1961. Rational expectations and the theory of price
movements. Econometrica 29, 315–335. URL: https://www.jstor.
org/stable/1909635 , doi: 10.2307/1909635 .
Pearl, J., 1995. Causal diagrams for empirical research. Biometrika
82(4), 669–710. URL: https://www.jstor.org/stable/2337329 ,
doi:10.2307/2337329 .
Pearl, J., 2009a. Causal inference in statistics: An overview. Statis-
tics Surveys 3, 96 – 146. URL: https://ftp.cs.ucla.edu/pub/stat_
ser/r350.pdf , doi: 10.1214/09-SS057 .
Pearl, J., 2009b. Causality: Models, reasoning and inference
2nd ed. Cambridge University Press, Cambridge. doi: 10.1017/
CBO9780511803161 .
Pearl, J., Glymour, M., Jewell, N., 2016. Causal inference in Statis-
tics: A primer. John Wiley and Sons. URL: https://www.wiley.
com/en-us/9781119186847 .
Polakow, D., 2010. If a portfolio manager who cannot count finds
a four-leaf clover, is he still lucky? Investment Analysts Journal
39, 53–71. doi: 10.1080/10293523.2010.11082523 .
Popper, K., 1957. The Poverty of Historicism. Routledge, London.
doi:10.2307/2216437 .
Rasmussen, S., Ludeke, S., Hjelmborg, J., 2019. A major lim-
itation of the direction of causation model: non-shared envi-
ronmental confounding. Twin Research and Human Genetics
22(1), 14–26. URL: https://pubmed.ncbi.nlm.nih.gov/30663577/ ,
doi:10.1017/thg.2018.67 .
Sanders, J.B., Farmer, J.D., Galla, T., 2018. The prevalence of
chaotic dynamics in games with many players. Nature Scien-
tific Reports 8(4902), 1232–1236. doi: https://doi.org/10.1038/
s41598-018-22013-5 .
Saylors, R., Trafimow, D., 2020. Why the increasing use of com-
plex causal models is a problem: On the danger sophisticated
theoretical narratives pose to truth. Organizational Research
Methods 24(3), 616–629. URL: https://journals.sagepub.com/
doi/abs/10.1177/1094428119893452 , doi: https://doi.org/10.1177/
1094428119893452 .
Scholl, M.P., Calinescu, A., Farmer, J.D., 2021. How market
ecology explains market malfunction. Proceedings of the Na-
tional Academy of Sciences 118, 2015574118. doi: 10.1073/pnas.
2015574118 .
Schoutens, W., Simons, E., Tistaert, J., 2004. A Perfect Calibra-
tion! Now What? Wilmott 2004(2), 66–78. URL: https://
perswww.kuleuven.be/ ~u0009713/ScSiTi03.pdf , doi: 10.1002/WILM.
42820040216 .
Shaikh, A., 2013. On the role of reflexivity in economic analysis.
Journal of Economic Methodology 20(4), 439–445. doi: https://
doi.org/10.1080/1350178X.2013.859414 .
Simon, H.A., 1954. Bandwagon and underdog effects and the possi-
bility of election predictions. Public Opinion Quarterly 18, 245–
253. URL: www.jstor.org/stable/2745982 , doi: 10.1086/266513 .
Soros, G., 2009. Soros: General theory of reflexiv-
ity. Financial Times URL: https://www.ft.com/content/
0ca06172-bfe9-11de-aed2-00144feab49a .
Soros, G., 2013. Fallibility, reflexivity and the human un-
certainty principle. Journal of Economic Methodology 20,
439–445. URL: https://www.tandfonline.com/doi/full/10.1080/
1350178X.2013.859415 , doi: 10.1080/1350178X.2013.859415 .
Spencer-Brown, G., 1969. Laws of form. George Allen and Un-
win, London. URL: http://www.siese.org/modulos/biblioteca/b/
G-Spencer-Brown-Laws-of-Form.pdf .
Stiglitz, J.E., 2018. Where modern macroeconomics went wrong.
Oxford Review of Economic Policy 43, 70–106. URL: https://
academic.oup.com/oxrep/article/34/1-2/70/4781816 , doi: 10.1093/
oxrep/grx057 .
Sugihara, G., May, R., Ye, H., hao Hsieh, C., Deyle, E., Fogarty,
M., Munch, S., 2012. Detecting causality in complex ecosystems.
Science 338, 496–500. URL: https://www.science.org/doi/abs/
10.1126/science.1227079 , doi: 10.1126/science.1227079 .
Umpleby, S., 2010. From Complexity to Reflexivity: The Next Step
8",2023-11-28T07:28:08Z,hi lard ucc kuss mc ga y mi berg lands t journal phosophical economics t future of terodox economics good hart problems monetary management t exience macmlaeducatlondoru nberg modi lia ni t journal political economy hands ru nberg modi lia ni researhistory economic thought methodology hard it science hardy self beyond journal cybernetics humaki harvey  evaluati journal tfmanagement hastis lex lessons ecology aug ebaker cooali ty journal nancial economics ns l t ru nberg modi lia ni t journal soc economics harsh leaf er lo  social journal economic dycontrho el ntropy hofstadter eitr baaeternal golbr articial intellence basic books new york hobehral journal economic re home  noolondokhawhite inticatt review corate nance studikauffmarefleity construivist foundatns lawsoso ros revue comique leymawarschmidt and ri recur iv it dresdiscusspa serieco lile m foundatns method causatophosop social scienransapubrs new brunswick new jersey lo t journal t management lo adaptive nancial princeto  princetorad t future emical nance journal tfmanagement rad causal faor investi cacambridge   cam rad t journal tfmanagement lucas econometric carnegie rocster conference seripublic policy lux caquantitative nance mac keie ahow cambridge  mac keie units siu eds do oprince   princetomay simple nature mc ose key t journal eco re mc leapontf dojournal nance mortot antics review mi row  machine dreams economics becomcyborg science cambridge   morgenstersiptive internatnal rei ve social sciencmath ratnal econometric pearl causal bme rica pearl causal astat is surveys pearl causality mols cambridge   cambridge pearl gay our jel causal stat is  rey sons poll   investment analysts journal  poverty historic is routledge londorasmussenu ke hj elm borg twiresearhumagenetics sanrs farmer gall t nature sci erets say lor a row w oorganizatnal researmethods schocal iescu farmer how proceedis na acamy sciencshortesimotis tae rt fe ca libre  what pot sc si ti sh articial intellence kh ojournal economic methodology simobandwagopublic opinquarterly so ros so ros genl nancial timso ros fallibity journal economic methodology spencer browlaws george alleulondospencer browlaws form stlitz wre oxford review economic policy ug chara may ye hsie yle fog party mutei science ump le by from lexity refleity t next step
paper_qf_38.pdf,9,"Epistemic Limits of Empirical Finance: Causal Reductionism and
  Self-Reference","  The clarion call for causal reduction in the study of capital markets is
intensifying. However, in self-referencing and open systems such as capital
markets, the idea of unidirectional causation (if applicable) may be limiting
at best, and unstable or fallacious at worst. In this research, we critically
assess the use of scientific deduction and causal inference within the study of
empirical finance and econometrics. We then demonstrate the idea of competing
causal chains using a toy model adapted from ecological predator/prey
relationships. From this, we develop the alternative view that the study of
empirical finance, and the risks contained therein, may be better appreciated
once we admit that our current arsenal of quantitative finance tools may be
limited to ex post causal inference under popular assumptions. Where these
assumptions are challenged, for example in a recognizable reflexive context,
the prescription of unidirectional causation proves deeply problematic.
","in the Systems Sciences, in: Trappl, R. (Ed.), Cybernetics and
Systems 2010, Austrian Society for Cybernetic Studies, Vienna.
p. 281–286. URL: https://api.semanticscholar.org/CorpusID:
209439003 . available at https://cepa.info/891.
Varela, F.J., 1975. A calculus for self-reference. International Jour-
nal of General Systems 2(1), 5–24. doi: https://doi.org/10.1080/
03081077508960828 .
Von Foerster, H., 2003. On constructing a reality, in: Understanding
Understanding. Springer-Verlag, New York, pp. 211–227. doi: 10.
1007/0-387-21722-3_8 .
Von Glasersfeld, E., 1984. An introduction to radical constructivism,
in: Watzlawick, P. (Ed.), The invented reality.. Norton, New York,
pp. 17–40. URL: https://www.vonglasersfeld.com/070.1 .
Wilcox, D.L., Gebbie, T.J., 2014. Hierarchical causality in finan-
cial economics. SSRN , 1–16,doi: http://dx.doi.org/10.2139/ssrn.
2544327 .
Wilcox, D.L., Gebbie, T.J., 2015. On pricing kernels, information
and risk. Investment Analysts Journal 44, 1–19. doi: 10.1080/
10293523.2014.994437 .
Ziliak, S.T., McCloskey, D.N., 2008. The Cult of Statistical Sig-
nificance. University of Michigan Press. doi: https://doi.org/10.
3998/mpub.186351 .
9",2023-11-28T07:28:08Z,tems scienrap ed cybernetics tems austriasociety cybernetic studivienna corpus are la internatnal jour genl tems vofo easter ounrstandi unrstandi  verlag new york volaser sf el awatzlawick ed t nortonew york wsonewbie hirchical wsonewbie oinvestment analysts journal lia mc ose key t cult statistical s  micha
paper_qf_39.pdf,1,Modelling Biochemical Operations on RNA Secondary Structures,"  In this paper we model several simple biochemical operations on RNA molecules
that modify their secondary structure by means of a suitable variation of
Gro\\\\ss e-Rhode's Algebra Transformation Systems.
","arXiv:cs/0306016v1  [cs.CE]  3 Jun 2003Modelling Biochemical Operations on RNA
Secondary Structures⋆
Merc` e Llabr´ es and Francesc Rossell´ o
Dept. of Mathematics and Computer Science,
Research Institute of Health Science (IUNICS),
University of the Balearic Islands,
07122 Palma de Mallorca (Spain)
{merce.llabres,cesc.rossello }@uib.es
Abstract. Inthispaper we model several simple biochemical operation s
on RNA molecules that modify their secondary structure by me ans of a
suitable variation of Große-Rhode’s Algebra Transformati on Systems.
1 Introduction
Biochemicalprocessesareresponsibleformostoftheinformation processingthat
takes place inside the cell. In the recent years, several represen tations and sim-
ulations of speciﬁc biochemical processes have been proposed usin g well known
rewriting formalisms borrowed from Theoretical Computer Science . Let us men-
tion, for instance, Fontana’s lambda calculus chemistry [3,4], recent ly revised by
M¨ uller [9] (for a recent survey on artiﬁcial chemistry, see [14]), t he stochastic
Petri net approach [5], the π-calculus representation of biochemical processes
carried out by networks of proteins [10], and the graph replaceme nt approach to
DNA operations [8]. In the latter, an ad hocgraph replacement formalism is de-
veloped to formalize DNA biochemical operations like annealing or dena turing,
by considering DNA double strands to be special graphs.
There is another popular line of research in theoretical biochemistr y that
aims to represent the three-dimensional structure of biopolymer s, and specially
of DNA and RNA, by means of diﬀerent kinds of formal grammars; se e, for
instance, [7,13] for two surveys on this topic. The ultimate goal of s uch a repre-
sentation is to understand how the three-dimensional structure of a biopolymer
is determined from its sequence of monomers (for instance, how th e sequence
of ribonucleotides of an RNA molecule determines its secondary stru cture; see
below for the relevant details of RNA’s biochemistry), and how this st ructure
evolves when the biopolymer is modiﬁed through biochemical process es.
Sooner or later, this two lines of research should intersect, and th e main
goal of this paper is to move these two lines of research a step close r. We for-
malize some simple biochemical processes on RNA molecules, like for inst ance
⋆This work has been partially supported by the Spanish DGES, g rant BFM2000-1113-
C02-01.",2003-06-03T08:48:54Z, jumolli bcmical oatns secondary struurmer ll abr francros sell pt matmatics uter science researinstitute alth science  barbaric islands alma mallory sp articial intellence abstra i pa gro rho algebra transform at tems introdubcmical processare responsible for most of t informatitoical uter science  montana i itre t sooner   spanish
paper_qf_39.pdf,2,Modelling Biochemical Operations on RNA Secondary Structures,"  In this paper we model several simple biochemical operations on RNA molecules
that modify their secondary structure by means of a suitable variation of
Gro\\\\ss e-Rhode's Algebra Transformation Systems.
","ribonucleotide removals or mutations, and their eﬀect on their thre e-dimensional
structures by means of a variant of Große Rhode’s Algebra Transf ormation Sys-
tems (ATS) [6] on partial algebras [1] representing RNA biomolecules .
Before entering into more details, it is time to introduce a little biochem istry.
As probably everybody knows, RNA molecules, together with DNA mo lecules
and proteins, form the molecular basis of life. An RNA molecule can be v iewed
as a chain of ribonucleotides, and each ribonucleotide is characteriz ed by the
base attached to it, which can be either adenine ( A), cytosine ( C), guanine
(G) or uracil ( U). An RNA molecule is uniquely determined by the sequence of
bases along its chain, and it has a deﬁnite orientation. Such an orient ed chain of
ribonucleotides is called the primary structure of the RNA molecule.
In the cell and in vitro, each RNA molecule folds into a speciﬁc three-
dimensionalstructurethat determinesits biochemicalactivity. To determine this
structure from the primary structure of the molecule is one of the main open
problems in computational biology, and partial solutions have been p roposed
using Stochastic Context Free Grammars and Dynamic Programming , among
other tools; see, for instance, [2, Chaps. 9, 10].
This three-dimensional structure is held together by weak interac tions called
hydrogen bonds between pairs of non-consecutive bases.1Almost all these bonds
form between complementary bases , i.e., between AandUand between Cand
G, but other pairings do also occur sporadically. For simplicity, in this pa per we
shall only consider pairings between complementary bases.
In most representations of RNA molecules, the detailed description of their
three-dimensional structure is overlookedand the attention is fo cused on its sec-
ondary structure : the set of its base pairs, or contacts. Secondary structures are
actually a simpliﬁed representation of RNA molecules’ three-dimensio nal struc-
ture, but that is enough in some applications, as diﬀerent levels of “g raining”
are suitable for diﬀerent problems. Two restrictions are usually add ed to the
deﬁnition of secondary structure:
–If two bases biandbjare paired, then neither biorbjcan bond with any
other base; this restriction is called the unique bonds condition .
–If contacts exist between bases biandbjand between bases bkandbl, and if
bklies between biandbj, thenblalso lies between biandbj; this restriction
is called the no-pseudoknots condition .
The unique bonds condition simply captures the fact that the “bond ” between
two consecutive bases is of diﬀerent nature, as a part of the molec ule’s back-
bone. The no-pseudoknots condition is usually added in order to ena ble the use
of dynamic programming methods to predict RNA secondary struct ures and,
although real three-dimensional RNA structures have (pseudo) knots, we impose
it here to show the scope of our approach:if pseudoknots are allow ed, one simply
has to allow them in the algebraic representation of RNA secondary s tructures
1Actually, for a hydrogen bond to be stable, the bases involve d in it must be several
nucleotides apart, but for simplicity we shall only conside r the restriction that they
must be non-consecutive.",2003-06-03T08:48:54Z,gro rho algebra trans ys before as aasuito stochastic context free graars dynamic prograi caps  almost and and and for isecondary two   t t aually
paper_qf_39.pdf,3,Modelling Biochemical Operations on RNA Secondary Structures,"  In this paper we model several simple biochemical operations on RNA molecules
that modify their secondary structure by means of a suitable variation of
Gro\\\\ss e-Rhode's Algebra Transformation Systems.
","and to remove the corresponding production rules from the rewrit ing system.
Thus,
This allows traditionally to represent an RNA molecule as a labelled graph ,
with nodes representing the ribonucleotides, and their labels denot ing the bases
attached to them, and arcs of two diﬀerent kinds: ones represen ting the order
of the bases in the primary structure (the backbone ) and the rest representing
the bonds that form the secondary structure (the contacts) [11,12]. Our repre-
sentation is slightly diﬀerent: the backbone is represented by a par tial algebra
corresponding, essentially, to a labelled ﬁnite chain, and then the co ntacts are
speciﬁed as arcs of a graph on the nodes of the backbone.
There are some biochemical operations that can be carried out on a n RNA
molecule. For instance, a ribonucleotide can be added or removed so mewhere in
the primary structure, a contact can form between two compleme ntary bases, or
it can be removed, and a base can mutate into another base. These operations
may havecollateraleﬀects: forinstance, if a basemutates into an otherone and it
was involved in a contact, then this contact will disappear, as the co rresponding
bases will no longer be complementary, and if a nucleotide is removed a nd as
a consequence two nucleotides forming a contact become consecu tive, then this
contact will also break.
It is precisely when we tried to specify these side eﬀects that we wer e not
able to use simple graph transformation systems in an easy way, and we decided
to use the ATS approach. ATS is a very powerful algebra rewriting f ormalism,
introduced by M. Große-Rhode in 1999 in order to specify the behav ior of com-
plex states software systems. It is operationally described, but n ot categorically
formalized, and it takes care of side eﬀects of the application of rule s, similar
to those found in our work. Unfortunately, even the ATS formalism as deﬁned
in [6], which was designed with software engineering speciﬁcation applic ations
in mind, was not suitable, as it stands, for our purposes. Thus we ha ve slightly
modiﬁed a simpliﬁed version of it, and we have dubbed the resulting for mal-
ismWithdrawal-based Algebra Transformation Systems (WATS). The reason is
that, in our approach, the inconsistencies are eliminated by retrea ting, i.e., by
removing, in a controlled way, the elements and operations that pro duce them,
while in the originalATS approachthe inconsistencieswereeliminated b yadding
operations and identifying points.
The rest of this paper is organized as follows. In Section 2 we repres ent RNA
moleculesassuitablepartialalgebras,inSection3webrieﬂyintroduc etheWATS
formalism, and then in Section 4 we show how to represent the afore mentioned
biochemical operations on RNA molecules by means of WATS productio n rules.
A ﬁnal section on Conclusions closes the paper.
2 RNA Molecules as Partial Algebras
Roughly speaking, we represent the primary structure of an RNA m olecule as a
chainnof length n∈Nwith a label in{A,C,G,U}attached to each element of",2003-06-03T08:48:54Z,  our tre for tse it gro rho it unfortunately  withdrawal algebra transformattems t t iseseseconusns moleculpartial algebras roughly with
paper_qf_39.pdf,4,Modelling Biochemical Operations on RNA Secondary Structures,"  In this paper we model several simple biochemical operations on RNA molecules
that modify their secondary structure by means of a suitable variation of
Gro\\\\ss e-Rhode's Algebra Transformation Systems.
","the chain, representing the base attached to the corresponding ribonucleotide,
and its secondary structure by means of ordered pairs in n×n.
LetΣpsbe the following many-sorted signature:
Sorts:Nat,Bases
Opns:suc:Nat→Nat
First,Last :→Nat
A,C,G,U :→Bases
minor:Nat,Nat→Nat
label:Nat→Bases
κ:Bases→Bases
AnRNA primary structure is a ﬁnite partial Σps-algebra
P= (PNat,PBases;
FirstP,LastP,AP,CP,GP,UP,sucP,minorP,labelP,κP)
such that:
i) (PNat;FirstP,LastP,sucP) is a chain with successor operation sucP, ﬁrst
elementFirstPand last element LastP.
ii) The operation minorPmodels the strict minority relation on this chain:
minorP(x,y) =xif and only if there exists some n≥1 such that y=
(sucP)n(x).
iii) The values of the nullary operations AP,CP,GP,UPare pairwise diﬀerent,
PBases={AP,CP,GP,UP}, and on this set the operation κPis given by
the involution
κP(AP) =UP, κP(UP) =AP, κP(CP) =GP, κP(GP) =CP.
iv) The operation labelPis total.
Notice that all these conditions except the last one cannot be spec iﬁed through
quasi-equations, since they are not satisﬁed by a trivial (with only o ne element
of each sort) total Σps-algebra.
LetΣssbe now the signature containing Σpsand, in addition, the following
sorts and operation symbols:
Sorts:Contacts
Opns:p1:Contacts→Nat
p2:Contacts→Nat
AnRNA secondary structure is a partial Σss-algebra
B= (BNat,BBases,BContacts;
FirstB,LastB,AB,CB,GB,UB,sucB,minorB,labelB,κB,pB
1,pB
2)",2003-06-03T08:48:54Z, sorts basns rst last basbasbasbasabasrst last rst last rst and last t mols t are basis t is notice  sorts contas ns contas contas abascontas rst last
paper_qf_39.pdf,5,Modelling Biochemical Operations on RNA Secondary Structures,"  In this paper we model several simple biochemical operations on RNA molecules
that modify their secondary structure by means of a suitable variation of
Gro\\\\ss e-Rhode's Algebra Transformation Systems.
","whoseΣps-reduct is an RNA primary structure and it satisﬁes moreover the
following quasi-equations:
(1)pB
1andpB
2are total
(2)pB
1(x) =pB
1(y)⇒x=y
(3)pB
2(x) =pB
2(y)⇒x=y
(4)pB
1(x) =pB
2(y)⇒x=y
(5)minor(succB(pB
1(x)),pB
2(x)) =succB(pB
1(x))
(6)minor(pB
1(x),pB
1(y)) =pB
1(x)∧minor(pB
1(y),pB
2(x)) =pB
1(y)
=⇒minor(pB
2(y),pB
2(x)) =pB
2(y)
(7)κB(labelB(pB
1(x))) =labelB(pB
2(x))
In such an RNA secondary structure, each element cof sortContacts repre-
sents, of course, a contact between nucleotides pB
1(c) andpB
2(c). Equations (2),
(3)and(4)representtheunique bondscondition,equation(5)re presentsthe fact
that there cannot exist a contact between a nucleotide and itself o r its successor
in the primary structure, equation (6) represents the no-pseud oknots condition,
and equation (7) represents the fact that a contact can only pair complementary
bases. Notice that, if we simply omit equation (6) then, pseudoknot s are allowed
in the representation of RNA molecules.
LetΓss= (Σss,CE) be the speciﬁcation whose set of consistence equations
CEare the quasi-equations (1) to (7) above. Let AlgΓssthe category whose ob-
jects are all partial Γss-algebras , i.e., those partial Σss-algebras satisfying equa-
tion (1) to (7), and the morphisms between them are the plain homom orphisms,
and let AlgRNAbe the full subcategory of AlgΓsssupported on the RNA sec-
ondary structures.
3 Withdrawal-based Σss-Algebra Transformation
Systems
Our Withdrawal-based Algebra Transformation Systems (WATS) ar e a modi-
ﬁcation of a simpliﬁed version of the Algebra Transformation System s (ATS)
introduced in [6]. This modiﬁcation only aﬀects the last step in the deﬁn ition of
the application of a rewriting rule through a matching, and therefor e all deﬁni-
tions previous to that one are the same as in the original ATS formalis m. Since
we are only interested in rewriting RNA secondary structures, we s hall only give
the main deﬁnitions for the signature Σssintroduced in the previous section.
So, to simplify the notations, let us denote by S,Ωandηthe set of sorts, the
set of operation symbols and the arity function of the signature Σss. For every
ϕ∈Ω, setη(ϕ) = (ω(ϕ),σ(ϕ))∈S∗×S.
AΣss-presentation is a pair P= (PS,PE) where PS= (Ps)s∈Sis anS-
set, whose elements will be called generators , andPEis a set of equations with
variables in PS
t=t′, t,t′∈TΣss(PS)s, s∈S.",2003-06-03T08:48:54Z,icontas equatns notice  are  lg lg be lg withdrawal algebra transformattems our withdrawal algebra transformattems algebra transformattem  since so for ps is is
paper_qf_39.pdf,6,Modelling Biochemical Operations on RNA Secondary Structures,"  In this paper we model several simple biochemical operations on RNA molecules
that modify their secondary structure by means of a suitable variation of
Gro\\\\ss e-Rhode's Algebra Transformation Systems.
","A special type of equations are the function entries , of the form
ϕ(a) =b, ϕ∈Ω, a∈Pω(ϕ)
S, b∈Pσ(ϕ).
A presentation is functional when all its equations are function entries, and a
functional presentation is consistently functional when for every ϕ∈Ωand
a∈Pω(ϕ)
S, there is at most one function entry of the form ϕ(a) =binPE.
Letp:PS→P′
Sbe a mapping of S-sets. Ifeis an equation t=t′with
t,t′∈TΣss(PS)s, then we shall denote by e[p] the equation t(p) =t′(p) where
t(p),t′(p)∈TΣss(P′
S)sare the terms obtained from tandt′, respectively, by
replacing all variables in them by their corresponding images under p. In partic-
ular, ifeis the function entry ϕ(a) =b, thene[p] stands for the function entry
ϕ(p(a)) =pσ(ϕ)(b). Given a mapping of S-setsp:Ps→P′
sand any set Eof
equations with variables in PS, let
E[p] ={e[p]|e∈E}.
Amorphism ofΣss-presentations p: (PS,PE)→(P′
S,P′
E) is then a mapping of
S-setsp:PS→P′
Ssuch that PE[p]⊆P′
E.
AΣss-rewriting rule is a pair of Σss-presentations, written r= (Pl←→Pr),
wherePl= (Xl,El) andPr= (Xr,Er) are functional presentations. Informally,
the left-hand side presentation in such a rule speciﬁes the elements and op-
erations that must be removed from the algebra which the rule is app lied to,
while its right-hand side presentation speciﬁes the elements and ope rations to
be added. The generators that occur in a rule play the role of variab les (and
therefore we shall usually call them variables ): those appearing in the left-hand
side presentation must be matched into the algebra to rewrite, and those ap-
pearing in the right-hand side presentation must be matched into th e resulting
algebra, in such a way that if a variable occurs in both parts of a rule, its image
must be preserved. In the sequel, we shall assume that all variable s that occur
in rewriting rules are taken from a universal S-setXthat is globally ﬁxed and
disjoint from all sets of operation symbols in the signatures we use. We shall also
assume that Xis large enough to contain equipotent copies of the carrier sets
of all algebras we are interested in.
For every Σss-rewriting rule r= (Pl←→Pr), withPl= (Xl,El) andPr=
(Xr,Er), let:
X0
l=Xl−Xr, X0
r=Xr−Xl,
E0
l=El−Er, E0
r=Er−El.
For every Γss-algebra A= (A,(ϕA)ϕ∈Ωss)), letAS=Aand
Ae={ϕ(a) =b|ϕ∈Ωss, a∈domϕA, ϕA(a) =b}.
Amatchmfor aΣss-rewriting rule r= (Pl←→Pr) inAis simply a presenta-
tion morphism m:Pl→(AS,AE). Theextension
m∗:Xl∪Xr→AS⊔X0
r",2003-06-03T08:48:54Z, be  is iveps of morphism supl pr pl xl el pr er informally t ithat  is for pl pr pl xl el pr er xl xl el er er el for and matfor pl pr is pl t extensxl
paper_qf_39.pdf,7,Modelling Biochemical Operations on RNA Secondary Structures,"  In this paper we model several simple biochemical operations on RNA molecules
that modify their secondary structure by means of a suitable variation of
Gro\\\\ss e-Rhode's Algebra Transformation Systems.
","ofmis deﬁned by2
m∗(x) =/braceleftbiggm(x)∈ASifx∈Xl
x∈X0
rifx∈X0
r=Xr−Xl
Theapplication ofrtoAthrough mrewrites then Ainto the partial Γss-
algebraBdeﬁned, step by step, as follows:
1) SetBS= (AS−m(X0
l))⊔X0
r. This step removes from Athe elements that
are images of elements in Xlthat do no longer belong to Xr, and adds to it
the elements in Xrthat did not belong to Xl.
2) SetBE= (AE−E0
l[m])∪E0
r[m∗]. This step removes from Athe operations
that are images of function entries in Elthat do no longer belong to Er, and
adds to it the equations in Erthat did not belong to El, with variables in
BS.
3) Since the presentation ( BS,BE) is functional, it deﬁnes a partial Σss-algebra
with carrier set B=BSby simply translating the function entries in BEinto
operations; if this presentation is not consistently functional, the n we must
identify elements in Bin order to remove inconsistencies. This step can be
formally described by means of a functor left adjoint to a functor t hat sends
everyΣss-algebra to its presentation ( AS,AE).
4) If the Σss-algebra deﬁned in this way satisﬁes equations (1) to (7), we are
done. Otherwise, there are two possibilities:
–Every contact x∈BContacts that violates equations (1), (5) or (7) is
removed.
–After performing all removals in the previous step, if there are still pairs
of contacts x,y∈BContacts that violate equations (2), (3), (4) or (6),
then, if one of them comes from X0
rand the other comes from AS, the
one from X0
ris removed and the other one preserved, and otherwise both
are removed.
It is in step (4) where the main diﬀerence between Große-Rhode’s or iginal
ATS formalism and our WATS formalism lies. In ATS, the Σss-algebra obtained
in (3) would be forced to satisfy equations (1) to (7) by taking its un iversal
solution in AlgΓss, and thus adding operations and identifying elements. In our
formalism, violations of equations (1) to (7) are obviated by simply re moving in
a controlled way the contacts that yield them.
4 Biochemical operations modelled by means of WATS
The biochemical operations considered in this paper are the addition , deletion
and mutation of a ribonucleotide and the addition and deletion of a con tact.
Each of these biochemical operations can be modelled as a rewriting s tep of a
2As always, we identify any set with its image into its disjoin t union with any other
set.",2003-06-03T08:48:54Z, xl xl t applicatthrough into  set  t xl that that xl set  t el that er er that el since by into bi  otrwise every contas after contas it gro rho ilg ibcmical t eaas
paper_qf_39.pdf,8,Modelling Biochemical Operations on RNA Secondary Structures,"  In this paper we model several simple biochemical operations on RNA molecules
that modify their secondary structure by means of a suitable variation of
Gro\\\\ss e-Rhode's Algebra Transformation Systems.
","WATS by the applications of a Σss-rewritingrule to a RNA secondarystructure.
The rewriting rules that model these biochemical operations are th e following
ones.
Adding a nucleotide: Wehavetoconsiderthreediﬀerentcases,corresponding
to adding the new nucleotide at the beginning of the chain, at the end , or
in the middle of it. In this case, each rule must be understood as havin g a
parameter xwhich corresponds to the base attached to the new nucleotide.
So, there are four diﬀerent values of this parameter, the nullary o peration
symbols A,U,CandG.
–RulePadd−base−first(x) has:
•asPlthe set of variables Xl={k1}of sortNatand the set of
equations El={First=k1};
•asPrthe set of variables Xr={t,k1,k0}, of sorts t∈Basesand
k1,k0∈Nat, and the set of equations Er={First=k0,suc(k0) =
k1,label(k0) =t,x=t}.
–RulePadd−base−last(x) has
•asPlthe set of variables Xl={kn}of sortNatand the set of
equations El={Last=kn};
•asPrthe set of variables Xr={t,kn+1,kn}, of sorts t∈Basesand
kn+1,kn∈Nat,andthesetofequations Er={Last=kn+1,suc(kn)
=kn+1,label(kn+1) =t,x=t}.
–RulePadd−base−middle(x) has:
•asPlthe set of variables Xl={ki,kj}of sortNatand the set of
equations El={suc(ki) =kj};
•asPrthe set of variables Xr={t,ki,kj,k}, of sorts t∈Basesand
ki,kj,k∈Nat, and the set of equations Er={suc(ki) =k,suc(k) =
kj,label(k) =t,x=t}.
Remove a nucleotide: We have to consider again three diﬀerent cases, corre-
sponding to removing the nucleotide at the beginning of the chain, at the
end, or in the middle of it.
–RulePdel−base−firsthas:
•asPlthe set of variables Xl={k1,k0}, both of sort Nat, and the
set of equations El={First=k0,suc(k0) =k1};
•asPrthe set of variables Xr={k1}of sortNatand the set of
equations Er={First=k1}.
–RulePdel−base−lasthas:
•asPlthe set of variables Xl={kn−1,kn}, both of sort Nat, and the
set of equations El={Last=kn,suc(kn−1) =kn};
•asPrthe set of variables Xr={kn−1}of sortNatand the set of
equations Er={Last=kn−1}.
–RulePdel−base−middlehas:
•asPlthe set of variables Xl={ki,kj,kl}, all of them of sort Nat,
and the set of equations El={suc(ki) =kl,suc(kl) =kj};
•asPrthe set of variables Xr={ki,kj}of sortNatand the set of
equations Er={suc(ki) =kj}.",2003-06-03T08:48:54Z,t addi  he to consir three di iso and rule add pl t xl and el rst pr t basand er rst rule add pl t xl and el last pr t basand er last rule add pl t xl and el pr t basand er remove  rule l pl t xl el rst pr t and er rst rule l pl t xl el last pr t and er last rule l pl t xl el pr t and er
paper_qf_39.pdf,9,Modelling Biochemical Operations on RNA Secondary Structures,"  In this paper we model several simple biochemical operations on RNA molecules
that modify their secondary structure by means of a suitable variation of
Gro\\\\ss e-Rhode's Algebra Transformation Systems.
","Mutating a base: The mutation of a base is speciﬁed by just redeﬁning the
operation label. Thus we consider the following rule:
–RulePmutation has:
•asPlthe set of variables Xl={x,y,k}, of sorts x,y∈Basesand
k∈Nat, and the set of equations El={label(k) =x};
•asPrthe set of variables Xr={x,y,k}, of sorts x,y∈Basesand
k∈Natand the set of equations Er={label(k) =y}.
Adding a contact: To add a contact we simply add a new element of sort
Contact and the projections from it to the nucleotides it bonds.
–RulePadd−contacthas:
•asPlthe set of variables Xl={x,y,ki,ki+1,kj}, of sorts x,y∈
Basesandki,ki+1,kj∈Nat, and the set of equations El={suc(ki)
=ki+1,minor(ki+1,kj) =ki+1,κ(x) =y,κ(y) =x,label(ki) =x,
label(kj) =y};
•asPrthe set of variables Xr={x,y,ki,ki+1,kj,c}, of sorts x,y∈
Bases,ki,ki+1,kj∈Natandc∈Contacts , and the set of equations
Er={suc(ki) =ki+1,minor(ki+1,kj) =ki+1,κ(x) =y,κ(y) =
x,p1(c) =ki,p2(c) =kj,label(ki) =x,label(kj) =y}.
Remove a contact: To remove a contact we simply delete it.
–RulePdel−contacthas:
•asPlthe set of variables Xl={ki,kj,c}, of sorts ki,kj∈Natand
c∈Contacts , and the set of equations El={p1(c) =ki,p2(c) =kj};
•asPrthe set of variables Xr={ki,kj}, both of sort Nat, and the
set of equations Er=∅.
It is not diﬃcult to check that an RNA secondary structure is always rewrit-
ten by the application of any one of these rules through any matchin g into an
RNA secondary structure, and that in each case their eﬀect is the desired one.
This must be done rule by rule and case by case.
5 Conclusion
We have modelled several simple biochemical operations on RNA molecu les that
modify their secondary structure by means of rewriting rules in a mo diﬁed ver-
sion of the Algebra Transformation Systems of Große-Rhode, whic h we have
dubbed Withdrawal-based Algebra Transformation Systems. This m odiﬁcation
has been made ad hocfor algebras representing RNA secondary structures, but
we feel that the philosophy ofremoving inconsistencies by retreat ing should have
applicationsinothercontexts,andcouldprobablybeformalizedfor algebrasover
arbitrary speciﬁcations.
In this paper we have made some simpliﬁcations on the RNA secondary
structure that could perfectly be avoided. For instance, if we wan t to allow
contacts between pairs of basis other than the usual complement ary pairs, like
for instance between G and U (they are called wobble pairs , not so uncommon),
then we only have to replace the involution κby a symmetric relation on the",2003-06-03T08:48:54Z,stati t  rule mtpl t xl basand el pr t basand and er addi to conta rule add pl t xl basand ki el pr t basand contas er remove to rule l pl t xl and contas el pr t er it  conus algebra transformattems gro rho withdrawal algebra transformattems  ifor
paper_qf_39.pdf,10,Modelling Biochemical Operations on RNA Secondary Structures,"  In this paper we model several simple biochemical operations on RNA molecules
that modify their secondary structure by means of a suitable variation of
Gro\\\\ss e-Rhode's Algebra Transformation Systems.
","carrier of sort Bases. And if we want to impose that two bases paired by a
contact must be at least at a ﬁxed distance, we only have to modify in a suitable
way equation (5).
There are also other collateral eﬀects that could, and probably sh ould, be
speciﬁed. For instance, isolated contacts tend to break, and pse udoknots should
be allowed under certain circumstances.
References
1. Burmeister, P., A Model Theoretic Oriented Approach to Partial Algebras . Math-
ematical Research 32, Akademie-Verlag (1986).
2. Durbin,R., Eddy,S., Krogh, A.,Mitchison, G., Biological SequenceAnalysis . Cam-
brideg Univ. Press (1998).
3. Fontana,W.,“Algorithmicchemistry.”In Artiﬁcial Life II (Addison-Wesley,1992),
159–210. Also Technical Report LA-UR90-1959, Los Alamos Na tional Lab. (1990).
4. Fontana, W., Buss, L. W. “The barrier of objects: from dyna mical systems to
bounded organization.” In Boundaries and Barriers (Addison-Wesley, 1996), 56–
116.
5. Goss, P., Peccoud, J. “Quantitative modeling of stochast ic systems in molecular
biology by using stochastic Petri nets.” Proc. Nat. Acad. Sicences USA 95 (1998),
6750–6755.
6. Große-Rhode, M. “Speciﬁcation of State Based Systems by A lgebra Rewrite Sys-
tems and Reﬁnements.” Technical Report 99-04, TU Berlin (Ma rch 1999).
7. Mayoh, B. “DNA Pattern multigrammars.” Technical Report (1994).
8. McCaskill, J., Niemann, U.“Graph replacementchemistry for DNAprocessing.” In
Proc. DNA6: 6th International Meeting on DNA based computer s, DIMACS Series
in Discrete Mathematics and Theoretical Computer Science ( AMS, to appear),
89–99.
9. M¨ uller, S. Functional Organization in Molecular Systems and the λ-calculus. PhD
Thesis, Univ. Wien (1999).
10. Regev, A., Silverman, W., Shapiro, E. “Representation a nd simulation of biochem-
ical processes using the π-calculus process algebra.” In Proc. Paciﬁc Symposyum
on Biocomputing 2001 (2001), 459–470.
11. C. Reidys, P. F. Stadler, Bio-molecular shapes and algeb raic structures. Computers
& Chemistry 20 (1996), 85–94.
12. P. Schuster, P. F. Stadler, Discrete models of biopolyme rs. Univ. Wien TBI
Preprint No. pks-99-012 (1999).
13. Searls, D. B. “Formal Language and Biological Macromole cules”. In Mathemati-
cal Support for Molecular Biology , DIMACS Series in Discrete Mathematics and
Theoretical Computer Science 47 (AMS, 1999), 117–140.
14. Speroni di Fenizio, P. “Artiﬁcial Chemistries.” Bulletin EATCS 76 (February
2002), 128–141.",2003-06-03T08:48:54Z,basand tre for referencbur mister mtoic oriented approapartial algebras math researakami verlag  eddy rog mitis oblogical sequence analysis cam uiv  montana algorithmic cmistry iart le additsley also technical ret los almost na lab montana bus t indaribarriers additsley loss pec oud quantitative i proc acad size cgro rho spec state based tems rewrite ys re technical ret berlima may oh paertechnical ret mc ca skl nie mangraph processi iproc internatnal meeti seridisete matmatics toical uter science funnal organizatmolecular tems ph tsis uiv wiere get svermashapi representatiproc pac sym pos yum b uti reid ys smaller b uters cmistry uster smaller disete uiv wiepre print no earls formal  blogical mao mole ima t math supt molecular blogy seridisete matmatics toical uter science soeni z art cmist ribuliruary
paper_qf_40.pdf,1,Filtrations,"  In this article, we define the notion of a filtration and then give the basic
theorems on initial and progressive enlargements of filtrations.
","arXiv:0712.0622v1  [math.PR]  4 Dec 2007FILTRATIONS
DELIA COCULESCU AND ASHKAN NIKEGHBALI
Abstract. In this article, we deﬁne the notion of a ﬁltration and then
give the basic theorems on initial and progressive enlargem ents of ﬁltra-
tions.
Definitions
Filtrations have been introduced by Doob and have been a fund amental
feature of the theory of stochastic processes. Most basic ob jects, such as
martingales, semimartingales, stopping times or Markov pr ocesses involve
the notion of ﬁltration.
Deﬁnition 1. Let (Ω ,F,P) be a probability space. A ﬁltration on (Ω,F,P)
is an increasing family ( Ft)t≥0of sub- σ-algebras of F. In other words, for
eacht,Ftis aσ-algebra included in Fand if s≤t,Fs⊂ F t. A proba-
bility space (Ω ,F,P) endowed with a ﬁltration ( Ft)t≥0is called a ﬁltered
probability space.
We now give a deﬁnition which is very closely related to that o f a ﬁltration:
Deﬁnition 2. A stochastic process ( Xt)t≥0on (Ω ,F,P) is adapted to the
ﬁltration ( Ft) if, for each t≥0,XtisFt-measurable.
A stochastic process Xis always adapted to its natural ﬁltration FX
t=
σ(Xs, s≤t) (the last notation meaning that Ftis the smallest σ-algebra
with respect to which all the variables ( Xs, s≤t) are measurable). ( FX
t)
is hence the smallest ﬁltration to which Xis adapted.
The parameter tis often thought of as time, and the σ-algebra Ftrep-
resents the set of information available at time t, that is events that have
occurred up to time t. The ﬁltration ( Ft)t≥0thus represents the evolution
of the information or knowledge of the world with time. If Xis an adapted
process, then Xt, its value at time t, only depends on the evolution of the
universe prior to t.
Deﬁnition 3. Let/parenleftBig
Ω,F,(Ft)t≥0,P/parenrightBig
be a ﬁltered probability space.
(i) The ﬁltration ( Ft)t≥0is said to be complete if (Ω,F,P) is complete
and if F0contains all the P-null sets.
2000Mathematics Subject Classiﬁcation. Primary 05C38, 15A15.
Key words and phrases. Filtrations, Enlargements of ﬁltrations, Initial enlarge ments
of ﬁltrations, progressive enlargements of ﬁltrations.
1",2007-12-04T21:57:21Z, c abstra initns ltratdo ob most mark   ft ift is and fs ft   xt ft xt is ft is xs ft is xs is t ft rep t ft  is xt   b ft b t ft matmatics subje ass primary key ltratenlargement initial
paper_qf_40.pdf,2,Filtrations,"  In this article, we define the notion of a filtration and then give the basic
theorems on initial and progressive enlargements of filtrations.
","2 DELIA COCULESCU AND ASHKAN NIKEGHBALI
(ii) The ﬁltration ( Ft)t≥0is said to satisfy the usual hypotheses if it is
complete and right continuous, that is Ft=Ft+, where
Ft+=/intersectiondisplay
u>tFu.
Some fundamental theorems, such as the Debut theorem, requi re the usual
hypotheses. Hence naturally, very often in the literature o n the theory of
stochastic processes and mathematical ﬁnance, the underly ing ﬁltered prob-
ability spaces are assumed to satisfy the usual hypotheses. This assumption
is not very restrictive for the following reasons:
(a) Any ﬁltration can easily be made complete and right conti nuous; indeed,
given a ﬁltered probability space/parenleftBig
Ω,F,(Ft)t≥0,P/parenrightBig
, we ﬁrst complete
the probability space (Ω ,F,P), and then we add all the P-null sets to
every Ft+,t≥0. The new ﬁltration thus obtained satisﬁes the usual
hypotheses and is called the usual augmentation of ( Ft)t≥0;
(b) Moreover, in most classical and encountered cases, the ﬁ ltration ( Ft)t≥0
is right continuous. Indeed, this is the case when for instan ce (Ft)t≥0
is the natural ﬁltration of a Brownian Motion, a L´ evy proces s, a Feller
process or a Hunt process (see [7, 8]).
Enlargements of filtrations
For more precise and detailed references, the reader can con sult the books
[3, 4, 5, 7] or the survey article [6].
Generalities. Let/parenleftBig
Ω,F,(Ft)t≥0,P/parenrightBig
be a ﬁltered probability space sat-
isfying the usual hypotheses. Let ( Gt)t≥0be another ﬁltration satisfying
the usual hypotheses and such that Ft⊂ Gtfor every t≥0. One natu-
ral question is: how are the ( Ft)-semimartingales modiﬁed when considered
as stochastic processes in the larger ﬁltration ( Gt)? Given the importance
of semimartingales and martingales (in particular in mathe matical ﬁnance
where they are used to model prices), it seems natural to char acterize situ-
ations where the semimartingale or martingale properties a re preserved:
Deﬁnition 4. We shall say that the pair of ﬁltrations ( Ft,Gt) satisﬁes the
(H′) hypothesis if every ( Ft)-semimartingale is a ( Gt)-semimartingale.
Remark 5.In fact, using a classical decomposition of semimartingale s due to
Jacod and M´ emin, it is enough to check that every ( Ft)-bounded martingale
is a (Gt)-semimartingale.
Deﬁnition 6. We shall say that the pair of ﬁltrations ( Ft,Gt) satisﬁes the
(H) hypothesis if every ( Ft)-local martingale is a ( Gt)-local martingale.
The techniques to answer such questions have been developed in the late
70’s under the name of the theory of enlargements of ﬁltratio ns. The theory",2007-12-04T21:57:21Z,t ft ft ft ft fu some but nce  any b ft b ft t ft oft ined ft brownish motseller hunt enlargement for genliti b ft b  gt ft gt for one ft gt give  ft gt ft gt remark ijack ft gt   ft gt ft gt t t
paper_qf_40.pdf,3,Filtrations,"  In this article, we define the notion of a filtration and then give the basic
theorems on initial and progressive enlargements of filtrations.
","FILTRATIONS 3
of enlargements of ﬁltrations has been recently very widely used in math-
ematical ﬁnance, specially in insider trading models and ev en more spec-
tacularly in models of default risk. The insider trading mod els are usually
based on the so called initial enlargements of ﬁltrations whereas the models
of default risk ﬁt perfectly well in the framework of the progressive en-
largements of ﬁltrations . More precisely, given a ﬁltered probability space
(Ω,F,(Ft),P), there are essentially two ways of enlarging ﬁltrations:
•initial enlargements , for which Gt=Ft/logicalortextH, i.e. the new information
His brought in at the origin of time; and
•progressive enlargements , for which Gt=Ft/logicalortextHt, i.e. the new infor-
mation is brought in progressively as the time tincreases.
Before presenting the basic theorems on enlargements of ﬁlt rations, we state
a useful theorem due to Stricker:
Theorem 7 (Stricker [9]) .Let(Ft)and(Gt)be two ﬁltrations as above,
such that for all t≥0,Ft⊂ Gt. If(Xt)is a(Gt)semimartingale which is
(Ft)adapted, then it is also an (Ft)semimartingale.
Initial enlargements of ﬁltrations. The most important theorem on ini-
tial enlargements of ﬁltrations is due to Jacod and deals wit h the special case
where the initial information brought in at the origin of tim e consists of the
σ-algebra generated by a random variable. More precisely let (Ω,F,(Ft),P)
be a ﬁltered probability space satisfying the usual assumpt ions. Let Zbe
anFmeasurable random variable. Deﬁne
Gt=/intersectiondisplay
ε>0/parenleftBig
Ft+ε/logicalordisplay
σ{Z}/parenrightBig
.
In ﬁnancial models, the ﬁltration ( Ft) represents the public information
in a ﬁnancial market and the random variable Zstands for the additional
(anticipating) information of an insider.
The conditional laws of Zgiven Ft, fort≥0 play a crucial role in initial
enlargements.
Theorem 8 (Jacod’s criterion) .LetZbe an Fmeasurable random vari-
able and let Qt(ω,dx)denote the regular conditional distribution of Zgiven
Ft, t≥0. Suppose that for each t≥0, there exists a positive σ-ﬁnite
measure ηt(dx)(on(R,B(R))) such that
Qt(ω,dx)≪ηt(dx) a.s.
Then every (Ft)-semimartingale is a (Gt)-semimartingale.
Remark 9.In fact this theorem still holds for random variables with va lues
in a standard Borel space. Moreover, the existence of the σ-ﬁnite measure
ηt(dx) is equivalent to the existence of one positive σ-ﬁnite measure η(dx)
such that Qt(ω,dx)≪η(dx) and in this case ηcan be taken to be the
distribution of Z.
Now we give classical corollaries of Jacod’s theorem.",2007-12-04T21:57:21Z,t  ft gt ft his gt ft ht before st kicker torem st kicker  ft gt ft gt  xt gt ft ft initial t jack  ft  be measurable  gt b ft b ift stands t giveft torem jack  be measurable qt giveft suppose qt tft gt remark ibore oqt  jack
paper_qf_40.pdf,4,Filtrations,"  In this article, we define the notion of a filtration and then give the basic
theorems on initial and progressive enlargements of filtrations.
","4 DELIA COCULESCU AND ASHKAN NIKEGHBALI
Corollary 10. LetZbe independent of F∞. Then every (Ft)-semimartingale
is a(Gt)-semimartingale.
Corollary 11. LetZbe a random variable taking on only a countable num-
ber of values. Then every (Ft)-semimartingale is a (Gt)-semimartingale.
It is possible to obtain in some cases an explicit decomposit ion of an ( Ft)-
local martingale as a ( Gt)-semimartingale (see [3, 4, 5, 6, 7]). For example,
ifZ=Bt0, for some ﬁxed time t0>0 and a Brownian Motion B, it can
be shown that Jacod’s criterion holds for t < t 0and that every ( Ft)-local
martingale is a semimartingale for 0 ≤t < t 0, but not necessarily including
t0. There are indeed in this case ( Ft)-local martingales which are not ( Gt)-
semimartingales. Moreover, Bis a (Gt)-semimartingale which decomposes
as:
Bt=B0+/tildewideBt+/integraldisplayt∧t0
0dsBt0−Bs
t0−s,
where/parenleftBig
/tildewideBt/parenrightBig
is a (Gt) Brownian Motion.
Remark 12.There are important cases where Jacod’s criterion does not h old
but where other methods apply ([3, 5, 6].
Progressive enlargements of ﬁltrations. Let/parenleftBig
Ω,F,(Ft)t≥0,P/parenrightBig
be a
ﬁltered probability space satisfying the usual hypotheses , and ρ: (Ω,F)→
(R+,B(R+)) be a random time. We enlarge the initial ﬁltration ( Ft) with
the process ( ρ∧t)t≥0, so that the new enlarged ﬁltration ( Fρ
t)t≥0is the
smallest ﬁltration (satisfying the usual assumptions) con taining ( Ft) and
making ρa stopping time (i.e. Fρ
t=Ko
t+, where Ko
t=Ft/logicalortextσ(ρ∧t)). One
may interpret ρas the instant of default of an issuer; the given ﬁltration
(Ft) can be thought of as the ﬁltration of default-free prices, f or which ρ
is not a stopping time. Then, the ﬁltration ( Fρ
t) is the defaultable market
ﬁltration used for the pricing of defaultable assets.
A few processes will play a crucial role in our discussion:
•the (Ft)-supermartingale
Zρ
t=P[ρ > t | Ft] (0.1)
chosen to be c` adl` ag, associated to ρby Az´ ema;
•the (Ft)-dual optional projection of the process 1 {ρ≤t}, denoted by
Aρ
t;
•the c` adl` ag martingale
µρ
t=E[Aρ
∞| Ft] =Aρ
t+Zρ
t.
Theorem 13. Every (Ft)-local martingale (Mt), stopped at ρ, is a (Fρ
t)-
semimartingale, with canonical decomposition:
Mt∧ρ=/tildewiderMt+/integraldisplayt∧ρ
0d/an}bracketle{tM,µρ/an}bracketri}hts
Zρ
s−(0.2)",2007-12-04T21:57:21Z,corollary  be tft gt corollary  be tft gt it ft gt for  brownish motjack ft tre ft gt ois gt    bs b  b gt brownish motremark tre jack progressive  b ft b  ft ft   ft one ft tft ft az ft ft torem every ft mt mt mt
paper_qf_40.pdf,5,Filtrations,"  In this article, we define the notion of a filtration and then give the basic
theorems on initial and progressive enlargements of filtrations.
","FILTRATIONS 5
where/parenleftBig
/tildewiderMt/parenrightBig
is an (Fρ
t)-local martingale.
The most interesting case in the theory of progressive enlar gements of ﬁltra-
tions is when ρis an honest time or equivalently the end of an ( Ft) optional
set Γ, i.e
ρ= sup {t: (t,ω)∈Γ}.
Indeed, in this case, the pair of ﬁltrations ( Ft,Fρ
t) satisﬁes the ( H′) hy-
pothesis: every ( Ft)-local martingale ( Mt), is an ( Fρ
t)-semimartingale, with
canonical decomposition:
Mt=/tildewiderMt+/integraldisplayt∧ρ
0d/an}bracketle{tM,µρ/an}bracketri}hts
Zρ
s−−/integraldisplayt
ρd/an}bracketle{tM,µρ/an}bracketri}hts
1−Zρ
s−.
The next decomposition formulae are widely used for pricing in default mod-
els:
Proposition 14. (i) Let ξ∈L1. Then a c` adl` ag version of the martingale
ξt=E[ξ|Fρ
t]is given by:
ξt=1
Zρ
t1t<ρE[ξ1t<ρ|Ft] +ξ1t≥ρ.
(ii) Let ξ∈L1and let ρbe an honest time. Then a c` adl` ag version of the
martingale ξt=E[ξ|Fρ
t]is given by:
ξt=1
Zρ
tE[ξ1t<ρ|Ft]1t<ρ+1
1−Zρ
tE[ξ1t≥ρ|Ft]1t≥ρ.
The(H)hypothesis. The (H) hypothesis is sometimes presented as a no-
abitrage condition in default models. Let (Ω ,F,P) be a probability space
satisfying the usual assumptions. Let ( Ft) and ( Gt) be two sub-ﬁltrations of
F, with
Ft⊂ Gt.
Br´ emaud and Yor [1] have proven the following characteriza tion of the ( H)
hypothesis:
Theorem 15. The following are equivalent:
(1) Every (Ft)martingale is a (Gt)martingale;
(2) For all t≥0, the sigma ﬁelds GtandF∞are independent condition-
ally on Ft.
Remark 16.We shall also say that ( Ft) isimmersed in (Gt).
In the framework of the progressive enlargement of some ﬁltr ation ( Ft)
with a random time ρ, the ( H) hypothesis is equivalent to one of the follow-
ing hypothesis:
(i)∀t, theσ-algebras F∞andFρ
tare conditionally independent given Ft.
(ii) For all bounded F∞measurable random variables Fand all bounded
Fρ
tmeasurable random variables Gt, we have
E[FGt| Ft] =E[F| Ft]E[Gt| Ft].",2007-12-04T21:57:21Z,b mt b t ft ined ft ft mt mt mt t proposit tft  tft ft t t   ft gt ft gt br or torem t every ft gt for gt and ft remark  ft gt ift ft for and gt gt ft ft gt ft
paper_qf_40.pdf,6,Filtrations,"  In this article, we define the notion of a filtration and then give the basic
theorems on initial and progressive enlargements of filtrations.
","6 DELIA COCULESCU AND ASHKAN NIKEGHBALI
(iii) For all bounded Fρ
tmeasurable random variables Gt:
E[Gt| F∞] =E[Gt| Ft].
(iv) For all bounded F∞measurable random variables F,
E[F| Fρ
t] =E[F| Ft].
(v) For all s≤t,
P[ρ≤s| Ft] =P[ρ≤s| F∞].
Now, a natural question, specially in view of applications t o ﬁnancial math-
ematics, is: how is the ( H) hypothesis aﬀected when we make an equivalent
change of probability measure?
Proposition 17. LetQbe a probability measure which is equivalent to P
(onF). Then every (F•,Q)-semimartingale is a (G•,Q)-semimartingale.
Now, deﬁne:
dQ
dP/vextendsingle/vextendsingle/vextendsingle
Ft=Rt;dQ
dP/vextendsingle/vextendsingle/vextendsingle
Gt=R′
t.
IfY=dQ
dP, then the hypothesis ( H) holds under Qif and only if:
∀X≥0, X∈ F∞,EP[XY|Gt]
R′
t=EP[XY|Ft]
Rt.
In particular, whendQ
dPisF∞measurable, Rt=R′
tand the hypothesis ( H)
holds under Q.
Now let us give a decomposition formula:
Theorem 18. If(Xt)is a(F•,Q)-local martingale, then the stochastic pro-
cess:
IX(t) =Xt+/integraldisplayt
0R′
s−
R′s/parenleftbigg1
Rs−d[X,R]s−1
R′
s−d[X,R′]s/parenrightbigg
is a(G•,Q)-local martingale.
References
[1]P. Br´emaud, M. Yor :Changes of ﬁltration and of probability measures , Z.f.W, 45,
269-295 (1978).
[2]J. Jacod :Grossissement initial, hypothse (H’), et th´ eor` eme de Gir sanov, In:
Grossissements de ﬁltrations: exemples et applications, ( ref [4], below) Springer
(1985), 15-35.
[3]T. Jeulin :Semi-martingales et grossissements d’une ﬁltration , Lecture Notes in
Mathematics 833, Springer (1980).
[4]T. Jeulin, M. Yor (eds) :Grossissements de ﬁltrations: exemples et applications ,
Lecture Notes in Mathematics 1118, Springer (1985).
[5]R. Mansuy, M. Yor :Random times and (enlargement of ﬁltrations) in a Brownian
setting , Lecture Notes in Mathematics, 1873, Springer (2006).
[6]A. Nikeghbali :An essay on the general theory of stochastic processes , Probability
Surveys, 3, 345-412, (2006).",2007-12-04T21:57:21Z,for gt gt gt ft for ft for ft  proposit be t ft rt gt   gt ft rt iis rt  torem  xt xt rs referencbr or jack gross is semegir ross is semets  jeu lisemi leure notmatmatics  jeu lior gross is semets leure notmatmatics  masu or random brownish leure notmatmatics  e gb ali aprobabity surveys
paper_qf_40.pdf,7,Filtrations,"  In this article, we define the notion of a filtration and then give the basic
theorems on initial and progressive enlargements of filtrations.
","FILTRATIONS 7
[7]P.E. Protter :Stochastic integration and diﬀerential equations , Springer. Second
edition (2005), version 2.1.
[8]D. Revuz, M. Yor :Continuous martingales and Brownian motion , Springer. Third
edition (1999).
[9]C. Stricker ,Quasi-martingales, martingales locales, semimartingale s et ﬁltration
naturelle , ZW, 39, (1977), 55-63.
ETHZ, Departement Mathematik, R ¨amistrasse 101, Z ¨urich 8092, Switzer-
land
E-mail address :delia.coculescu@math.ethz.ch
Institut f ¨ur Mathematik, Universit ¨at Z¨urich, Winterthurerstrasse 190 CH-
8057 Z ¨urich, Switzerland
E-mail address :ashkan.nikeghbali@math.unizh.ch",2007-12-04T21:57:21Z,pr oer stochastic  second rev uz or continuous brownish  third st kicker quasi partment ma tm tik twier institut ma tm tik  winterthur east pass switzerland
paper_qf_41.pdf,1,"What is the Sharpe Ratio, and how can everyone get it wrong?","  The Sharpe ratio is the most widely used risk metric in the quantitative
finance community - amazingly, essentially everyone gets it wrong. In this
note, we will make a quixotic effort to rectify the situation.
","arXiv:1802.04413v1  [q-fin.PM]  13 Feb 2018WHAT IS THE SHARPE RATIO, AND HOW CAN EVERYONE
GET IT WRONG?
IGOR RIVIN
Abstract. TheSharperatioisthemostwidelyusedriskmetricin thequantitative
ﬁnance community - amazingly, essentially everyone gets it wrong. I n this note, we
will make a quixotic eﬀort to rectify the situation.
1.Introduction
The Sharpe ratio was introduced over ﬁfty years ago by William F. Sha rpe in
[Sha66] (Sharpe revised the deﬁnition slightly almost thirty years later in [ Sha94]).
The Sharpe ratio is a measure of risk-adjusted returns, and was in itially intended to
distinguish truly superior strategies from ones where the portfolio manager simply
levered up a mediocre strategy. Such levering up would outperform the market in
good times, and then crash in burn when things turned bad. The Sha rpe ratio (in its
most current incarnation) is deﬁned as:
(1) Sa=E(Ra−Rb)
σa,
whereRais the expected return of the asset, Rbis the risk-free return rate, and σais
the standard deviation of the asset returns. A statistically inclined reader will be sure
to note the more-than-passing resemblance of the Sharpe ratio t o thet-statistic, and
so quantiﬁes the evidence that the investment strategy is better than the proverbial
monkeys throwing darts at the (by now virtual) stock table.
While the Sharpe ratio is widely used by investors (to decide which inves tment ve-
hicle is preferable), it isalso very signiﬁcant as aninternal measure in thequantitative
ﬁnance community - a high Sharpe ratio indicates that it is extremely u nlikely that
the strategy will lose money in any given year, and so allows the portf olio manager
to lever up the portfolio without too much risk of ruin.
2.How is the Sharpe ratio computed?
The quantities in formula ( 1) areannualquantities, and so, in principle, to get
reasonable estimates of all the variable, we should examine a portfo lio manager’s
return over a few decades. This is obviously impractical - the portf olio manager may
well beretired(or dead) bythetime areasonable valueisobtain, and thevaluewill be
meaningless in any case, since the character of the market change s considerably over
such lengthy time scales. Consequently, in practice, Sharpe ratio is computed using
thedaily(sometimes monthly) return stream. In order to make this comput ation
feasible, a basic assumption is made:
1",2018-02-13T00:49:20Z,  abstra t share rat is t most wy used risk metric iintrodut share wliam sha sha share sha t share sut sha ra rb ra is rb is share w share share how share t  consequently share in
paper_qf_41.pdf,2,"What is the Sharpe Ratio, and how can everyone get it wrong?","  The Sharpe ratio is the most widely used risk metric in the quantitative
finance community - amazingly, essentially everyone gets it wrong. In this
note, we will make a quixotic effort to rectify the situation.
","2 IGOR RIVIN
The daily returns are independent identically distributed (i.i.d) random
variables.
While this assumption is clearly false on a number of grounds (there is s easonality in
the markets, so the returns are not identically distributed, and th ere are momentum
and reversal phenomena, which means that they are not independ ent), these assump-
tions are not too far from reality, and we will not quarrel with them h ere, since the
real confusion is just beginning:
An assumption is made that (since the returns are small) the return over a number
of days equals sumof the returns on the individual days, or
n/productdisplay
i=1(1+ri) = 1+n/summationdisplay
i=1ri.
This is then used in the following way: the yearly return is the sum of 25 2 (the
traditionally accepted number of trading days in a year) daily return s. Since the
mean and variance are both expectations, both grow linearly with th e size of sample.
The standard deviation is the square root of variance, so, when th e smoke clears, the
formula used universally is:
(2) Sa=√
252E(Ra(d)−Rb(d)
σa(d),
where the dnow indicates dailyreturns.
3.The correct way
Let us now see what the truth is.
As mentioned above, the actual annual return over nperiods is
Xn=n/productdisplay
i=1(1+ri).
Since the daily returns are i.i.d, the expectation of the product is the product of the
expectations, so
E(Xn) = (1+ µ(d))n.
What about the variance?
Var(Xn) =E(X2
n)−E(Xn)2=
E(/productdisplay
i= 1n(1+ri)2]−(1+µ(d))2n=
n/productdisplay
i=1E(1+ri)2)−(1+µ(d))2n=n/productdisplay
i=1((1+µ)2)+σ2)−n/productdisplay
i=1(1+µ)2=
n/summationdisplay
i=1/parenleftbiggn
i/parenrightbigg
σ2i(1+µ)2n−2i",2018-02-13T00:49:20Z,t w a since t ra rb t  as xsince xwhat var xxn
paper_qf_41.pdf,3,"What is the Sharpe Ratio, and how can everyone get it wrong?","  The Sharpe ratio is the most widely used risk metric in the quantitative
finance community - amazingly, essentially everyone gets it wrong. In this
note, we will make a quixotic effort to rectify the situation.
","WHAT IS THE SHARPE RATIO, AND HOW CAN EVERYONE GET IT WRONG? 3
So, ﬁnally, we have the following formula for the Sharpe ratio under t he identical
independent daily returns assumption:
Ia=(1+µ)n−1/radicalBig/summationtextn
i=1/parenleftbign
i/parenrightbig
σ2i(1+µ)2n−2i,
Whereµ,σis the mean and standard deviation (respectively) of daily returns. The
ﬁrst question is:
Question 3.1.isIaclose toSaunder the assumption of small returns?
The answer is: NO.Indeed, if the returns are quite small, then it is not unrea-
sonable to approximate the numerator of Iabynµ.If the volatility is also quite low,
it is quite reasonable to say that only the ﬁrst term in the sum in the de nominator
contributes signiﬁcantly. When the smoke clears, we get the followin g approximation:
Forµ,σ≪1,
Ia≈√nµ
(1+(n−1)µ)σ.
Notice that this diﬀers from Saby a factor of (1 + ( n−1)µ).Since (n−1)µis
approximately the yearly return (under our hypothesis) the erro r is nontrivial even
under the small returns and volatilities hypothesis.
The errors are much more egregious a little further away from the ” heat death”
limit. Consider, for example, the performance of crypto-currenc ies. Bitcoin has run
up hugely over the last few years, and if the usual formula Sais used, the (one year
lookback) Sharpe of Bitcoin (at the time of writing), is around 2 .5 (see, for example
https://www.sifrdata.com/cryptocurrency-sharpe-rati os/. By contrast, the
correctSharperatio(aspostedon http://cci30.com )is0.83,whichisnotsodiﬀerent
from the S&P 500 over the same period.
4.Why is the standard calculation so wrong?
As far as this author can tell, the reason is more sociological than ma thematical.
Remember that the primary audience for Sharpe consisted of inves tment managers -
people who were probably good salesmen with good connections, but no understand-
ing of mathematics. In particular, no understanding of logarithms. So, for them, life
became much simpler if log(1+ x) =x,as a direct consequence, the product of returns
is roughly the sum of returns. Before we heap derision on these peo ple, we should
note that from the viewpoint of Kelly betting (see the canonical ref erence [MTZ11])
wewantto deal with the logs, and so a very reasonable quantity to use is Sha rpe
ratio in log space, computed as:
La=√nE(logRa(d)−log(Rb(d)))
σ(logRa(d).
Now,Lais much closer to Sathan toIa,and arguably it is a more reasonable risk
measure: Suppose you have an investment that (with equal proba bility) multiplies
by 8 or halves by 2. The mean return is 3 .25,the variance of returns is 38 .6875.The
”correct Sharpe ratio” as computed by Iais 1.34810−72,so it judges this a rather poor
investment.By contrast, the log-sharpe Laratio is 5 .29,indicating that this is a very
good investment. The regular sharpe ratio Sais 8.29.It is clear that the last two",2018-02-13T00:49:20Z,so share b wre t tose unr t ined by  wfor notice by since t consir bitcoiarticial intellence share bitcoiby share rat w as remember share iso before kelly sha la ra rb ra  la is thasuppose t t share is by la rat t articial intellence it
paper_qf_41.pdf,4,"What is the Sharpe Ratio, and how can everyone get it wrong?","  The Sharpe ratio is the most widely used risk metric in the quantitative
finance community - amazingly, essentially everyone gets it wrong. In this
note, we will make a quixotic effort to rectify the situation.
","4 IGOR RIVIN
numbers are more indicative than the ﬁrst one. Why is this happening ? Because the
the ﬂuctuations in the tail end of the year (by which time the accoun t holder almost
certainly owns this, and all other, universes) dwarf the returns f ormostof the year.
5.Conclusion
It is this author’s strongly held opinion that the Log-Sharpe ratio Lais the right
metric to use. However, if you do want to compute the actual Shar pe ratio (and many
portfolio managers are required to do so by their investors), then useIa.
References
[MTZ11] Leonard C MacLean, Edward O Thorp, and William T Ziemba. The Kelly capital growth
investment criterion: Theory and practice , volume 3. world scientiﬁc, 2011.
[Sha66] William F Sharpe. Mutual fund performance. The Journal of business , 39(1):119–138,
1966.
[Sha94] William F Sharpe. The sharpe ratio. Journal of portfolio management , 21(1):49–58, 1994.
Mathematics Department, Temple University and The Cryptos Fund
E-mail address :rivin@temple.edu",2018-02-13T00:49:20Z,w because conusit log share la is sha referencleonard mac leaedward thor wliam zi mba t kelly tory sha wliam share mutual t journal sha wliam share t journal matmatics partment temple  t ypto fund
paper_qf_42.pdf,1,"FinRL: Deep Reinforcement Learning Framework to Automate Trading in
  Quantitative Finance","  Deep reinforcement learning (DRL) has been envisioned to have a competitive
edge in quantitative finance. However, there is a steep development curve for
quantitative traders to obtain an agent that automatically positions to win in
the market, namely \\\\textit{to decide where to trade, at what price} and
\\\\textit{what quantity}, due to the error-prone programming and arduous
debugging. In this paper, we present the first open-source framework
\\\\textit{FinRL} as a full pipeline to help quantitative traders overcome the
steep learning curve. FinRL is featured with simplicity, applicability and
extensibility under the key principles, \\\\textit{full-stack framework,
customization, reproducibility} and \\\\textit{hands-on tutoring}.
  Embodied as a three-layer architecture with modular structures, FinRL
implements fine-tuned state-of-the-art DRL algorithms and common reward
functions, while alleviating the debugging workloads. Thus, we help users
pipeline the strategy design at a high turnover rate. At multiple levels of
time granularity, FinRL simulates various markets as training environments
using historical data and live trading APIs. Being highly extensible, FinRL
reserves a set of user-import interfaces and incorporates trading constraints
such as market friction, market liquidity and investor's risk-aversion.
Moreover, serving as practitioners' stepping stones, typical trading tasks are
provided as step-by-step tutorials, e.g., stock trading, portfolio allocation,
cryptocurrency trading, etc.
","FinRL: Deep Reinforcement Learning Framework to Automate
Trading in Quantitative Finance
Xiao-Yang Liu, Hongyang Yang
{xl2427,hy2500}@columbia.edu
Columbia UniversityJiechao Gao
jg5ycn@virginia.edu
University of VirginiaChristina Dan Wang∗
christina.wang@nyu.edu
New York University Shanghai
ABSTRACT
Deep reinforcement learning (DRL) has been envisioned to have a
competitive edge in quantitative finance. However, there is a steep
development curve for quantitative traders to obtain an agent that
automatically positions to win in the market, namely to decide where
to trade, at what price andwhat quantity , due to the error-prone
programming and arduous debugging. In this paper, we present
the first open-source framework FinRL as a full pipeline to help
quantitative traders overcome the steep learning curve. FinRL is
featured with simplicity, applicability and extensibility under the
key principles, full-stack framework, customization, reproducibility
andhands-on tutoring .
Embodied as a three-layer architecture with modular structures,
FinRL implements fine-tuned state-of-the-art DRL algorithms and
common reward functions, while alleviating the debugging work-
loads. Thus, we help users pipeline the strategy design at a high
turnover rate. At multiple levels of time granularity, FinRL simu-
lates various markets as training environments using historical data
and live trading APIs. Being highly extensible, FinRL reserves a set
of user-import interfaces and incorporates trading constraints such
as market friction, market liquidity and investor’s risk-aversion.
Moreover, serving as practitioners’ stepping stones, typical trad-
ing tasks are provided as step-by-step tutorials, e.g., stock trading,
portfolio allocation, cryptocurrency trading, etc.
CCS CONCEPTS
•Computing methodologies →Machine learning ;Markov
decision processes ;Reinforcement learning .
KEYWORDS
Deep reinforcement learning, automated trading, quantitative fi-
nance, Markov Decision Process, portfolio allocation.
ACM Reference Format:
Xiao-Yang Liu, Hongyang Yang, Jiechao Gao, and Christina Dan Wang. 2021.
FinRL: Deep Reinforcement Learning Framework to Automate Trading in
Quantitative Finance. In 2nd ACM International Conference on AI in Finance
∗Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
New York ’21, Nov. 3–5, 2021, New York, NY
©2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-9148-1/21/11. . . $15.00
https://doi.org/10.1145/3490354.3494366(ICAIF’21), November 3–5, 2021, Virtual Event, USA. ACM, New York, NY,
USA, 9 pages. https://doi.org/10.1145/3490354.3494366
1 INTRODUCTION
Deep reinforcement learning (DRL), that balances exploration (of
uncharted territory) and exploitation (of current knowledge), is a
promising approach to automate trading in quantitative finance
[50][51][47][54][21][13]. DRL algorithms are powerful in solving
dynamic decision making problems by learning through interac-
tions with an unknown environment, and offer two major advan-
tages of portfolio scalability and market model independence [6].
In quantitative finance, algorithmic trading is essentially making
dynamic decisions, namely to decide where to trade, at what price
and what quantity , in a highly stochastic and complex financial
market. Incorporating many financial factors, as shown in Fig. 1,
a DRL trading agent builds a multi-factor model to trade automat-
ically, which is difficult for human traders to accomplish [ 4,53].
Therefore, DRL has been envisioned to have a competitive edge in
quantitative finance.
Many existing works have applied DRL in quantitative finan-
cial tasks. Both researchers and industry practitioners are actively
designing trading strategies fueled by DRL, since deep neural net-
works are significantly powerful at estimating the expected return
of taking a certain action at a state. Moody and Saffell [ 33] utilized a
policy search for stock trading; Deng et al. [9] showed that DRL can
obtain more profits than conventional methods. More applications
include stock trading [ 35,47,51,54], futures contracts [ 54], alter-
native data (news sentiments) [ 22,35], high frequency trading [ 15],
liquidation strategy analysis [ 3], and hedging [ 6]. DRL is also being
actively explored in the cryptocurrency market, e.g., automated
trading, portfolio allocation, and market making.
However, designing a DRL trading strategy is not easy. The pro-
gramming is error-prone with tedious debugging. The development
pipeline includes preprocessing market data, building a training
environment, managing trading states, and backtesting trading per-
formance. These steps are standard for implementation but yet
time consuming especially for beginners. Therefore, there is an
urgent demand for an open-source library to help researchers and
quantitative traders to overcome the steep learning curve.
In this paper, we present a FinRL framework that automatically
streamlines the development of trading strategies, so as to help
researchers and quantitative traders to iterate their strategies at a
high turnover rate. Users specify the configurations, such as pick-
ing data APIs and DRL algorithms, and analyze the performance
of trading results. To achieve this, FinRL introduces a three-layer
framework. At the bottom is an environment layer that simulates
financial markets using actual historical data, such as closing price,
shares, trading volume, and technical indicators. In the middle is
the agent layer that implements fine-tuned DRL algorithms andarXiv:2111.09395v1  [q-fin.TR]  7 Nov 2021",2021-11-07T00:34:32Z,ep reinforcement learni framework automate tradi quantitative nance  ya  ho ya ya columbia  jie chagao  virginia christina dawa new york  e articial intellence ep iembodied  at is bei outi machine mark reinforcement ep mark cisprocess reference format  ya  ho ya ya jie chagao christina dawa ep reinforcement learni framework automate tradi quantitative nance iinternatnal conference nance correspondi misscopyrhts arai to  new york nov new york associatuti machinery november virtual event new york ep iincorati  trefore many both moody af fell e  t t tse trefore iusers is to at i nov
paper_qf_42.pdf,2,"FinRL: Deep Reinforcement Learning Framework to Automate Trading in
  Quantitative Finance","  Deep reinforcement learning (DRL) has been envisioned to have a competitive
edge in quantitative finance. However, there is a steep development curve for
quantitative traders to obtain an agent that automatically positions to win in
the market, namely \\\\textit{to decide where to trade, at what price} and
\\\\textit{what quantity}, due to the error-prone programming and arduous
debugging. In this paper, we present the first open-source framework
\\\\textit{FinRL} as a full pipeline to help quantitative traders overcome the
steep learning curve. FinRL is featured with simplicity, applicability and
extensibility under the key principles, \\\\textit{full-stack framework,
customization, reproducibility} and \\\\textit{hands-on tutoring}.
  Embodied as a three-layer architecture with modular structures, FinRL
implements fine-tuned state-of-the-art DRL algorithms and common reward
functions, while alleviating the debugging workloads. Thus, we help users
pipeline the strategy design at a high turnover rate. At multiple levels of
time granularity, FinRL simulates various markets as training environments
using historical data and live trading APIs. Being highly extensible, FinRL
reserves a set of user-import interfaces and incorporates trading constraints
such as market friction, market liquidity and investor's risk-aversion.
Moreover, serving as practitioners' stepping stones, typical trading tasks are
provided as step-by-step tutorials, e.g., stock trading, portfolio allocation,
cryptocurrency trading, etc.
","New York ’21, Nov. 3–5, 2021, New York, NY Xiao-Yang Liu, Hongyang Yang, Jiechao Gao, and Christina Dan Wang
EnvironmentShare of StocksRemaining BalancePrices of StocksTrading AgentsDQNDDPGMulti-Agent DDPGPPOSACA2CTD3SellHoldBuyProfitLossBalance SharesPricesTechnical IndicatorsStateRewardAction𝑺𝒕𝑹𝒕𝑨𝒕𝑹𝒕""𝟏𝒔𝒕""𝟏
Figure 1: Overview of automated trading in FinRL, using
deep reinforcement learning.
common reward functions. The agent interacts with the environ-
ment through properly defined reward functions on the state space
and action space. The top layer includes applications in automated
trading, where we demonstrate several use cases, namely stock trad-
ing, portfolio allocation, cryptocurrency trading, etc. We provide
baseline trading strategies to alleviate debugging workloads.
Under the three-layer framework, FinRL is developed with three
primary principles:
•Full-stack framework . To provide a full-stack DRL framework
with finance-oriented optimizations, including market data APIs,
data preprocessing, DRL algorithms, and automated backtesting.
Users can transparently make use of such a development pipeline.
•Customization . To maintain modularity and extensibility in
development by including state-of-the-art DRL algorithms and
supporting design of new algorithms. The DRL algorithms can
be used to construct trading strategies by simple configurations.
•Reproducibility and hands-on tutoring . To provide tutorials
such as step-by-step Jupyter notebooks and user guide to help
users walk through the pipeline and reproduce the use cases.
This leads to a unified framework where developers are able to
efficiently explore ideas through high-level configurations and spec-
ifications, and to customize their own strategies at request.
Our contributions are summarized as follows:
•FinRL is the first open-source framework that demonstrates the
great potential of applying DRL algorithms in quantitative fi-
nance. We build an ecosystem around the FinRL framework,
which seeds the rapidly growing AI4Finance community.
•The application layer provides interfaces for users to customize
FinRL to their own trading tasks. Automated backtesting mod-
ule and performance metrics are provided to help quantitative
traders iterate trading strategies at a high turnover rate. Prof-
itable trading strategies are reproducible and hands-on tutorials
are provided in a beginner-friendly fashion. Adjusting the trained
models to the rapid changing markets is also possible.
•The agent layer provides state-of-the-art DRL algorithms that
are adapted to finance with fine-tuned hyperparameters. Users
can add new DRL algorithms.
•The environment layer includes not only a collection of histori-
cal data APIs, but also live trading APIs. They are reconfigured
into standard OpenAI gym-style environments [ 5]. Moreover, it
incorporates market frictions and allows users to customize the
trading time granularity.The remainder of this paper is organized as follows. Section 2
reviews related works. Section 3 presents the FinRL framework.
Section 4 demonstrates benchmark trading tasks using FinRL. We
conclude this paper in Section 5.
2 RELATED WORKS
We review the state-of-the-art DRL algorithms, relevant open-
source libraries, and applications of DRL in quantitative finance.
2.1 Deep Reinforcement Learning Algorithms
Many DRL algorithms have been developed. They fall into three
categories: value based, policy based, and actor-critic based.
A value based algorithm estimates a state-action value function
that guides the optimal policy. Q-learning [ 49] approximates a Q-
value (expected return) by iteratively updating a Q-table, which
works for problems with small discrete state spaces and action
spaces. Researchers proposed to utilize deep neural networks for
approximating Q-value functions, e.g., deep Q-network (DQN) and
its variants double DQN and dueling DQN [1].
A policy based algorithm directly updates the parameters of a
policy through policy gradient [ 45]. Instead of value estimation,
policy gradient uses a neural network to model the policy directly,
whose input is a state and output is a probability distribution ac-
cording to which the agent takes an action at the input state.
An actor-critic based algorithm combines the advantages of value
based and policy based algorithms. It updates two neural networks,
namely, an actor network updates the policy (probability distribu-
tion) while a critic network estimates the state-action value function.
During the training process, the actor network takes actions and the
critic network evaluates those actions. The state-of-art actor-critic
based algorithms are deep deterministic policy gradient (DDPG),
proximal policy optimization (PPO), asynchronous advantage actor
critic (A3C), advantage actor critic (A2C), soft actor-critic (SAC),
multi-agent DDPG, and twin-delayed DDPG (TD3) [1].
2.2 Deep Reinforcement Learning Libraries
We summarize relevant open-source DRL libraries as follows:
OpenAI Gym [5] provides standardized environments for vari-
ous DRL tasks. OpenAI baselines [ 10] implements common DRL
algorithms, while stable baselines 3 [ 37] improves [ 10] with code
cleanup and user-friendly examples.
Google Dopamine [7] aims for fast prototyping of DRL algorithms.
It features good plugability and reusability.
RLlib [25] provides highly scalable DRL algorithms. It has modular
framework and is well maintained.
TensorLayer [11] is designed for researchers to customize neural
networks for various applications. TensorLayer is a wrapper of
TensorFlow and supports the OpenAI gym-style environments.
However, it is not user-friendly.
2.3 Deep Reinforcement Learning in Finance
Many recent works have applied DRL to quantitative finance. Stock
trading is considered as the most challenging task due to its noisy
and volatile features, and various DRL based approaches [ 15,35,54]
have been applied. Volatility scaling was incorporated in DRL algo-
rithms to trade futures contracts, which considered market volatility",2021-11-07T00:34:32Z,new york nov new york  ya  ho ya ya jie chagao christina dawa environment share stocks rem articial intellence ni balance pricstocks tradi  multi agent sell hold buy prot loss balance sharpriechnical indicators state reward a overview t t  unr full to is users custoatto t reproducibity to up ter  our  nance t automated prof adjusti t users t is is ty opeot sesese se ep reinforcement learni algorithms many ty researcrs instead ait  t ep reinforcement learni librari opegym ope dopamine it lli it tensor layer tensor layer tensor flow opeep reinforcement learni nance many stock volatity
paper_qf_42.pdf,3,"FinRL: Deep Reinforcement Learning Framework to Automate Trading in
  Quantitative Finance","  Deep reinforcement learning (DRL) has been envisioned to have a competitive
edge in quantitative finance. However, there is a steep development curve for
quantitative traders to obtain an agent that automatically positions to win in
the market, namely \\\\textit{to decide where to trade, at what price} and
\\\\textit{what quantity}, due to the error-prone programming and arduous
debugging. In this paper, we present the first open-source framework
\\\\textit{FinRL} as a full pipeline to help quantitative traders overcome the
steep learning curve. FinRL is featured with simplicity, applicability and
extensibility under the key principles, \\\\textit{full-stack framework,
customization, reproducibility} and \\\\textit{hands-on tutoring}.
  Embodied as a three-layer architecture with modular structures, FinRL
implements fine-tuned state-of-the-art DRL algorithms and common reward
functions, while alleviating the debugging workloads. Thus, we help users
pipeline the strategy design at a high turnover rate. At multiple levels of
time granularity, FinRL simulates various markets as training environments
using historical data and live trading APIs. Being highly extensible, FinRL
reserves a set of user-import interfaces and incorporates trading constraints
such as market friction, market liquidity and investor's risk-aversion.
Moreover, serving as practitioners' stepping stones, typical trading tasks are
provided as step-by-step tutorials, e.g., stock trading, portfolio allocation,
cryptocurrency trading, etc.
","FinRL: Deep Reinforcement Learning Framework to Automate Trading in Quantitative Finance New York ’21, Nov. 3–5, 2021, New York, NY
Applications
User-defined
Tasks
DRLLibraries:ElegantRL,RLlib,StableBaselines3DRLAgents
MarketEnvironments
LiveTradingAPI:
CCXT,Alpaca,
QuantConnect
Reward State ActionUser-designed
DRLAlgorithmsA2C,
SACPPOMADDPG,
MAPPOTD3 DDPGDQN,D3QN
DoubleDQNCryptocurrency
TradingHigh-Frequency
Trading
HistoricalDataAPI:
WRDS
Yahoo!FinanceUser-imported
DatasetsMarket
Regulations
Market
SimulationsStockTrading,
PortfolioAllocationApplications
User-defined
Tasks
DRLLibraries:ElegantRL,RLlib,StableBaselines3DRLAgents
MarketEnvironments
LiveTradingAPI:
CCXT,Alpaca,
QuantConnect
Reward State ActionUser-designed
DRLAlgorithmsA2C,
SACPPOMADDPG,
MAPPOTD3 DDPGDQN,D3QN
DoubleDQNCryptocurrency
TradingHigh-Frequency
Trading
HistoricalDataAPI:
WRDS
Yahoo!FinanceUser-imported
DatasetsMarket
Regulations
Market
SimulationsStockTrading,
PortfolioAllocation
Figure 2: Overview of FinRL: application layer at the top, agent layer in the middle and environment layer at the bottom.
in a reward function. News headline sentiments and knowledge
graphs, as alternative data, can be combined with the price-volume
data as time series to train a DRL trading agent. High frequency
trading using DRL [38] is a hot topic.
Deep Hedging [ 6] designed hedging strategies with DRL algo-
rithms that manages the risk of liquid derivatives. It has shown two
advantages of DRL in mathematical finance, scalable andmodel-free .
DRL driven strategy would become more efficient as the scale of the
portfolio grows. It uses DRL to manage the risk of liquid derivatives,
which indicates further extension of our FinRL library into other
asset classes and topics in mathematical finance.
Cryptocurrencies are rising in the digital financial market, such
as Bitcoin (BTC) [ 40], and are considered more volatile than stocks.
DRL is also being actively explored in automated trading, portfolio
allocation, and market making for cryptocurrencies [20, 39, 41].
3 THE PROPOSED FINRL FRAMEWORK
In this section, we first present an overview of the FinRL framework
and describe its layers. Then, we propose a training-testing-trading
pipeline as a standard evaluation of the trading performance.
3.1 Overview of FinRL Framework
The FinRL framework has three layers, application layer, agent
layer, and environment layer, as shown in Fig. 2.
•On the application layer, FinRL aims to provide hundreds of
demonstrative trading tasks, serving as stepping stones for users
to develop their strategies.
•On the agent layer, FinRL supports fine-tuned DRL algorithms
from DRL libraries in a plug-and-play manner, following the
unified workflow in Fig. 1.
•On the environment layer, FinRL aims to wrap historical data
and live trading APIs of hundreds of markets into training envi-
ronments, following the defacto standard Gym [5].Upper-layer trading tasks can directly call DRL algorithms in the
agent layer and market environments in the environment layer.
The FinRL framework has the following features:
•Layered architecture : The lower layer provides APIs for the
upper layer, ensuring transparency . The agent layer interacts with
the environment layer in an exploration-exploitation manner.
Updates in each layer is independent, as long as keeping the APIs
in Table 2 unchanged.
•Modularity and extensibility : Each layer has modules that
define self-contained functions. A user can select certain modules
to implement her trading task. We reserve interfaces for users to
develop new modules, e.g., adding new DRL algorithms.
•Simplicity and applicability : FinRL provides benchmark trad-
ing tasks that are reproducible for users, and also enables users
to customize trading tasks via simple configurations. In addition,
hands-on tutorials are provided in a beginner-friendly fashion.
3.2 Application Layer
On the application layer, users map an algorithmic trading strategy
into the DRL language by specifying the state space, action space
and reward function. For example, the state, action and reward
for several use cases are given in Table 1. Users can customize
according to their own trading strategies.
State spaceS. The state space describes how the agent perceives
the environment. A trading agent observes many features to make
sequential decisions in an interactive market environment. We
allow the time step 𝑡to have multiple levels of granularity , e.g.,
daily, hourly or a minute basis. We provide various features for
users to select and update, in each time step 𝑡:
•Balance 𝑏𝑡∈R+: the account balance at the current time step 𝑡.
•Shares 𝒌𝑡∈Z𝑛+: current shares for each asset, where 𝑛represents
the number of stocks in the portfolio.
•Open-high-low-close (OHLC) prices 𝒐𝑡,𝒉𝑡,𝒍𝑡,𝒑𝑡∈R𝑛+and trad-
ing volume 𝒗𝑡∈R𝑛+.",2021-11-07T00:34:32Z,ep reinforcement learni framework automate tradi quantitative nance new york nov new york applicatns user tasks librarielegant lli stable baseline  market environments live tradi alpha quant conne reward state auser algorithms double ypto currency tradi hh frequency tradi historical data yahoo nance user datasets market regulatns market simulatns stock tradi tfallocatapplicatns user tasks librarielegant lli stable baseline  market environments live tradi alpha quant conne reward state auser algorithms double ypto currency tradi hh frequency tradi historical data yahoo nance user datasets market regulatns market simulatns stock tradi tfallocat overview news hh ep edgi it it ypto currencibitcoiitoverview framework t  oo ois gym up t layered t is t updatis table modular it ea simplicity iapplicatlayer ofor table users state t   balance shar
paper_qf_42.pdf,4,"FinRL: Deep Reinforcement Learning Framework to Automate Trading in
  Quantitative Finance","  Deep reinforcement learning (DRL) has been envisioned to have a competitive
edge in quantitative finance. However, there is a steep development curve for
quantitative traders to obtain an agent that automatically positions to win in
the market, namely \\\\textit{to decide where to trade, at what price} and
\\\\textit{what quantity}, due to the error-prone programming and arduous
debugging. In this paper, we present the first open-source framework
\\\\textit{FinRL} as a full pipeline to help quantitative traders overcome the
steep learning curve. FinRL is featured with simplicity, applicability and
extensibility under the key principles, \\\\textit{full-stack framework,
customization, reproducibility} and \\\\textit{hands-on tutoring}.
  Embodied as a three-layer architecture with modular structures, FinRL
implements fine-tuned state-of-the-art DRL algorithms and common reward
functions, while alleviating the debugging workloads. Thus, we help users
pipeline the strategy design at a high turnover rate. At multiple levels of
time granularity, FinRL simulates various markets as training environments
using historical data and live trading APIs. Being highly extensible, FinRL
reserves a set of user-import interfaces and incorporates trading constraints
such as market friction, market liquidity and investor's risk-aversion.
Moreover, serving as practitioners' stepping stones, typical trading tasks are
provided as step-by-step tutorials, e.g., stock trading, portfolio allocation,
cryptocurrency trading, etc.
","New York ’21, Nov. 3–5, 2021, New York, NY Xiao-Yang Liu, Hongyang Yang, Jiechao Gao, and Christina Dan Wang
Key components Attributes
StateBalance 𝑏𝑡∈R+; Shares 𝒌𝑡∈Z𝑛+
OHLCV data 𝒐𝑡,𝒉𝑡,𝒍𝑡,𝒑𝑡,𝒗𝑡∈R𝑛+
Technical indicators; Fundamental indicators
Smart beta
NLP market sentiment features
ActionBuy/Sell/Hold; Short/Long
Portfolio weights
RewardsChange of portfolio value
Portfolio log-return
Shape ratio
EnvironmentDow- 30, NASDAQ- 100, S&P- 500
Cryptocurrencies
Foreign currency and exchange
Futures and options
Living trading
Table 1: Key components and attributes. OHLCV stands for
Open, High, Low, Close and Volume.
•Technical indicators, including Moving Average Convergence
Divergence (MACD) 𝑴𝑡∈R𝑛, Relative Strength Index (RSI)
𝑹𝑡∈R𝑛+, etc.
•Fundamental indicators, including return on assets (ROA), return
on equity (ROE), net profit margin (NPM), price-to-earnings (PE)
ratio, price-to-book (PB) ratio, etc.
Action spaceA. The action space describes the allowed actions
that an agent can take at a state. An action of one share is 𝑎∈
{−1,0,1}where−1,0,1represent selling, holding, and buying, re-
spectively; an action of multiple shares is 𝑎∈{− 𝑘, ...,−1,0,1, ..., 𝑘}
where 𝑘denotes the maximum number of shares to buy or sell, e.g.,
""Buy/Sell 10 shares of AAPL"" is 10or−10, respectively.
Reward function . The reward function 𝑟(𝑠, 𝑎, 𝑠′)is the incen-
tive for an agent to learn a better policy. FinRL supports user-defined
reward functions to reflect risk-aversion or market friction [6, 54]
and provides commonly used ones [13] as follows:
•The change of the portfolio value when taking action 𝑎at state 𝑠
and arriving at new state 𝑠′[35,50,51],𝑟(𝑠, 𝑎, 𝑠′)=𝑣′−𝑣, where
𝑣′and𝑣are portfolio values at 𝑠′and𝑠, respectively.
•The portfolio log return [18], 𝑟(𝑠, 𝑎, 𝑠′)=log(𝑣′/𝑣).
•The Sharpe ratio for trading periods 𝑡=1, ...,𝑇 [34],
Sharpe ratio =(E(𝑅𝑡)−𝑅𝑓)/std(𝑅𝑡), (1)
where 𝑅𝑡=𝑣𝑡−𝑣𝑡−1, and 𝑅𝑓is the risk-free rate.
3.3 Agent Layer
FinRL allows users to plug in and play with standard DRL algo-
rithms, following the unified workflow in Fig. 1. As a backbone, we
fine-tune three representative open-source DRL libraries, namely
Stable Baseline 3 [ 37], RLlib [ 25] and ElegantRL [ 28]. User can also
design new DRL algorithms by adapting existing ones.
3.3.1 Agent APIs. FinRL uses unified Python APIs for training a
trading agent. The Python APIs are flexible so that a DRL algo-
rithm can be easily plugged in. To train a DRL trading agent, as
in Fig. 2, a user chooses an environment (i,e., StockTradingEnv,
StockPortfolioEnv) built on historical data or live trading APIs with
StabilityEfficiency
Stable Baselines 3RLlib ElegantRLHigh
Low
Low High/StabilityEfficiency
Stable Baselines 3RLlib ElegantRLHigh
Low
Low High/Figure 3: Comparison of DRL libraries.
default parameters (env_kwargs), and picks a DRL algorithm (e.g.,
PPO [ 42]). Then, FinRL initializes the agent class with the envi-
ronment, sets a DRL algorithm with its default hyperparameters
(model_kwargs), then launches a training process and returns a
trained model.
The main APIs are given in Table 2, while the details of building
an environments, importing an algorithm, and constructing an
agents are hidden in the API calls.
3.3.2 Plug-and-Play DRL Libraries. Fig. 3 compares the three DRL
libraries. The details of each library are summarised as follows.
Stable Baselines 3 [37] is a set of improved implementations of
DRL algorithms over the OpenAI Baselines [ 10]. FinRL chooses to
support Stable baselines 3 due to its advantages : 1). User-friendly,
2). Easy to replicate, refine, and identify new ideas, and 3). Good
documentation. Stable Baselines 3 is used as a base around which
new ideas can be added, and as a tool for comparing a new approach
against existing ones. The purpose is that the simplicity of these
tools will allow beginners to experiment with a more advanced tool
set, without being buried in implementation details.
RLlib [25] is an open-source high performance library for a
variety of general applications. FinRL chooses to support RLlib due
to its advantages : 1). High performance and parallel DRL training
framework; 2). Scale training onto large-scale distributed servers;
and 3). Allowing the multi-processing technique to efficiently train
on laptops. RLlib natively supports TensorFlow, TensorFlow Eager,
and PyTorch, but most of its internals are framework agnostic.
ElegantRL [28] is designed for researchers and practitioners
with finance-oriented optimizations. FinRL chooses to support El-
egantRL due to its advantages : 1). Lightweight: core codes have
less than 1,000 lines, less dependable packages, only using PyTorch
(train), OpenAI Gym [ 5] (env), NumPy, Matplotlib (plot); 2). Cus-
tomization: Due to the completeness and simplicity of the code
structure, users can easily customize their own agents; 3). Efficient:
Performance is comparable with RLlib [ 25]; and 4). Stable: As stable
as Stable baseline 3 [37].
ElegantRL supports state-of-the-art DRL algorithms, including
both discrete and continuous ones, and provides user-friendly tuto-
rials in Jupyter Notebooks. ElegantRL implements DRL algorithms
under the Actor-Critic framework, where an agent consists of an
actor network and a critic network. The ElegantRL library enables
researchers and practitioners to pipeline the disruptive “design,
development and deployment” of DRL technology.
Customizing trading strategies . Due to the uniqueness of dif-
ferent financial markets, customization becomes a vital character
to design trading strategies. Users are able to select a DRL algo-
rithm and easily customize it for their trading tasks by specifying
the state-action-reward tuple in Table 1. We believe that among",2021-11-07T00:34:32Z,new york nov new york  ya  ho ya ya jie chagao christina dawa key aributstate balance shartechnical fundamental smart abuy sell hold short lo tfrewards e tfshape environment dow ypto currenciforefuturlivi table key opehh low ose volume technical movi ge convergence divergence relative sth inx fundamental at abuy sell reward t t t t share share agent layer  as stable baseline lli elegant user agent is pythois t pythois to  stock tradi estock tfeis stabity efciency stable baseline lli elegant hh low low hh stabity efciency stable baseline lli elegant hh low low hh  arisott is table plug play librari t stable baseline opebaseline stable user easy good stable baseline t lli lli hh scale allowi lli tensor flow tensor flow eager py torelegant el lhtht py toropegym num py mat plot lib us due efcient formance lli stable as stable elegant up ter notebooks elegant aor itic t elegant custoi due users table 
paper_qf_42.pdf,5,"FinRL: Deep Reinforcement Learning Framework to Automate Trading in
  Quantitative Finance","  Deep reinforcement learning (DRL) has been envisioned to have a competitive
edge in quantitative finance. However, there is a steep development curve for
quantitative traders to obtain an agent that automatically positions to win in
the market, namely \\\\textit{to decide where to trade, at what price} and
\\\\textit{what quantity}, due to the error-prone programming and arduous
debugging. In this paper, we present the first open-source framework
\\\\textit{FinRL} as a full pipeline to help quantitative traders overcome the
steep learning curve. FinRL is featured with simplicity, applicability and
extensibility under the key principles, \\\\textit{full-stack framework,
customization, reproducibility} and \\\\textit{hands-on tutoring}.
  Embodied as a three-layer architecture with modular structures, FinRL
implements fine-tuned state-of-the-art DRL algorithms and common reward
functions, while alleviating the debugging workloads. Thus, we help users
pipeline the strategy design at a high turnover rate. At multiple levels of
time granularity, FinRL simulates various markets as training environments
using historical data and live trading APIs. Being highly extensible, FinRL
reserves a set of user-import interfaces and incorporates trading constraints
such as market friction, market liquidity and investor's risk-aversion.
Moreover, serving as practitioners' stepping stones, typical trading tasks are
provided as step-by-step tutorials, e.g., stock trading, portfolio allocation,
cryptocurrency trading, etc.
","FinRL: Deep Reinforcement Learning Framework to Automate Trading in Quantitative Finance New York ’21, Nov. 3–5, 2021, New York, NY
Function Description
env = StockTradingEnv(df, **env_kwargs) Return an environment instance of the Env class with data and default
parameters.
agent = DRLAgent(env) Instantiate a DRL agent with a given environment env.
model = agent.get_model(model_name, **model_kwargs) Return a model with name and default hyperparameters.
trained_model = agent.train_model(model) Launch the training process for the agent and return a trained model.
Table 2: Main APIs of FinRL.
the three state-of-the-art DRL libraries, ElegentRL is a practically
useful option for financial tasks because of its completeness and
simplicity along with its comparable performance with RLlib [ 25]
and stability with Stable Baselines 3 [37].3.4 Environment Layer
Environment design is crucial in DRL, because the agent learns
by interacting with the environment in a trial and error manner.
A good environment that simulates real-world market will help
the agent learn a better strategy. Considering the stochastic and
interactive nature, a financial task is modeled as a Markov Decision
Process (MDP), whose state transition is shown in Fig. 1.
The environment layer in FinRL is responsible for observing
current market information and translating those information into
states of the MDP problem. The state variables can be categorized
into the state of an agent and the state of the market. For example,
in the use case stock trading, the state of the market includes the
open-high-low-close prices and volume (OHLCV) and technical
indicators; the state of an agent includes the account balance and
the shares for each stock.
The RL training process involves observing price change, taking
an action and calculating a reward. By interacting with the envi-
ronment, the agent updates iteratively and eventually obtains a
trading strategy to maximize the expected return. We reconfigure
real market data into gym-style training environments according
to the principle of time-driven simulation . Inspired by OpenAI Gym
[5], FinRL provides strategy builders with a collection of universal
training environments for various trading tasks.
3.4.1 Standard Datasets and Live Trading APIs. DRL in finance is
different from chess, card games and robotics [ 44,52], which may
have physical engines or simulators. Different financial tasks may
require different market simulators. Building such training environ-
ments is time-consuming, so FinRL provides a set of representative
ones and also supports user-import data, aiming to free users from
such tedious and time-consuming work.
NASDAQ-100 index constituents are100stocks that are char-
acterized by high technology and high growth.
Dow Jones Industrial Average (DJIA) index is made up of 30
representative constituent stocks. DJIA is the most cited market
indicator to examine market overall performance.
Standard & Poor’s 500 (S&P 500) index constituents consist of
500largest U.S. publicly traded companies.
Hang Seng Index Index (HSI) constituents are grouped into
Finance, Utilities, Properties and Commerce & Industry [ 19]. HSI is
the most widely quoted indicator of the Hong Kong stock market.
SSE 50 Index constituents [12] include the best representative
companies (in 10industries) of A shares listed at Shanghai Stock
Exchange (SSE) with considerable size and liquidity.
CSI 300 Index constituents [8] consist of the 300largest and
most liquid A-share stocks listed on Shenzhen Stock Exchange
Training Testing TradingFine 
tuningPast Future
Trained
ModelTraining Testing TradingFine 
tuningPast Future
Trained
ModelFigure 4: The training-testing-trading pipeline.
and SSE. This index reflects the performance of the China A-share
market.
Bitcoin (BTC) Price Index consists of the quote and trade data
on Bitcoin market, available at https://public.bitmex.com/.
3.4.2 User-Imported Data. Users may want to train agents on their
own data sets. FinRL provides convenient support for users to
import data, adjust the time granularity, and perform the training-
testing-trading data split. We specify the format for different trading
tasks, and users preprocess and format the data according to our
instructions. Stock statistics and indicators can be calculated us-
ing our support, which provides more features for the state space.
Furthermore, episodic total return and Sharpe ratio can also assist
performance evaluation.
3.5 Training-Testing-Trading Pipeline
The ""training-testing"" workflow used by conventional machine
learning methods falls short for financial tasks. It splits the data
into training set and testing set. On the training data, users select
features and tune parameters; then evaluate on the testing data.
However, financial tasks will experience a simulation-to-reality gap
between the testing performance and real-live market performance.
Because the testing here is offline backtesting, while the users’ goal
is to place orders in a real-world market.
FinRL employs a “training-testing-trading"" pipeline to reduce
the simulation-to-reality gap. We use historical data (time series)
for the “training-testing"" part, which is the same as conventional
machine learning tasks, and this testing period is for backtesting
purpose. For the “trading"" part, we use live trading APIs, such as
CCXT, Alpaca, or Interactive Broker, allowing users carry out trades
directly in a trading system. Therefore, FinRL directly connects
with live trading APIs: 1). downloads live data, 2). feeds data to the
trained DRL model and obtains the trading positions, and 3). allows
users to place trades.
Fig. 4 illustrates the “training-testing-trading” pipeline:
Step 1) . A training window to retrain an agent.
Step 2) . A testing window to evaluate the trained agent, while
hyperparameters can be tuned iteratively.
Step 3) . Use the trained agent to trade in a trading window.
Rolling window is used in the training-testing-trading pipeline,
because the investors and portfolio managers need to retrain the
model periodically as time goes ahead. FinRL provides flexible",2021-11-07T00:34:32Z,ep reinforcement learni framework automate tradi quantitative nance new york nov new york funsiptstock tradi eureagent instantiate urlauntable articial intellence is ele gent lli stable baseline environment layer environment consiri mark cisprocess  t t for t by  insed opegym standard datasets live tradi is dferent budi dow jonindustrial ge standard poor ha so inx inx nance utitiproticoerce industry ho  inx e articial intellence stock exe inx snzstock exe tr articial intellence ni testi tradi ne past future tr articial intellence ned mtr articial intellence ni testi tradi ne past future tr articial intellence ned m t   bitcoiprice inx bitcoiuser imted data users  stock furtr share tr articial intellence ni testi tradi pipeline t it obecause  for is alpha intive broker trefore is  step step step use rolli n
paper_qf_42.pdf,6,"FinRL: Deep Reinforcement Learning Framework to Automate Trading in
  Quantitative Finance","  Deep reinforcement learning (DRL) has been envisioned to have a competitive
edge in quantitative finance. However, there is a steep development curve for
quantitative traders to obtain an agent that automatically positions to win in
the market, namely \\\\textit{to decide where to trade, at what price} and
\\\\textit{what quantity}, due to the error-prone programming and arduous
debugging. In this paper, we present the first open-source framework
\\\\textit{FinRL} as a full pipeline to help quantitative traders overcome the
steep learning curve. FinRL is featured with simplicity, applicability and
extensibility under the key principles, \\\\textit{full-stack framework,
customization, reproducibility} and \\\\textit{hands-on tutoring}.
  Embodied as a three-layer architecture with modular structures, FinRL
implements fine-tuned state-of-the-art DRL algorithms and common reward
functions, while alleviating the debugging workloads. Thus, we help users
pipeline the strategy design at a high turnover rate. At multiple levels of
time granularity, FinRL simulates various markets as training environments
using historical data and live trading APIs. Being highly extensible, FinRL
reserves a set of user-import interfaces and incorporates trading constraints
such as market friction, market liquidity and investor's risk-aversion.
Moreover, serving as practitioners' stepping stones, typical trading tasks are
provided as step-by-step tutorials, e.g., stock trading, portfolio allocation,
cryptocurrency trading, etc.
","New York ’21, Nov. 3–5, 2021, New York, NY Xiao-Yang Liu, Hongyang Yang, Jiechao Gao, and Christina Dan Wang
selections of rolling windows, such as monthly, quarterly, yearly
windows, or by users’ specifications.
4 HANDS-ON TUTORIALS AND
BENCHMARK PERFORMANCE
We provide hands-on tutorials and reproduce existing works as use
cases. Their configurations and commands are available on Github.
4.1 Backtesting Module
Backtesting plays a key role in evaluating a trading strategy. FinRL
library provides an automated backtesting module based on Quan-
topian pyfolio package [ 36]. It is easy to use and consists of various
individual plots that provide a comprehensive image of the perfor-
mance. In order to facilitate users, FinRL also incorporates market
frictions, market liquidity and the investor’s degree of risk-aversion.
4.1.1 Incorporating Trading Constraints. Transaction costs incur
when executing a trade, such as broker commissions and the SEC
fee. We allow users to treat transaction costs as parameters in the
environments: 1). Flat fee is a fixed amount per trade; and 2). Per
share percentage is a percentage rate for every share, e.g., 0.1%
or0.2%are most commonly used.
Moreover, we need to consider market liquidity for stock trading,
e.g., the bid-ask spread that is the difference between the best bid
and ask prices. In our environment, users can add the bid-ask spread
as a parameter. For different levels of risk-aversion, users can add
the standard deviation of the portfolio returns into the reward
function or use a risk-adjusted Sharpe ratio as the reward function.
4.1.2 Risk-aversion. An investor may prefer conservative trad-
ing in highly volatile markets. For a worst case scenario as the
2008 global financial crisis, FinRL employs the turbulence index
turbulence 𝑡to measure extreme fluctuation [23]:
turbulence 𝑡=(𝒚𝒕−𝝁)𝑇𝚺−1(𝒚𝒕−𝝁)∈R, (2)
where 𝒚𝒕∈R𝑛is the return at 𝑡,𝝁∈R𝑛is the average of histor-
ical returns, and 𝚺∈R𝑛×𝑛is the covariance matrix of historical
returns. turbulence 𝑡can be used to control buying/selling actions.
Ifturbulence 𝑡is higher than a preset threshold, the agent halts and
will resume when turbulence 𝑡becomes lower than the threshold.
4.2 Baseline Strategies and Trading Metrics
Baseline trading strategies are provided to compare with DRL strate-
gies. Investors usually have two mutually conflicting objectives:
the highest possible profits and the lowest possible risks [ 43]. We
include three conventional strategies as baselines.
Passive trading strategy [31] is an easy and popular strategy
that has the minimal trading activities. Investors simply buy and
hold index ETFs [ 46] to replicate a broad market index or indices
such as Dow Jones Industrial Average (DJIA) index and Standard &
Poor’s 500 (S&P 500) index.
Mean-variance and min-variance strategy [2] both aim to
achieve an optimal balance between the risks and profits. It selects
a diversified portfolio with risky assets, and the risk is diversified
when traded together.
Equally weighted strategy is a type of portfolio allocation
method. It gives the same importance to each asset in a portfolio.
FinRL includes common metrics to evaluate trading performance:
Final portfolio value : the amount of money at the end of the
trading period.Cumulative return : subtracting the initial value from the final
portfolio value, then dividing by the initial value.
Annualized return and standard deviation : geometric average
return in a yearly sense, and the corresponding deviation.
Maximum drawdown ratio : the maximum observed loss from
a historical peak to a trough of a portfolio, before a new peak is
achieved. Maximum drawdown is an indicator of downside risk
over a time period.
Sharpe ratio in (1) is the average return earned in excess of the
risk-free rate per unit of volatility.
4.3 Hands-on Tutorials
We provide tutorials to help users walk through the strategy design
pipeline, i.e., get familiar with the stat-action-reward specifications
in Table 1 and the agent-environment interactions in Fig. 1.
Tutorial 1: Stock trading
First, users specify the state at the application layer, i.e., the num-
ber of stocks, technical indicators, the initial capital, etc. Second,
users provide start/end dates for training/testing periods, set the
time granularity. FinRL instantiates an environment for the task,
while the operations are transparent to users. FinRL uses standard
APIs to download data and obtains a Pandas DataFrame containing
the open-high-low-close prices and volume (OHLCV) data. FinRL
preprocesses the OHLCV data by filling missing data and calculates
technical indicators that are passed into the state. Third, users select
a DRL library and a DRL algorithm. FinRL has default hyperparam-
eters for daily stock trading task. During the testing period, users
can tune these parameters to improve the trading performance.
Finally, FinRL feeds time series data of the portfolio value into a
backtesting module to plot charts. Please see examples in Section
4.4 and Section 4.6.
Tutorial 2: Analyzing Trading Performance
Before deploying a trading strategy, users need to fully evaluate
its trading performance via backtesting. The trading performance
can be easily evaluated using the automatic backtesting module
in Section 4.1. The commonly used trading metrics and baseline
strategies are given in Section 4.2.
Cumulative return and Sharpe ratio are widely used metrics to
evaluate overall performance of trading strategies. To gain more
details about the strategy, the distribution of returns over the test-
ing period and annualized return are provided to examine if the
return is stable and consistent. Annualized volatility and maximum
drawdown measure the robustness.
4.4 Use Case I: Stock Trading
We use FinRL to reproduce both [ 50] and [ 51] for stock trading.
The ensemble strategy [ 51] combines three DRL algorithms (PPO
[42], A2C [32] and DDPG [26]) to improve the robustness.
The implementation is easy with FinRL. We choose three algo-
rithms (PPO, A2C, DDPG) in the agent layer, and an environment
with start and end dates in the environment layer. The implemen-
tations of DRL algorithms and data preprocessing are transparent
to users, alleviating the programming and debugging workloads.
Thus, FinRL greatly facilitates the strategy design, allowing users
to focus on improving the trading performance.
Fig. 5 and Table 3 show the backtesting performance on Dow
30 constituent stocks, accessed at 2020/07/01. The training period",2021-11-07T00:34:32Z,new york nov new york  ya  ho ya ya jie chagao christina dawa  tir it hub back testi module back testi quait iincorati tradi const articial intellence nts transa flat  oifor share risk afor  turbulence baseline strategitradi metrics baseline investors  passive investors fs dow jonindustrial ge standard poor meait equally it nal cumulative annual zed maximum maximum share hands tutorials  table  tutorial stock rst second is panda data frame third  nally please sesetutorial analyzi tradi formance before t set secumulative share to annual zed use case stock tradi  t t  t   table dow t
paper_qf_42.pdf,7,"FinRL: Deep Reinforcement Learning Framework to Automate Trading in
  Quantitative Finance","  Deep reinforcement learning (DRL) has been envisioned to have a competitive
edge in quantitative finance. However, there is a steep development curve for
quantitative traders to obtain an agent that automatically positions to win in
the market, namely \\\\textit{to decide where to trade, at what price} and
\\\\textit{what quantity}, due to the error-prone programming and arduous
debugging. In this paper, we present the first open-source framework
\\\\textit{FinRL} as a full pipeline to help quantitative traders overcome the
steep learning curve. FinRL is featured with simplicity, applicability and
extensibility under the key principles, \\\\textit{full-stack framework,
customization, reproducibility} and \\\\textit{hands-on tutoring}.
  Embodied as a three-layer architecture with modular structures, FinRL
implements fine-tuned state-of-the-art DRL algorithms and common reward
functions, while alleviating the debugging workloads. Thus, we help users
pipeline the strategy design at a high turnover rate. At multiple levels of
time granularity, FinRL simulates various markets as training environments
using historical data and live trading APIs. Being highly extensible, FinRL
reserves a set of user-import interfaces and incorporates trading constraints
such as market friction, market liquidity and investor's risk-aversion.
Moreover, serving as practitioners' stepping stones, typical trading tasks are
provided as step-by-step tutorials, e.g., stock trading, portfolio allocation,
cryptocurrency trading, etc.
","FinRL: Deep Reinforcement Learning Framework to Automate Trading in Quantitative Finance New York ’21, Nov. 3–5, 2021, New York, NY
Figure 5: Performance of stock trading [51] using the FinRL framework.
Figure 6: Performance of portfolio allocation [21] using the FinRL framework.
07/01/2020-06/30/2021 Ensemble [51] A2C PPO DDPG TD3 Min-Var. DJIA
Initial value 1M 1M 1M 1M 1M 1M 1M
Final value 1.52M 1.46M; 1.43M 1.42M; 1.36M 1.40M; 1.36M 1.39M 1.24M 1.33M
Annualized return 52.61% 46.65%; 42.57% 41.90%; 36.17% 40.34%; 36.01% 39.38% 24.10% 32.84%
Annualized Std 15.53% 17.86%; 15.51% 16.33%; 15.20% 17.28%; 14.38% 15.08% 11.2% 14.5%
Sharpe ratio 2.81 2.24; 2.36 2.23; 2.11 2.05; 2.21 2.28 1.98 2.02
Max drawdown -7.09% -7.59%; -9.04% -9.41%; -8.68% -8.10%; -8.46% -8.92% -6.97% -8.93%
Table 3: Performance of stock trading and portfolio allocation over the DJIA constituents stocks using FinRL. The Sharpe ratios
of the ensemble strategy and the individual DRL agents excceed those of the DJIA index, and of the min-variance strategy.
is from 2009/01/01 to 2020/06/30 on a daily basis, and the testing
period is from 2020/07/01 to 2021/06/30. The performance in terms
of multiple metrics is consistent with the results reported in [ 51] and
[50], and here we show results in a recent trading period. We can
see from the DJIA index that the trading period is a bullish market
with an annual return of 32.84% . The ensemble strategy achieves
a Sharpe ratio of 2.81and an annual return of 52.61%. It beatsA2C with a Sharpe ratio of 2.24, PPO with a Sharpe ratio of 2.23,
DDPG with a Sharpe ratio of 2.05, DJIA with a Sharpe ratio of 2.02,
and min-variance portfolio allocation with a Sharpe ratio of 1.98,
respectively. Therefore, the backtesting performance demonstrates
that FinRL successfully reproduces the ensemble strategy [51].",2021-11-07T00:34:32Z,ep reinforcement learni framework automate tradi quantitative nance new york nov new york  formance  formance ensemble mivar initial nal annual zed annual zed std share max table formance t share t  t share it share share share share share trefore n
paper_qf_42.pdf,8,"FinRL: Deep Reinforcement Learning Framework to Automate Trading in
  Quantitative Finance","  Deep reinforcement learning (DRL) has been envisioned to have a competitive
edge in quantitative finance. However, there is a steep development curve for
quantitative traders to obtain an agent that automatically positions to win in
the market, namely \\\\textit{to decide where to trade, at what price} and
\\\\textit{what quantity}, due to the error-prone programming and arduous
debugging. In this paper, we present the first open-source framework
\\\\textit{FinRL} as a full pipeline to help quantitative traders overcome the
steep learning curve. FinRL is featured with simplicity, applicability and
extensibility under the key principles, \\\\textit{full-stack framework,
customization, reproducibility} and \\\\textit{hands-on tutoring}.
  Embodied as a three-layer architecture with modular structures, FinRL
implements fine-tuned state-of-the-art DRL algorithms and common reward
functions, while alleviating the debugging workloads. Thus, we help users
pipeline the strategy design at a high turnover rate. At multiple levels of
time granularity, FinRL simulates various markets as training environments
using historical data and live trading APIs. Being highly extensible, FinRL
reserves a set of user-import interfaces and incorporates trading constraints
such as market friction, market liquidity and investor's risk-aversion.
Moreover, serving as practitioners' stepping stones, typical trading tasks are
provided as step-by-step tutorials, e.g., stock trading, portfolio allocation,
cryptocurrency trading, etc.
","New York ’21, Nov. 3–5, 2021, New York, NY Xiao-Yang Liu, Hongyang Yang, Jiechao Gao, and Christina Dan Wang
Figure 7: Cumulative returns (5-minute) of top 10 market cap cryptocurrencies trading using FinRL.
4.5 Use Case II: Portfolio Allocation
We reproduce a portfolio allocation strategy [ 21] that uses a DRL
agent to allocate capital to a set of stocks and reallocate periodically.
FinRL improves the reproducibility by allowing users to easily
compare the results of different settings, such as the pool of stocks to
trade, the initial capital, and the model hyperparameters. It utilizes
the agent layer to specify the state-of-the-art DRL libraries. Users
do not need to redevelop the neural networks and instead they can
just plug-and-play with any DRL algorithm.
Fig. 6 and Table 3 depict the backtesting performance on Dow
30 constituent stocks. The training and testing period is the same
with Case I. It shows that each DRL agent, namely A2C [ 32], TD3
[14], PPO [ 42], and DDPG [ 26], outperforms the DJIA index and
the min-variance strategy. A2C has the best performance with a
Sharpe ratio of 2.36and an annual return of 42.57%; TD3 is the
second best agent with a Sharpe ratio of 2.28and an annual return
of39.38%; PPO with a Sharpe ratio of 2.11and an annual return
of36.17% and DDPG with a Sharpe ratio of 2.21and an annual
return of 36.01% . Therefore, using FinRL, users can easily compare
the agents’ performance with each other and with the baselines.
4.6 Use Case III: Cryptocurrencies Trading
We use FinRL to reproduce [ 20] for top 10 market cap cryptocurren-
cies1. FinRL provides a full-stack development pipeline, allowing
users to have an end-to-end walk-through of how to download mar-
ket data using APIs, perform data preprocessing, pick and fine-tune
DRL algorithms, and get automated backtesting performance.
Fig. 7 describes the backtesting performance on the ten cryp-
tocurrencies with transaction cost. The training period is from
2021/10/01 to 2021/10/20 on a 5-minute basis, and the testing period
is from 2021/10/21 to 2021/10/30. The portfolio with the PPO algo-
rithm from the ElegantRL library has the highest cumulative return
of103% ; Equally weighted portfolio strategy has the second high-
est cumulative return of 99%; BTC buy and hold strategy with a
cumulative return of 93% . Therefore, the backtesting performance
shows that FinRL successfully reproduce [ 20] with completeness
and simplicity.
1The top 10 market cap cryptocurrencies as of Oct 2021 are: Bitcoin (BTC), Ethereum
(ETH), Cardano (ADA), Binance Coin (BNB), Ripple (XRP), Solana (SOL), Polkadot
(DOT), Dogecoin (DOGE), Avalanche (AVAX), Uniswap (UNI).5 ECOSYSTEM OF FINRL AND CONCLUSIONS
In this paper, we have developed an open-source framework, FinRL,
to help quantitative traders overcome the steep learning curve.
Customization is accessible on all layers, from market environments,
trading agents up towards trading tasks. FinRL follows a training-
testing-trading pipeline to reduce the simulation-to-reality gap.
Within FinRL, historical market data and live trading platforms are
reconfigured into standardized environments in OpenAI gym-style;
state-of-the-art DRL algorithms are implemented for users to train
trading agents in a pipeline; and an automated backtesting module
is provided to evaluate trading performance. Moreover, benchmark
schemes on typical trading tasks are provided as practitioners’
stepping stones.
Ecosystem of FinRL Framework . We believe that the rise of
the open-source community fostered the development of AI in Fi-
nance for both academia and industry side. As the need of utilizing
open-source AI for finance ecosystem is imminent in the finance
community, FinRL provides a ecosystem that features Deep Rein-
forcement Learning in finance comprehensively to fulfill such need
for all-level users in our open-source community.
FinRL offers an overall framework to utilize DRL agents for vari-
ous markets, SOTA DRL algorithms, finance tasks (portfolio alloca-
tion, cryptocurrency trading, high-frequency trading), live trading
support, etc. For entry-level users, FinRL aims to provide a demon-
strative and educational atmosphere with hands-on documents to
help beginners get familiar with DRL in Finance applications. For
intermediate-level users, such as full-stack developers and profes-
sionals, FinRL provides ElegantRL [ 28], a lightweight and scalable
DRL library for FinRL with finance-oriented optimizations. For
advanced-level users, such as investment banks and hedge funds.
FinRL delivers FinRL-Podracer [ 24,29], a cloud-native solution for
FinRL with high performance and high scalability training.
FinRL also develops other useful tools to support the ecosystem.
FinRL-Meta [ 30] adds financial data engineering for FinRL with
unified data processor and hundreds of market environments. Ex-
plainable DRL for portfolio management [ 17] and DRL ensemble
strategy for stock trading [50, 51] are also implemented.
Future work . Future research directions would be investiaging
DRL’s potential on limit order book [ 48], hedging [ 6], market mak-
ing [16], liquidation [3], and trade execution [27].",2021-11-07T00:34:32Z,new york nov new york  ya  ho ya ya jie chagao christina dawa  cumulative use case tfallocat it users  table dow t case it share share share share trefore use case ypto currencitradi  is  t t elegant equally trefore t  bitcoitre um cardano biance coiripple solar polka dot done coialanc uni swap icustoatwithiopeoecotem framework   as ep re ilearni for nance for elegant for  racer meta ex future future
paper_qf_42.pdf,9,"FinRL: Deep Reinforcement Learning Framework to Automate Trading in
  Quantitative Finance","  Deep reinforcement learning (DRL) has been envisioned to have a competitive
edge in quantitative finance. However, there is a steep development curve for
quantitative traders to obtain an agent that automatically positions to win in
the market, namely \\\\textit{to decide where to trade, at what price} and
\\\\textit{what quantity}, due to the error-prone programming and arduous
debugging. In this paper, we present the first open-source framework
\\\\textit{FinRL} as a full pipeline to help quantitative traders overcome the
steep learning curve. FinRL is featured with simplicity, applicability and
extensibility under the key principles, \\\\textit{full-stack framework,
customization, reproducibility} and \\\\textit{hands-on tutoring}.
  Embodied as a three-layer architecture with modular structures, FinRL
implements fine-tuned state-of-the-art DRL algorithms and common reward
functions, while alleviating the debugging workloads. Thus, we help users
pipeline the strategy design at a high turnover rate. At multiple levels of
time granularity, FinRL simulates various markets as training environments
using historical data and live trading APIs. Being highly extensible, FinRL
reserves a set of user-import interfaces and incorporates trading constraints
such as market friction, market liquidity and investor's risk-aversion.
Moreover, serving as practitioners' stepping stones, typical trading tasks are
provided as step-by-step tutorials, e.g., stock trading, portfolio allocation,
cryptocurrency trading, etc.
","FinRL: Deep Reinforcement Learning Framework to Automate Trading in Quantitative Finance New York ’21, Nov. 3–5, 2021, New York, NY
REFERENCES
[1]Joshua Achiam. 2018. Spinning Up in Deep Reinforcement Learning. https:
//spinningup.openai.com
[2]Andrew Ang. August 10, 2012. Mean-variance investing. Columbia Business
School Research Paper No. 12/49. (August 10, 2012).
[3]Wenhang Bao and Xiao-Yang Liu. 2019. Multi-agent deep reinforcement learning
for liquidation strategy analysis. ICML Workshop on Applications and Infrastruc-
ture for Multi-Agent Learning (2019).
[4]Stelios D Bekiros. 2010. Fuzzy adaptive decision-making for boundedly rational
traders in speculative stock markets. European Journal of Operational Research
202, 1 (2010), 285–293.
[5]Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schul-
man, Jie Tang, and Wojciech Zaremba. 2016. OpenAI Gym. arXiv preprint
arXiv:1606.01540 (2016).
[6]Hans Buehler, Lukas Gonon, Josef Teichmann, Ben Wood, Baranidharan Mohan,
and Jonathan Kochems. 2019. Deep hedging: Hedging derivatives under generic
market frictions using reinforcement learning. Swiss Finance Institute Research
Paper 19-80 (2019).
[7]Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and
Marc G. Bellemare. 2018. Dopamine: A research framework for deep reinforce-
ment learning. http://arxiv.org/abs/1812.06110 (2018).
[8]Ltd China Securities Index Co. 2017. CSI 300. http://www.csindex.com.cn/
uploads/indices/detail/files/en/145_000300_Fact_Sheet_en.pdf
[9]Yue Deng, Feng Bao, Youyong Kong, Zhiquan Ren, and Qionghai Dai. 2016. Deep
direct reinforcement learning for financial signal representation and trading.
IEEE Transactions on Neural Networks and Learning Systems 28, 3 (2016), 653–664.
[10] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plap-
pert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov.
2017. OpenAI baselines. https://github.com/openai/baselines.
[11] Hao Dong, Akara Supratak, Luo Mai, Fangde Liu, Axel Oehmichen, Simiao
Yu, and Yike Guo. 2017. TensorLayer: A versatile library for efficient deep
learning development. In Proceedings of the 25th ACM International Conference
on Multimedia . 1201–1204.
[12] Shanghai Stock Exchange. 2018. SSE 180 Index Methodology.
http://www.sse.com.cn/market/sseindex/indexlist/indexdetails/indexmethods/
c/IndexHandbook_EN_SSE180.pdf
[13] Thomas G. Fischer. 2018. Reinforcement learning in financial markets - a survey .
FAU Discussion Papers in Economics. Friedrich-Alexander University Erlangen-
Nuremberg, Institute for Economics.
[14] Scott Fujimoto, Herke Van Hoof, and David Meger. 2018. Addressing function
approximation error in actor-critic methods. International Conference on Machine
Learning (2018).
[15] Prakhar Ganesh and Puneet Rakheja. 2018. Deep reinforcement learning in high
frequency trading. ArXiv abs/1809.01506 (2018).
[16] Sumitra Ganesh, Nelson Vadori, Mengda Xu, Hua Zheng, Prashant Reddy, and
Manuela Veloso. 2019. Reinforcement Learning for Market Making in a Multi-
agent Dealer Market. NeurIPS’19 Workshop on Robust AI in Financial Services .
[17] Mao Guan and Xiao-Yang Liu. 2021. Explainable Deep Reinforcement Learning
for Portfolio Management: An Empirical Approach. ACM International Conference
on AI in Finance (ICAIF) (2021).
[18] Chien Yi Huang. 2018. Financial trading as a game: A deep reinforcement learning
approach. arXiv preprint arXiv:1807.02787 (2018).
[19] Hang Seng Index. 2020. Hang Seng Index and Sub-indexes. https://www.hsi.
com.hk/eng/indexes/all-indexes/hsi
[20] Zhengyao Jiang and J. Liang. 2017. Cryptocurrency portfolio management with
deep reinforcement learning. Intelligent Systems Conference (IntelliSys) (2017),
905–913.
[21] Zhengyao Jiang, Dixing Xu, and J. Liang. 2017. A deep reinforcement
learning framework for the financial portfolio management problem. ArXiv
abs/1706.10059.
[22] Prahlad Koratamaddi, Karan Wadhwani, Mridul Gupta, and Sriram G. Sanjeevi.
2021. Market sentiment-aware deep reinforcement learning approach for stock
portfolio allocation. Engineering Science and Technology, an International Journal .
[23] Mark Kritzman and Yuanzhen Li. 2010. Skulls, financial turbulence, and risk
management. Financial Analysts Journal 66 (10 2010).
[24] Zechu Li, Xiao-Yang Liu, jiahao Zheng, Zhaoran Wang, Anwar Walid, and Jian
Guo. 2021. FinRL-Podracer: High Performance and Scalable Deep Reinforcement
Learning for Quantitative Finance. ACM International Conference on AI in Finance
(ICAIF) (2021).
[25] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Gold-
berg, Joseph E. Gonzalez, Michael I. Jordan, and Ion Stoica. 2018. RLlib: Ab-
stractions for Distributed Reinforcement Learning. In International Conference
on Machine Learning (ICML) .
[26] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez,
Yuval Tassa, David Silver, and Daan Wierstra. 2016. Continuous control with
deep reinforcement learning. ICLR (2016).[27] Siyu Lin and P. Beling. 2020. A Deep Reinforcement Learning Framework for
Optimal Trade Execution. In ECML/PKDD .
[28] Xiao-Yang Liu, Zechu Li, Zhaoran Wang, and Jiahao Zheng. 2021. ElegantRL:
A scalable and elastic deep reinforcement learning library. https://github.com/
AI4Finance-Foundation/ElegantRL.
[29] Xiao-Yang Liu, Zechu Li, Zhuoran Yang, Jiahao Zheng, Zhaoran Wang, Anwar
Walid, Jian Guo, and Michael Jordan. 2021. ElegantRL-Podracer: Scalable and Elas-
tic Library for Cloud-Native Deep Reinforcement Learning. Deep RL Workshop,
NeurIPS 2021 (2021).
[30] Xiao-Yang Liu, Jingyang Rui, Jiechao Gao, Liuqing Yang, Hongyang Yang, Zhao-
ran Wang, Christina Dan Wang, and Guo Jian. 2021. Data-Driven Deep Rein-
forcement Learning in Quantitative Finance. Data-Centric AI Workshop, NeurIPS .
[31] B. G. Malkiel. 2003. Passive investment strategies and efficient markets. European
Financial Management 9 (2003), 1–10.
[32] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timo-
thy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchro-
nous methods for deep reinforcement learning. In International Conference on
Machine Learning . 1928–1937.
[33] John Moody and Matthew Saffell. 2001. Learning to trade via direct reinforcement.
IEEE Transactions on Neural Networks 12, 4 (2001), 875–889.
[34] J. Moody, L. Wu, Y. Liao, and M. Saffell. 1998. Performance functions and rein-
forcement learning for trading systems and portfolios. Journal of Forecasting 17
(1998), 441–470.
[35] Abhishek Nan, Anandh Perumal, and Osmar R Zaiane. 2020. Sentiment and
knowledge based algorithmic trading with deep reinforcement learning. ArXiv
abs/2001.09403 (2020).
[36] Quantopian. 2019. Pyfolio: A toolkit for Portfolio and risk analytics in Python.
https://github.com/quantopian/pyfolio.
[37] Antonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto,
and Noah Dormann. 2019. Stable Baselines3. https://github.com/DLR-RM/stable-
baselines3.
[38] Francesco Rundo. 2019. Deep LSTM with reinforcement learning layer for finan-
cial trend prediction in FX high frequency trading systems. Applied Sciences 9
(10 2019), 1–18.
[39] Jonathan Sadighian. 2019. Deep reinforcement learning in Cryptocurrency
market making. arXiv: Trading and Market Microstructure (2019).
[40] Svetlana Sapuric and A. Kokkinaki. 2014. Bitcoin is volatile! Isn’t that right?. In
BIS.
[41] Otabek Sattarov, Azamjon Muminov, Cheol Lee, Hyun Kang, Ryumduck Oh,
Junho Ahn, Hyung Oh, and Heung Jeon. 2020. Recommending cryptocurrency
trading points with deep reinforcement learning approach. Applied Sciences 10.
[42] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347
(2017).
[43] William F Sharpe. 1970. Portfolio theory and capital markets . McGraw-Hill
College.
[44] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George
Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershel-
vam, Marc Lanctot, et al .2016. Mastering the game of Go with deep neural
networks and tree search. Nature 529, 7587 (2016), 484.
[45] Richard S. Sutton, David Mcallester, Satinder Singh, and Yishay Mansour. 2000.
Policy gradient methods for reinforcement learning with function approximation.
InAdvances in Neural Information Processing Systems 12 . MIT Press, 1057–1063.
[46] Evgeni B. Tarassov. 2016. Exchange traded funds (ETF): History, mechanism,
academic literature review and research perspectives. Microeconomics: General
Equilibrium & Disequilibrium Models of Financial Markets eJournal (2016).
[47] Nelson Vadori, Sumitra Ganesh, Prashant Reddy, and Manuela Veloso. 2020. Risk-
sensitive reinforcement learning: a martingale approach to reward uncertainty.
International Conference on AI in Finance (ICAIF) (2020).
[48] Svitlana Vyetrenko, David Byrd, Nick Petosa, Mahmoud Mahfouz, Danial Der-
vovic, Manuela Veloso, and Tucker Hybinette Balch. 2020. Get real: Realism
metrics for robust limit order book market simulations. International Conference
on AI in Finance (ICAIF) (2020).
[49] Christopher JCH Watkins and Peter Dayan. 1992. Q-learning. Machine Learning
8, 3-4 (1992), 279–292.
[50] Zhuoran Xiong, Xiao-Yang Liu, Shan Zhong, Hongyang Yang, and Anwar Walid.
2018. Practical deep reinforcement learning approach for stock trading. NeurIPS
Workshop (2018).
[51] Hongyang Yang, Xiao-Yang Liu, Shan Zhong, and Anwar Walid. 2020. Deep
reinforcement learning for automated stock trading: An ensemble strategy. ACM
International Conference on AI in Finance (ICAIF) (2020).
[52] Daochen Zha, Kwei-Herng Lai, Kaixiong Zhou, and X. X. Hu. 2019. Experi-
ence replay optimization. International Joint Conference on Artificial Intelligence
(IJCAI) .
[53] Yong Zhang and Xingyu Yang. 2017. Online portfolio selection strategy based on
combining experts’ advice. Computational Economics 50, 1 (2017), 141–159.
[54] Zihao Zhang, Stefan Zohren, and Stephen Roberts. 2020. Deep reinforcement
learning for trading. The Journal of Financial Data Science 2, 2 (2020), 25–40.",2021-11-07T00:34:32Z,ep reinforcement learni framework automate tradi quantitative nance new york nov new york joshua chi am spinni up ep reinforcement learni andrew a august meacolumbia business schoresearpa no august ha bao  ya  multi workshapplicatns infra st ru multi agent learni ste libe ki ros fuz ajournal oatnal researgreg brockmanicki chu ludw petersojonas schneir  chul jie ta wojcieare mba opegym   hans bu hler lucas go nojosef eichmanbewood barid har amorajonathaems ep edgi swiss nance institute researpa pablo samuel castro sub ho ep moi tra charlgel da laura bh kumar marc belle mare dopamine ltd  securitiinx co fa set yue e fe bao you yo  zhi quareqi o articial intellence articial intellence ep transans neural networks learni tems pra full hari wal  jesse leg kl imo alex nic hmatias lap alec bradford  sc humasalmosid or yu hu articial intellence wu peter zh  opehao do kara supra tak lu articial intellence fa   axe oe hm csi m yu ike guo tensor layer iproceedis internatnal conference multimedia e articial intellence stock exe inx methodology inx handbook thomas scr reinforcement discusspas economics friedrialexanr  erlaenuremberg institute economics  fimoto r ke vahoof did me ger addressi internatnal conference machine learni pra khagampune et rak  ep ar  su mira gamnelsova doi megda xu hua c rash ant reddy manuel veto so reinforcement learni market maki multi aler market eur workshrobust nancial servicmao gu ya   articial intellence able ep reinforcement learni tfmanagement aemical approainternatnal conference nance chieyi hu nancial   ha so inx ha so inx sub c yao lia lia ypto currency intellent tems conference itell ys c yao lia di ki xu lia ar  prasad rea tam add karawad hwarid ul gupta sri ram sajeep market eineeri science technology internatnal journal mark rlitz mayuawli skulls nancial analysts journal ze chu li  ya  c hao rawa near valid jaguo  racer hh formance scalable ep reinforcement learni quantitative nance internatnal conference nance eric lia richard lia robert ishihara phip merit roy fox kegold joseph gonzalez michael jordastic lli ab distributed reinforcement learni iinternatnal conference machine learni timot l li ap jonathahunt alexanr rlitz el nicolas  ess tom rez yu val as did sawie stra continuous si yu libe li ep reinforcement learni framework optimal tra executi ya  ze chu li hao rawa jia hao c elegant nance foundatelegant  ya  ze chu li zhu raya jia hao c hao rawa near valid jaguo michael jordaelegant  racer scalable el as library oud native ep reinforcement learni ep worksheur  ya  ji ya rui jie chagao  ki ya ho ya ya hao wa christina dawa guo jadata driveep re ilearni quantitative nance data centric worksheur mal kiel passive anancial management vdy myr nih adr p domenemedia media mira alex grtimo l li ap tim harley did s ray ka uk co lu synchro iinternatnal conference machine learni  moody maw af fell learni transans neural networks moody wu lia af fell formance journal forecasti abi sk naaand u mal oscar articial intellence ne sentiment ar  quato plapy ftfpythoantofaff iashley hl maximiaernest us adam lee assi kane vis to noah dorm anstable baseline france rudo ep applied sciencjonathad hi aep ypto currency  tradi market m struure svetlana sap ric k kiaki bitcoiisistate star ov adam jomum iov c uka ryu duck oh juho ahu oh hu  recoendi applied scienc sc humaflip  pra full hari wal alec bradford leg kl imo proximal   wliam share tfmc raw hl college did saja hu chris additarthur guez laurent  re george vadrisc juliascar it wise loais antoog lou ved anne ers l marc la to masteri go nature richard suodid call ester tenr sih yi sha manor policy iadvancneural informatprocessi tems  evetras sov exe history maoeconomic genl equibrium is equibrium mols nancial markets journal nelsova doi su mira gamrash ant reddy manuel veto so risk internatnal conference nance sv it lana yet re did bird nick pet osa mahmoud mahfouz dani al r manuel veto so tucker  bint te batget ream internatnal conference nance  wats peter day amachine learni zhu raso  ya  shaho ho ya ya near valid praical eur workshho ya ya  ya  shaho near valid ep ainternatnal conference nance dao csha i r  articial intellence articial intellence so hou hu ex i internatnal joint conference articial intellence yo  ki yu ya online tnal economics zi hao  stefaoh resteproberts ep t journal nancial data science
paper_qf_43.pdf,1,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Factor Models for Cancer Signatures
Zura Kakushadzexy1and Willie Yu]2
xQuantigicrSolutions LLC
1127 High Ridge Road #135, Stamford, CT 069053
yFree University of Tbilisi, Business School & School of Physics
240, David Agmashenebeli Alley, Tbilisi, 0159, Georgia
]Centre for Computational Biology, Duke-NUS Medical School
8 College Road, Singapore 169857
(April 28, 2016)
Abstract
We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
nance to cancer genome data. Using 1389 whole genome sequenced samples
from 14 cancers, we identify an \\\\overall"" mode of somatic mutational noise.
We give a prescription for factoring out this noise and source code for xing
the number of signatures. We apply nonnegative matrix factorization (NMF)
to genome data aggregated by cancer subtype and ltered using our method.
The resultant signatures have substantially lower variability than those from
unltered data. Also, the computational cost of signature extraction is cut by
about a factor of 10. We nd 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature
(70% contribution). Our method accelerates nding new cancer signatures
and improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative nance.
1Zura Kakushadze, Ph.D., is the President of QuantigicrSolutions LLC, and a Full Professor
at Free University of Tbilisi. Email: zura@quantigic.com
2Willie Yu, Ph.D., is a Research Fellow at Duke-NUS Medical School. Email: willie.yu@duke-
nus.edu.sg
3DISCLAIMER: This address is used by the corresponding author for no purpose other than
to indicate his professional aliation as is customary in publications. In particular, the contents
of this paper are not intended as an investment, legal, tax or any other such advice, and in no
way represent views of QuantigicrSolutions LLC, the website www.quantigic.com or any of their
other aliates.arXiv:1604.08743v4  [q-bio.GN]  23 Jan 2017",2016-04-29T09:11:34Z,faor mols cancer snatur u sha xy wlie yu quant ic solutns hh ridge road stanford free  tbisi business schoschopsics did gma s ne belt alley tbisi georgia centre tnal blogy duke medical schocollege road siae apr abstra  usi   t also  our reciprocal  u sha ph presint quant ic solutns full professor free  tbisi em articial intellence wlie yu ph researfellow duke medical schoem articial intellence  iquant ic solutns  
paper_qf_43.pdf,2,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","1 Introduction and Summary
One in eight human deaths is caused by cancer. Cancer stands out among diseases
for it stems from somatic alterations in the genome. One common type of somatic
alterations found in cancer is due to single nucleotide variations (SNVs) or alter-
ations to single bases in the genome. These SNVs are accumulated throughout the
lifetime of the cancer via exposures to dierent mutational processes. These pro-
cesses can be endogenous to the cell such as imperfect DNA replication during cell
division or spontaneous cytosine deamination [Goodman and Fygenson, 1998], [Lin-
dahl, 1993]. They can also be exogenous due to exposures to chemical insults or
ultraviolet radiation [Loeb and Harris, 2008], [Ananthaswamy and Pierceall, 1990].
All of these mutational processes, whether extrinsic or intrinsic, will leave evidence
of their activity in the cancer genome characterized by distinctive alteration pat-
terns or mutational signatures. From a knowledge standpoint, if one can identify all
signatures and thus all mutational processes contributing to cancer, then one can
begin to understand the origins of cancer and its development. From a therapeutic
point of view, if there are no discernible patterns of mutations between dierent
cancer types, then dierent cancers will mostly likely require their own type-specic
or even patient-specic therapeutics. However, if there is a much smaller number of
mutational signatures describing all or most cancer types, then a therapeutic for one
cancer type with certain mutational signatures present may very well be applicable
across other cancer types with the same or similar mutational signatures.4
At present, the identication of mutational signatures involves analyzing SNV
patterns present in a cohort of DNA sequenced whole cancer genomes. SNVs found
in each cancer genome can be classied into 96 distinct mutation categories.5The
data is organized into a matrix Gis, where the rows correspond to the N= 96
mutation categories, the columns correspond to dsamples, and each element is a
nonnegative occurrence count of a given mutation in a given sample. The commonly
accepted method for extracting cancer signatures from Gis[Alexandrov et al, 2013a]
is via nonnegative matrix factorization (NMF) [Paatero and Tapper, 1994], [Lee and
Seung, 1999]. Under NMF the matrix Gis approximated via GW H , whereWiA
is anNKmatrix,HAsis aKdmatrix, and both WandHare nonnegative. The
appeal of NMF is its biologic interpretation whereby the Kcolumns of the matrix
Ware interpreted as the weights with which the Kcancer signatures contribute into
theN= 96 mutation categories, and the columns of the matrix Hare interpreted
4Another practical motivation for identifying cancer signatures is prevention, by pairing the
signatures observed in cancer samples with those caused by exposure to various carcinogens.
5In brief, DNA is a double helix of two strands, and each strand is a string of letters A, C,
G, T corresponding to adenine, cytosine, guanine and thymine, respectively. In the double helix,
A in one strand always binds with T in the other, and G always binds with C. This is known
as base complementarity. Thus, there are six possible base mutations C >A, C>G, C>G,
T>A, T>C, T>G, whereas the other six base mutations are equivalent to these by base
complementarity. Each of these 6 possible base mutations is anked by 4 possible bases on each
side thereby producing 4 64 = 96 distinct mutation categories.
1",2016-04-29T09:11:34Z,introdusuary one cancer one vs tse vs tse goodmafy gens olity lobe harris samantha swamp pierce all all from from at vs t is t is alexandra pa at ero tap   seu unr is wi matrix as is wand hare t columns ware cancer hare anotr ii  each
paper_qf_43.pdf,3,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","as the exposures to the Ksignatures in each sample. The price to pay for this is
that NMF, which is an iterative procedure, is computationally costly and depending
on the number of samples dit can take days or even weeks to run it. Furthermore,
it does not automatically x the number of signatures K, which must either be
guessed or obtained via trial and error, thereby adding to the computational cost.
Additional considerations include: i) out-of-sample instability, i.e., the signatures
obtained from non-overlapping sets of samples can be dramatically dierent; ii) in-
sample instability, i.e., the signatures can have a strong dependence on the initial
iteration choice; and iii) samples with low counts or sparsely populated samples
(i.e., those with many zeros { such samples are ubiquitous, e.g., in exome data) are
usually deemed not too useful as they contribute to the in-sample instability.
Happily, a conceptually similar problem is well-studied in quantitative nance
and we can simply borrow from the arsenal of tools developed there once we establish
adictionary between the biologic and nance quantities. Thus, in the quant nance
context one deals with a portfolio of Nstocks, which are analogous to the N= 96
mutation categories. The data consists of a time series of d(e.g., daily) stock returns
for each stock, so we have an NdmatrixRis. Thedobservations in the time series
of stock returns are analogous to the dsamples in the cancer data. The returns Ris
are analogous to the counts Gisexcept that the returns Risneed not be positive.
However, this does not aect what we wish to borrow from quantitative nance.
The sample correlation6matrix 	 ijcomputed based on the time series of stock re-
turns contains important information about the correlation structure of the returns.
Its spectral decomposition via principal components provides a tool for identify-
ing common risk factors underlying the returns, i.e., up to an error term, we have
R
F, where the columns of the NK(so-called factor loadings) matrix 
 iA
are related to the rst Kprincipal components of 	 ij, and the columns of the Kd
matrixFAsare the time series of the factor returns. In our dictionary, the matrix

 is analogous to the matrix W, and the matrix Fis analogous to the matrix H.
So, why is this useful, especially considering that the matrices 
 and Fare not
nonnegative in the nance context? There are two pieces of useful information we
can extract from this analogy. First, algorithms for xing the number of factors K
are readily available [Kakushadze and Yu, 2016b]. So, if we compute the sample cor-
relation matrix 	 ijbased on our occurrence count matrix Gisand apply the methods
employed in statistical risk models , we can x the number of cancer signatures (or at
least a useful expected range for it) based on purely statistical methods.7E.g., one
such method proposed in [Kakushadze and Yu, 2016b] is based on eRank (eective
rank) [Roy and Vetterli, 2007] of 	 ijand appears to work well for cancer signatures.
Second, intuitively it is clear that there is a lot of noise in the occurrence count
dataGis. There are mutations that also occur in healthy humans, e.g., via imper-
fections in DNA repair. Furthermore, one can expect that in the presence of cancer
6The sample covariance matrix Cij=ij	ij, where2
iare the sample variances and 	 ii1.
7We review these methods in detail below. The gist of the idea is to identify Kwhich optimizes
the contributions from the factors (signatures) and the error terms into the diagonal of 	 ij.
2",2016-04-29T09:11:34Z,snaturt furtr additnal happy  stocks t is t observatns t is is except is need t its principal as are iis so fare tre rst u sha yu so is and u sha yu rank roy ve er li second is tre furtr t ci  t which
paper_qf_43.pdf,4,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","such or similar mutations not directly associated with cancer signatures may become
more ubiquitous due to disruption in the normal operation of various processes (in-
cluding repair) in DNA. This \\\\background noise"" obscures the signatures and must
be identied and factored out of the data prior to attempting any signature ex-
traction. In the context of nance this is well-known as the \\\\market"" mode, which
corresponds to the overall movement of the broad market aecting all stocks (to
varying degrees) { cash inow (outow) into (from) the market tends to push stock
prices higher (lower). This is the market risk factor. To mitigate this risk factor,
one can, e.g., hold a dollar-neutral portfolio of stocks (the same dollar holdings for
long and short positions).8And we can use this analogy for cancer signatures.
Based on our empirical analysis, we indeed nd what we term the \\\\overall"" mode
{ the analog of the \\\\market"" mode in nance { in the occurrence count data. It is
unequivocally present. Here is a simple way to understand this \\\\overall"" mode. The
average pair-wise correlation 	 ijbetween dierent mutations ( i6=j) is nonzero and
is in fact high for most cancer types we study. This is noise that must be factored
out. If we aggregate samples by cancer type and compute the sample correlation
matrix 	 ijfor the so-aggregated data (across the n= 14 cancer types we study),
the average correlation is about 75% if we base it on Gis, and over whopping 96%
if we use the log-based matrix instead (see below). Another way of thinking about
this is that the occurrence counts in dierent samples are not normalized uniformly
across all samples. Therefore, running NMF on a vanilla matrix Giscould amount
to mixing apples with oranges thereby obscuring the true underlying signatures.
Factoring out the \\\\overall"" mode (or \\\\de-noising"" the matrix Gis) therefore most
simply would amount to cross-sectional (i.e., across the 96 mutation categories) de-
meaning. Simply put, we could demean the columns of Gis. One evident issue with
this is that, while the so-demeaned Giscan be used in the context of applying sta-
tistical factor model methods to it (recall that the returns Risneed not be positive)
to x the number of signatures, we would not be able to run NMF on such a matrix
as it is no longer nonnegative. Another, more subtle issue is that distributions of
counts inGis{ the counts being nonnegative numbers { are not (quasi) normal but
skewed, with long tails at the higher end. In fact, they are quasi log-normal, which is
common for nonnegative quantities. Therefore, instead of demeaning the columns of
G, it makes much more sense to demean the columns of ln( G) (and re-exponentiate
for the purpose of running NMF). A minor hiccup is that some elements of Giscan
be 0. A simple way to deal with this is to set Ris= ln(1 +Gis) and construct the
correlation matrix 	 ijbased onRis(as opposed to Gis) orR0
is, which isRiswith
columns demeaned { this amounts to factoring out the \\\\overall"" mode. We run our
analysis using Ris,R0
isas well asGisandG0
is(which isGiswith columns demeaned)
and unequivocally nd that using \\\\de-noised"" log-based matrix R0
isworks best.9
8The \\\\market"" mode is the (quasi uniform) 1st principal component of 	 ij:V(1)
i1=p
N.
9Table 1 summarizes the Mean/Median ratio and skewness for the matrices GisandRisacross
cancer types and mutation categories and makes the skewed nature of the counts evident. This
skewness is exacerbated when we consider it across samples for many individual cancer types. Also,
3",2016-04-29T09:11:34Z, i to and based it re t   is anotr trefore is could faoriis simply is one is cais need anotr is itrefore is cais is is is is with  is is and is with t table meamediais and is aoss  also
paper_qf_43.pdf,5,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","So, here is a simple prescription for xing the number of signatures using our
statistical factor model based methodology. Compute R0
isas above for the occur-
rence counts aggregated by cancer type. I.e., Gisis anNnmatrix, where the
number of cancer types n= 14 in our case. Compute the sample correlation matrix
	ijbased onR0
is, i.e., the correlations are computed across the 14 cancer types.
Compute eRank(	 ij) and round it to the nearest integer. This is the expected num-
ber of cancer signatures K(excluding the \\\\overall"" mode, which is noise). This
simple procedure appears to work well for this purpose and we explain in detail
why this is the case based on the statistical factor model methodology, along with
another method for xing K, which gives similar results. A complementary way
of xingKis to compute the sample correlation matrices [	( )]ijfor each cancer
type labeled by = 1;:::;n ([	()]ijis computed based on the samples for the -th
cancer type), take the rst principal component [ V()](1)
ifor each correlation matrix
[	()]ij, compute the nnmatrix of inner products E(1)
=PN
i=1[V()](1)
i[V()](1)
i,
computeE1= eRank(E(1)
), and identify Kwith rounded E1. This method produces
essentially the same prediction for Kas the aforesaid method using eRank(	 ij).
Once we x the expected number of signatures K, we are ready to use NMF to
extract cancer signatures. However, as mentioned above, running NMF on Gisis
suboptimal as it contains noise due to the \\\\overall"" mode. A simple way to eliminate
the \\\\overall"" mode is to run NMF on the re-exponentiated matrix eGis= exp(R0
is).
Note that the elements in eGisare no longer interpreted as \\\\counts"" { they are
fractional and low. We can include an overall normalization10to make it look more
like the original matrix Gis, however, this does not aect the signatures extracted
via NMF.11Now we are in good shape: we have the expected number of signatures
Kand the \\\\de-noised"" matrix eGisfrom which we can extract signatures via NMF.
Remarkably, what we nd is 4 previously known signatures12plus 3 new signa-
tures. One of the new signatures dominates liver cancer (with over 96% contribu-
tion), with almost no peak variability. Another new signature to a lesser degree
dominates renal cell carcinoma (with over 70% contribution). The third new signa-
ture appears mostly in bone cancer, brain lower grade glioma and medulloblastoma
(and also 5 other cancers to lesser degrees). We nd the same signatures (plus the
\\\\overall"" mode) if we use Gisinstead ofeGis, but the signatures are unequivocally
more stable when using eGis. Simply put, removing the \\\\overall"" mode (the noise)
Figures 1 and 2 help visualize why factoring out the \\\\overall"" mode reduces noise.
10E.g., we can take eGis= exp(Mean( Ris) +R0
is), oreGis= exp(Median( Ris) +R0
is), etc.
11Technically speaking, after re-exponentiating we should subtract the extra 1 we added in the
denitionRis= ln(1+Gis). This can be done using the denitions in footnote 10 and the (relatively
scarce) negative elements resulting from subtracting 1 should be zeroed out. However, this does
not seem to aect the results much, so not to overcomplicate things we work with eGis= exp(R0
is).
12To wit, mutational signatures 1 (spontaneous cytosine deamination), 2+13 (APOBEC medi-
ated cytosine deamination), 4 (tobacco carcinogen related exposure) and 17 (appearing in oesoph-
agus cancer, breast cancer, liver cancer, lung adenocarcinoma, B-cell lymphoma, stomach cancer
and melanoma; mutational process unknown) of [Nik-Zainal et al, 2012], [Alexandrov et al, 2013b].
4",2016-04-29T09:11:34Z,so ute is is ute ute rank   is rank with  as rank once is is is note is are  is  and is from remarkably one anotr t  is instead is is simply s is meais is mediais technically is is  is to  articial intellence nal alexandra
paper_qf_43.pdf,6,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","pays o high dividends. Now, we emphasize that our results are based on using
the occurrence counts aggregated by cancer type. The advantages of this method
include: i) the data is much less noisy than for samples by individual cancer type;
and ii) it allows us to use allgenomic data, including that with low counts. In
this regard, our approach here can be readily applied to exome data, which we will
report elsewhere along with extending our analysis to individual cancer types.
The remainder of this paper is organized as follows. In Sections 2-4 we review the
quantitative nance machinery we borrow from. Section 5 applies this machinery to
cancer signatures. Section 6 discusses empirical results based on the published data
for the 14 cancer types. Section 7 discusses our NMF results. We briey conclude
in Section 8. Appendix A lists the genome data sample IDs we use. Appendix B
contains our R source code for factor models. Appendix C contains some legalese.
2 Sample Covariance Matrix
2.1 Sample Data
In many practical applications we have Nobjects characterized by an observable
quantity, which is measured over dobservations for each object. The resulting
data is anNdmatrix { call it Ris{ where the rows correspond to the objects
labeled byi= 1;:::;N , and the columns correspond to the observations labeled by
s= 1;:::;d . In general there can be some missing observations, i.e., NAs in Ris.
However, for our purposes here it will suce to assume that there are no NAs.
Here are some examples of such data. In nance we have Nstocks,dtrading
days,13and we measure daily stock returns14Ris. Or, e.g., ilabels large cities in
the US (or, alternatively, zip codes), slabels years, and Risis violent crime rate per
capita. In the context of this paper, we have N= 96 mutation types15occurring in
various types of cancers, dis the number of collected samples, and Risis (related to
{ see below) the occurrence count for the mutation type iin the sample s.
2.2 Serial Covariances and Correlations
We can think of the matrix RisasNseries ofd=M+ 1 observations.16The
sample covariance matrix (SCM) is dened as an NNmatrix of pair-wise serial
13A trading day refers to a day on which the stock market is open.
14E.g., the so-called close-to-close return, i.e., the return from yesterday's closing price to today's
closing price. This return can be dened as Ris=Pis=Pi;(s+1)",2016-04-29T09:11:34Z, t it isens sesese se ds   sample co variance matrix sample data iobjes t is ias is as re istocks is or is is iis is serial co variance correlatns  is as serit matrix  is is pi
paper_qf_43.pdf,7,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","covariances:17
Cij=1
MM+1X
s=1XisXjs (1)
whereXis=Ris",2016-04-29T09:11:34Z,ci is xj is is
paper_qf_43.pdf,8,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","2.4 Correlations, Not Covariances
In many applications involving SCM, it must be invertible and, furthermore, out-
of-sample stable.19As mentioned above, in many cases SCM does not satisfy these
requirements and one replaces it with a constructed matrix such that it is positive
denite and more stable. However, in practice it is convenient to model the sample
correlation matrix 	 ijinstead ofCij, for two reasons. First, since sample variances
Ciiare relatively stable and can be readily computed, there is no need to model
them; it is the pair-wise correlations 	 ij(i6=j) that require modeling. Second, in
many cases, the sample variances Ciihave a skewed cross-sectional20(e.g., (quasi)
log-normal) distribution, as is often the case with positive-valued quantities. It is
therefore convenient to factor iout of SCM, i.e., to work with the sample correlation
matrix 	 ij=Cij=ij. Its diagonal elements are nicely uniform (	 ii1), and the
o-diagonal elements 	 ij(i6=j) take values in ( ",2016-04-29T09:11:34Z,correlatns not co variance ias ci rst ii are second ii he it ci its
paper_qf_43.pdf,9,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","As above, Cov(;) are serial covariances.21A nice feature of ",2016-04-29T09:11:34Z,as cov
paper_qf_43.pdf,10,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","3.2 Statistical Factor Models
In many applications \\\\binary"" and \\\\analog"" factors mentioned above are unattain-
able or unreliable. In this case, we can resort to statistical factor models [Kakushadze
and Yu, 2016b]. The idea is simple. We have our data eRis. What if we construct

iAbased on this data and no other input? I.e., we have to take an Ndmatrix
and somehow distill it down to a smaller NKmatrix. The question is how and
what should Kbe? And this is precisely where the factor model approximation via
",2016-04-29T09:11:34Z,statistical faor mols iiu sha yu t  is what based matrix t be and
paper_qf_43.pdf,11,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","This corresponds to taking the factor loadings matrix and factor covariance matrix
of the form

iA=p
(A)V(A)
i; A = 1;:::;K (17)
AB=AB (18)
This construction is nicely simple. However, what should Kbe? Two simple meth-
ods for xing Kare discussed in [Kakushadze and Yu, 2016b], where R source code
for constructing statistical factor models is also given. We briey review them here.
4 Fixing Factor Number
WhenK=Mwe have ",2016-04-29T09:11:34Z,  be two are u sha yu  xi faor number w
paper_qf_43.pdf,12,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Here eRank( Z) is the eective rank [Roy and Vetterli, 2007] of a symmetric semi-
positive-denite (which suces for our purposes here) matrix Z. It is dened as
eRank(Z) = exp(H) (22)
H=",2016-04-29T09:11:34Z,re rank roy ve er li it rank
paper_qf_43.pdf,13,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","where:= 1;:::;n labelsndierent cancer types; as before, i= 1;:::;N = 96; and
s= 1;:::;d (). Hered() is the number of samples for the cancer type labeled by
. The combined matrix Gisis obtained simply by appending the matrices [ G()]is
together column-wise. We will discuss a renement of this data structure below.
The simplest thing we can do is to identify the matrix Risin our discussion
above with Gis(or [G()]is).27However, this may not be the most optimal choice.
The issue is this. The elements of the matrix Gisare populated by nonnegative
occurrence counts. Nonnegative quantities with large numbers of samples tend to
have skewed distributions with long tails at higher values. I.e., such distributions
are not normal but (in many cases) roughly log-normal. One simple way to deal
with this is to identify Riswith a (natural) logarithm of Gis(instead of Gisitself).
A minor hiccup here is that some elements of Giscan be 0. We can do a lot of
complicated and even convoluted things to deal with this issue. Here we will follow
a pragmatic approach and do something simple instead { there is so much noise in
the data that doing otherwise simply does not pay o. So, we will simply take
Ris= ln (1 +Gis) (26)
This takes care of the Gis= 0 cases; for Gis1 we haveRisln(Gis), as desired.
Now we can construct statistical factor models for cancer signatures using the
\\\\minimization"" and eRank based methods (with or without the K0based variation)
for xing the number of cancer signatures K. In fact, for the sake of completeness
and comparative purposes, below we will construct such factor models assuming
both (26) and Ris=Gis. Happily, qualitatively the results turn out to be similar.
6 Empirical Results
6.1 Data Summary
In our empirical analysis below we use genome data from published samples only.
This data is summarized in Table 2, where we give total counts, number of samples
and the data sources, which are as follows: A1 = [Alexandrov et al, 2013b], A2
= [Love et al, 2012], B1 = [Tirode et al, 2014], C1 = [Zhang et al, 2013], D1 = [Nik-
Zainal et al, 2012], E1 = [Puente et al, 2011], E2 = [Puente et al, 2015], F1 = [Cheng
et al, 2016], G1 = [Wang et al, 2014], H1 = [Sung et al, 2012], H2 = [Fujimoto et al,
2016], I1 = [Imielinksi et al, 2012], J1 = [Jones et al, 2012], K1 = [Patch et al, 2015],
L1 = [Waddell et al, 2015], M1 = [Gundem et al, 2015], N1 = [Scelo et al, 2014].
Sample IDs with the corresponding publication sources are given in Appendix A.
6.2 Genome Data Results
In our genome dataset we have 14 cancer types. Using the denition (26), we apply
the \\\\minimization"" and eRank based methods (with and without the K0based
27The discussion below focuses on Gisand, unless stated otherwise, also applies to [ G()]is.
12",2016-04-29T09:11:34Z, red t is is  t is iis t t is are nonegative one is with is is itself is ca re so is is  is is is is  rank iis is happy emical results data suary i table alexandra love tico    articial intellence nal plenty plenty c wa su fimoto mie links jonpatwadll gum cell sample ds   data results iusi rank t is and
paper_qf_43.pdf,14,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","variation) for xing the number of cancer signatures K. We use the R functions
bio.erank.pc() and bio.cov.pc() in Appendix B hereof, which are adapted from
Appendix A of [Kakushadze and Yu, 2016b]. The results are summarized in Table
3. Unless we use the K0based variation, the value of Ktends to be low. If we
combine the samples from all 14 cancer types into a single \\\\big"" matrix (in our
case, of dimension 96 1389), then we get K= 2 for the eRank based method and
K= 1 for the\\\\minimization"" based method (without the K0based variation). Both
of these methods produce K= 1 if we aggregate all samples within each cancer type
and run them on the resulting 96 14 matrix. The question is, how come?
The answer is quite prosaic. Table 4 provides the average pair-wise correlation 	
(as dened in footnote 26) and the rst 5 eigenvalues of the sample correlation matrix
	ij. Except for Brain Lower Grade Glioma, Esophageal Cancer and Pancreatic
Cancer (for which cancer types the matrix Gisis sparsely populated with many 0s),
these average correlations are rather high and there is a large gap between the rst
and higher eigenvalues. Therefore, the rst eigenvector dominates in the spectral
decomposition (14). Excluding it via the K0based variation then produces higher
values ofK. However, on general grounds we expect the higher principal components
to be out-of-sample unstable. That is, if we compute them based on two or more
sets of non-overlapping samples, there is no guarantee that they will be stable from
set to set. Therefore, we must address the issue out-of-sample stability rst.
6.2.1 Out-of-sample (In)stability
A convenient way of addressing this issue is by checking whether the rst and higher
principal components computed for each cancer type are stable from one cancer type
to another. As above, let [ G()]isbe the occurrence count matrix for the cancer type
labeled by(for our genome data takes 14 values). We then compute the corre-
sponding matrix [ R()]isvia (26) and the correlation matrix [	( )]ij. Let [V()](a)
i
be thea-th principal component of [	( )]ij. We then dene a very informative
matrix of inner products
E(a)
=NX
i=1[V()](a)
i[V()](a)
i (27)
By denition, E(a)
1, andjE(a)
j<1 for6=. This matrix can be thought of
as a measure of how \\\\correlated"" the a-th principal components are across dierent
cancer types. Table 5 gives summaries of jE(a)
jfora= 1;2;3 and6=(there are
1413=2 = 91 independent values for each a). For illustrative purposes, in the
fourth row we also give a summary of a similar matrix of inner products based on a
union of the second and third principal components. From Table 5 it is evident that
the rst principal component is extremely stable from one cancer type to another.
However, higher principal components appear to be rather unstable. In this regard
it is informative to compute the eRank of the matrix E(a)
(using the calc.erank()
13",2016-04-29T09:11:34Z,   u sha yu t table unless tends  rank both t t table except br articial intellence lor gra lima esophageal cancer paneatic cancer is is trefore exudi that trefore out ias    by  table for from table irank
paper_qf_43.pdf,15,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","subfunction in the bio.erank.pc() function in Appendix B hereof). For each athis
is a measure of how independent of each other the principal components [ V()](a)
iare
across the 14 cancer types: the lower the eRank, the less independent they are, and
the more stable they are from one cancer type to another. So, for Ea= eRank(E(a)
)
we getE1= 1:31,E2= 9:49,E3= 10:59, andE2+3= 15:54, whereE2+3is based
on the union of the second and third principal components as above. Based on
the foregoing, it appears that higher principal components are highly out-of-sample
unstable. Put dierently, higher (than the rst) principal components computed for
one cancer type apparently have little predictive power for other cancer types.28
6.2.2 The \\\\Overall"" Mode
The rst principal component is highly stable from one cancer type to another.
The values of E(1)
in Table 5 are mostly above 90%. This implies that we have a
dominant \\\\overall"" mode. In nance the analog of this is the so-called \\\\market""
mode29corresponding to the overall movement of the broad market, which aects
all stocks (to varying degrees) { cash inow (outow) into (from) the market tends
to push stock prices higher (lower). This is the market risk factor. To mitigate
this risk factor, one can, e.g., hold a dollar-neutral portfolio of stocks (i.e., the same
dollar holdings for long and short positions). And we can draw from this analogy.
We can think of the \\\\overall"" mode as follows. We can always write the sample
correlation matrix as
	ij= (1",2016-04-29T09:11:34Z, for rank so ea rank based put t ovll mo t t table  i to and  
paper_qf_43.pdf,16,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Factoring out the \\\\overall"" mode is nothing but cross-sectionally demeaning the
matrixRis, i.e., instead of Riswe use
R0
is=Ris",2016-04-29T09:11:34Z,faoriis is  is
paper_qf_43.pdf,17,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","7 Nonnegative Matrix Factorization
7.1 First, A Multiplicative Model
Using the statistical factor model approach allows us to: i) x the number of factors
K; and ii) remove the \\\\overall"" mode. The number of factors K1excluding the
\\\\overall"" mode predicted by the eRank based method agrees with that obtained via
E1in Subsection 6.2.3. However, a priori the statistical factor model approach would
appear to lack biologic interpretation. If we apply it directly to the no log denition
Ris=Gis(irrespective of the \\\\overall"" mode), the matrices 
 iAandFAsgenerally
can have negative elements. If we apply it to the log-based denition (26), then we
can re-exponentiate (11) via (recall that 2
iis the sample variance, and the factor
model is for the correlation matrix, which is why iappears in the exponent)31
bGis= exp 
i""is+iKX
A=1
iAFAs!
=isKY
A=1(ZAs)iA(30)
whereis= exp (i""is),ZAs= exp(FAs) andiA=i
iA. So, ignoring the \\\\multi-
plicative error"" term isfor a moment, bGisprovides a positive decomposition of the
matrix 1 + Gis, except that it is a multiplicative decomposition (as opposed to an
additive one, as in NMF). So, instead of \\\\weights"", here we have the powers isfor
the \\\\exposures"" ZAs. In fact, such a multiplicative model may not be too farfetched.
The processes inside DNA do appear to have \\\\exponential"" tendencies. We intend
to discuss this approach in more detail in a forthcoming paper. Instead, here we
will apply the improvement we get from factoring out the \\\\overall"" mode to NMF.
7.2 NMF: Vanilla Counts Matrix
The commonly accepted method for extracting cancer signatures from Gis[Alexan-
drov et al, 2013a] is via nonnegative matrix factorization (NMF) [Paatero and Tap-
per, 1994], [Lee and Seung, 1999]. Under NMF the matrix Gis approximated via
GW H , whereWiAis anNKmatrix,HAsis aKdmatrix, and both Wand
Hare nonnegative. The appeal of NMF is its biologic interpretation whereby the
Kcolumns of the matrix Ware interpreted as the weights with which the Kcancer
signatures contribute into the N= 96 mutation categories, and the columns of the
matrixHare interpreted as the exposures to the Ksignatures in each sample.
Usually, NMF is applied either to individual cancer types or to a \\\\big matrix""
obtained by combining the samples from all cancer types. Here we apply NMF in
a novel fashion to the 96 14 matrix obtained by aggregating samples by cancer
type. The advantage of this approach is that we get to include low count samples
31Technically speaking, we should subtract the 1 we added inside the log in the denition (26).
Then we have to deal with negative values. This would obscure our discussion here with no benet.
16",2016-04-29T09:11:34Z,nonegative matrix faorizatrst multiplicative musi t rank subse is is and as genlly  is as as as as so is provis is so as it  instead vanla counts matrix t is alex apa at ero tap  seu unr is wi is matrix as is wand hare t columns ware cancer hare snaturusually re t technically t
paper_qf_43.pdf,18,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","without destabilizing the results (in-sample), and this way we also avoid undesirable
proliferation of signatures which can occur when the number of samples is large.32
We use organic R code for running NMF (and check that it produces the same re-
sults as the R package \\\\NMF"", https://cran.r-project.org/package=NMF). We run
NMF for 100 \\\\samplings"" using random starting WandHfor each \\\\sampling"".33
Figure 3 gives the Pearson correlations between the vanilla matrix Gand the re-
constructed matrix G=W H for 5 to 9 signatures.34The highest reconstruction
accuracy is achieved for K= 8 signatures, which is what we anticipated above for
the vanilla matrix ( K1= 7 plus the \\\\overall"" mode). Figures 4-11 plot the 8 sig-
natures. For each signature, the corresponding weights in the Wcolumns (for each
of the 96 mutation categories) are averages over the 100 \\\\samplings"", and the error
bars are the standard deviations.35We discuss the interpretation of the signatures
below. Here we note that the error bars for the vanilla matrix are substantial. Also,
Signature 8 has substantial presence in most cancer types. This is the noise largely
stemming from the \\\\overall"" mode. Figure 12 summarizes signature contributions.
7.3 NMF: \\\\Overall"" Mode Factored Out
We now repeat the NMF procedure of the last subsection using the data with the
\\\\overall"" mode factored about. For this purpose, we simply re-exponentiate the
column-wise demeaned matrix R0
is, i.e., we take
eGis= exp(R0
is) (31)
and run NMF on eGis. We can include an overall normalization by taking eGis=
exp(Mean(Ris)+R0
is), oreGis= exp(Median( Ris)+R0
is), oreGis= exp(Median( Rs)+
R0
is) (recall that Rsis the vector of column means of Ris), etc., to make it look more
like the original matrix Gis, however, this does not aect the signatures extracted
via NMF.36Again, technically speaking, after re-exponentiating we should subtract
the extra 1 we added in the denition (26) (assuming we include one of the aforesaid
overall normalizations). However, this does not seem to aect the results much.
Figure 13 gives the Pearson correlations between the vanilla matrix eGand the
reconstructed matrix eG=W H for 4 to 8 signatures. The highest reconstruction
accuracy is achieved for K1= 7 signatures, which is what we anticipated above.
Figures 14-20 plot the 7 signatures, and the error bars are the standard deviations
for each mutation category as a result of the 100 \\\\samplings"". Here we note that
the error bars for the \\\\de-noised"" matrix eG(Figures 14-20) are substantially smaller
than for the vanilla matrix G(Figures 4-11) due to factoring out the \\\\overall"" mode.
32Such ubiquity of signatures generally makes them less useful.
33Each \\\\sampling"" nds a local optimum { NMF does not guarantee global convergence.
34This range is based on the values of Kin the last rows of Tables 3 and 7.
35We use the k-means clustering to sort the resultant signatures across the 100 \\\\samplings"".
36This is because each column of W, being weights, is normalized to add up to 1.
17",2016-04-29T09:11:34Z,  wand for  pearsoand t s for columns  re also snature   ovll mo faored out  for is is  is meais is mediais is mediars rs is is is ag articial intellence  pearsoand t s re s s suea kitabl 
paper_qf_43.pdf,19,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Figure 21 summarizes signature contributions. Our Signatures 1-4 are previously
known signatures, to wit, mutational signatures 1 (spontaneous cytosine deamina-
tion), 2+13 (APOBEC mediated cytosine deamination),374 (tobacco carcinogen
related exposure) and 17 (appearing in oesophagus cancer, breast cancer, liver can-
cer, lung adenocarcinoma, B-cell lymphoma, stomach cancer and melanoma; mu-
tational process unknown) of [Nik-Zainal et al, 2012], [Alexandrov et al, 2013b].38
Our Signatures 5-7 are new. New Signature 5 dominates liver cancer (with over 96%
contribution), with almost no peak variability. New Signature 6 to a lesser degree
dominates renal cell carcinoma (with over 70% contribution). New Signature 7 ap-
pears mostly in bone cancer, brain lower grade glioma and medulloblastoma (and
also 5 other cancers to lesser degrees). The super-dominant liver cancer signature
is exciting. Tables 13 and 14 give weights with errors for the 7 signatures.
8 Concluding Remarks
Out-of-sample (in)stability. This is a sticking point for any statistically based
method, which includes NMF. Usually, \\\\stability"" is addressed in the context of
NMF by perturbing the matrix Gand checking whether the signatures are stable.
However, this does not address out-of-sample stability. Out-of-sample stability is
well-understood and is the bread-and-butter in the context of quantitative trading.
Since there one deals with time series and forecasting, if a given model lacks out-
of-sample stability, it is pretty much useless. This is because time ows only in one
direction and if a model built using parameters computed based on a time period in
the past does not perform well during a future time period { that is, out-of-sample
{ it has no predictive (i.e., forecasting) power. In quantitative nance money is at
stake so methods for checking out-of-sample stability are rather well understood.
Based on those methods, a true test for out-of-sample stability in the context
of cancer signatures would be to take a set of samples, split it into 2 (or more)
non-overlapping subsets, independently extract signatures based on these subsets
and compare them. In fact, to have any kind of statistical signicance, we would
need an ensemble of such non-overlapping sets. E.g., we could take some number
of samples and split them randomly into two halves a number of times. For this
to be meaningful, we need a sizable number of samples to begin with. The data
we work with in this paper is rather limited in this sense because it includes only
published genome samples. Not only is the number of cancer types limited to 14,
but the number of samples within each cancer type is also limited. E.g., for prostate
cancer we have mere 5 samples and any meaningful out-of-sample stability test for
that cancer type is unattainable. On the other hand, for liver cancer we have a
37This was reported as a single signature (almost identical to our Signature 2) in [Alexandrov
et al, 2013b], however, subsequently, it was split into 2 distinct signatures, which usually appear
in the same samples. For detailed comments, see http://cancer.sanger.ac.uk/cosmic/signatures.
38Recovering these signatures is not surprising as we use data from these references.
18",2016-04-29T09:11:34Z, our snatur articial intellence nal alexandra our snaturnew snature new snature new snature t tablconudi remarks out  usually and out since  ibased ifor t not o snature alexandra for recoveri
paper_qf_43.pdf,20,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","substantial number of samples (389), which appears to be a contributing factor to
the extraction of a super-dominant signature for this cancer type, albeit not the
leading factor { without \\\\de-noising"" this signature is not as dominant, nor was it
found in [Fujimoto et al, 2016], where most of the liver cancer samples are published.
We need as much data as possible to study out-of-sample stability in any meaningful
fashion. The (still embargoed) ICGC data appears to hold promise in this regard.
What about individual cancer types? We ran our NMF analysis on the data
aggregated by cancer type. Can we do the same for individual cancer types? The
answer is yes { after all, this is how NMF is usually applied { but with caveats.
One of the advantages of aggregating the data by cancer type is that it reduces
the noise level. Individual cancer type samples generally are too noisy. Low count
samples exacerbate this issue. Table 7 is an apt testament to this. Once we remove
the \\\\overall"" mode (which articially lowers the value of K), we get too many
factors based on the statistical factor model analysis, and we therefore can expect
a proliferation of signatures as well. As mentioned above, too many signatures
are useless. In fact, high values of Kfor individual cancer types indicate out-of-
sample instability of any potential signatures. There are methods to reduce noise
for individual cancer types, however, they are outside of the scope hereof and will
be reported elsewhere. A practical motivation for considering individual cancer
types is that within each cancer type there may be biologic factors one may wish to
understand, e.g., mutational spectra of liver cancers can have substantial regional
dependence as they are mutagenized by exposures to dierent chemicals.39However,
aggregation by regions within a cancer type may still be warranted to reduce noise.40
Exome data. The volume of published exome data is substantially higher than
that of the published genome data. In this regard, it would make sense to apply
our methods to the exome data. The caveat is that the exome data is much more
sparsely populated than the genome data, which has the same eect as the low count
samples in the context of the genome data. Aggregation by cancer type is a natural
remedy to this. The main issue is that meticulously ascertaining which samples are
published is time consuming. We plan to discuss the exome data separately.
\\\\Minimization"" and eRank based algorithms. The former typically leads to
lower values of Kthan the latter. For the data at hand, the eRank based algorithm is
right on the money for xing the number of signatures. In this regard, it appears that
the eRank based algorithm should be the go-to method, however, the\\\\minimization""
based algorithm is still useful as the two algorithms set the expected range of the
values ofKwhere the search should be performed. Having a (tight) range of K
helps reduce computational cost { as mentioned above, NMF, being an iterative
procedure, is computationally costly. Speaking of which, we observed a reduction of
the number of iterations (within each \\\\sampling"" { see above) by about a factor of 10
between the vanilla and \\\\de-noised"" matrices. Not only does \\\\de-noising"" improve
the quality of the resultant signatures, but it also provides substantial computational
39We would like to thank Steven Rozen for emphasizing this point to us.
40Or else expectedly unstable factors in individual cancer types appear too copious (Table 7).
19",2016-04-29T09:11:34Z,fimoto  t what  cat one individual low table once as ifor tre exo me t it aregatt  miniatrank t thafor rank irank wre hi speaki not  steverole or table
paper_qf_43.pdf,21,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","cost savings. This is not surprising in retrospect. \\\\De-noising"" (not aggregation by
cancer type) is the key factor in improving the overall stability { the signatures
based on aggregated data without \\\\de-noising"" have much larger error bars.
Framework. This paper is not intended to be exhaustive in any way. As men-
tioned above, the data we work with here is limited, etc. Rather, the purpose of this
paper is to set forth the framework of factor models for cancer signatures, including
its application as an improvement to NMF. We hope it facilitates further research
and helps identify the (hopefully not too many) underlying cancer signatures.
Acknowledgments
We would like to thank Steven Rozen for valuable discussions and comments.
A Genome Sample IDs
In this Appendix we give the sample IDs with the corresponding publication refer-
ences for the genome data we use.
B Cell Lymphoma :
[Alexandrov et al, 2013b]:
4101316, 4105105, 4108101, 4112512, 4116738, 4119027, 4121361, 4125240, 4133511, 4135350, 4142267, 4158726, 4159170, 4163639,
4175837, 4177856, 4182393, 4189200, 4189998, 4190495, 4193278, 4194218, 4194891.
[Love et al, 2012]:
G1.
Bone Cancer :
[Tirode et al, 2014]:
IC009T, IC015T, IC024T, IC034T, IC044T, IC046T, IC049T, IC053T, IC054T, IC057T, IC058T, IC066T, IC067T, IC071T, IC076T,
IC077T, IC080T, IC082T, IC086T, IC092T, IC093T, IC096T, IC1057T, IC105T, IC106T, IC111T, IC112T, IC114T, IC116T, IC121T,
IC128T, IC130T, IC136T, IC147T, IC149T, IC151T, IC158T, IC165T, IC168T, IC174T, IC193T, IC196T, IC197T, IC198T, IC204T,
IC213T, IC215T, IC224T, IC242T, IC248T, IC254T, IC262T, IC263T, IC264T, IC267T, IC268T, IC270T, IC271T, IC272T, IC273T,
IC274T, IC275T, IC277T, IC278T, IC279T, IC280T, IC282T, IC283T, IC284T, IC286T, IC288T, IC294T, IC295T, IC296T, IC297T,
IC299T, IC300T, IC301T, IC302T, IC303T, IC305T, IC306T, IC309T, IC310T, IC311T, IC315T, IC316T, IC318T, IC319T, IC323T,
IC324T, IC325T, IC340T, IC343T, IC349T, IC831T, IC929T, IC973T.
Brain Lower Grade Glioma :
[Alexandrov et al, 2013b]:
PA10, PA102, PA103, PA105, PA107, PA109, PA11, PA110, PA112, PA116, PA117, PA12, PA131, PA134, PA136, PA138, PA14,
PA143, PA145, PA148, PA149, PA157, PA166, PA17, PA20, PA21, PA22, PA25, PA3, PA36, PA4, PA41, PA43, PA46, PA48, PA5,
PA53, PA54, PA55, PA56, PA58, PA59, PA62, PA63, PA64, PA65, PA69, PA70, PA73, PA75, PA79, PA8, PA81, PA82, PA83, PA84,
PA85, PA86, PA87, PA9, PA90, PA93, PA96.
[Zhang et al, 2013]:
SJLGG001, SJLGG002, SJLGG003, SJLGG004, SJLGG005, SJLGG006, SJLGG006R, SJLGG007, SJLGG008, SJLGG009,
SJLGG010, SJLGG011, SJLGG012, SJLGG013, SJLGG015, SJLGG016, SJLGG018, SJLGG019, SJLGG020, SJLGG021, SJLGG022,
SJLGG024, SJLGG025, SJLGG026, SJLGG027, SJLGG028, SJLGG029, SJLGG030, SJLGG031, SJLGG032, SJLGG033, SJLGG034,
20",2016-04-29T09:11:34Z,  framework  as ratr  ackledgment  steverole  sample ds i ds cell lymphoma alexandra love bone cancer tico  br articial intellence lor gra lima alexandra 
paper_qf_43.pdf,22,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","SJLGG035, SJLGG037, SJLGG038, SJLGG039, SJLGG040, SJLGG042.
Breast Cancer :
[Nik-Zainal et al, 2012]:
PD3851a, PD4085a, PD4088a, PD4103a, PD4120a, PD4194a, PD4192a, PD4198a, PD4199a, PD4248a, PD4086a, PD4109a, PD4107a,
PD3890a, PD3905a, PD4005a, PD4006a, PD3904a, PD3945a, PD4115a, PD4116a.
[Alexandrov et al, 2013b]:
PD3989a, PD4069a, PD4072a, PD4080a, PD4224a, PD4225a, PD4255a, PD4261a, PD4266a, PD4267a, PD4315a, PD4604a, PD4605a,
PD4606a, PD4607a, PD4608a, PD4613a, PD4826a, PD4833a, PD4836a, PD4841a, PD4847a, PD4951a, PD4952a, PD4953a, PD4954a,
PD4955a, PD4957a, PD4958a, PD4959a, PD4962a, PD4963a, PD4965a, PD4966a, PD4967a, PD4968a, PD4970a, PD4971a, PD4972a,
PD4975a, PD4976a, PD4980a, PD4981a, PD4982a, PD4983a, PD4985a, PD4986a, PD5928a, PD5934a, PD5935a, PD5936a, PD5942a,
PD5944a, PD5947a, PD5951a, PD5956a, PD6018a, PD6041a, PD6042a, PD6043a, PD6044a, PD6045a, PD6046a, PD6049a, PD6409a,
PD6410a, PD6411a, PD6413a, PD6417a, PD6418a, PD6422a, PD6466b, PD6719a, PD6720a, PD6721a, PD6722a, PD7199a, PD7201a,
PD7207a, PD7208a, PD7209a, PD7210a, PD7212a, PD7214a, PD7215a, PD7216a, PD7217a, PD7218a, PD7219a, PD7221a, PD7321a,
PD7404a, PD7409a, PD7431a, PD7433a, PD8618a, PD8622a, PD8623a.
Chronic Lymphocytic Leukemia :
[Alexandrov et al, 2013b]:
001-0002-03TD, 003-0005-09TD, 012-02-1TD, 004-0012-05TD, 005-0015-01TD, 006-0018-01TD, 007-0020-01TD, 008-0022-01TD, 009-
0026-02TD, 013-0035-01TD, 016-0040-02TD, 017-0042-01TD, 018-0046-01TD, 019-0047-01TD, 020-0049-01TD, 022-0053-01TD, 023-
0056-01TD, 027-0063-01TD, 029-0065-01TD, 030-0066-01TD, 032-0069-01TD, 033-0070-01TD, 038-0087-01TD, 039-0076-01TD, 040-
0088-01TD, 041-0090-01TD, 042-0091-01TD, 043-0094-01TD, 044-0092-01TD, 045-0082-04TD, 048-0089-01TD, 049-0086-01TD, 051-
0099-05TD, 052-0103-01TD, 053-0104-02TD, 054-0114-05TD, 063-0127-01TD, 064-0128-01TD, 082-02-1TD, 083-01-2TD, 090-02-1TD,
091-01-6TD, 100-02-2TD, 110-0218-04TD, 117-01-1TD, 124-01-1TD, 136-02-3TD, 141-02-3TD, 144-01-1TD, 145-01-1TD, 146-01-5TD,
148-02-3TD, 152-01-4TD, 155-01-1TD, 156-01-1TD, 157-01-1TD, 159-01-1TD, 165-01-5TD, 166-01-4TD, 168-02-2TD, 170-01-3TD,
171-01-2TD, 172-01-1TD, 173-01-3TD, 174-01-3TD, 175-01-3TD, 178-01-2TD, 181-01-3TD, 182-01-4TD, 184-01-4TD, 185-01-6TD,
186-01-6TD, 188-01-2TD, 189-01-1TD, 191-01-3TD, 192-01-4TD, 193-01-1TD, 194-01-2TD, 195-01-5TD, 197-01-3TD, 264-01-7TD,
266-01-7TD, 267-01-6TD, 270-01-2TD, 272-01-2TD, 273-01-5TD, 274-01-3TD, 275-01-2TD, 276-01-4TD, 278-01-4TD, 279-01-4TD,
280-01-4TD, 282-01-12TD, 290-1950-01TD, 319-01-1TD, 321-01-1TD, 322-01-1TD, 323-01-1TD, 324-01-1TD, 325-01-1TD, 326-01-
1TD, 328-01-1TD, 375-1099-15TD, 618-1503-04TD, 642-1991-01TD, 680-1992-01TD, 758-2041-01TD, 761-01-1TD, 785-1836-01TD.
[Puente et al, 2011]:
CLL4-ARTICLE.
[Puente et al, 2015]:
125, 128, 137, 141, 151, 178, 192, 26, 277, 282, 294, 306, 308, 318, 342, 343, 367, 393, 467, 473, 477, 519, 523, 564.
Esophageal Cancer :
[Cheng et al, 2016]:
ESCC-ESCC-001T, ESCC-ESCC-002T, ESCC-ESCC-003T, ESCC-ESCC-004T, ESCC-ESCC-005T, ESCC-ESCC-006T,
ESCC-ESCC-008T, ESCC-ESCC-009T, ESCC-ESCC-010T, ESCC-ESCC-011T, ESCC-ESCC-012T, ESCC-ESCC-013T,
ESCC-ESCC-014T, ESCC-ESCC-015T, ESCC-ESCC-016T, ESCC-ESCC-017T, ESCC-ESCC-018T.
Gastric Cancer :
[Wang et al, 2014]:
pfg005T, pfg008T, pfg022T, pfg023T, pfg030T, pfg031T, pfg032T, pfg034T, pfg035T, pfg036T, pfg038T, pfg039T, pfg043T, pfg050T,
pfg052T, pfg053T, pfg054T, pfg057T, pfg058T, pfg059T, pfg060T, pfg062T, pfg064T, pfg065T, pfg068T, pfg069T, pfg072T, pfg073T,
pfg076T, pfg081T, pfg082T, pfg088T, pfg089T, pfg092T, pfg094T, pfg097T, pfg099T, pfg100T, pfg102T, pfg103T, pfg104T, pfg105T,
pfg106T, pfg107T, pfg108T, pfg115T, pfg116T, pfg118T, pfg119T, pfg120T, pfg121T, pfg122T, pfg123T, pfg124T, pfg125T, pfg127T,
21",2016-04-29T09:11:34Z,breast cancer  articial intellence nal alexandra chronic lymph cy tic leukemia alexandra plenty plenty esophageal cancer c gastric cancer wa
paper_qf_43.pdf,23,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","pfg129T, pfg130T, pfg132T, pfg135T, pfg136T, pfg138T, pfg142T, pfg143T, pfg144T, pfg145T, pfg146T, pfg151T, pfg156T, pfg157T,
pfg160T, pfg164T, pfg166T, pfg167T, pfg173T, pfg180T, pfg181T, pfg182T, pfg205T, pfg212T, pfg213T, pfg217T, pfg220T, pfg222T,
pfg228T, pfg258T, pfg272T, pfg277T, pfg282T, pfg311T, pfg316T, pfg317T, pfg344T, pfg373T, pfg375T, pfg378T, pfg398T, pfg413T,
pfg416T, pfg424T.
Liver Cancer :
[Sung et al, 2012]:
HK101T, HK105T, HK106T, HK108T, HK113T, HK114T, HK115T, HK116T, HK117T, HK11T, HK122T, HK126T, HK131T, HK13T,
HK145T, HK14T, HK154T, HK159T, HK169T, HK172T, HK174T, HK177T, HK179T, HK17T, HK180T, HK181T, HK182T, HK186T,
HK193T, HK198T, HK19T, HK200T, HK203T, HK204T, HK205T, HK206T, HK207T, HK21T, HK22T, HK23T, HK260T, HK261T,
HK262T, HK266T, HK267T, HK268T, HK26T, HK272T, HK273T, HK274T, HK276T, HK29T, HK30T, HK32T, HK34T, HK35T,
HK36T, HK38T, HK39T, HK41T, HK43T, HK45T, HK46T, HK49T, HK53T, HK55T, HK58T, HK60T, HK62T, HK63T, HK64T,
HK65T, HK67T, HK68T, HK70T, HK71T, HK73T, HK75T, HK76T, HK79T, HK81T, HK82T, HK84T, HK87T, HK90T, HK92T,
HK95T, HK98T.
[Fujimoto et al, 2016]:
RK001 C01, RK002 C, RK003 C, RK004 C01, RK005 C, RK006 C1, RK006 C2, RK007 C01, RK010 C, RK012 C01, RK014 C01,
RK015 C, RK016 C01, RK018 C01, RK019 C, RK020 C01, RK021 C01, RK022 C01, RK023 C, RK024 C, RK025 C, RK026 C01,
RK027 C01, RK028 C01, RK029 C, RK030 C01, RK031 C01, RK032 C01, RK033 C01, RK034 C, RK035 C01, RK036 C01,
RK037 C01, RK038 C01, RK040 C01, RK041 C01, RK042 C, RK043 C01, RK044 C01, RK046 C01, RK046 C02, RK047 C01,
RK048 C, RK049 C01, RK050 C, RK051 C01, RK052 C01, RK053 C01, RK054 C01, RK055 C01, RK056 C01, RK057 C01,
RK058 C01, RK059 C01, RK060 C01, RK061 C01, RK062 C01, RK063 C, RK064 C01, RK065 C01, RK066 C01, RK067 C01,
RK068 C, RK069 C01, RK070 C01, RK071 C01, RK072 C01, RK073 C01, RK074 C01, RK075 C01, RK076 C01, RK077 C01,
RK079 C01, RK080 C01, RK081 C01, RK082 C01, RK083 C01, RK084 C01, RK085 C01, RK086 C01, RK087 C01, RK088 C01,
RK089 C01, RK090 C01, RK091 C01, RK092 C01, RK093 C01, RK095 C01, RK096 C01, RK098 C01, RK099 C01, RK100 C01,
RK101 C01, RK102 C01, RK103 C01, RK104 C01, RK105 C01, RK106 C01, RK107 C01, RK108 C01, RK109 C01, RK110 C01,
RK111 C01, RK112 C01, RK113 C01, RK115 C01, RK116 C01, RK117 C01, RK118 C01, RK119 C01, RK120 C01, RK121 C01,
RK122 C01, RK123 C01, RK124 C01, RK125 C01, RK126 C01, RK128 C01, RK130 C01, RK131 C01, RK133 C01, RK134 C01,
RK135 C01, RK136 C01, RK137 C01, RK138 C01, RK139 C01, RK140 C01, RK141 C01, RK142 C01, RK143 C01, RK144 C01,
RK145 C01, RK146 C01, RK147 C01, RK148 C01, RK149 C01, RK150 C01, RK151 C01, RK152 C01, RK153 C01, RK154 C01,
RK155 C01, RK156 C01, RK157 C01, RK159 C01, RK162 C01, RK163 C01, RK164 C01, RK165 C01, RK166 C01, RK167 C01,
RK169 C01, RK170 C01, RK171 C01, RK172 C01, RK175 C01, RK176 C01, RK177 C01, RK178 C01, RK179 C01, RK180 C01,
RK180 C02, RK181 C01, RK182 C01, RK183 C01, RK184 C01, RK185 C01, RK186 C01, RK187 C01, RK188 C01, RK189 C01,
RK190 C01, RK191 C01, RK193 C01, RK194 C01, RK195 C01, RK196 C01, RK197 C01, RK198 C01, RK199 C01, RK200 C01,
RK201 C01, RK202 C01, RK204 C02, RK205 C01, RK206 C01, RK207 C01, RK208 C01, RK209 C01, RK210 C01, RK211 C01,
RK212 C01, RK213 C01, RK214 C01, RK215 C01, RK216 C01, RK217 C01, RK219 C01, RK220 C01, RK221 C01, RK222 C01,
RK223 C01, RK224 C01, RK225 C01, RK226 C01, RK227 C01, RK228 C01, RK229 C01, RK230 C01, RK232 C01, RK233 C01,
RK233 C02, RK234 C01, RK235 C01, RK236 C01, RK237 C01, RK240 C01, RK241 C01, RK243 C01, RK244 C01, RK245 C01,
RK254 C01, RK256 C01, RK257 C01, RK258 C01, RK259 C01, RK260 C01, RK261 C01, RK261 C02, RK262 C01, RK263 C01,
RK264 C01, RK265 C01, RK266 C01, RK267 C01, RK268 C01, RK269 C01, RK270 C01, RK272 C01, RK275 C01, RK277 C01,
RK278 C01, RK279 C01, RK280 C01, RK281 C01, RK282 C01, RK283 C01, RK284 C01, RK285 C01, RK287 C01, RK288 C01,
RK289 C01, RK290 C01, RK297 C01, RK298 C01, RK303 C01, RK304 C01, RK305 C01, RK306 C01, RK307 C01, RK308 C01,
RK309 C01, RK310 C01, RK312 C01, RK316 C01, RK317 C01, RK326 C01, RK337 C01, RK338 C01, HX10T, HX11T, HX12T,
HX13T, HX14T, HX15T, HX16T, HX17T, HX18T, HX19T, HX20T, HX21T, HX22T, HX23T, HX24T, HX25T, HX26T, HX27T,
HX28T, HX29T, HX30T, HX31T, HX32T, HX33T, HX34T, HX35T, HX36T, HX37T, HX4T, HX5T, HX9T.
22",2016-04-29T09:11:34Z,licancer su fimoto
paper_qf_43.pdf,24,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Lung Cancer :
[Imielinksi et al, 2012]:
LU-A08-43, LUAD-2GUGK, LUAD-5V8LT, LUAD-AEIUF, LUAD-D02326, LUAD-E00934, LUAD-E01014, LUAD-E01278, LUAD-
E01317, LUAD-FH5PJ, LUAD-QY22Z, LUAD-S00488, LUAD-S01302, LUAD-S01331, LUAD-S01341, LUAD-S01345, LUAD-S01346,
LUAD-S01356, LUAD-S01381, LUAD-S01404, LUAD-S01405, LUAD-S01467, LUAD-S01478, LUAD-U6SJ7.
Medulloblastoma :
[Jones et al, 2012]:
LFS MB1, LFS MB2, LFS MB4, MB1, MB101, MB102, MB104, MB106, MB107, MB108, MB110, MB112, MB113, MB114, MB115,
MB117, MB119, MB12, MB121, MB122, MB124, MB125, MB126, MB127, MB128, MB129, MB130, MB131, MB132, MB134, MB139,
MB15, MB16, MB17, MB18, MB19, MB2, MB20, MB21, MB23, MB24, MB26, MB28, MB3, MB31, MB32, MB34, MB35, MB36,
MB37, MB38, MB39, MB40, MB45, MB46, MB49, MB5, MB50, MB51, MB518, MB53, MB56, MB57, MB58, MB59, MB6, MB60,
MB61, MB612, MB63, MB64, MB66, MB67, MB69, MB7, MB70, MB74, MB75, MB77, MB78, MB79, MB8, MB800, MB81, MB82,
MB83, MB84, MB85, MB86, MB88, MB89, MB9, MB90, MB91, MB92, MB94, MB95, MB96, MB98, MB99.
Ovarian Cancer :
[Patch et al, 2015]:
AOCS-001-1, AOCS-004-1, AOCS-005-1, AOCS-034-1, AOCS-055-1, AOCS-056-1, AOCS-057-1, AOCS-058-1, AOCS-059-1, AOCS-
060-1, AOCS-061-1, AOCS-063-1, AOCS-064-1, AOCS-065-1, AOCS-075-1, AOCS-076-1, AOCS-077-1, AOCS-078-1, AOCS-079-1,
AOCS-080-1, AOCS-081-1, AOCS-083-1, AOCS-084-1, AOCS-085-1, AOCS-086-1, AOCS-088-1, AOCS-090-1, AOCS-091-1, AOCS-
092-1, AOCS-093-1, AOCS-093-12, AOCS-094-1, AOCS-095-1, AOCS-096-1, AOCS-097-1, AOCS-104-1, AOCS-105-1, AOCS-106-1,
AOCS-107-1, AOCS-108-1, AOCS-109-1, AOCS-111-1, AOCS-112-1, AOCS-113-1, AOCS-114-1, AOCS-115-1, AOCS-116-1, AOCS-
122-1, AOCS-123-1, AOCS-124-1, AOCS-125-1, AOCS-126-1, AOCS-128-1, AOCS-130-1, AOCS-131-1, AOCS-132-1, AOCS-133-1,
AOCS-137-12, AOCS-139-1, AOCS-143-1, AOCS-144-1, AOCS-145-1, AOCS-146-1, AOCS-147-1, AOCS-148-1, AOCS-149-1, AOCS-
152-1, AOCS-153-1, AOCS-157-1, AOCS-158-1, AOCS-159-1, AOCS-160-1, AOCS-161-1, AOCS-162-1, AOCS-163-1, AOCS-164-1,
AOCS-165-1, AOCS-166-1, AOCS-168-1, AOCS-169-1, AOCS-170-1, AOCS-170-12, AOCS-171-1, AOCS-171-12.
Pancreatic Cancer :
[Waddell et al, 2015]:
TDICGC 0002, TD ICGC 0004, TD ICGC 0005, TD ICGC 0006, TD ICGC 0007, TD ICGC 0008, TD ICGC 0009, TD ICGC 0016,
TDICGC 0025, TD ICGC 0026, TD ICGC 0031, TD ICGC 0032, TD ICGC 0033, TD ICGC 0034, TD ICGC 0035, TD ICGC 0036,
TDICGC 0037, TD ICGC 0040, TD ICGC 0042, TD ICGC 0051, TD ICGC 0052, TD ICGC 0054, TD ICGC 0055, TD ICGC 0059,
TDICGC 0061, TD ICGC 0062, TD ICGC 0063, TD ICGC 0066, TD ICGC 0069, TD ICGC 0072, TD ICGC 0075, TD ICGC 0076,
TDICGC 0077, TD ICGC 0087, TD ICGC 0088, TD ICGC 0089, TD ICGC 0098, TD ICGC 0103, TD ICGC 0105, TD ICGC 0109,
TDICGC 0114, TD ICGC 0115, TD ICGC 0116, TD ICGC 0118, TD ICGC 0119, TD ICGC 0121, TD ICGC 0131, TD ICGC 0134,
TDICGC 0135, TD ICGC 0137, TD ICGC 0138, TD ICGC 0139, TD ICGC 0140, TD ICGC 0141, TD ICGC 0143, TD ICGC 0144,
TDICGC 0146, TD ICGC 0149, TD ICGC 0153, TD ICGC 0154, TD ICGC 0169, TD ICGC 0182, TD ICGC 0185, TD ICGC 0188,
TDICGC 0199, TD ICGC 0201, TD ICGC 0205, TD ICGC 0206, TD ICGC 0207, TD ICGC 0212, TD ICGC 0214, TD ICGC 0215,
TDICGC 0223, TD ICGC 0235, TD ICGC 0239, TD ICGC 0242, TD ICGC 0255, TD ICGC 0257, TD ICGC 0283, TD ICGC 0285,
TDICGC 0289, TD ICGC 0290, TD ICGC 0295, TD ICGC 0296, TD ICGC 0300, TD ICGC 0303, TD ICGC 0304, TD ICGC 0309,
TDICGC 0312, TD ICGC 0315, TD ICGC 0321, TD ICGC 0326, TD ICGC 0327, TD ICGC 0328, TD ICGC 0329, TD ICGC 0338,
TDICGC 0420, TD ICGC 0532, TD ICGC 0548, TD ICGC 0549.
Prostate Cancer :
[Gundem et al, 2015]:
A10E-0015 CRUK PC0015 T1,
A22C-0016 CRUK PC0016 T1,
23",2016-04-29T09:11:34Z,lu cancer mie links me dull oblast oma jonovariacancer patpaneatic cancer wadll prostate cancer gum
paper_qf_43.pdf,25,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","A29C-0017 CRUK PC0017 T1,
A31C-0018 CRUK PC0018 T1,
A32C-0019 CRUK PC0019 T1.
Renal Cell Carcinoma :
[Scelo et al, 2014]:
C0001T, C0002T, C0004T, C0005T, C0006T, C0007T, C0008T, C0009T, C0010T, C0011T, C0012T, C0013T, C0014T, C0015T,
C0016T, C0017T, C0018T, C0019T, C0020T, C0021T, C0022T, C0023T, C0024T, C0025T, C0026T, C0027T, C0028T, C0029T,
C0030T, C0031T, C0032T, C0033T, C0034T, C0035T, C0036T, C0037T, C0038T, C0039T, C0040T, C0041T, C0042T, C0043T,
C0044T, C0045T, C0046T, C0047T, C0048T, C0049T, C0050T, C0051T, C0052T, C0053T, C0054T, C0055T, C0056T, C0057T,
C0058T, C0059T, C0060T, C0061T, C0062T, C0063T, C0064T, C0065T, C0066T, C0067T, C0068T, C0069T, C0070T, C0071T,
C0072T, C0073T, C0074T, C0075T, C0076T, C0077T, C0078T, C0079T, C0080T, C0081T, C0082T, C0083T, C0084T, C0085T,
C0086T, C0089T, C0091T, C0092T, C0094T, C0096T, C0097T, C0098T, C0099T, C0100T.
B R Source Code
In this appendix we give the R (R Package for Statistical Computing, http://www.r-
project.org) source code for building purely statistical factor models (using principal
components) based on the algorithm we discuss in Sections 3.2 and 4, including the
\\\\minimization"" and eRank based algorithms for xing the number of factors Kin
Section 4. The two functions below are self-explanatory and straightforward as they
follow the formulas therein. This source code is an adaptation of that in Appendix A
of [Kakushadze and Yu, 2016b] reecting peculiarities in the mutation count data.41
The function bio.cov.pc(ret, use.cor = T, excl.first = F) corresponds to
the \\\\minimization"" based method for xing K. The input is: i) ret, anNdmatrix
Ris(see Section 5), where Nis the number of mutation types, and dis the number
of samples (or, in the case where we aggregate samples by cancer type, the number
of cancer types); ii) use.cor , where for TRUE (default) the factors are computed
based on the principal components of the sample correlation matrix 	 ij, whereas
forFALSE they are computed based on the sample covariance matrix Cij;excl.first ,
where for TRUE theK0based method of Subsection 4.3 is used. The output is a list:
result$pc are the rst Kprincipal components of a) Cijforuse.cor = F , and b) 	 ij
foruse.cor = T . For details of the other named list members, which are not needed
for our purposes here, see Appendix A of [Kakushadze and Yu, 2016b]. However,
we keep this output as it will be useful in future research projects.
The second function is bio.erank.pc(ret, use.cor = T, do.trunc = F, k = 0,
excl.first = F) and corresponds to the eRank based method for xing Kfor the
default parameter k = 0 . The input is the same as in the bio.cov.pc() function
except for the additional parameters do.trunc = F and k = 0 . For a positive integer
kthe code simply takes its value as the number of factors K. For k = 0 (default) the
code uses the eRank method: if do.trunc = F (default), then K= Round(eRank()),
41E.g., we can have cases with vanishing serial variance, and the code deals with this accordingly.
In nancial applications this would correspond to a stock that does not trade at all.
24",2016-04-29T09:11:34Z,renal cell carcinoma cell source co ipackage statistical uti sens rank kiset   u sha yu t t is seis ci subset principal ci for use for  u sha yu t rank for t for for rank round rank in
paper_qf_43.pdf,26,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","while if do.trunc = T , thenK= oor(eRank()). (The argument of eRank( ) is the
matrixCijifuse.cor = F , and the matrix 	 ijifuse.cor = T ). The output is the
same as in the bio.cov.pc() function. We use defaults unless stated otherwise.
The source code we give in this appendix is not written to be \\\\fancy"" or opti-
mized for speed or in any other way { e.g, using the functions qrm.calc.eigen()
and qrm.calc.eigen.eff() , as applicable, provided in Appendix B and Appendix
C of [Kakushadze and Yu, 2016b], respectively, instead of the built-in R function
eigen() may speed up the code. The sole purpose of the code herein is to illustrate
the algorithms described in the main text in a simple-to-understand fashion. See
Appendix C for some important legalese.
bio.cov.pc <- function (ret, use.cor = T, excl.first = F)
f
print(""Running bio.cov.pc()..."")
tr <- apply(ret, 1, sd)
take <- tr == 0
tr[take] <- 1
if(use.cor)
ret <- ret / tr
d <- ncol(ret)
x <- ret
x <- x - rowMeans(x)
x <- x %*% t(x) / (ncol(x) - 1)
tv <- diag(x)
x <- eigen(x)
if(excl.first)
f
k1 <- 2
y1 <- sqrt(x$values[1]) * matrix(x$vectors[, 1], nrow(ret), 1)
x1 <- y1 %*% t(y1)
tv <- tv - diag(x1)
g
else
f
k1 <- 1
x1 <- 0
g
g.prev <- 999
25",2016-04-29T09:11:34Z,rank t rank ii fuse t  t   u sha yu t   runni means
paper_qf_43.pdf,28,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","result <- new.env()
result$spec.risk <- spec.risk
result$fac.load <- fac.load
result$fac.cov <- fac.cov
result$cov.mat <- cov.mat
result$inv.cov <- inv.cov
result$pc <- x$vectors[, 1:ncol(fac.load)]
result <- as.list(result)
return(result)
g
bio.erank.pc <- function (ret, use.cor = T, do.trunc = F, k = 0,
excl.first = F)
f
print(""Running bio.erank.pc()..."")
calc.erank <- function(x, excl.first)
f
take <- x > 0
x <- x[take]
if(excl.first)
x <- x[-1]
p <- x / sum(x)
h <- - sum(p * log(p))
er <- exp(h)
if(excl.first)
er <- er + 1
return(er)
g
if(use.cor)
f
tr <- apply(ret, 1, sd)
take <- tr == 0
tr[take] <- 1
ret <- ret / tr
g
x <- ret
x <- x - rowMeans(x)
x <- x %*% t(x) / (ncol(x) - 1)
tv <- diag(x)
27",2016-04-29T09:11:34Z,runni means
paper_qf_43.pdf,30,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","return(result)
g
C DISCLAIMERS
Wherever the context so requires, the masculine gender includes the feminine and/or
neuter, and the singular form includes the plural and vice versa . The author of this
paper (\\\\Author"") and his aliates including without limitation QuantigicrSolu-
tions LLC (\\\\Author's Aliates"" or \\\\his Aliates"") make no implied or express
warranties or any other representations whatsoever, including without limitation
implied warranties of merchantability and tness for a particular purpose, in con-
nection with or with regard to the content of this paper including without limitation
any code or algorithms contained herein (\\\\Content"").
The reader may use the Content solely at his/her/its own risk and the reader
shall have no claims whatsoever against the Author or his Aliates and the Author
and his Aliates shall have no liability whatsoever to the reader or any third party
whatsoever for any loss, expense, opportunity cost, damages or any other adverse
eects whatsoever relating to or arising from the use of the Content by the reader
including without any limitation whatsoever: any direct, indirect, incidental, spe-
cial, consequential or any other damages incurred by the reader, however caused
and under any theory of liability; any loss of prot (whether incurred directly or
indirectly), any loss of goodwill or reputation, any loss of data suered, cost of pro-
curement of substitute goods or services, or any other tangible or intangible loss;
any reliance placed by the reader on the completeness, accuracy or existence of the
Content or any other eect of using the Content; and any and all other adversities
or negative eects the reader might encounter in using the Content irrespective of
whether the Author or his Aliates is or are or should have been aware of such
adversities or negative eects.
The R code included in Appendix B hereof is part of the copyrighted R code
of QuantigicrSolutions LLC and is provided herein with the express permission of
QuantigicrSolutions LLC. The copyright owner retains all rights, title and interest
in and to its copyrighted source code included in Appendix B hereof and any and
all copyrights therefor.
References
Alexandrov, L.B., Nik-Zainal, S., Wedge, D.C., Campbell, P.J., Stratton, M.R.
(2013a) Deciphering Signatures of Mutational Processes Operative in Human
Cancer. Cell Reports 3(1): 246-259.
Alexandrov, L.B., Nik-Zainal, S., Wedge, D.C., Aparicio, S.A., Behjati, S.,
Biankin, A.V., Bignell, G.R., Bolli, N., Borg, A., Brresen-Dale, A.L., Boyault,
29",2016-04-29T09:11:34Z,w author quant ic so lu author content t content author author content content content content author t  quant ic solutns quant ic solutns t  referencalexandra  articial intellence nal dge campbell statcipri snaturmtal processoative humacancer cell rets alexandra  articial intellence nal dge par c be jai bikib  bold borg dale boy all
paper_qf_43.pdf,31,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","S., Burkhardt, B., Butler, A.P., Caldas, C., Davies, H.R., Desmedt, C., Eils,
R., Eyfj ord, J.E., Foekens, J.A., Greaves, M., Hosoda, F., Hutter, B., Ilicic,
T., Imbeaud, S., Imielinski, M., J ager, N., Jones, D.T., Jones, D., Knappskog,
S., Kool, M., Lakhani, S.R., L opez-Ot n, C., Martin, S., Munshi, N.C., Naka-
mura, H., Northcott, P.A., Pajic, M., Papaemmanuil, E., Paradiso, A., Pearson,
J.V., Puente, X.S., Raine, K., Ramakrishna, M., Richardson, A.L., Richter, J.,
Rosenstiel, P., Schlesner, M., Schumacher, T.N., Span, P.N., Teague, J.W.,
Totoki, Y., Tutt, A.N., Vald es-Mas, R., van Buuren, M.M., van 't Veer, L.,
Vincent-Salomon, A., Waddell, N., Yates, L.R.; Australian Pancreatic Can-
cer Genome Initiative; ICGC Breast Cancer Consortium; ICGC MMML-Seq
Consortium; ICGC PedBrain, Zucman-Rossi, J., Futreal, P.A., McDermott,
U., Lichter, P., Meyerson, M., Grimmond, S.M., Siebert, R., Campo, E., Shi-
bata, T., Pster, S.M., Campbell, P.J., Stratton, M.R. (2013b) Signatures of
mutational processes in human cancer. Nature 500(7463): 415-421.
Ananthaswamy, H.N. and Pierceall, W.E. (1990) Molecular mechanisms of
ultraviolet radiation carcinogenesis. Photochemistry and Photobiology 52(6):
1119-1136.
Bai, Z.D. and Yin, Y.Q. (1993) Limit of the smallest eigenvalue of a large
dimensional sample covariance matrix. The Annals of Probability 21(3): 1275-
1294.
Bouchaud, J.-P. and Potters, M. (2011) Financial applications of random matrix
theory: a short review. In: Akemann, G., Baik, J. and Di Francesco, P. (eds.)
The Oxford Handbook of Random Matrix Theory. Oxford, United Kingdom:
Oxford University Press.
Campbell, L.L. (1960) Minimum coecient rate for stationary random pro-
cesses. Information and Control 3(4): 360-371.
Cheng, C., Zhou, Y., Li, H., Xiong, T., Li, S., Bi, Y., Kong, P., Wang, F.,
Cui, H., Li, Y., Fang, X., Yan, T., Li, Y., Wang, J., Yang, B., Zhang, L., Jia,
Z., Song, B., Hu, X., Yang, J., Qiu, H., Zhang, G., Liu, J., Xu, E., Shi, R.,
Zhang, Y., Liu, H., He, C., Zhao, Z., Qian, Y., Rong, R., Han, Z., Zhang, Y.,
Luo, W., Wang, J., Peng, S., Yang, X., Li, X., Li, L., Fang, H., Liu, X., Ma,
L., Chen, Y., Guo, S., Chen, X., Xi, Y., Li, G., Liang, J., Yang, X., Guo, J.,
Jia, J., Li, Q., Cheng, X., Zhan, Q., Cui, Y. (2016) Whole-Genome Sequencing
Reveals Diverse Models of Structural Variations in Esophageal Squamous Cell
Carcinoma. The American Journal of Human Genetics 98(2): 256-274.
Fujimoto, A., Furuta, M., Totoki, Y., Tsunoda, T., Kato, M., Shiraishi, Y.,
Tanaka, H., Taniguchi, H., Kawakami, Y., Ueno, M., Gotoh, K., Ariizumi, S.,
Wardell, C.P., Hayami, S., Nakamura, T., Aikata, H., Arihiro, K., Boroevich,
K.A., Abe, T., Nakano, K., Maejima, K., Sasaki-Oku, A., Ohsawa, A., Shibuya,
30",2016-04-29T09:11:34Z,bulk hard butler calls dis edit  ey fj foe lens grho soda uer  ici im be aud me i jonjonapp s tola khaot martimushi aka northco pa ji papa ea nui dise pearsoplenty articial intellence ne ramakrishna richardsorichter rosetl slner schumacr spaleague to toi bu valid mas but reevincent salmowadll dataustraliapaneatic ca initiative breast cancer consortium seq consortium ped br articial intellence zu cm arossi fu real mc r most liter meters ogri obieber campo shi campbell statsnaturnature samantha swamp pierce all molecular photo cmistry photo blogy articial intellence yilimit t annals probabity  chaters nancial ike manarticial intellence di france t oxford handbook random matrix tory oxford united kidom oxford   campbell minimum informatcontrc hou li so li bi  wa cui li fa yali wa ya  jia so hu ya qi   xu shi    hao iaso ha lu wa pe ya li li fa  ma cguo cxi li lia ya guo jia li c zh acui whole  sequenci reveals diverse mols struural variatns esophageal quad us cell carcinoma t journal humagenetics fimoto futura to toi tsu no kate ship articial intellence shi tank tanuchi kawakami xeno got oh ari zum war ll hay mi naka aik at ari ro borribe nano mae jima asahi oku oh saw shibuya
paper_qf_43.pdf,32,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","T., Nakamura, H., Hama, H., Hosoda, F., Arai, Y., Ohashi, S., Urushidate, T.,
Nagae, G., Yamamoto, S., Ueda, H., Tatsuno, K., Ojima, H., Hiraoka, N.,
Okusaka, T., Kubo, M., Marubashi, S., Yamada, T., Hirano, S., Yamamoto,
M., Ohdan, H., Shimada, K., Ishikawa, O., Yamaue, H., Chayama, K., Miyano,
S., Aburatani, H., Shibata, T., Nakagawa, H. (2016) Whole-genome mutational
landscape and characterization of noncoding and structural mutations in liver
cancer. Nature Genetics , doi:10.1038/ng3547.
Goodman, M.F. and Fygenson, K.D. (1998) DNA polymerase delity: from
genetics toward a biochemical understanding. Genetics 148(4): 1475-1482.
Grinold, R.C. and Kahn, R.N. (2000) Active Portfolio Management. New York,
NY: McGraw-Hill.
Gundem, G., Van Loo, P., Kremeyer, B., Alexandrov, L.B., Tubio, J.M., Pa-
paemmanuil, E., Brewer, D.S., Kallio, H.M., H ogn as, G., Annala, M., Kiv-
inummi, K., Goody, V., Latimer, C., O'Meara, S., Dawson, K.J., Isaacs,
W., Emmert-Buck, M.R., Nykter, M., Foster, C., Kote-Jarai, Z., Easton, D.,
Whitaker, H.C.; ICGC Prostate UK Group, Neal, D.E., Cooper, C.S., Eeles,
R.A., Visakorpi, T., Campbell, P.J., McDermott, U., Wedge, D.C., Bova, G.S.
(2015) The evolutionary history of lethal metastatic prostate cancer. Nature
520(7547): 353-357.
Imielinski, M., Berger, A.H., Hammerman, P.S., Hernandez, B., Pugh, T.J.,
Hodis, E., Cho, J., Suh, J., Capelletti, M., Sivachenko, A., Sougnez, C., Au-
clair, D., Lawrence, M.S., Stojanov, P., Cibulskis, K., Choi, K., de Waal, L.,
Sharifnia, T., Brooks, A., Greulich, H., Banerji, S., Zander, T., Seidel, D.,
Leenders, F., Ans en, S., Ludwig, C., Engel-Riedel, W., Stoelben, E., Wolf, J.,
Goparju, C., Thompson, K., Winckler, W., Kwiatkowski, D., Johnson, B.E.,
J anne, P.A., Miller, V.A., Pao, W., Travis, W.D., Pass, H.I., Gabriel, S.B., Lan-
der, E.S., Thomas, R.K., Garraway, L.A., Getz, G., Meyerson, M. (2012) Map-
ping the hallmarks of lung adenocarcinoma with massively parallel sequencing.
Cell150(6): 1107-1120.
Jones, D.T., J ager, N., Kool, M., Zichner, T., Hutter, B., Sultan, M., Cho, Y.J.,
Pugh, T.J., Hovestadt, V., St utz, A.M., Rausch, T., Warnatz, H.J., Ryzhova,
M., Bender, S., Sturm, D., Pleier, S., Cin, H., Pfa, E., Sieber, L., Wittmann,
A., Remke, M., Witt, H., Hutter, S., Tzaridis, T., Weischenfeldt, J., Raeder, B.,
Avci, M., Amstislavskiy, V., Zapatka, M., Weber, U.D., Wang, Q., Lasitschka,
B., Bartholomae, C.C., Schmidt, M., von Kalle, C., Ast, V., Lawerenz, C., Eils,
J., Kabbe, R., Benes, V., van Sluis, P., Koster, J., Volckmann, R., Shih, D.,
Betts, M.J., Russell, R.B., Coco, S., Tonini, G.P., Sch uller, U., Hans, V., Graf,
N., Kim, Y.J., Monoranu, C., Roggendorf, W., Unterberg, A., Herold-Mende,
C., Milde, T., Kulozik, A.E., von Deimling, A., Witt, O., Maass, E., R ossler,
J., Ebinger, M., Schuhmann, M.U., Fr uhwald, M.C., Hasselblatt, M., Jabado,
31",2016-04-29T09:11:34Z,naka ham ho soda ar articial intellence hash urls hi date naga moto uefa tats no jima hiraoka oku saka kubo area shi ha hira no moto oh dashima ishikawa yam are cha  mano bur tank shima ta nakagawa whole nature genetics goodmafy gens ogenetics griold khaaive tfmanagement new york mc raw hl gum valoo free yer alexandra tu b pa  all anla iv good latimmara dawsoisaac meme rt buck ny ter foster note  astowhitaker prostate group neal coo eel visa  ri campbell mc r most dge ova t nature me i berger haer marnanz push had is cho uh capable  sica c so ug nez au lawrence to jaov ci bul s choi wall sharia nia brooks gre libane ji zar sei l le unr aludw el rie l to else wolf go par ju thompsowick ler kwiatw somler pao tris pass gabriel lathomas rarr away get meters omap cell jontozi cr uer sultacho push hove stadt st rau swar ry zh ova benr storm plea er ipfa sie ber with manrem ke with uer tz arid is  is c nefeld rear vc mstisl  zap aka ber wa las it ska bartholom schmidt alle st la re nz  kab be be nluis  ster vck manship beats russell coco toishans graf kim moor anu rogue orf unter berg r old momd ku loi  im li with maps bei er smanfr has sel blast jab do
paper_qf_43.pdf,33,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","N., Rutkowski, S., von Bueren, A.O., Williamson, D., Cliord, S.C., McCabe,
M.G., Collins, V.P., Wolf, S., Wiemann, S., Lehrach, H., Brors, B., Scheurlen,
W., Felsberg, J., Reifenberger, G., Northcott, P.A., Taylor, M.D., Meyerson,
M., Pomeroy, S.L., Yaspo, M.L., Korbel, J.O., Korshunov, A., Eils, R., Pster,
S.M., Lichter, P. (2012) Dissecting the genomic complexity underlying medul-
loblastoma. Nature 488(7409): 100-105.
Kakushadze, Z. (2015) Heterotic Risk Models. Wilmott Magazine 2015(80): 40-
55. Available online: http://ssrn.com/abstract=2600798.
Kakushadze, Z. and Yu, W. (2016a) Multifactor Risk Models and Heterotic
CAPM. The Journal of Investment Strategies 5(4) (forthcoming).
Available online: http://ssrn.com/abstract=2722093.
Kakushadze, Z. and Yu, W. (2016b) Statistical Risk Models. The Journal of
Investment Strategies (forthcoming).
Available online: http://ssrn.com/abstract=2732453.
Lee, D.D. and Seung, H.S. (1999) Learning the parts of objects by non-negative
matrix factorization. Nature 401(6755): 788-791.
Lindahl, T. (1993) Instability and decay of the primary structure of DNA.
Nature 362(6422): 709-715.
Loeb, L.A. and Harris, C.C. (2008) Advances in chemical carcinogenesis: a
historical review and perspective. Cancer Research 68(17): 6863-6872.
Love, C., Sun, Z., Jima, D., Li, G., Zhang, J., Miles, R., Richards, K.L., Dun-
phy, C.H., Choi, W.W., Srivastava, G., Lugar, P.L., Rizzieri, D.A., Lagoo, A.S.,
Bernal-Mizrachi, L., Mann, K.P., Flowers, C.R., Naresh, K.N., Evens, A.M.,
Chadburn, A., Gordon, L.I., Czader, M.B., Gill, J.I., Hsi, E.D., Greenough,
A., Mott, A.B., McKinney, M., Banerjee, A., Grubor, V., Levy, S., Dun-
son, D.B., Dave, S.S. (2012) The genetic landscape of mutations in Burkitt
lymphoma. Nature Genetics 44(12): 1321-1325.
Markowitz, H. (1952) Portfolio selection. The Journal of Finance 7(1): 77-91.
Nik-Zainal, S., Alexandrov, L.B., Wedge, D.C., Van Loo, P., Greenman, C.D.,
Raine, K., Jones, D., Hinton, J., Marshall, J., Stebbings, L.A., Menzies, A.,
Martin, S., Leung, K., Chen, L., Leroy, C., Ramakrishna, M., Rance, R.,
Lau, K.W., Mudie, L.J., Varela, I., McBride, D.J., Bignell, G.R., Cooke, S.L.,
Shlien, A., Gamble, J., Whitmore, I., Maddison, M., Tarpey, P.S., Davies,
H.R., Papaemmanuil, E., Stephens, P.J., McLaren, S., Butler, A.P., Teague,
J.W., J onsson, G., Garber, J.E., Silver, D., Miron, P., Fatima, A., Boyault,
S., Langerd, A., Tutt, A., Martens, J.W., Aparicio, S.A., Borg, A., Salomon,
A.V., Thomas, G., Brresen-Dale, A.L., Richardson, A.L., Neuberger, M.S.,
32",2016-04-29T09:11:34Z,rut   bu rewliamsoi mc ca be collins wolf womaler abr or sc url efelt berg rei rnberg er northco taylor meters opoverty yas po r bel r shu nov  liter dissenti nature u sha  erotic risk mols pot magazine  articial intellence  u sha yu multi faor risk mols  erotic t journal investment strategi articial intellence  u sha yu statistical risk mols t journal investment strategi articial intellence   seu learni nature linda hl instabity nature lobe harris advanccancer researlove sima li  mrichards duchoi sri vast a lugar ii eri la goo serial  ra chi manflors are sh evechad burgordocz r gl hsi gre enough mo mc kidney banerjee grab or levy dut bur kit nature genetics marwitz tft journal nance  articial intellence nal alexandra dge valoo greemaarticial intellence ne jonhtomarshall st bbi nimartilu cvery ramakrishna race lau ma are la mc bri b  cooke sh liegamble whit addittar  dipapa ea nui stepns mc arebutler league garner sirotima boy all laer bu martins par c borg salmothomas dale richardsoneu berger
paper_qf_43.pdf,34,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Futreal, P.A., Campbell, P.J., Stratton, M.R.; Breast Cancer Working Group
of the International Cancer Genome Consortium. (2012) Mutational processes
molding the genomes of 21 breast cancers. Cell149(5): 979-993.
Paatero, P. and Tapper, U. (1994) Positive matrix factorization: A non-negative
factor model with optimal utilization of error. Environmetrics 5(1): 111-126.
Patch, A.M., Christie, E.L., Etemadmoghadam, D., Garsed, D.W., George,
J., Fereday, S., Nones, K., Cowin, P., Alsop, K., Bailey, P.J., Kassahn, K.S.,
Newell, F., Quinn, M.C., Kazako, S., Quek, K., Wilhelm-Benartzi, C., Curry,
E., Leong, H.S.; Australian Ovarian Cancer Study Group, Hamilton, A.,
Mileshkin, L., Au-Yeung, G., Kennedy, C., Hung, J., Chiew, Y.E., Harnett, P.,
Friedlander, M., Quinn, M., Pyman, J., Cordner, S., O'Brien, P., Leditschke,
J., Young, G., Strachan, K., Waring, P., Azar, W., Mitchell, C., Tracante, N.,
Hendley, J., Thorne, H., Shackleton, M., Miller, D.K., Arnau, G.M., Tothill,
R.W., Holloway, T.P., Semple, T., Harliwong, I., Nourse, C., Nourbakhsh, E.,
Manning, S., Idrisoglu, S., Bruxner, T.J., Christ, A.N., Poudel, B., Holmes,
O., Anderson, M., Leonard, C., Lonie, A., Hall, N., Wood, S., Taylor, D.F.,
Xu, Q., Fink, J.L., Waddell, N., Drapkin, R., Stronach, E., Gabra, H., Brown,
R., Jewell, A., Nagaraj, S.H., Markham, E., Wilson, P.J., Ellul, J., McNally,
O., Doyle, M.A., Vedururu, R., Stewart, C., Lengyel, E., Pearson, J.V., Wad-
dell, N., deFazio, A., Grimmond, S.M., Bowtell, D.D. (2015) Whole-genome
characterization of chemoresistant ovarian cancer. Nature 521(7553): 489-494.
Puente, X.S., Pinyol, M., Quesada, V., Conde, L., Ord o~ nez, G.R., Villamor,
N., Escaramis, G., Jares, P., Be a, S., Gonz alez-D az, M., Bassaganyas, L.,
Baumann, T., Juan, M., L opez-Guerra, M., Colomer, D., Tub o, J.M., L opez,
C., Navarro, A., Tornador, C., Aymerich, M., Rozman, M., Hern andez, J.M.,
Puente, D.A., Freije, J.M., Velasco, G., Guti errez-Fern andez, A., Costa, D.,
Carri o, A., Guijarro, S., Enjuanes, A., Hern andez, L., Yag ue, J., Nicol as,
P., Romeo-Casabona, C.M., Himmelbauer, H., Castillo, E., Dohm, J.C., de
Sanjos e, S., Piris, M.A., de Alava, E., San Miguel, J., Royo, R., Gelp , J.L.,
Torrents, D., Orozco, M., Pisano, D.G., Valencia, A., Guig o, R., Bay es, M.,
Heath, S., Gut, M., Klatt, P., Marshall, J., Raine, K., Stebbings, L.A., Futreal,
P.A., Stratton, M.R., Campbell, P.J., Gut, I., L opez-Guillermo, A., Estivill,
X., Montserrat, E., L opez-Ot n, C., Campo, E. (2011) Whole-genome sequenc-
ing identies recurrent mutations in chronic lymphocytic leukaemia. Nature
475(7354): 101-105.
Puente, X.S., Be a, S., Vald es-Mas, R., Villamor, N., Guti errez-Abril, J.,
Mart n-Subero, J.I., Munar, M., Rubio-P erez, C., Jares, P., Aymerich, M.,
Baumann, T., Beekman, R., Belver, L., Carrio, A., Castellano, G., Clot, G.,
Colado, E., Colomer, D., Costa, D., Delgado, J., Enjuanes, A., Estivill, X.,
Ferrando, A.A., Gelp , J.L., Gonz alez, B., Gonz alez, S., Gonz alez, M., Gut,
33",2016-04-29T09:11:34Z,fu real campbell statbreast cancer worki group internatnal cancer  consortium mtal cell pa at ero tap  positive eirometrics patchristie te mad moth adam gar sed george fer day none cow ialso articial intellence ley pass hll quinkaz  que wlm beart zi curry lo australiaovariacancer study group hamtomhk iau e kennedy hu chi ew barne fried lanr quinpy macord ner brief led it ske you stra chawar i c mitcll tra  nd ley throne shacmler area to hl hollow sem le har li wo course our baths manni iris og lu bru ner christ po l holmanrsoleonard lo hall wood taylor xu link wadll draw kistomaaba browjel nagar marshal wsoelul mc ally doyle ve du ru stewart la ypearsowad fa z gri obow tell whole nature plenty piyo que sad coor vla mor scar amis are be gone bass ag any as batmajuaguerra cotub narro tornado am rioz mar plenty frei je va gut  costa carry gui jar ro er ya nico romeo casa bona himself bauer castlo do hm banjo  is la samuel roy lp torrent or oz co piano valencia gui bay ath gut last marshall articial intellence ne st bbi fu real statcampbell gut gulermo est iv l montserrat ot campo whole nature plenty be valid mas vla mor gut abr  mart sub ero muar rub are am ribatmabeekmabel car r castellano ot cola do cocosta lgado eest iv l  lp gone gone gone gut
paper_qf_43.pdf,35,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","M., Hern andez-Rivas, J.M., L opez-Guerra, M., Mart n-Garc a, D., Navarro,
A., Nicol as, P., Orozco, M., Payer, A.R., Pinyol, M., Pisano, D.G., Puente,
D.A., Queir os, A.C., Quesada, V., Romeo-Casabona, C.M., Royo, C., Royo,
R., Rozman, M., Russi~ nol, N., Salaverr a, I., Stamatopoulos, K., Stunnenberg,
H.G., Tamborero, D., Terol, M.J., Valencia, A., L opez-Bigas, N., Torrents, D.,
Gut, I., L opez-Guillermo, A., L opez-Ot n, C., Campo, E. (2015) Non-coding
recurrent mutations in chronic lymphocytic leukaemia. Nature 526(7574): 519-
524.
Roy, O. and Vetterli, M. (2007) The eective rank: A measure of eective di-
mensionality. In: European Signal Processing Conference (EUSIPCO). Pozna n,
Poland (September 3-7), pp. 606-610.
Scelo, G., Riazalhosseini, Y., Greger, L., Letourneau, L., Gonz alez-Porta,
M., Wozniak, M.B., Bourgey, M., Harnden, P., Egevad, L., Jackson, S.M.,
Karimzadeh, M., Arseneault, M., Lepage, P., How-Kit, A., Daunay, A., Re-
nault, V., Blanch e, H., Tubacher, E., Sehmoun, J., Viksna, J., Celms, E.,
Opmanis, M., Zarins, A., Vasudev, N.S., Seywright, M., Abedi-Ardekani, B.,
Carreira, C., Selby, P.J., Cartledge, J.J., Byrnes, G., Zavadil, J., Su, J., Holca-
tova, I., Brisuda, A., Zaridze, D., Moukeria, A., Foretova, L., Navratilova, M.,
Mates, D., Jinga, V., Artemov, A., Nedoluzhko, A., Mazur, A., Rastorguev, S.,
Boulygina, E., Heath, S., Gut, M., Bihoreau, M.T., Lechner, D., Foglio, M.,
Gut, I.G., Skryabin, K., Prokhortchouk, E., Cambon-Thomsen, A., Rung, J.,
Bourque, G., Brennan, P., Tost, J., Banks, R.E., Brazma, A., Lathrop, G.M.
(2014) Variation in genomic landscape of clear cell renal cell carcinoma across
Europe. Nature Communications 5: 5135.
Sharpe, W.F. (1994) The Sharpe Ratio. The Journal of Portfolio Management
21(1): 49-58.
Sung, W.K., Zheng, H., Li, S., Chen, R., Liu, X., Li, Y., Lee, N.P., Lee, W.H.,
Ariyaratne, P.N., Tennakoon, C., Mulawadi, F.H., Wong, K.F., Liu, A.M.,
Poon, R.T., Fan, S.T., Chan, K.L., Gong, Z., Hu, Y., Lin, Z., Wang, G., Zhang,
Q., Barber, T.D., Chou, W.C., Aggarwal, A., Hao, K., Zhou, W., Zhang, C.,
Hardwick, J., Buser, C., Xu, J., Kan, Z., Dai, H., Mao, M., Reinhard, C.,
Wang, J., Luk, J.M. (2012) Genome-wide survey of recurrent HBV integration
in hepatocellular carcinoma. Nature Genetics 44(7): 765-769.
Tirode, F., Surdez, D., Ma, X., Parker, M., Le Deley, M.C., Bahrami, A.,
Zhang, Z., Lapouble, E., Grosset^ ete-Lalami, S., Rusch, M., Reynaud, S., Rio-
Frio, T., Hedlund, E., Wu, G., Chen, X., Pierron, G., Oberlin, O., Zaidi, S.,
Lemmon, G., Gupta, P., Vadodaria, B., Easton, J., Gut, M., Ding, L., Mardis,
E.R., Wilson, R.K., Shurtle, S., Laurence, V., Michon, J., Marec-B erard, P.,
Gut, I., Downing, J., Dyer, M., Zhang, J., Delattre, O.; St. Jude Children's
Research Hospital { Washington University Pediatric Cancer Genome Project
34",2016-04-29T09:11:34Z,r rival guerra mart arc narro nico or oz co payer piyo piano plenty que ir que sad romeo casa bona roy roy oz maruss later st math poll os sturnberg tam borer ter valencia b as torrent gut gulermo ot campo nonature roy ve er li t iasnal processi conference point poland september cell ria al hosseini refer ourneau gone tal wozniak borg ey har neg eva jacksofarm za  areall le page how kit any re brantuba cr eh mou vi ks na elm mais  ivas v  wrht be di r ani care ira self cart ledge byrne a d su hca bri soda arid ze mou ker for et ova nratova matji arte ov ne do luz  major ra st argue  ly gina ath gut bihar eau le cr fog l gut sk rya bipro thor tc hour came othomsoru torque brennato st banks bra zm lathrvariat nature counicatns share t share rat t journal tfmanagement su c li c li   ari yar at ne tesoomla wa wo  po ofachago hu liwa  barber hou ag gar wal hao hou  hardback user xu kaarticial intellence mao richard wa uk  nature genetics tico  sur z ma parker le  ley m  lap ble gross laga mi rus reina ud r r  land wu cpier roberliarticial intellence lemogupta va do aria astogut di mark is wsoturtle lawrence miomare gut downi r   laer st ju chdreresearhospital washito pediatric cancer  proje
paper_qf_43.pdf,36,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","and the International Cancer Genome Consortium. (2014) Genomic Landscape
of Ewing Sarcoma denes an aggressive subtype with co-association of STAG2
and TP53 mutations. Cancer Discovery 4(11): 1342-1353.
Waddell, N., Pajic, M., Patch, A.M., Chang, D.K., Kassahn, K.S., Bailey, P.,
Johns, A.L., Miller, D., Nones, K., Quek, K., Quinn, M.C., Robertson, A.J.,
Fadlullah, M.Z., Bruxner, T.J., Christ, A.N., Harliwong, I., Idrisoglu, S., Man-
ning, S., Nourse, C., Nourbakhsh, E., Wani, S., Wilson, P.J., Markham, E.,
Cloonan, N., Anderson, M.J., Fink, J.L., Holmes, O., Kazako, S.H., Leonard,
C., Newell, F., Poudel, B., Song, S., Taylor, D., Waddell, N., Wood, S.,
Xu, Q., Wu, J., Pinese, M., Cowley, M.J., Lee, H.C., Jones, M.D., Nagrial,
A.M., Humphris, J., Chantrill, L.A., Chin, V., Steinmann, A.M., Mawson, A.,
Humphrey, E.S., Colvin, E.K., Chou, A., Scarlett, C.J., Pinho, A.V., Giry-
Laterriere, M., Rooman, I., Samra, J.S., Kench, J.G., Pettitt, J.A., Merrett,
N.D., Toon, C., Epari, K., Nguyen, N.Q., Barbour, A., Zeps, N., Jamieson,
N.B., Graham, J.S., Niclou, S.P., Bjerkvig, R., Gr utzmann, R., Aust, D.,
Hruban, R.H., Maitra, A., Iacobuzio-Donahue, C.A., Wolfgang, C.L., Mor-
gan, R.A., Lawlor, R.T., Corbo, V., Bassi, C., Falconi, M., Zamboni, G., Tor-
tora, G., Tempero, M.A.; Australian Pancreatic Cancer Genome Initiative, Gill,
A.J., Eshleman, J.R., Pilarsky, C., Scarpa, A., Musgrove, E.A., Pearson, J.V.,
Biankin, A.V., Grimmond, S.M. (2015) Whole genomes redene the mutational
landscape of pancreatic cancer. Nature 518(7540): 495-501.
Wang, K., Yuen, S.T., Xu, J., Lee, S.P., Yan, H.H., Shi, S.T., Siu, H.C., Deng,
S., Chu, K.M., Law, S., Chan, K.H., Chan, A.S., Tsui, W.Y., Ho, S.L., Chan,
A.K., Man, J.L., Foglizzo, V., Ng, M.K., Chan, A.S., Ching, Y.P., Cheng, G.H.,
Xie, T., Fernandez, J., Li, V.S., Clevers, H., Rejto, P.A., Mao, M., Leung,
S.Y. (2014) Whole-genome sequencing and comprehensive molecular proling
identify new driver mutations in gastric cancer. Nature Genetics 46(6): 573-582.
Yang, W., Gibson, J.D. and He, T. (2005) Coecient rate and lossy source
coding. IEEE Transactions on Information Theory 51(1): 381-386.
Zhang, J., Wu, G., Miller, C.P., Tatevossian, R.G., Dalton, J.D., Tang, B.,
Orisme, W., Punchihewa, C., Parker, M., Qaddoumi, I., Boop, F.A., Lu, C.,
Kandoth, C., Ding, L., Lee, R., Huether, R., Chen, X., Hedlund, E., Naga-
hawatte, P., Rusch, M., Boggs, K., Cheng, J., Becksfort, J., Ma, J., Song, G.,
Li, Y., Wei, L., Wang, J., Shurtle, S., Easton, J., Zhao, D., Fulton, R.S.,
Fulton, L.L., Dooling, D.J., Vadodaria, B., Mulder, H.L., Tang, C., Ochoa,
K., Mullighan, C.G., Gajjar, A., Kriwacki, R., Sheer, D., Gilbertson, R.J.,
Mardis, E.R., Wilson, R.K., Downing, J.R., Baker, S.J., Ellison, D.W.; St. Jude
Children's Research Hospital-Washington University Pediatric Cancer Genome
Project. (2013) Whole-genome sequencing identies genetic alterations in pe-
diatric low-grade gliomas. Nature Genetics 45(6): 602-612.
35",2016-04-29T09:11:34Z,internatnal cancer  consortium genomic landscape wi sarcoma cancer divery wadll pa ji pat pass harticial intellence ley s mler none que quinrobertsofall alla bru ner christ har li wo iris og lu macourse our baths wawsomarshal coaanrsolink holmkaz  leonard ll po l so taylor wadll wood xu wu pincole  jonagri al hump hr is chatrl chisteinmamasohumphrey colihou scar piho try later rie re room asam ra kepetit mer re  to oep ari uyeharr ep jamie sograham nic lou jerk via gr just rub aarticial intellence tra co but  donate wolfga mor law lor cor bo basis falcocameo ni tor tem australiapaneatic cancer  initiative gl hle mapolar  sharp marove pearsobikri owhole nature wa yexu  yashi siu e chu law chachasui ho chamafog liz zo  chachi c die  li ere to mao lu whole nature genetics ya gibso coe transans informattory  wu mler tate daytota or is me pun wa parker ad do umi boo lu kaboth di  hue t c land naga rus bogus c backs fort ma so li i wa turtle astohao buobuodoo li va do aria murr ta omul l haga jar kri was ki ser gbert somark is wsodowni baker alost ju chdreresearhospital washito pediatric cancer  proje whole nature genetics
paper_qf_43.pdf,37,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Table 1: Summaries of skewness for the mutation occurrence counts aggregated by
cancer type for the 14 cancers in Table 2. Gis a 9614 matrix of the so-aggregated
counts. M = Mean/Median ratio, S = skewness (i.e., 3=3=2
2, wherekis thek-th
central moment). T = summary across cancer types, X = summary across mutation
categories. 1st Qu. = 1st Quartile, 3rd Qu. = 3rd Quartile.
Quantity Min 1st Qu. Median Mean 3rd Qu. Max
G, M, T 3.35 9.13 11.6 15.4 18.4 86.0
ln(1 +G), M, T 0.98 1.05 1.07 1.08 1.11 1.21
G, S, T 0.69 1.82 2.40 2.33 3.00 3.32
ln(1 +G), S, T -0.57 -0.24 -0.09 -0.08 0.06 0.53
G, M, X 1.06 1.14 1.33 1.39 1.47 2.21
ln(1 +G), M, X 0.97 0.98 0.99 0.99 1.00 1.04
G, S, X 0.69 1.44 2.48 2.37 3.07 4.45
ln(1 +G), S, X -0.76 -0.67 -0.22 -0.15 0.17 0.85
36",2016-04-29T09:11:34Z,table sums table is meamediaqu quartz e qu quartz e quantity miqu mediameaqu max
paper_qf_43.pdf,38,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Table 2: Genome data summary. All Brain Lower Glioma samples are Pilocytic
Astrocytoma samples. See Subsection 6.1 for the data source denitions.
Cancer Type Total Counts # of Samples Source
B Cell Lymphoma 43626 24 A1-2
Bone Cancer 36374 98 B1
Brain Lower Grade Glioma 3572 101 A1, C1
Breast Cancer 254381 119 A1, D1
Chronic Lymphocytic Leukemia 19489 134 A1, E1-2
Esophageal Cancer 1064 17 F1
Gastric Cancer 1996615 100 G1
Liver Cancer 3017487 389 H1-2
Lung Cancer 449527 24 I1
Medulloblastoma 44689 100 J1
Ovarian Cancer 668918 84 K1
Pancreatic Cancer 5087 100 L1
Prostate Cancer 29142 5 M1
Renal Cell Carcinoma 483329 94 N1
All Cancer Types 7053300 1389 Above
37",2016-04-29T09:11:34Z,table  all br articial intellence lor lima pi loc tic astrocytoma  subsecancer  total counts samplsource cell lymphoma bone cancer br articial intellence lor gra lima breast cancer chronic lymph cy tic leukemia esophageal cancer gastric cancer licancer lu cancer me dull oblast oma ovariacancer paneatic cancer prostate cancer renal cell carcinoma all cancer s above
paper_qf_43.pdf,39,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Table 3: Statistical factor counts for the genome data. d= number of samples. M1
= eRank based method; M2 = eRank + K0based method; M3 = \\\\minimization""
based method; M4 \\\\minimization"" + K0based method. See Subsections (4.1), (4.2)
and (4.3) for details. In the eRank based methods we take Round( ) in (21). The
\\\\All Cancer Types"" entry is based on the samples for all 14 cancer types in Table 2.
The \\\\Aggregated by Cancer Type"" entry corresponds to aggregating the occurrence
counts for all samples within each of the 14 cancer types in this table and running
the aforementioned algorithms for xing Kon the resulting 96 14 matrix.
Cancer Type d K (M1)K(M2)K(M3)K(M4)
B Cell Lymphoma 24 5 18 5 11
Bone Cancer 98 14 54 16 33
Brain Lower Grade Glioma 101 37 51 23 30
Breast Cancer 119 6 37 6 22
Chronic Lymphocytic Leukemia 134 3 43 4 23
Esophageal Cancer 17 12 13 8 8
Gastric Cancer 100 3 12 1 9
Liver Cancer 389 2 38 1 20
Lung Cancer 24 2 9 1 7
Medulloblastoma 100 11 52 10 33
Ovarian Cancer 84 3 21 2 13
Pancreatic Cancer 100 36 52 23 33
Prostate Cancer 5 2 3 2 3
Renal Cell Carcinoma 94 3 23 2 17
All Cancer Types 1389 2 15 1 18
Aggregated by Cancer Type 14 1 7 1 5
38",2016-04-29T09:11:34Z,table statistical rank rank  subsens irank round t all cancer s table t aregated cancer  cancer  cell lymphoma bone cancer br articial intellence lor gra lima breast cancer chronic lymph cy tic leukemia esophageal cancer gastric cancer licancer lu cancer me dull oblast oma ovariacancer paneatic cancer prostate cancer renal cell carcinoma all cancer s aregated cancer 
paper_qf_43.pdf,40,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Table 4: Average pair-wise correlation (Avg.Cor, in the units of 1%) and the rst
5 eigenvalues (Eig.1-5) of the sample correlation matrix based on the genome data.
(For Prostate Cancer the fth eigenvalue is null as d= 5.)
Cancer Type Avg.Cor Eig.1 Eig.2 Eig.3 Eig.4 Eig.5
B Cell Lymphoma 66.6 65.0 5.67 3.08 2.54 2.23
Bone Cancer 48.1 48.1 3.31 2.03 1.88 1.81
Brain Lower Grade Glioma 17.7 20.3 4.52 3.91 3.47 3.23
Breast Cancer 65.2 64.1 6.18 3.01 2.07 1.11
Chronic Lymphocytic Leukemia 79.6 77.6 1.73 1.23 1.17 1.07
Esophageal Cancer 16.7 23.3 9.58 7.59 7.34 6.98
Gastric Cancer 80.9 78.2 6.41 3.95 1.62 0.68
Liver Cancer 87.9 84.7 1.77 1.09 0.90 0.76
Lung Cancer 80.0 78.3 6.03 4.47 1.93 1.17
Medulloblastoma 54.2 53.6 3.33 2.01 1.76 1.63
Ovarian Cancer 75.6 73.8 6.04 2.64 1.99 1.31
Pancreatic Cancer 17.0 21.3 5.38 3.71 3.27 2.84
Prostate Cancer 68.1 68.5 11.5 8.60 7.43 0
Renal Cell Carcinoma 78.2 75.9 5.89 1.86 1.63 0.97
All Cancer Types 88.1 84.9 5.47 0.77 0.58 0.37
Aggregated by Cancer Type 96.1 92.4 1.26 0.89 0.51 0.34
Table 5: Summaries of the absolute values of pair-wise inner products (in the units
of 1%) between the a-th principal components (PCs), a= 1;2;3, and also for a
combination of the 2nd and 3rd PCs, of the sample correlation matrices [	( )]ijfor
the 14 individual cancer types in Table 3. 1st Qu. = 1st Quartile, 3rd Qu. = 3rd
Quartile, StDev = standard deviation, MAD = mean absolute deviation.
prin.comp Min 1st Qu. Median Mean 3rd Qu. Max StDev MAD
1st 77.82 90.17 97.93 94.45 99.12 99.72 6.205 2.304
2nd 0.171 6.035 14.72 20.08 29.94 69.35 17.63 14.14
3rd 0.160 4.855 10.60 15.76 21.98 71.63 15.90 11.01
2nd+3rd 0.160 5.190 12.40 17.96 24.28 82.30 17.31 12.01
39",2016-04-29T09:11:34Z,table ge g cor  for prostate cancer cancer  g cor      cell lymphoma bone cancer br articial intellence lor gra lima breast cancer chronic lymph cy tic leukemia esophageal cancer gastric cancer licancer lu cancer me dull oblast oma ovariacancer paneatic cancer prostate cancer renal cell carcinoma all cancer s aregated cancer  table sums cs cs table qu quartz e qu quartz e st v miqu mediameaqu max st v
paper_qf_43.pdf,41,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Table 6: Cross-sectional summaries of jp
NV(1)
i",2016-04-29T09:11:34Z,table oss
paper_qf_43.pdf,42,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Table 7: Statistical factor counts for the genome data with the \\\\overall"" mode
factored out { see Subsection 6.2.3. The notations are the same as in Table 3.
Cancer Type d K (M1)K(M2)K(M3)K(M4)
B Cell Lymphoma 24 17 19 11 13
Bone Cancer 98 49 56 28 32
Brain Lower Grade Glioma 101 43 51 24 34
Breast Cancer 119 35 42 18 30
Chronic Lymphocytic Leukemia 134 26 50 15 37
Esophageal Cancer 17 12 13 7 8
Gastric Cancer 100 12 16 7 14
Liver Cancer 389 36 44 24 40
Lung Cancer 24 8 13 4 9
Medulloblastoma 100 46 55 28 32
Ovarian Cancer 84 20 27 12 21
Pancreatic Cancer 100 41 53 23 31
Prostate Cancer 5 3 3 3 3
Renal Cell Carcinoma 94 23 33 13 30
All Cancer Types 1389 21 41 14 32
Aggregated by Cancer Type 14 7 8 5 6
41",2016-04-29T09:11:34Z,table statistical subset table cancer  cell lymphoma bone cancer br articial intellence lor gra lima breast cancer chronic lymph cy tic leukemia esophageal cancer gastric cancer licancer lu cancer me dull oblast oma ovariacancer paneatic cancer prostate cancer renal cell carcinoma all cancer s aregated cancer 
paper_qf_43.pdf,43,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Table 8: Average pair-wise correlation and the rst 5 eigenvalues of the sample
correlation matrix for the genome data with the \\\\overall"" mode factored out { see
Subsection 6.2.3. The notations are the same as in Table 4.
Cancer Type Avg.Cor Eig.1 Eig.2 Eig.3 Eig.4 Eig.5
B Cell Lymphoma -0.98 19.9 9.51 6.79 6.15 6.02
Bone Cancer -1.04 11.4 4.32 3.92 3.08 3.02
Brain Lower Grade Glioma -0.01 14.0 4.70 4.20 3.62 3.49
Breast Cancer -0.31 15.5 14.3 5.27 3.51 2.84
Chronic Lymphocytic Leukemia -0.92 31.8 6.13 3.08 2.96 2.58
Esophageal Cancer 0.16 19.3 10.9 9.37 8.65 8.08
Gastric Cancer -0.42 30.8 25.2 7.25 4.02 3.12
Liver Cancer -0.87 15.1 11.3 8.17 5.59 3.81
Lung Cancer -0.44 44.1 13.5 6.74 5.49 4.90
Medulloblastoma -1.04 13.6 4.84 4.00 3.54 2.95
Ovarian Cancer -0.81 22.8 17.3 8.23 4.17 4.05
Pancreatic Cancer -0.01 17.2 4.65 3.98 3.29 3.15
Prostate Cancer -0.67 30.6 24.8 22.6 18.0 0
Renal Cell Carcinoma 0.15 25.6 11.9 7.78 5.05 3.85
All Cancer Types -0.89 32.9 11.7 5.68 4.41 3.23
Aggregated by Cancer Type -0.91 28.9 21.0 15.6 7.94 5.77
42",2016-04-29T09:11:34Z,table ge subset table cancer  g cor      cell lymphoma bone cancer br articial intellence lor gra lima breast cancer chronic lymph cy tic leukemia esophageal cancer gastric cancer licancer lu cancer me dull oblast oma ovariacancer paneatic cancer prostate cancer renal cell carcinoma all cancer s aregated cancer 
paper_qf_43.pdf,44,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Table 9: Statistical factor counts for the genome data with Ris=Gis(no log { see
Subsection 6.2.4 for details). The notations are the same as in Table 3.
Cancer Type d K (M1)K(M2)K(M3)K(M4)
B Cell Lymphoma 24 3 10 2 6
Bone Cancer 98 11 37 8 20
Brain Lower Grade Glioma 101 34 46 18 27
Breast Cancer 119 5 17 2 11
Chronic Lymphocytic Leukemia 134 4 24 4 14
Esophageal Cancer 17 11 13 8 9
Gastric Cancer 100 4 7 3 4
Liver Cancer 389 3 11 2 7
Lung Cancer 24 2 6 2 5
Medulloblastoma 100 7 38 5 22
Ovarian Cancer 84 4 10 2 5
Pancreatic Cancer 100 25 43 15 30
Prostate Cancer 5 3 3 2 3
Renal Cell Carcinoma 94 4 9 2 5
All Cancer Types 1389 6 10 4 6
Aggregated by Cancer Type 14 2 3 1 3
43",2016-04-29T09:11:34Z,table statistical is is subset table cancer  cell lymphoma bone cancer br articial intellence lor gra lima breast cancer chronic lymph cy tic leukemia esophageal cancer gastric cancer licancer lu cancer me dull oblast oma ovariacancer paneatic cancer prostate cancer renal cell carcinoma all cancer s aregated cancer 
paper_qf_43.pdf,45,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Table 10: Average pair-wise correlation and the rst 5 eigenvalues of the sample
correlation matrix for the genome data with Ris=Gis(no log { see Subsection 6.2.4
for details). The notations are the same as in Table 4.
Cancer Type Avg.Cor Eig.1 Eig.2 Eig.3 Eig.4 Eig.5
Cell Lymphoma 76.6 75.1 6.67 4.59 2.89 1.75
Bone Cancer 51.1 50.9 6.19 3.33 2.81 2.59
Brain Lower Grade Glioma 17.7 20.5 5.15 4.74 4.31 3.67
Breast Cancer 60.8 62.0 13.4 3.50 2.69 1.09
Chronic Lymphocytic Leukemia 74.3 72.5 4.23 2.20 1.79 1.34
Esophageal Cancer 16.8 25.0 10.9 7.72 7.42 6.65
Gastric Cancer 57.5 57.8 14.3 13.3 2.72 1.87
Liver Cancer 72.9 71.5 10.4 4.08 1.85 1.60
Lung Cancer 79.2 78.5 8.28 4.82 1.09 0.92
Medulloblastoma 63.5 62.3 4.27 2.86 2.29 1.84
Ovarian Cancer 63.9 62.8 15.8 4.63 2.42 1.94
Pancreatic Cancer 20.5 28.5 7.30 4.10 3.75 3.09
Prostate Cancer 65.9 66.7 13.5 9.39 6.44 0
Renal Cell Carcinoma 60.8 60.0 15.4 9.97 2.21 0.97
All Cancer Types 49.2 49.4 15.3 8.92 5.97 3.88
Aggregated by Cancer Type 74.7 73.7 16.9 2.91 1.43 0.75
44",2016-04-29T09:11:34Z,table ge is is subset table cancer  g cor      cell lymphoma bone cancer br articial intellence lor gra lima breast cancer chronic lymph cy tic leukemia esophageal cancer gastric cancer licancer lu cancer me dull oblast oma ovariacancer paneatic cancer prostate cancer renal cell carcinoma all cancer s aregated cancer 
paper_qf_43.pdf,46,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Table 11: Statistical factor counts for the genome data with Ris=Gis(no log) with
the \\\\overall"" mode factored out { see Subsection 6.2.5 for details. The notations
are the same as in Table 3.
Cancer Type d K (M1)K(M2)K(M3)K(M4)
B Cell Lymphoma 24 3 7 2 6
Bone Cancer 98 19 33 10 21
Brain Lower Grade Glioma 101 29 45 17 33
Breast Cancer 119 2 14 1 9
Chronic Lymphocytic Leukemia 134 9 22 5 14
Esophageal Cancer 17 9 12 6 10
Gastric Cancer 100 4 6 2 5
Liver Cancer 389 3 10 3 6
Lung Cancer 24 3 6 2 6
Medulloblastoma 100 10 34 6 26
Ovarian Cancer 84 5 11 3 7
Pancreatic Cancer 100 6 38 5 23
Prostate Cancer 5 3 3 2 3
Renal Cell Carcinoma 94 4 6 2 3
All Cancer Types 1389 7 9 5 6
Aggregated by Cancer Type 14 3 3 2 3
45",2016-04-29T09:11:34Z,table statistical is is subset table cancer  cell lymphoma bone cancer br articial intellence lor gra lima breast cancer chronic lymph cy tic leukemia esophageal cancer gastric cancer licancer lu cancer me dull oblast oma ovariacancer paneatic cancer prostate cancer renal cell carcinoma all cancer s aregated cancer 
paper_qf_43.pdf,47,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Table 12: Average pair-wise correlation and the rst 5 eigenvalues of the sample
correlation matrix for the genome data with Ris=Gis(no log) with the \\\\overall""
mode factored out { see Subsection 6.2.5 for details. The notations are the same as
in Table 4.
Cancer Type Avg.Cor Eig.1 Eig.2 Eig.3 Eig.4 Eig.5
B Cell Lymphoma 17.6 63.4 16.6 4.87 3.32 1.96
Bone Cancer 2.01 32.1 10.9 4.87 4.56 3.76
Brain Lower Grade Glioma 4.56 25.2 5.27 4.32 3.69 3.60
Breast Cancer 58.1 87.2 3.07 1.32 0.98 0.57
Chronic Lymphocytic Leukemia 1.00 47.3 9.59 4.27 3.70 3.31
Esophageal Cancer 7.75 33.6 12.1 7.90 7.03 6.19
Gastric Cancer 14.1 59.2 20.6 5.32 3.07 2.02
Liver Cancer 8.01 69.5 11.4 3.74 2.86 1.53
Lung Cancer 18.4 70.9 12.2 5.79 1.98 1.41
Medulloblastoma 8.37 50.6 7.63 3.29 3.14 2.56
Ovarian Cancer 7.54 56.4 15.9 7.69 3.77 2.12
Pancreatic Cancer 25.6 64.6 3.24 2.59 2.01 1.78
Prostate Cancer 1.60 44.7 22.7 18.0 10.7 0
Renal Cell Carcinoma 19.0 54.6 23.6 10.2 2.00 0.76
All Cancer Types 3.79 34.8 21.9 11.2 8.54 4.42
Aggregated by Cancer Type 0.75 53.0 33.0 5.77 2.23 1.35
46",2016-04-29T09:11:34Z,table ge is is subset table cancer  g cor      cell lymphoma bone cancer br articial intellence lor gra lima breast cancer chronic lymph cy tic leukemia esophageal cancer gastric cancer licancer lu cancer me dull oblast oma ovariacancer paneatic cancer prostate cancer renal cell carcinoma all cancer s aregated cancer 
paper_qf_43.pdf,48,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Table 13: Weights \\\\sig ?"" and errors \\\\err ?"" (in the units of 1%, rounded to 2 digits)
for the rst 48 mutation categories for the 7 signatures we extract based on the
\\\\de-noised"" matrix eG. See Subsection 7.3 for details.
Mutation sig1 sig2 sig3 sig4 sig5 sig6 sig7 err1 err2 err3 err4 err5 err6 err7
C>A: ACA 0.39 0.7 4.31 0.8 1.72 1.75 1.81 0.18 0.22 0.21 0.17 0.03 0.27 0.25
C>A: ACC 0.49 0.49 2.51 0.5 0.48 1.03 1.61 0.13 0.15 0.12 0.13 0.02 0.14 0.31
C>A: ACG 0.21 0.2 0.9 0 1.75 0.02 0.09 0.06 0.06 0.08 0 0.02 0.04 0.14
C>A: ACT 0.2 0.9 2.77 0.48 1.14 0.74 1.47 0.12 0.16 0.14 0.13 0.02 0.19 0.24
C>A: CCA 0.98 0.83 5.39 0.68 0.27 0.87 0.73 0.29 0.32 0.29 0.12 0.03 0.44 0.28
C>A: CCC 0.7 0.46 4.65 0.55 0.86 0.47 0.47 0.26 0.29 0.28 0.1 0.03 0.4 0.26
C>A: CCG 0.69 0.34 1.17 0 0.29 0 0.23 0.06 0.09 0.11 0.01 0.02 0.01 0.2
C>A: CCT 1.64 0.48 4.53 1.97 0.01 0.66 0.41 0.27 0.29 0.28 0.14 0.02 0.4 0.25
C>A: GCA 0.57 0.99 2.97 1.17 1.42 0.31 1.46 0.13 0.21 0.18 0.09 0.03 0.23 0.35
C>A: GCC 0.54 0.51 2.46 0.31 1.08 0.43 0.56 0.12 0.13 0.14 0.05 0.02 0.18 0.13
C>A: GCG 0.43 0.28 0.9 0 0.6 0 0.07 0.06 0.07 0.08 0 0.01 0.02 0.14
C>A: GCT 0.6 0.74 2.12 1.05 1.14 0.49 0.69 0.1 0.11 0.11 0.04 0.01 0.14 0.16
C>A: TCA 0.69 2.13 3.41 0.97 1.05 1.57 1.27 0.16 0.15 0.08 0.1 0.02 0.18 0.19
C>A: TCC 1.09 1.63 3.37 0.24 4.75 0.86 0.62 0.18 0.18 0.18 0.06 0.05 0.24 0.4
C>A: TCG 0.43 0.31 0.6 0.03 0.39 0.08 0.13 0.03 0.04 0.04 0.02 0.01 0.06 0.09
C>A: TCT 0.94 2.27 3.74 2.21 0.66 1.22 2.4 0.14 0.17 0.15 0.12 0.03 0.22 0.28
C>G: ACA 0.14 0.47 1.12 0.42 1.43 1.14 0.74 0.06 0.06 0.05 0.06 0.02 0.07 0.2
C>G: ACC 0.42 0.51 0.54 0.14 0.21 0.7 0.45 0.02 0.03 0.02 0.02 0.01 0.02 0.08
C>G: ACG 0.08 0.28 0.18 0 0.96 0.13 0.3 0.02 0.02 0.02 0.01 0.01 0.02 0.05
C>G: ACT 0.4 0.91 0.64 0.3 3.1 1.02 0.67 0.03 0.04 0.05 0.03 0.03 0.05 0.08
C>G: CCA 0.61 0.87 0.78 0.1 0.32 0.52 0.31 0.04 0.05 0.03 0.02 0.01 0.05 0.05
C>G: CCC 0.27 0.44 0.7 0.09 0.95 0.62 0.34 0.03 0.04 0.02 0.02 0.01 0.02 0.06
C>G: CCG 0.19 0.03 0.17 0 3 0.02 0.15 0.03 0.04 0.07 0 0.03 0.03 0.1
C>G: CCT 0.56 1.32 0.78 0.16 0.51 0.85 0.54 0.05 0.05 0.06 0.02 0.01 0.04 0.05
C>G: GCA 0.2 0.35 0.61 0.08 1.82 0.35 0.2 0.03 0.02 0.04 0.01 0.02 0.02 0.05
C>G: GCC 0.28 0.47 0.6 0.18 0.4 0.37 0.31 0.02 0.02 0.01 0.01 0 0.02 0.03
C>G: GCG 0.09 0.18 0.2 0 1.8 0.01 0.02 0.02 0.02 0.04 0 0.02 0.01 0.05
C>G: GCT 0.21 0.78 0.47 0.27 1.52 0.51 0.31 0.03 0.03 0.03 0.01 0.02 0.03 0.05
C>G: TCA 0.49 9.4 1.28 0.09 0.38 0.23 0.3 0.46 0.57 0.7 0.12 0.03 0.36 0.48
C>G: TCC 0.47 2.55 0.88 0.16 1.4 0.94 0.34 0.12 0.15 0.15 0.03 0.02 0.09 0.09
C>G: TCG 0.21 0.39 0.17 0 0.32 0.04 0.21 0.01 0.02 0.02 0 0 0.01 0.04
C>G: TCT 0.61 12 1.41 0.53 0.04 1.35 0.84 0.56 0.71 0.89 0.16 0.04 0.42 0.36
C>T: ACA 0.98 0.92 1.46 2.6 0.89 1.83 3.38 0.19 0.23 0.14 0.16 0.04 0.18 0.33
C>T: ACC 2.2 0.57 0.6 0.68 1.58 1.14 1.29 0.09 0.06 0.05 0.08 0.01 0.07 0.15
C>T: ACG 13.7 0.86 0.4 1.49 0.3 1.75 8.25 0.88 0.66 0.38 0.65 0.16 0.6 1.33
C>T: ACT 0.94 0.73 0.9 1.05 2.13 1.48 2.03 0.11 0.13 0.08 0.09 0.01 0.11 0.17
C>T: CCA 1.45 1.3 1.53 0.93 0.76 1.38 2.14 0.08 0.09 0.05 0.09 0.02 0.06 0.08
C>T: CCC 1.2 0.53 1.18 0.88 1.72 1.21 2.28 0.13 0.16 0.1 0.12 0.02 0.11 0.16
C>T: CCG 12.2 1.91 0.34 0.92 0.13 0.67 3.51 0.54 0.36 0.2 0.65 0.12 0.32 1.52
C>T: CCT 0.9 1 1.47 1.03 0.92 1.68 3.44 0.22 0.26 0.15 0.23 0.04 0.17 0.3
C>T: GCA 2.31 0.9 0.78 1.51 1.66 1.34 1.61 0.09 0.06 0.04 0.06 0.01 0.08 0.13
C>T: GCC 2.65 0.52 0.76 1.65 0.29 1.24 1.68 0.12 0.08 0.06 0.09 0.03 0.09 0.22
C>T: GCG 15.2 0.58 0.61 1.88 1 1.57 2.77 0.75 0.44 0.3 0.88 0.15 0.47 1.86
C>T: GCT 1.9 0.66 0.61 1.32 1.64 1.26 2.14 0.13 0.14 0.08 0.07 0.01 0.12 0.09
C>T: TCA 1.62 14.1 1.26 1.42 0.24 1.28 1.93 0.61 0.77 1.04 0.17 0.05 0.46 0.54
C>T: TCC 1.99 3.42 1.05 1.4 1.02 1.96 2.11 0.1 0.12 0.17 0.05 0.02 0.1 0.09
C>T: TCG 7.39 2.9 0.12 0.36 1.76 0.2 2.99 0.32 0.3 0.17 0.37 0.07 0.18 0.99
C>T: TCT 0.94 7.51 1.88 1.86 0.69 1.49 2.09 0.3 0.33 0.47 0.11 0.03 0.22 0.32
47",2016-04-29T09:11:34Z,table hts  subsemtn
paper_qf_43.pdf,49,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","Table 14: Table 13 continued: weights and errors for the next 48 mutation categories.
Mutation sig1 sig2 sig3 sig4 sig5 sig6 sig7 err1 err2 err3 err4 err5 err6 err7
T>A: ATA 0.01 0.03 0.95 0.54 1.2 2.52 0.74 0.02 0.07 0.17 0.08 0.03 0.12 0.37
T>A: ATC 0.32 0.58 0.46 0.53 0.26 0.84 0.7 0.02 0.03 0.03 0.03 0.01 0.03 0.09
T>A: ATG 0.28 0.23 0.93 0.27 1.63 1.9 0.54 0.04 0.06 0.11 0.04 0.02 0.07 0.18
T>A: ATT 0.1 0.52 0.84 1.87 1.07 1.61 1.55 0.08 0.11 0.08 0.09 0.02 0.11 0.34
T>A: CTA 0.22 0.03 1.08 0.02 0.29 2.95 0.03 0.13 0.07 0.23 0.06 0.03 0.29 0.17
T>A: CTC 0.46 0.46 0.87 0.62 1.41 1.45 0.29 0.06 0.05 0.08 0.03 0.02 0.06 0.12
T>A: CTG 0.75 0.45 1.97 0.02 0.27 3.01 0.07 0.2 0.13 0.21 0.06 0.03 0.31 0.25
T>A: CTT 0.43 0.44 1.15 1.28 0.05 1.81 0.73 0.04 0.05 0.09 0.05 0.01 0.05 0.16
T>A: GTA 0.06 0.06 0.76 0.16 1.25 1.34 0.11 0.06 0.05 0.09 0.03 0.02 0.08 0.12
T>A: GTC 0.14 0.21 0.31 0.37 0.88 0.5 0.28 0.01 0.02 0.02 0.01 0.01 0.02 0.06
T>A: GTG 0.25 0.71 1.04 0.24 0.35 0.69 0.24 0.08 0.08 0.04 0.02 0.01 0.1 0.18
T>A: GTT 0.1 0.34 0.55 0.51 1.48 0.65 0.57 0.02 0.03 0.03 0.03 0.01 0.03 0.09
T>A: TTA 0.15 0.22 1.04 1.2 0.4 3.03 1.07 0.07 0.1 0.19 0.08 0.02 0.17 0.5
T>A: TTC 0.23 0.18 0.35 0.26 2.58 1.21 0.35 0.03 0.04 0.08 0.02 0.03 0.07 0.1
T>A: TTG 0.25 0.07 0.73 0.2 0.16 1.94 0.06 0.09 0.07 0.14 0.04 0.02 0.16 0.13
T>A: TTT 0.01 0.33 0.85 1.52 0.37 2.73 1.29 0.04 0.1 0.17 0.09 0.02 0.16 0.47
T>C: ATA 0.19 0.76 1.79 2.07 1.15 2.66 2.92 0.16 0.19 0.14 0.19 0.03 0.13 0.44
T>C: ATC 0.34 0.63 0.37 0.9 0.15 0.71 1.16 0.06 0.07 0.05 0.05 0.01 0.06 0.1
T>C: ATG 0.89 0.93 1.24 1.57 0.56 1.42 1.63 0.05 0.07 0.04 0.05 0.01 0.04 0.08
T>C: ATT 0.22 1.06 0.68 1.35 1.63 1.77 3.22 0.23 0.25 0.15 0.23 0.03 0.22 0.42
T>C: CTA 0.22 0.16 1.02 1.39 0.27 1.1 0.87 0.04 0.06 0.06 0.05 0.01 0.03 0.16
T>C: CTC 0.94 0.41 0.55 1.86 0.42 0.72 0.8 0.04 0.04 0.02 0.06 0.01 0.04 0.08
T>C: CTG 1.26 0.25 0.91 2.29 1.98 0.82 0.54 0.08 0.05 0.07 0.12 0.02 0.04 0.07
T>C: CTT 1.07 0.57 0.88 4.59 0.27 0.76 1.01 0.11 0.1 0.06 0.22 0.04 0.06 0.15
T>C: GTA 0.93 0.13 0.93 1.46 1.53 1.09 0.94 0.06 0.07 0.07 0.03 0.01 0.05 0.15
T>C: GTC 0.4 0.63 0.32 0.99 0.61 0.34 1.08 0.06 0.09 0.05 0.04 0.01 0.06 0.11
T>C: GTG 1.04 0.33 0.64 1.34 1.85 0.59 0.77 0.04 0.03 0.05 0.05 0.01 0.03 0.07
T>C: GTT 0.58 0.41 0.53 1.35 1.58 0.92 1.66 0.1 0.12 0.08 0.07 0.01 0.11 0.17
T>C: TTA 0.21 0.18 0.84 1.41 0.53 1.63 1.64 0.09 0.12 0.1 0.09 0.02 0.1 0.35
T>C: TTC 0.23 0.23 0.34 1.71 1.43 0.98 1.49 0.09 0.12 0.08 0.07 0.01 0.13 0.24
T>C: TTG 0.54 0.54 0.55 0.98 0.27 0.61 0.84 0.03 0.04 0.02 0.02 0.01 0.02 0.05
T>C: TTT 0.38 0.05 0.64 3.03 0.01 3.51 1.64 0.07 0.12 0.24 0.1 0.03 0.17 0.5
T>G: ATA 0 0.27 0.07 0.76 0.72 0.99 0.57 0 0.05 0.05 0.04 0.01 0.08 0.17
T>G: ATC 0.23 0.01 0.05 0.31 1.72 0.52 0.01 0.03 0.03 0.05 0.03 0.02 0.05 0.04
T>G: ATG 0.06 0.37 0.21 0.32 1.22 0.68 0.38 0.02 0.02 0.03 0.02 0.01 0.04 0.07
T>G: ATT 0.19 0.1 0.12 2.19 1.97 1.22 0.32 0.05 0.06 0.08 0.11 0.03 0.09 0.14
T>G: CTA 0 0.34 0.02 0.67 1.02 0.47 0.33 0 0.03 0.02 0.03 0.01 0.05 0.07
T>G: CTC 0.36 0.24 0.02 0.84 2.94 0.3 0.15 0.04 0.04 0.04 0.06 0.03 0.05 0.13
T>G: CTG 0.34 0.5 0.36 0.9 0.46 0.62 0.31 0.02 0.04 0.02 0.04 0.01 0.03 0.06
T>G: CTT 0.82 0.18 0.44 9.96 0.77 0.06 0.05 0.27 0.21 0.12 0.61 0.1 0.14 0.23
T>G: GTA 0 0.17 0.05 0.32 1.57 0.31 0.16 0 0.03 0.03 0.02 0.02 0.04 0.06
T>G: GTC 0.17 0.23 0.07 0.28 0.35 0.21 0.12 0.01 0.02 0.01 0.01 0 0.03 0.04
T>G: GTG 0.06 1.24 0.53 0.3 0.71 0.38 0.34 0.06 0.06 0.07 0.03 0.01 0.08 0.12
T>G: GTT 0.06 0.53 0 3.2 1.61 0.29 0.44 0.06 0.06 0.01 0.17 0.03 0.08 0.09
T>G: TTA 0.02 0.27 0.03 1.04 0.39 1.04 0.89 0.04 0.08 0.04 0.05 0.01 0.11 0.22
T>G: TTC 0.27 0.15 0.1 0.45 1.09 0.84 0.24 0.02 0.03 0.05 0.02 0.01 0.05 0.08
T>G: TTG 0.17 0.57 0.29 0.68 2.15 0.65 0.38 0.05 0.04 0.03 0.03 0.02 0.04 0.12
T>G: TTT 0.69 0.14 0.31 5.45 0.89 2.83 0.78 0.13 0.14 0.19 0.27 0.05 0.19 0.38
48",2016-04-29T09:11:34Z,table table mtn
paper_qf_43.pdf,50,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","2 4 6 8 10 12 142 4 6 8 10
cancer typeX−sectional Mean(R)Figure 1: Variability in cross-sectional means. x-axis: the cancer types labeled 1-14
in the order in Table 2. y-axis: the column means of the matrix Ris= ln(1 +Gis).
49",2016-04-29T09:11:34Z,mea variabity table is is
paper_qf_43.pdf,51,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","2 4 6 8 10 12 140 5000 10000 15000 20000
cancer typeExp(X−sectional Mean(R))Figure 2: Variability in cross-sectional means. x-axis: the cancer types labeled 1-
14 in the order in Table 2. y-axis: exponents of the column means of the matrix
Ris= ln(1 +Gis).
50",2016-04-29T09:11:34Z,e mea variabity table is is
paper_qf_43.pdf,52,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","  00.10.20.30.40.50.60.70.80.91Reconstruction accuracy by cancer subtypes
5 signatures 6 signatures 7 signatures 8 signatures 9 signatures
Cancer subtypePearson Correlation
51",2016-04-29T09:11:34Z,reconstrucancer pearsocorrelatn
paper_qf_43.pdf,61,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","  0%10%20%30%40%50%60%70%80%90%100%Signature contribution to 14 cancer subtypes
Signature 8
Signature 7
Signature 6
Signature 5
Signature 4
Signature 3
Signature 2
Signature 1
Cancer subtypes% contribution
60",2016-04-29T09:11:34Z,snature snature snature snature snature snature snature snature snature cancer
paper_qf_43.pdf,62,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","  00.10.20.30.40.50.60.70.80.91Reconstruction accuracy by cancer types
4 signatures 5 signatures 6 signatures 7 signatures 8 signatures
Cancer subtypePearson Correlation
61",2016-04-29T09:11:34Z,reconstrucancer pearsocorrelatn
paper_qf_43.pdf,70,Factor Models for Cancer Signatures,"  We present a novel method for extracting cancer signatures by applying
statistical risk models (http://ssrn.com/abstract=2732453) from quantitative
finance to cancer genome data. Using 1389 whole genome sequenced samples from
14 cancers, we identify an ""overall"" mode of somatic mutational noise. We give
a prescription for factoring out this noise and source code for fixing the
number of signatures. We apply nonnegative matrix factorization (NMF) to genome
data aggregated by cancer subtype and filtered using our method. The resultant
signatures have substantially lower variability than those from unfiltered
data. Also, the computational cost of signature extraction is cut by about a
factor of 10. We find 3 novel cancer signatures, including a liver cancer
dominant signature (96% contribution) and a renal cell carcinoma signature (70%
contribution). Our method accelerates finding new cancer signatures and
improves their overall stability. Reciprocally, the methods for extracting
cancer signatures could have interesting applications in quantitative finance.
","  0%10%20%30%40%50%60%70%80%90%100%Signature contribution to 14 cancer subtypes
Signature 7
Signature 6
Signature 5
Signature 4
Signature 3
Signature 2
Signature 1
Cancer subtypes% contribution
69",2016-04-29T09:11:34Z,snature snature snature snature snature snature snature snature cancer
paper_qf_44.pdf,1,Biased Roulette Wheel: A Quantitative Trading Strategy Approach,"  The purpose of this research paper it is to present a new approach in the
framework of a biased roulette wheel. It is used the approach of a quantitative
trading strategy, commonly used in quantitative finance, in order to assess the
profitability of the strategy in the short term. The tools of backtesting and
walk-forward optimization were used to achieve such task. The data has been
generated from a real European roulette wheel from an on-line casino based in
Riga, Latvia. It has been recorded 10,980 spins and sent to the computer
through a voice-to-text software for further numerical analysis in R. It has
been observed that the probabilities of occurrence of the numbers at the
roulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is shown
that a flat betting system against Kelly Criterion was more profitable in the
short term.
","Biased roulette wheel: A Quantitative Trading Strategy Approach
Giancarlo Salirrosas Mart nez, CQF
Department of Economics, Universidad San Ignacio de Loyola
Abstract
The purpose of this research paper it is to present a new approach in the framework of a biased roulette
wheel. It is used the approach of a quantitative trading strategy, commonly used in quantitative nance, in
order to assess the protability of the strategy in the short term. The tools of backtesting andwalk-forward
optimization were used to achieve such task. The data has been generated from a real European roulette wheel
from an on-line casino based in Riga, Latvia. It has been recorded 10,980 spins1and sent to the computer
through a voice-to-text software for further numerical analysis in R. It has been observed that the probabilities
of occurrence of the numbers at the roulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is
shown that a at betting system against Kelly Criterion was more protable in the short term.
1 Introduction
Roulette wheel is one of the most famous games in the
context of casino gambling, although its invention has
no clear author, it is meant to be introduced by the
french mathematician Blaise Pascal, in his eorts for
building a perpetual motion machine around the 17th
century . Currently, two types of roulette wheels can
be found in a casino: the European and the Ameri-
can. The dierence between them relies in the quan-
tity of numbered pockets. An European roulette wheel
presents 37 numbered pockets, from 0 to to 36, whereas
an American roulette has one additional compartment:
the double zero. In both cases, roulette's non-zero
numbers are equally distributed into black and red co-
lors, i.e. 18 red and 18 black. Thus, it is clear to realize
the house advantage at the American roulette wheel is
higher, such that if a player wagers one monetary unit,
the average expected value is ",2016-09-30T05:39:05Z,biased quantitative tradi strategy approagian sal ir rosa mart partment economics universidad sainic loyal abstra t it t t ara latvia it it or nste ihlebeck okelly iterintroduroute bl articial intellence se pascal currently amerit t aai n
paper_qf_44.pdf,2,Biased Roulette Wheel: A Quantitative Trading Strategy Approach,"  The purpose of this research paper it is to present a new approach in the
framework of a biased roulette wheel. It is used the approach of a quantitative
trading strategy, commonly used in quantitative finance, in order to assess the
profitability of the strategy in the short term. The tools of backtesting and
walk-forward optimization were used to achieve such task. The data has been
generated from a real European roulette wheel from an on-line casino based in
Riga, Latvia. It has been recorded 10,980 spins and sent to the computer
through a voice-to-text software for further numerical analysis in R. It has
been observed that the probabilities of occurrence of the numbers at the
roulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is shown
that a flat betting system against Kelly Criterion was more profitable in the
short term.
","
2.1 Data
This work was done with data gathered from a real on-
line casino based in Riga, Latvia. The time taken to
achieve this task was 23 days (1 July 2016 until 23 July
2016). A collection of 10,980 observations were stored
and distributed into 5,000 spins for in sample analysis
and 5,980 spins for out of sample statistical analysis.
In order to test the strategy into dierent time hori-
zons, the out of sample group has been divided into
seven dierent subgroups. Each subgroup division has
dierent time intervals.
The reason to collect a large sample size it is because
it allows to have more accurate information about the
population, as it reects more reliably the population
mean and other statistical properties that are discussed
in the theoretical framework section. While larger the
sample size, better level of condence in the sample
estimates, and lower uncertainty about the matter of
study.
Since the data was not provided by the casino, nor
was requested, this has been taken through screen-
shots that allowed to visualize the last 500 outcomes
from the roulette's spins. First, the raw information
was stored as an image data type, then it was repro-
duced to the computer through a voice-to-text soft-
ware, in order to avoid typographical errors. Finally,
this text le was converted into a numerical data type
with an algorithm in R.
2.2 Methodology
2.2.1 Theoretical Framework
LetX=fx1;x2;x3;:::;xngbe a nite set that repre-
sents a discrete random variable with a population of
sizen, wherexisymbolize the elements of X. Let 
 be
a set of all possible outcomes of X, the sample space .
Finally, let Rbe a-algebra of subsets of the sam-
ple space 
, i.e a collection of one or more outcomes.
Such that, if we spin the roulette wheel ntimes, the
measurable space components are:

n=fX:X= (x1;:::;xn);xi: 0;1;2;:::;36g(1)
R=fxi2
ng (2)
Wherefxigis an elementary event. The probability
measure Pdened over the sample space 
, such that
the probability of an event R
 is denoted as P(R) =P
x2Rp(x). Thus, the probability space is dened as
(
;R;P), which satises the following axioms:
8x2R: 0P(x)1 (3)
P(
) = 1 (4)
P(n[
i=1Ri) =nX
i=1P(Ribe a countable sequence
of pairwise disjoint events. Axioms (3), (4) and (5)
were published initially by Andrei Kolmogorov in his
work Grundbegrie der Wahrscheinlichkeitsrechnung
in 1956.
Under the classical approach we can dene the pro-
bability of an event, and an elementary event as:
P(R) =jRj
j
j(6)
P(fxg) =1
j
j=1
nr(7)
Where in (6), we note jRjandj
j, as the cardinals
ofRand 
, i.e. the number of elements in the event
subset and the sample space set respectively. In (7),
we noteras the number of trials, and we make the
assumption that each elementary event, x, has the
same probability of occurrence.
Example 2.2.1. A ball is spun once in an unbiased
roulette wheel, i.e a roulette wheel where each outcome
has the same probability of occurrence, such that
P(x= 0) = P(x= 1) = P(x= 2) =:::=P(x= 36).
The probability space (
 ;R;P) is dened as:

 =f0;1;2;:::;36g
R=fxig
P(R) =1
370:027
Thus, in this context, it is convenient to state the
concept of probability mass function (PMF), where
we noteFX(xi) =P(X=xi), and it is dened as
FX:R![0;1]. The rst property of the mass
function states that FX(xi)0;8xiin the state
space, and the second property isP
xFX(xi) = 1.
In this example, our PMF is dened as an uniform
distribution:
FX(xi) =8
<
:1
37;ifx2f0;1;2;:::;36g;
0;ifx =2f0;1;2;:::;36g:(8)
Figure 1: PMF of an unbiased roulette wheel
2",2016-09-30T05:39:05Z,data  ra latvia t july july ieat w since rst nally methodology toical framework   nally be suwre fx  is t  rp  ri rise axms andrew lmogorov gr und be gri wah rsc  ilike its rec hu  unr rj wre rj and rand i t  t i
paper_qf_44.pdf,3,Biased Roulette Wheel: A Quantitative Trading Strategy Approach,"  The purpose of this research paper it is to present a new approach in the
framework of a biased roulette wheel. It is used the approach of a quantitative
trading strategy, commonly used in quantitative finance, in order to assess the
profitability of the strategy in the short term. The tools of backtesting and
walk-forward optimization were used to achieve such task. The data has been
generated from a real European roulette wheel from an on-line casino based in
Riga, Latvia. It has been recorded 10,980 spins and sent to the computer
through a voice-to-text software for further numerical analysis in R. It has
been observed that the probabilities of occurrence of the numbers at the
roulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is shown
that a flat betting system against Kelly Criterion was more profitable in the
short term.
"," approach, the probabi-
lity of occurrence of an event, xi, of a discrete random
variableXcan be measured as the limit of its rela-
tive frequency of occurrence in an innite sequence of
repetitions, such that:
P(X=xi) = lim
n!1n(xi)
n(9)
Wheren(xi) is the number of times the event xi
has taken place in the experiment and nis the total
number of repetitions. The element n!1 satises
the condition of the Law of Large Numbers, which
holds that as the number of observations increases
to innity, the cumulative relative frequency of an
event will converge to the true probability of the event
itself, and it was proved by Etemadi [7] in his work An
elementary proof of the strong law of large numbers
published in 1981. We will review the two types of
Law of Large Numbers: the weak law and the strong
law.
Theorem 2.2.1 [Weak Law of Large Numbers] . Let
(Xn)n2Nbe a sequence of i:i:d: random variables, each
with nite mean =E[Xi] and variance 2=Var[Xi].
We deneSn=1
nPn
i=1Xi. Then,8>0 :
lim
n!1P(jSn",2016-09-30T05:39:05Z,cawre t law large numbers et em adi a law large numbers torem ak law large numbers  xbe xi var xi  spxi tsn
paper_qf_44.pdf,4,Biased Roulette Wheel: A Quantitative Trading Strategy Approach,"  The purpose of this research paper it is to present a new approach in the
framework of a biased roulette wheel. It is used the approach of a quantitative
trading strategy, commonly used in quantitative finance, in order to assess the
profitability of the strategy in the short term. The tools of backtesting and
walk-forward optimization were used to achieve such task. The data has been
generated from a real European roulette wheel from an on-line casino based in
Riga, Latvia. It has been recorded 10,980 spins and sent to the computer
through a voice-to-text software for further numerical analysis in R. It has
been observed that the probabilities of occurrence of the numbers at the
roulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is shown
that a flat betting system against Kelly Criterion was more profitable in the
short term.
","We note our cumulative distribution function (CDF)
asGX(xi) dened by GX:R![0;1], such that:
GX(xi) =P(Xxi) =X
x1x2g(x1);8xi2R(17)
A cumulative distribution function has the following
properties:
a).GX(xi) is a non-decreasing step ifx1x2then
GX(x1)GX(x2).
b). 0GX(xi)1;8xi2R.
c).GX(x+
i) =GX(xi)8xi2R:Thus,GXis
right",2016-09-30T05:39:05Z,  is
paper_qf_44.pdf,5,Biased Roulette Wheel: A Quantitative Trading Strategy Approach,"  The purpose of this research paper it is to present a new approach in the
framework of a biased roulette wheel. It is used the approach of a quantitative
trading strategy, commonly used in quantitative finance, in order to assess the
profitability of the strategy in the short term. The tools of backtesting and
walk-forward optimization were used to achieve such task. The data has been
generated from a real European roulette wheel from an on-line casino based in
Riga, Latvia. It has been recorded 10,980 spins and sent to the computer
through a voice-to-text software for further numerical analysis in R. It has
been observed that the probabilities of occurrence of the numbers at the
roulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is shown
that a flat betting system against Kelly Criterion was more profitable in the
short term.
","As show in (22), a probability of occurrence of 3% can
increase the expected value dramatically. We will use
the 3% level as a lter for selecting the numbers to bet.
From this we begin to compute the lter of our back-
tester as boolean, where 1 means True and 0 stands
forFalse , as follows:
BX(xi) =(
1;ifP(X=xi)3:0%;
0;otherwise:(23)
We dene the payo function for a single number bet
as:
JX(xi) =(
35b;ifBX(xi) = 1;
",2016-09-30T05:39:05Z,as  from true false 
paper_qf_44.pdf,13,Biased Roulette Wheel: A Quantitative Trading Strategy Approach,"  The purpose of this research paper it is to present a new approach in the
framework of a biased roulette wheel. It is used the approach of a quantitative
trading strategy, commonly used in quantitative finance, in order to assess the
profitability of the strategy in the short term. The tools of backtesting and
walk-forward optimization were used to achieve such task. The data has been
generated from a real European roulette wheel from an on-line casino based in
Riga, Latvia. It has been recorded 10,980 spins and sent to the computer
through a voice-to-text software for further numerical analysis in R. It has
been observed that the probabilities of occurrence of the numbers at the
roulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is shown
that a flat betting system against Kelly Criterion was more profitable in the
short term.
","
0 2.84% 2.74% 0.18%
1 2.76% 2.82% 0.28%
2 2.84% 2.67% 0.21%
3 2.64% 2.43% 0.17%
4 2.60% 2.60% 0.13%
5 2.44% 2.50% 0.11%
6 2.50% 2.67% 0.29%
7 2.90% 2.94% 0.14%
8 2.86% 3.22% 0.50%
9 3.22% 3.33% 0.36%
10 2.90% 3.30% 0.21%
11 2.78% 2.51% 0.21%
12 2.44% 2.51% 0.11%
13 2.68% 2.48% 0.29%
14 2.76% 2.41% 0.40%
15 2.78% 2.59% 0.22%
16 2.36% 2.30% 0.13%
17 2.70% 3.14% 0.35%
18 2.32% 2.08% 0.27%
19 2.44% 2.24% 0.17%
20 2.96% 3.03% 0.20%
21 2.56% 2.25% 0.26%
22 3.08% 3.36% 0.24%
23 2.86% 3.31% 0.58%
24 2.72% 2.78% 0.15%
25 2.86% 2.76% 0.16%
26 2.62% 2.58% 0.11%
27 2.66% 2.75% 0.19%
28 2.60% 2.51% 0.29%
29 2.98% 3.21% 0.21%
30 2.86% 2.98% 0.16%
31 2.56% 2.73% 0.17%
32 2.70% 2.68% 0.10%
33 2.42% 2.29% 0.30%
34 2.48% 2.46% 0.15%
35 2.74% 2.69% 0.17%
36 2.58% 2.56% 0.16%
Table 1: Probability of occurrence for in-sample data
As we can see, the numbers that has higher proba-
bility of occurrence in the sample of 5,000 spins are:
9, 22 and 29; with probabilities of 3 :22%, 3:08% and
2:98% respectively.
It has also been found, that the probability of
occurrence of the roulette wheel numbers seems to
follow an stochastic process and, in some specic cases,
the way they converge to its mean are more stable
than other numbers. This is the case for numbers
such as: 1, 6, 8, 9, 17 and 23, which coincidently, are
the numbers with higher standard deviations than the
other ones: 0 :28%, 0:29%, 0:50%, 0:36%, 0:35% and
0:58% respectively.
We can try to model this stable convergence to the
mean as a mean-reverting process, with an Ornstein-
Uhlenbeck process , which is commonly used in quanti-
tative nance to model interest rates [16], [17], [18].As the number with the highest probability of oc-
currence is the number 9 we will use it to try to model
the way its probability evolve over time. We dene the
following stochastic dierential equation as:
dP(t) =A(P;t)dt+B(P;t)dW(t) (29)
WhereP(t) is the process of P(X= 9) over a time
interval [0;T]. In the RHS, the term A(P;t)dtis de-
terministic, and the coecient of dtis know as the
drift of the process; the other term B(P;t)dW(t) is
random. The coecient of dW(t) is known as the di-
usion of the process, where W(t) follows a standard
Wiener process . This can be written in integral form
as:
P(t) =P(0) +Zt
0A(P;)d+Zt
0B(P;)dW() (30)
From this, we can begin to state our Ornstein-
Uhlenbeck process as:
dP(t) =",2016-09-30T05:39:05Z,table probabity as it   or nste ihlebeck as  wre it winner  from or nste ihlebeck
paper_qf_44.pdf,14,Biased Roulette Wheel: A Quantitative Trading Strategy Approach,"  The purpose of this research paper it is to present a new approach in the
framework of a biased roulette wheel. It is used the approach of a quantitative
trading strategy, commonly used in quantitative finance, in order to assess the
profitability of the strategy in the short term. The tools of backtesting and
walk-forward optimization were used to achieve such task. The data has been
generated from a real European roulette wheel from an on-line casino based in
Riga, Latvia. It has been recorded 10,980 spins and sent to the computer
through a voice-to-text software for further numerical analysis in R. It has
been observed that the probabilities of occurrence of the numbers at the
roulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is shown
that a flat betting system against Kelly Criterion was more profitable in the
short term.
","Figure 4: Cumulative return, returns and drawdown from in-sample backtesting for wagers on numbers 9 and
22. Conservative at betting system established by half a Kelly Criterion equal to 1:81%
Despite the fact that the roulette wheel is not
statistically biased due to a Chi square test, where
X2
calc = 28:1076 is less than X2
stat = 50:999 with
36degrees of freedom and 95% condence level,
we can still have prots if we select only the num-
bers that pass the lter of greater or equal than
three percent of probability of occurrence, which
in this case are the numbers 9 and 22. Moreover,
we will compare the performance of a at betting
system (FBS) against Kelly . The wager's size for
each FBS are dened as the average of all Kelly's
position size in each Walk-Forward Optimization.
An initial capital of $2 ;000 was selected arbitrary in
order to assess the statistical properties of the strategy.
For this in-sample backtesting we can see the
presence of curve tting which is evident in the
distribution of the returns gure from above. This
issue is generated because the strategy was designed
and over-optimized to this specic data set, i.e. a good
metaphor would be: buying a lottery ticket with a
previous knowledge of the winning numbers. In order
to avoid curve tting issues and test if our strategy
it is robust we will use the tool of walk-forward
optimization . The results from the walk-forward
optimization and backtesting are presented in the
same way as the back-tester used in Bloomberg L.P. 's
software as follows:Table 2: Performance Summary
Number of obs. 5000
Wins 315
Losses 4685
P&L 29,979.14
% P&L 1,498.96
Total Wins 199,710
Total Losses 169,730.9
Avg. Win 634
Avg. Loss 36.22
Max. Win 634
Max. Loss 36.22
Max. Drawdown 5,053.89
Calmar Ratio 5.93
14",2016-09-30T05:39:05Z, cumulative conservative kelly iterspite chi okelly t kelly walk forward optiatafor  it bloomberg table formance suary number wins losstotal wins total lossg w loss max wimax loss max draw dowalma rat
paper_qf_44.pdf,15,Biased Roulette Wheel: A Quantitative Trading Strategy Approach,"  The purpose of this research paper it is to present a new approach in the
framework of a biased roulette wheel. It is used the approach of a quantitative
trading strategy, commonly used in quantitative finance, in order to assess the
profitability of the strategy in the short term. The tools of backtesting and
walk-forward optimization were used to achieve such task. The data has been
generated from a real European roulette wheel from an on-line casino based in
Riga, Latvia. It has been recorded 10,980 spins and sent to the computer
through a voice-to-text software for further numerical analysis in R. It has
been observed that the probabilities of occurrence of the numbers at the
roulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is shown
that a flat betting system against Kelly Criterion was more profitable in the
short term.
","Figure 5: Wagers only on number 9. Probability of occurrence equal to 3 :22%. FBS' position size is $30 :86.
Kelly's position size is 0 :4549%. Probabilities gathered from in-sample analysis of 5,000 observations.
Table 3: Performance Summary
Statistics Kelly Criterion Flat wager
Number of obs. 2479 2479
Wins 84 84
Losses 2395 2395
P&L 6,839.57 16,790.21
% P&L 341.98 839.51
Total Wins 81,066.99 90,741,23
Total Losses 74,218.33 73,920.15
Avg. Win 965.08 1080.25
Avg. Loss 30.99 30.86
Max. Win 1,726.00 1080.25
Max. Loss 57.16 30.86
Max. Drawdown 7,167.49 5,524.72
Calmar Ratio 0.95 3.04
Figure 6: Histogram of consecutive losses for Walk-
Forward Optimization No. 1. Maximum consecutive
loss equal to 132 losses in a row.
15",2016-09-30T05:39:05Z, waters probabity kelly probabititable formance suary statistics kelly iterflat number wins losstotal wins total lossg w loss max wimax loss max draw dowalma rat  piogram walk forward optiatno maximum
paper_qf_44.pdf,16,Biased Roulette Wheel: A Quantitative Trading Strategy Approach,"  The purpose of this research paper it is to present a new approach in the
framework of a biased roulette wheel. It is used the approach of a quantitative
trading strategy, commonly used in quantitative finance, in order to assess the
profitability of the strategy in the short term. The tools of backtesting and
walk-forward optimization were used to achieve such task. The data has been
generated from a real European roulette wheel from an on-line casino based in
Riga, Latvia. It has been recorded 10,980 spins and sent to the computer
through a voice-to-text software for further numerical analysis in R. It has
been observed that the probabilities of occurrence of the numbers at the
roulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is shown
that a flat betting system against Kelly Criterion was more profitable in the
short term.
","Figure 7: Wagers on numbers 9 and 20. Probabilities of occurence equal to 3 :2758% and 2 :9282% respectively.
FBS' position size is $18 :81. Kelly's position size is 3 :5242%. Probabilities gathered from in-sample analysis of
7,479 observations.
Table 4: Performance Summary
Statistics Kelly Criterion Flat wager
Number of obs. 499 499
Wins 24 24
Losses 475 475
P&L -1,992.21 -1,053.43
% P&L -99.61 -52.67
Total Wins 7,012.65 7,900.69
Total Losses 8,934.38 8,935.304
Avg. Win 292.19 329.20
Avg. Loss 18.81 18.81
Max. Win 1,294.00 329.20
Max. Loss 119.60 18.81
Max. Drawdown 3,384.99 2,238.53
Calmar Ratio -0.59 -0.47
Figure 8: Histogram of consecutive losses for Walk-
Forward Optimization No. 2. Maximum consecutive
loss equal to 68 losses in a row.
16",2016-09-30T05:39:05Z, waters probabitikelly probabititable formance suary statistics kelly iterflat number wins losstotal wins total lossg w loss max wimax loss max draw dowalma rat  piogram walk forward optiatno maximum
paper_qf_44.pdf,17,Biased Roulette Wheel: A Quantitative Trading Strategy Approach,"  The purpose of this research paper it is to present a new approach in the
framework of a biased roulette wheel. It is used the approach of a quantitative
trading strategy, commonly used in quantitative finance, in order to assess the
profitability of the strategy in the short term. The tools of backtesting and
walk-forward optimization were used to achieve such task. The data has been
generated from a real European roulette wheel from an on-line casino based in
Riga, Latvia. It has been recorded 10,980 spins and sent to the computer
through a voice-to-text software for further numerical analysis in R. It has
been observed that the probabilities of occurrence of the numbers at the
roulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is shown
that a flat betting system against Kelly Criterion was more profitable in the
short term.
","Figure 9: Wagers on numbers 9 and 10. Probabilities of occurrence equal to 3 :2214% and 3 :0083% respectively.
FBS' position size is $49 :22. Kelly's position size is 3 :5505%. Probabilities gathered from in-sample analysis of
7,978 observations.
Table 5: Performance Summary
Statistics Kelly Criterion Flat wager
Number of obs. 501 501
Wins 33 33
Losses 468 468
P&L -1,269.61 5340.02
% P&L -63.48 267.00
Total Wins 22,170.33 28,422.68
Total Losses 23,368.93 23,033.45
Avg. Win 671.83 861.29
Avg. Loss 49.93 49.22
Max. Win 2,181.00 861.29
Max. Loss 202.10 49.22
Max. Drawdown 5,433.79 4,084.99
Calmar Ratio -0.23 1.31
Figure 10: Histogram of consecutive losses for Walk-
Forward Optimization No. 3. Maximum consecutive
loss equal to 83 losses in a row.
17",2016-09-30T05:39:05Z, waters probabitikelly probabititable formance suary statistics kelly iterflat number wins losstotal wins total lossg w loss max wimax loss max draw dowalma rat  piogram walk forward optiatno maximum
paper_qf_44.pdf,18,Biased Roulette Wheel: A Quantitative Trading Strategy Approach,"  The purpose of this research paper it is to present a new approach in the
framework of a biased roulette wheel. It is used the approach of a quantitative
trading strategy, commonly used in quantitative finance, in order to assess the
profitability of the strategy in the short term. The tools of backtesting and
walk-forward optimization were used to achieve such task. The data has been
generated from a real European roulette wheel from an on-line casino based in
Riga, Latvia. It has been recorded 10,980 spins and sent to the computer
through a voice-to-text software for further numerical analysis in R. It has
been observed that the probabilities of occurrence of the numbers at the
roulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is shown
that a flat betting system against Kelly Criterion was more profitable in the
short term.
","Figure 11: Wagers on numbers 9 and 10. Probabilities of occurrence equal to 3 :1961% and 3 :0546% respectively.
FBS' position size is $42 :49. Kelly's position size is 3 :5722%. Probabilities gathered from in-sample analysis of
8,479 observations
Table 6: Performance Summary
Statistics Kelly Criterion Flat wager
Number of obs. 365 365
Wins 23 23
Losses 342 342
P&L -1,459.41 2,528.17
% P&L -72.97 126.41
Total Wins 13,330.28 17,102.31
Total Losses 14,718.25 14,531.65
Avg. Win 579.58 743.58
Avg. Loss 43.04 42.49
Max. Win 1572.00 743.58
Max. Loss 145.90 42.49
Max. Drawdown 3,714.64 2,273.23
Calmar Ratio -0.39 1.11
Figure 12: Histogram of consecutive losses for Walk-
Forward Optimization No. 4. Maximum consecutive
loss equal to 49 losses in a row.
18",2016-09-30T05:39:05Z, waters probabitikelly probabititable formance suary statistics kelly iterflat number wins losstotal wins total lossg w loss max wimax loss max draw dowalma rat  piogram walk forward optiatno maximum
paper_qf_44.pdf,19,Biased Roulette Wheel: A Quantitative Trading Strategy Approach,"  The purpose of this research paper it is to present a new approach in the
framework of a biased roulette wheel. It is used the approach of a quantitative
trading strategy, commonly used in quantitative finance, in order to assess the
profitability of the strategy in the short term. The tools of backtesting and
walk-forward optimization were used to achieve such task. The data has been
generated from a real European roulette wheel from an on-line casino based in
Riga, Latvia. It has been recorded 10,980 spins and sent to the computer
through a voice-to-text software for further numerical analysis in R. It has
been observed that the probabilities of occurrence of the numbers at the
roulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is shown
that a flat betting system against Kelly Criterion was more profitable in the
short term.
","Figure 13: Wagers on numbers 9 and 10. Probabilities of occurrence equal to 3 :1547% and 3 :0981% respectively.
FBS' position size is $13 :33. Kelly's position size is 3 :5743%. Probabilities gathered from in-sample analysis of
8,844 observations
Table 7: Performance Summary
Statistics Kelly Criterion Flat wager
Number of obs. 492 492
Wins 23 23
Losses 469 469
P&L -1,994.70 -899.51
% P&L -99.73 -44.98
Total Wins 4,327.74 5,263.73
Total Losses 6,250.95 6,249.91
Avg. Win 188.16 233.21
Avg. Loss 13.33 13.33
Max. Win 1,222.00 233.21
Max. Loss 113.50 13.33
Max. Drawdown 3,170.51 1,359.26
Calmar Ratio -0.63 -0.66
Figure 14: Histogram of consecutive losses for Walk-
Forward Optimization No. 5. Maximum consecutive
loss equal to 80 losses in a row.
19",2016-09-30T05:39:05Z, waters probabitikelly probabititable formance suary statistics kelly iterflat number wins losstotal wins total lossg w loss max wimax loss max draw dowalma rat  piogram walk forward optiatno maximum
paper_qf_44.pdf,20,Biased Roulette Wheel: A Quantitative Trading Strategy Approach,"  The purpose of this research paper it is to present a new approach in the
framework of a biased roulette wheel. It is used the approach of a quantitative
trading strategy, commonly used in quantitative finance, in order to assess the
profitability of the strategy in the short term. The tools of backtesting and
walk-forward optimization were used to achieve such task. The data has been
generated from a real European roulette wheel from an on-line casino based in
Riga, Latvia. It has been recorded 10,980 spins and sent to the computer
through a voice-to-text software for further numerical analysis in R. It has
been observed that the probabilities of occurrence of the numbers at the
roulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is shown
that a flat betting system against Kelly Criterion was more profitable in the
short term.
","Figure 15: Wagers on numbers 9 and 10. Probabilities of occurrence equal to 3 :1170% and 3 :0527% respectively.
FBS' position size is $6 :27. Kelly's position size is 3 :4888%. Probabilities gathered from in-sample analysis of
9,336 observations
Table 8: Performance Summary
Statistics Kelly Criterion Flat wager
Number of obs. 759 759
Wins 47 47
Losses 712 712
P&L -1,892.20 686.10
% P&L -94.61 34.31
Total Wins 2,714.65 5,153.59
Total Losses 4,537.08 4,461.22
Avg. Win 57.76 109.65
Avg. Loss 6.37 6.26
Max. Win 316.70 109.65
Max. Loss 67.34 6.26
Max. Drawdown 1,924.77 626.58
Calmar Ratio -0.98 1.10
Figure 16: Histogram of consecutive losses for Walk-
Forward Optimization No. 6. Maximum consecutive
loss equal to 85 losses in a row.
20",2016-09-30T05:39:05Z, waters probabitikelly probabititable formance suary statistics kelly iterflat number wins losstotal wins total lossg w loss max wimax loss max draw dowalma rat  piogram walk forward optiatno maximum
paper_qf_44.pdf,21,Biased Roulette Wheel: A Quantitative Trading Strategy Approach,"  The purpose of this research paper it is to present a new approach in the
framework of a biased roulette wheel. It is used the approach of a quantitative
trading strategy, commonly used in quantitative finance, in order to assess the
profitability of the strategy in the short term. The tools of backtesting and
walk-forward optimization were used to achieve such task. The data has been
generated from a real European roulette wheel from an on-line casino based in
Riga, Latvia. It has been recorded 10,980 spins and sent to the computer
through a voice-to-text software for further numerical analysis in R. It has
been observed that the probabilities of occurrence of the numbers at the
roulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is shown
that a flat betting system against Kelly Criterion was more profitable in the
short term.
","Figure 17: Wagers on numbers 10 and 9. Probabilities of occurrence equal to 3 :1105% and 3 :0609% respectively.
FBS' position size is $15 :25. Kelly's position size is 3 :4906%. Probabilities gathered from in-sample analysis of
10,095 observations
Table 9: Performance Summary
Statistics Kelly Criterion Flat wager
Number of obs. 885 885
Wins 50 50
Losses 835 835
P&L -1,994.323 594.66
% P&L -99.72 29.73
Total Wins 10,944.32 13,341.75
Total Losses 12,868.84 -12,731.84
Avg. Win 218.89 266.83
Avg. Loss 15.41 15.25
Max. Win 811.80 266.83
Max. Loss 74.72 15.25
Max. Drawdown 2,135.71 1913.59
Calmar Ratio -0.93 0.31
Figure 18: Histogram of consecutive losses for Walk-
Forward Optimization No. 7. Maximum consecutive
loss equal to 67 losses in a row.
21",2016-09-30T05:39:05Z, waters probabitikelly probabititable formance suary statistics kelly iterflat number wins losstotal wins total lossg w loss max wimax loss max draw dowalma rat  piogram walk forward optiatno maximum
paper_qf_44.pdf,22,Biased Roulette Wheel: A Quantitative Trading Strategy Approach,"  The purpose of this research paper it is to present a new approach in the
framework of a biased roulette wheel. It is used the approach of a quantitative
trading strategy, commonly used in quantitative finance, in order to assess the
profitability of the strategy in the short term. The tools of backtesting and
walk-forward optimization were used to achieve such task. The data has been
generated from a real European roulette wheel from an on-line casino based in
Riga, Latvia. It has been recorded 10,980 spins and sent to the computer
through a voice-to-text software for further numerical analysis in R. It has
been observed that the probabilities of occurrence of the numbers at the
roulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is shown
that a flat betting system against Kelly Criterion was more profitable in the
short term.
","
In this paper, we have seen that is not necessary that
the roulette wheel has to be statistically biased in
order to have prots in the short-term. Importantly,
however, it is needed a probability of occurrence
approximately to greater or equal than three percent
for make a bet on a specic number.
Moreover, the probability of occurrence of each
number at the roulette wheel can be modelled very
accurately as an Ornstein-Uhlenbeck process. This
result showed consistency with the Monte Carlo
simulations shown in Figure 3. Further analysis is
needed in order to assess if it is possible to make
forecasts of how this probabilities evolve over time
with this same model.
Furthermore, the at betting system (FBS) has
shown a better performance than the Kelly criterion's
system, due that Kelly's carries higher volatility and
it is more aggressive than the FBS, thus, as more
losses in a row comes into the strategy it is more
dicult for Kelly's system to recover. It would be a
matter of further study to assess the performance of
half a Kelly criterion as a position size. Similarly,
another matter of further study might involve perform
aNon-anchored Walk-forward optimization, in order
to get more up-to-date probabilities of occurrence and
include the momentum factor of the roulette wheel
into the strategy.
In addition, it has also been observed that roulette
outcomes get less predictable over time, which may be
an evidence of the Second Law of Thermodynamics,
which states that disorder, known as entropy , increases
with time.
5 Acknowledgements
This research paper would not have been possible
without the support of the following people, whom I
am grateful:
My family, in alphabetical order: my brother, father
and mother. For their tremendous support in my
personal life over time.
My ex-wife, for her big support during the ve years of
my undergraduate education and sharing those years
of her life with me.
My three godsons, in alphabetical order: Anthony,
C esar and Piero, for took the crazy decision of choo-
sing me as their godfather.
Pedro Cornejo, mentor and friend who brought me
into the nancial industry, to whom I am deeply
indebted.Gabriel Chirre, for his loyal and disinterested friend-
ship during my professional career.
David Hafermann, who taught me the concepts
of backtesting and walk-forward optimization, and
showed me the importance of programming.
Dennis La Cotera, a true Peruvian quant, who taught
me the amazing properties of computational nance
and applied mathematics in Wolfram Mathematica.
My professors at the CQF Institute, for sharing their
great knowledge about quantitative nance with me.
My professors at Universidad San Ignacio de Loyola,
who gave me the very rst concepts of nance and
econometrics at the Faculty of Economics.
In order of meeting to: Megija, Darja, Laura, Sigita
and Jana, blackjack and roulette dealers at Evolution
Gaming, for their invaluable motivational words that
made this research paper possible. In this group, a
very special thanks to Jana Zidere for verify in a daily
basis my progress on this paper and for share the same
crazy spirit as the author.
Above all, thanks to the Intelligent Designer .
6 References
[1] Pozdnyakov, Vladimir and Steele, Michael (2009).
Martingale Methods for Patterns and Scan Statistics.
Statistics for Industry and Technology . pp. 289-317
[2] Farmer, Doyne and Sidorowich John (1987). Pre-
dicting Chaotic Time Series. Physical Review Letters .
Vol 59, No. 8.
[3] Small, Michael and Tse, Chi Kong (2012).
Predicting the outcome of roulette. Chaos: An
Interdisciplinary Journal of Nonlinear Science
[4] Jirina, Miroslav (1978). A biased roulette. Annales
de l'Institute Henri Poincar e . Vol 14, No 1, p. 1-23.
[5] Sundali, James and Croson, Rachel (2006). Biases
in casino betting: The hot hand and the gambler's
fallacy. Judgement and Decision Making . Vol. 1, No.
1, pp. 1-12.
[6] Ethier, S.N. (1982). Testing for favourable num-
bers on a roulette wheel. Journal of the American
Statistical Association . Vol. 77, No. 379, pp. 660-665.
[7] Etemadi, N. Z. (1981). An elementary proof
of the strong law of large numbers. Zeitschrift f ur
Wahrscheinlichkeitstheorie und Verwandte Gebiete .
Vol. 55, Issue 1, pp. 119-122.
22",2016-09-30T05:39:05Z,iimtantly oor nste ihlebeck     furtr furtr kelly kelly kelly it kelly simarly alk isecond law trmodynamics ackledgement  my for my my anthony pier pedro core jo gabriel chi re did ha germannis la co t siawolfram matmatics my institute my universidad sainic loyal faculty economics ime gi dar laura s it jaevolutgami ijasi re above intellent sner referencpo zd v vladimir steele michael martale methods paerns castatistics statistics industry technology farmer do ne si row i pre chaotic time seripsical review ters vno small michael tse chi  predii chaos ainterdisciplinary journal nonlinear science ina mirosl annals institute nri pointer vno sudali jamo soracl biast judgement cismaki vno thier testi journal statistical associatvno et em adi azeitschrt wah rsc  ilike its t or ie wand te b ie te vissue
paper_qf_44.pdf,23,Biased Roulette Wheel: A Quantitative Trading Strategy Approach,"  The purpose of this research paper it is to present a new approach in the
framework of a biased roulette wheel. It is used the approach of a quantitative
trading strategy, commonly used in quantitative finance, in order to assess the
profitability of the strategy in the short term. The tools of backtesting and
walk-forward optimization were used to achieve such task. The data has been
generated from a real European roulette wheel from an on-line casino based in
Riga, Latvia. It has been recorded 10,980 spins and sent to the computer
through a voice-to-text software for further numerical analysis in R. It has
been observed that the probabilities of occurrence of the numbers at the
roulette wheel follows an Ornstein-Uhlenbeck process. Moreover, it is shown
that a flat betting system against Kelly Criterion was more profitable in the
short term.
","[8] Ni, Jiarui and Zhang, Chengqi (2005). An Ecient
Implementation of the Backtesting of Trading Strate-
gies. Lecture Notes in Computer Science . Vol. 3758,
pp. 126-131.
[9] Dixon M., Klabjan, D. and Bang, J. (2015).
Backtesting Trading Strategies with Deep Neural Net-
works on the Intel Xeon Phi. SSRN Electronic Journal .
[10] Wong. W (2008). Backtesting trading risk of
commercial banks using expected shortfall. Journal of
Banking and Finance . Vol, 32, Issue 7, pp. 1404-1415.
[11] Menkveld, Albert J. (2013). High frequency
trading and the new market makers. Vol. 16, Issue 4,
pp. 712-740.
[12] Schneider, D. (2011). Trading at the speed of
light. IEEE Spectrum . Vol. 48, Issue 10, pp. 11-12.
[13] Scholtus M., and Dijk D. (2012). High-Frequency
Technical Trading: The Importance of Speed. Tinber-
gen Institute Discussion Paper . p. 63.
[14] Kelly, John L. Jr. (1956). A New Interpretation
of Information Rate. Bell Systems Technical Journal .
Vol. 35, pp. 917-926.
[15] Young, T. (1991). Calmar Ratio: A Smoother
Tool. Future Magazine . Vol. 20, Issue 1, p. 40.
[16] Barndor-Nielsen, O. and Shephard, N. (2001).
Non-Gaussian Ornstein-Uhlenbeck-Based models and
some of their uses in nancial economics. Journal
of the Royal Statistical Society. Series B (Statistical
Methodology) Vol. 63, Issue 2, pp. 167-241.
[17] Maller R., M uller G. and Szimayer A. (2009).
Ornstein-Uhlenbeck Processes and Extensions. Hand-
book of Financial Time Series . Issue 1930, pp.
421-437.
[18] Onalan, O. (2009). Financial Modelling with
Ornstein-Uhlenbeck Processes Driven by L evy Process.
Lecture Notes in Engineering and Computer Science .
Vol. 2177, Issue 1, pp. 1350-1355.
23",2016-09-30T05:39:05Z,ni jia rui  c qi aimplementatback testi tradi st rate leure notuter science vdixolab jaba back testi tradi strategiep neural net intel xeno phi eleronic journal wo back testi journal banki nance vissue meld albert hh vissue schneir tradi sperum vissue slt us di jk hh frequency technical tradi t imtance speed tiber institute discusspa kelly  jr new interpatinformatrate bell tems technical journal vyou alma rat smootr tofuture magazine vissue bardor nielsesprd noor nste ihlebeck based journal royal statistical society seristatistical methodology vissue aller zi mayer or nste ihlebeck processextensns hand nancial time seriissue oalanancial molli or nste ihlebeck processdriveprocess leure noteineeri uter science vissue
paper_qf_45.pdf,1,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","*K-means and Cluster Models for Cancer Signatures
Zura Kakushadzexy1and Willie Yu]2
xQuantigicrSolutions LLC
1127 High Ridge Road #135, Stamford, CT 069053
yFree University of Tbilisi, Business School & School of Physics
240, David Agmashenebeli Alley, Tbilisi, 0159, Georgia
]Centre for Computational Biology, Duke-NUS Medical School
8 College Road, Singapore 169857
(January 30, 2017)
Abstract
We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753
to quantitative nance. *K-means is statistically deterministic without speci-
fying initial centers, etc. We apply *K-means to extracting cancer signatures
from genome data without using nonnegative matrix factorization (NMF).
*K-means' computational cost is a fraction of NMF's. Using 1,389 published
samples for 14 cancer types, we nd that 3 cancers (liver cancer, lung cancer
and renal cell carcinoma) stand out and do not have cluster-like structures.
Two clusters have especially high within-cluster correlations with 11 other can-
cers indicating common underlying structures. Our approach opens a novel
avenue for studying such structures. *K-means is universal and can be applied
in other elds. We discuss some potential applications in quantitative nance.
1Zura Kakushadze, Ph.D., is the President of QuantigicrSolutions LLC, and a Full Professor
at Free University of Tbilisi. Email: zura@quantigic.com
2Willie Yu, Ph.D., is a Research Fellow at Duke-NUS Medical School. Email: willie.yu@duke-
nus.edu.sg
3DISCLAIMER: This address is used by the corresponding author for no purpose other than
to indicate his professional aliation as is customary in publications. In particular, the contents
of this paper are not intended as an investment, legal, tax or any other such advice, and in no
way represent views of QuantigicrSolutions LLC, the website www.quantigic.com or any of their
other aliates.arXiv:1703.00703v4  [q-bio.GN]  18 Jul 2017",2017-03-02T10:39:59Z,uster mols cancer snatur u sha xy wlie yu quant ic solutns hh ridge road stanford free  tbisi business schoschopsics did gma s ne belt alley tbisi georgia centre tnal blogy duke medical schocollege road siae uary abstra   usi two our   u sha ph presint quant ic solutns full professor free  tbisi em articial intellence wlie yu ph researfellow duke medical schoem articial intellence  iquant ic solutns  jul
paper_qf_45.pdf,2,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","1 Introduction and Summary
Every time we can learn something new about cancer, the motivation goes without
saying. Cancer is dierent. Unlike other diseases, it is not caused by \\\\mechani-
cal"" breakdowns, biochemical imbalances, etc. Instead, cancer occurs at the DNA
level via somatic alterations in the genome structure. A common type of somatic
mutations found in cancer is due to single nucleotide variations (SNVs) or alter-
ations to single bases in the genome, which accumulate through the lifespan of the
cancer via imperfect DNA replication during cell division or spontaneous cytosine
deamination [Goodman and Fygenson, 1998], [Lindahl, 1993], or due to exposures
to chemical insults or ultraviolet radiation [Loeb and Harris, 2008], [Ananthaswamy
and Pierceall, 1990], etc. These mutational processes leave a footprint in the cancer
genome characterized by distinctive alteration patterns or mutational signatures.
If we can identify all underlying signatures, this could greatly facilitate progress
in understanding the origins of cancer and its development. Therapeutically, if there
are common underlying structures across dierent cancer types, then a therapeutic
for one cancer type might be applicable to other cancers, which would be a great
news.4However, it all boils down to the question of usefulness, i.e., is there a small
enough number of cancer signatures underlying all (100+) known cancer types, or is
this number too large to be meaningful or useful? Indeed, there are only 96 SNVs,5
so we cannot have more than 96 signatures.6Even if the number of true underlying
signatures is, say, of order 50, it is unclear whether they would be useful, especially
within practical applications. On the other hand, if there are only a dozen or so
underlying signatures, then we could hope for an order of magnitude simplication.
To identify mutational signatures, one analyzes SNV patterns in a cohort of
DNA sequenced whole cancer genomes. The data is organized into a matrix Gis,
where the rows correspond to the N= 96 mutation categories, the columns corre-
spond todsamples, and each element is a nonnegative occurrence count of a given
mutation category in a given sample. Currently, the commonly accepted method
for extracting cancer signatures from Gis[Alexandrov et al, 2013a] is via nonnega-
tive matrix factorization (NMF) [Paatero and Tapper, 1994], [Lee and Seung, 1999].
Under NMF the matrix Gis approximated via GW H , whereWiAis anNK
matrix,HAsis aKdmatrix, and both WandHare nonnegative. The appeal
of NMF is its biologic interpretation whereby the Kcolumns of the matrix Ware
4Another practical application is prevention by pairing the signatures extracted from cancer
samples with those caused by known carcinogens (e.g., tobacco, aatoxin, UV radiation, etc).
5In brief, DNA is a double helix of two strands, and each strand is a string of letters A, C,
G, T corresponding to adenine, cytosine, guanine and thymine, respectively. In the double helix,
A in one strand always binds with T in the other, and G always binds with C. This is known
as base complementarity. Thus, there are six possible base mutations C >A, C >G, C >T,
T>A, T >C, T >G, whereas the other six base mutations are equivalent to these by base
complementarity. Each of these 6 possible base mutations is anked by 4 possible bases on each
side thereby producing 4 64 = 96 distinct mutation categories.
6Nonlinearities could undermine this argument. However, again, it all boils down to usefulness.
1",2017-03-02T10:39:59Z,introdusuary every cancer unlike instead vs goodmafy gens olinda hl lobe harris samantha swamp pierce all tse  trapeutically ined vs eveoto t is currently is alexandra pa at ero tap   seu unr is wi is as is wand hare t columns ware anotr ii  eanonlinear tiver
paper_qf_45.pdf,3,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","interpreted as the weights with which the Kcancer signatures contribute into the
N= 96 mutation categories, and the columns of the matrix Hare interpreted as
the exposures to the Ksignatures in each sample. The price to pay for this is that
NMF, which is an iterative procedure, is computationally costly and depending on
the number of samples dit can take days or even weeks to run it. Furthermore, it
does not automatically x the number of signatures K, which must be either guessed
or obtained via trial and error, thereby further adding to the computational cost.7
Some of the aforesaid issues were recently addressed in [Kakushadze and Yu,
2016b], to wit: i) by aggregating samples by cancer types, we can greatly improve
stability and reduce the number of signatures;8ii) by identifying and factoring out
the somatic mutational noise, or the \\\\overall"" mode (this is the \\\\de-noising"" proce-
dure of [Kakushadze and Yu, 2016b]), we can further greatly improve stability and,
as a bonus, reduce computational cost; and iii) the number of signatures can be xed
borrowing the methods from statistical risk models [Kakushadze and Yu, 2017b] in
quantitative nance, by computing the eective rank (or eRank) [Roy and Vetterli,
2007] for the correlation matrix 	 ijcalculated across cancer types or samples (see
below). All this yields substantial improvements [Kakushadze and Yu, 2016b].
In this paper we push this program to yet another level. The basic idea here is
quite simple (but, as it turns out, nontrivial to implement { see below). We wish to
apply clustering techniques to the problem of extracting cancer signatures. In fact,
we argue in Section 2 that NMF is, to a degree, \\\\clustering in disguise"". This is for
two main reasons. The prosaic reason is that NMF, being a nondeterministic algo-
rithm, requires averaging over many local optima it produces. However, each run
generally produces a weights matrix WiAwith columns (i.e., signatures) not aligned
with those in other runs. Aligning or matching the signatures across dierent runs
(before averaging over them) is typically achieved via nondeterministic clustering
such as k-means. So, not only is clustering utilized at some layer, the result, even
after averaging, generally is both noisy9and nondeterministic! I.e., if this computa-
tionally costly procedure (which includes averaging) is run again and again on the
same data, generally it will yield dierent looking cancer signatures every time!
The second, not-so-prosaic reason is that, while NMF generically does not pro-
duce exactly null weights, it does produce low weights, such that they are within
error bars. For all practical purposes we might as well set such weights to zero.
NMF requires nonnegative weights. However, we could as reasonably require that
the weights should be, say, outside error bars (e.g., above one standard deviation {
7Other issues include: i) out-of-sample instability, i.e., the signatures obtained from non-
overlapping sets of samples can be dramatically dierent; ii) in-sample instability, i.e., the signa-
tures can have a strong dependence on the initial iteration choice; and iii) samples with low counts
or sparsely populated samples (i.e., those with many zeros { such samples are ubiquitous, e.g., in
exome data) are usually deemed not too useful as they contribute to the in-sample instability.
8As a result, now we have the so-aggregated matrix Gis, where s= 1; : : : ; d , and d=nis the
number of cancer types, not of samples. This matrix is much less noisy than the sample data.
9By \\\\noise"" we mean the statistical errors in the weighs obtained by averaging. Typically,
such error bars are not reported in the literature on cancer signatures. Usually they are large.
2",2017-03-02T10:39:59Z,cancer hare snaturt furtr some u sha yu u sha yu u sha yu rank roy ve er li all u sha yu it  ise t wi with alni so t for otr as is  by typically usually
paper_qf_45.pdf,4,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","this would render the algorithm highly recursive and potentially unstable or compu-
tationally too costly) or above some minimum threshold (which would still further
complicated as-is complicated NMF), or else the non-compliant weights are set to
zero. As we increase this minimum threshold, the matrix WiAwill start to have more
and more zeros. It may not exactly have a binary cluster-like structure, but it may
at least have some substructures that are cluster-like. It then begs the question: are
there cluster-like (sub)structures present in WiAor, generally, in cancer signatures?
To answer this question, we can apply clustering methods directly to the matrix
Gis, or, more, precisely, to its de-noised version G0
is(see below) [Kakushadze and
Yu, 2016b]. The na ve, brute-force approach where one would simply cluster Gis
orG0
isdoes not work for a variety of reasons, some being more nontrivial or subtle
than others. Thus, e.g., as discussed in [Kakushadze and Yu, 2016b], the counts
Gishave skewed, long-tailed distributions and one should work with log-counts, or,
more precisely, their de-noised versions. This applies to clustering as well. Further,
following a discussion in [Kakushadze and Yu, 2016c] in the context of quantitative
trading, it would be suboptimal to cluster de-noised log-counts. Instead, it pays
to cluster their normalized variants (see Section 2 hereof). However, taking care of
such subtleties does not alleviate one big problem: nondeterminism!10If we run a
vanilla nondeterministic algorithm such as k-means on the data however massaged
with whatever bells and whistles, we will get random-looking disparate results every
time we run k-means with no stability in sight. We need to address nondeterminism!
Our solution to the problem is what we term *K-means . The idea behind *K-
means, which essentially achieves determinism statistically , is simple. Suppose we
have anNdmatrixXis, i.e., we have N d-vectors Xi. If we run k-means with the
input number of clusters Kbut initially unspecied centers, every run will generally
produce a new local optimum. *K-means reduces and in fact essentially eliminates
this indeterminism via two levels. At level 1 it takes clusterings obtained via M
independent runs or samplings. Each sampling produces a binary NKmatrix 
 iA,
whose element equals 1 if Xibelongs to the cluster labeled by A, and 0 otherwise.
The aggregation algorithm and the source code therefor are given in [Kakushadze
and Yu, 2016c]. This aggregation { for the same reasons as in NMF (see above)
{ involves aligning clusters across the Mruns, which is achieved via k-means, and
so the result is nondeterministic. However, by aggregating a large number Mof
samplings, the degree of nondeterminism is greatly reduced. The \\\\catch"" is that
sometimes this aggregation yields a clustering with K0<K clusters, but this does
not pose an issue. Thus, at level 2, we take a large number Pof such aggregations
(each based on Msamplings). The occurrence counts of aggregated clusterings are
not uniform but typically have a (sharply) peaked distribution around a few (or
manageable) number of aggregated clusterings. So this way we can pinpoint the
\\\\ultimate"" clustering, which is simply the aggregated clustering with the highest
occurrence count. This is the gist of *K-means and it works well for genome data.
10Deterministic (e.g., agglomerative hierarchical) algorithms have their own issues (see below).
3",2017-03-02T10:39:59Z,as wi wl it it wi or to is u sha yu t is  u sha yu is he  furtr u sha yu instead se  our t suppose is xi  but at eamatrix xi belos t u sha yu  runs of t  of sampli t so  terministic
paper_qf_45.pdf,5,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","So, we apply *K-mean to the same genome data as in [Kakushadze and Yu,
2016b] consisting of 1,389 (published) samples across 14 cancer types (see below).
Our target number of clusters is 7, which was obtained in [Kakushadze and Yu,
2016b] using the eRank based algorithm (see above). We aggregated 1,000 samplings
into clusterings, and we constructed 150,000 such aggregated clusterings (i.e., we ran
150 million k-means instances). We indeed found the \\\\ultimate"" clustering with 7
clusters. Once the clustering is xed, it turns out that within-cluster weights can be
computed via linear regressions (with some bells and whistles) and the weights are
automatically positive. That is, we do not need NMF at all! Once we have clusters
and weights, we can study reconstruction accuracy and within-cluster correlations
between the underlying data and the tted data that the cluster model produces.
We nd that clustering works well for 10 out the 14 cancer types we study. The
cancer types for which clustering does not appear to work all that well are Liver
Cancer, Lung Cancer, and Renal Cell Carcinoma. Also, above 80% within-cluster
correlations arise for 5 out of 7 clusters. Furthermore, remarkably, one cluster has
high within-cluster correlations for 9 cancer types, and another cluster for 6 cancer
types. These appear to be the leading clusters. Together they have high within-
cluster correlations in 11 out of 14 cancer types. So what does all this mean?
Additional insight is provided by looking at the within-cluster correlations be-
tween signatures Sig1 through Sig7 extracted in [Kakushadze and Yu, 2016b] and
our clusters. High within-cluster correlations arise for Sig1, Sig2, Sig4 and Sig7,
which are precisely the signatures with \\\\peaks"" (or \\\\spikes"" { \\\\tall mountain land-
scapes""), whereas Sig3, Sig5 and Sig6 do not have such \\\\peaks"" (\\\\at"" or \\\\rolling
hills landscapes""); see Figures 14 through 20 of [Kakushadze and Yu, 2016b]. The
latter 3 signatures simply do not have cluster-like structures. Looking at Figure 21
in [Kakushadze and Yu, 2016b], it becomes evident why clustering does not work
well for Liver Cancer { it has a whopping 96% contribution from Sig5! Similarly,
Renal Cell Carcinoma has a 70% contribution from Sig6. Lung Cancer is domi-
nated by Sig3, hence no cluster-like structure. So, Liver Cancer, Lung Cancer and
Renal Cell Carcinoma have little in common with other cancers (and each other)!
However, 11 other cancers, to wit, B Cell Lymphoma, Bone Cancer, Brain Lower
Grade Glioma, Breast Cancer, Chronic Lymphocytic Leukemia, Esophageal Cancer,
Gastric Cancer, Medulloblastoma, Ovarian Cancer, Pancreatic Cancer and Prostate
Cancer, have 5 (with 2 leading) cluster structures substantially embedded in them.
In Section 2 we i) discuss why applying clustering algorithms to extracting cancer
signatures makes sense, ii) argue that NMF, to a degree, is \\\\clustering in disguise"",
and iii) give the machinery for building cluster models via *K-means, including
various details such as what to cluster, how to x the number of clusters, etc. In
Section 3 we discuss i) cancer genome data we use, ii) our application of *K-means
to it, and iii) the interpretation of our empirical results. Section 4 contains some
concluding remarks, including a discussion of potential applications of *K-means in
quantitative nance, where we outline some concrete problems where *K-means can
be useful. Appendix A contains R source code for *K-means and cluster models.
4",2017-03-02T10:39:59Z,so u sha yu our u sha yu rank   once that once  t licancer lu cancer renal cell carcinoma also furtr tse togetr so additnal s s u sha yu hh s s s s s s s s u sha yu t looki  u sha yu licancer s simarly renal cell carcinoma s lu cancer s so licancer lu cancer renal cell carcinoma cell lymphoma bone cancer br articial intellence lor gra lima breast cancer chronic lymph cy tic leukemia esophageal cancer gastric cancer me dull oblast oma ovariacancer paneatic cancer prostate cancer iseisese
paper_qf_45.pdf,6,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","2 Cluster Models
The chief objective of this paper is to introduce a novel approach to identifying
cancer signatures using clustering methods. In fact, as we discuss below in detail,
our approach is more than just clustering. Indeed, it is evident from the get-go
that blindly using nondeterministic clustering algorithms,11which typically produce
(unmanageably) large numbers of local optima, would introduce great variability
into the resultant cancer signatures.12On the other hand, deterministic algorithms
such as agglomerative hierarchical clustering13typically are (substantially) slower
and require essentially \\\\guessing"" the initial clustering,14which in practical appli-
cations15can often turn out to be suboptimal. So, both to motivate and explain our
new approach employing clustering methods, we rst { so to speak { \\\\break down""
the NMF approach and argue that it is in fact a clustering method in disguise!
2.1 \\\\Breaking Down"" NMF
The current \\\\lore"" { the commonly accepted method for extracting Kcancer signa-
tures from the occurrence counts matrix Gis(see above) [Alexandrov et al, 2013a]
{ is via nonnegative matrix factorization (NMF) [Paatero and Tapper, 1994], [Lee
and Seung, 1999]. Under NMF the matrix Gis approximated via GW H , where
WiAis anNKmatrix of weights, HAsis aKdmatrix of exposures, and both W
andHare nonnegative. However, not only is the number of signatures Knot xed
via NMF (and must be either guessed or obtained via trial and error), NMF too is a
nondeterministic algorithm and typically produces a large number of local optima.
So, in practice one has no choice but to execute a large number NSof NMF runs {
which we refer to as samplings { and then somehow extract cancer signatures from
these samplings. Absent a guess for what Kshould be, one executes NSsamplings
for a range of values of K(say,KminKKmax, whereKminandKmaxare basi-
cally guessed based on some reasonable intuitive considerations), for each Kextracts
cancer signatures (see below), and then picks Kand the corresponding signatures
with the best overall t into the underlying matrix G. For a given K, dierent sam-
plings generally produce dierent weights matrices W. So, to extract a single matrix
Wfor each value of Koneaverages over the samplings. However, before averaging,
one must match the Kcancer signatures across dierent samplings { indeed, in a
given sampling X the columns in the matrix WiAare not necessarily aligned with
11Such as k-means [Steinhaus, 1957], [Lloyd, 1957], [Forgy, 1965], [MacQueen, 1967], [Hartigan,
1975], [Hartigan and Wong, 1979], [Lloyd, 1982].
12As we discuss below, in this regard NMF is not dissimilar.
13E.g., SLINK [Sibson, 1973], etc. (see, e.g., [Murtagh and Contreras, 2011], [Kakushadze and
Yu, 2016c], and references therein).
14E.g., splitting the data into 2 initial clusters.
15Such as quantitative trading, where out-of-sample performance can be objectively measured.
There empirical evidence suggests that such deterministic algorithms underperform so long as
nondeterministic ones are used thoughtfully [Kakushadze and Yu, 2016c].
5",2017-03-02T10:39:59Z,uster mols t iined oso breaki do cancer is alexandra pa at ero tap   seu unr is wi is matrix as is hare knot so of absent should sampli km imax km iand max are extras and for so for one gcancer wi are susteihas lloyd for gy mac queepartisapartisawo lloyd as sib somurtagh contrs u sha yu sutre u sha yu
paper_qf_45.pdf,7,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","the columns in the matrix WiAin a dierent sampling Y. To align the columns in
the matrices Wacross theNSsamplings, once often uses a clustering algorithm such
as k-means. However, since k-means is nondeterministic, such alignment of the W
columns is not guaranteed to { and in fact does not { produce a unique answer. Here
one can try to run multiple samplings of k-means for this alignment and aggregate
them, albeit such aggregation itself would require another level of alignment (with
its own nondeterministic clustering such as k-means).16And one can do this ad
innitum . In practice, one must break the chain at some level of alignment, either
ad hoc (essentially by heuristically observing sucient stability and \\\\convergence"")
or via using a deterministic algorithm (see fn. 16). Either way, invariably all this
introduces (overtly or covertly) systematic and statistical errors into the resultant
cancer signatures and often it is unclear if they are meaningful without invoking
some kind empirical biologic \\\\experience"" or \\\\intuition"" (often based on already
well-known eects of, e.g., exposure to various well-understood carcinogens such as
tobacco, ultraviolet radiation, aatoxin, etc.). At the end of the day it all boils down
to how useful { or predictive { the resultant method of extracting cancer signatures
is, including signature stability. With NMF, the answer is not at all evident...
2.2 Clustering in Disguise?
So, in practice, under the hood, NMF already uses clustering methods. However, it
goes deeper than that. While NMF generically does not produce vanishing weights
for a given signature, some weights are (much) smaller than others. E.g., often
one has several \\\\peaks"" with high concentration of weights, with the rest of the
mutation categories having relatively low weights. In fact, many weights can even
be within the (statistical plus systematic) error bars.17Such weights can for all
practical purposes be set to zero. In fact, we can take this further and ask whether
proliferation of low weights adds any explanatory power. One way to address this is
to run NMF with an additional constraint that the weights (obtained via averaging
{ see above) should be higher than either i) some multiple of the corresponding error
bars18or ii) some preset xed minimum weight. This certainly sounds reasonable,
so why is this not done in practice? A prosaic answer appears to be that this would
complicate the already nontrivial NMF algorithm even further, require additional
coding and computation resources, etc. However, arguendo , let us assume that we
require, say, that the weights be higher than a preset xed minimum weight wminor
else the weights are set to zero. As we increase wmin, the so-modied NMF would
produce more and more zeros. This does not mean that the resulting matrix WiA
16We should point out that at some level of alignment one may employ a deterministic (e.g.,
agglomerative hierarchical { see above) clustering algorithm to terminate the malicious circle, which
can be a reasonable approach assuming there is enough stability in the data. However, this too
adds a(n often hard to quantify and therefore hidden) systematic error to the resultant signatures.
17And such error bars are rarely displayed in the prevalent literature...
18This would require a highly recursive algorithm.
6",2017-03-02T10:39:59Z,wi ito aoss sampli re and ieitr at with usteri disguise so w isuione  as  wi  and 
paper_qf_45.pdf,8,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","would have a binary cluster structure, i.e., that WiA=wiG(i);A, whereABis a
Kronecker delta and G:f1;:::;Ng7!f 1;:::;Kgis a map from N= 96 mutation
categories to Kclusters. Put another way, this does not mean that in the resulting
matrixWiAfor a given i(i.e., mutation category) we would have a nonzero element
for one and only one value of A(i.e., signature). However, as we gradually increase
wmin, generally the matrix WiAis expected to look more and more like having a
binary cluster structure, albeit with some \\\\overlapping"" signatures (i.e., such that in
a given pair of signatures there are nonzero weights for one or more mutations). We
can achieve a binary structure via a number of ways. Thus, a rudimentary algorithm
would be to take the matrix WiA(equally successfully before or after achieving some
zeros in it via nonzero wmin) and for a given value of iset all weights WiAto zero
except in the signature Afor whichWiA= max(WiAjA= 1;:::;K ). Note that this
might result in some empty signatures (clusters), i.e., signatures with WiA= 0 for
all values of i. This can be dealt with by i) ether simply dropping such signatures
altogether and having fewer K0< K signatures (binary clusters) at the end, or ii)
augmenting the algorithm to avoid empty clusters, which can be done in a number
of ways we will not delve into here. The bottom line is that NMF essentially can be
made into a clustering algorithm by reasonably modifying it, including via getting
rid of ubiquitous and not-too-informative low weights. However, the downside would
be an even more contrived algorithm, so this is not what we are suggesting here.
Instead, we are observing that clustering is already intertwined in NMF and the
question is whether we can simplify things by employing clustering methods directly.
2.3 Making Clustering Work
Happily, the answer is yes. Not only can we have much simpler and apparently more
stable clustering algorithms, but they are also computationally much less costly than
NMF. As mentioned above, the biggest issue with using popular nondeterministic
clustering algorithms such as k-means19is that they produce a large number of local
optima. For deniteness in the remainder of this paper we will focus on k-means,
albeit the methods described herein are general and can be applied to other such
algorithms. Fortunately, this very issue has already been addressed in [Kakushadze
and Yu, 2016c] in the context of constructing statistical industry classications (i.e.,
clustering models for stocks) for quantitative trading, so here we simply borrow
therefrom and further expand and adapt that approach to cancer signatures.
2.3.1 K-means
A popular clustering algorithm is k-means [Steinhaus, 1957], [Lloyd, 1957], [Forgy,
1965], [MacQueen, 1967], [Hartigan, 1975], [Hartigan and Wong, 1979], [Lloyd, 1982].
The basic idea behind k-means is to partition Nobservations into Kclusters such
that each observation belongs to the cluster with the nearest mean. Each of the N
19Which are preferred over deterministic ones for the reasons discussed above.
7",2017-03-02T10:39:59Z,wi is krone cker  kg is usters put wi for wi is   wi wi to for wi wi note wi  t instead maki usteri work happy not as for fortunately u sha yu steihas lloyd for gy mac queepartisapartisawo lloyd t observatns usters eawhich
paper_qf_45.pdf,9,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","observations is actually a d-vector, so we have an NdmatrixXis,i= 1;:::;N ,
s= 1;:::;d . LetCabe theKclusters,Ca=fiji2Cag,a= 1;:::;K . Then
k-means attempts to minimize20
g=KX
a=1X
i2CadX
s=1(Xis",2017-03-02T10:39:59Z,is  ca be usters ca ca tcad is
paper_qf_45.pdf,10,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","
r
iainto a single matrix e
ia=PM
r=1
r
ia. Now, this matrix does not look like a
binary clustering matrix. Instead, it is a matrix of occurrence counts, i.e., it counts
how many times a given mutation was assigned to a given cluster in the process
ofMsamplings. What we need to construct is a map Gsuch that one and only
one mutation belongs to each of the Kclusters. The simplest criterion is to map
a given mutation to the cluster in which e
iais maximal, i.e., where said mutation
occurs most frequently. A caveat is that there may be more than one such clusters.
A simple criterion to resolve such an ambiguity is to assign said mutation to the
cluster with most cumulative occurrences (i.e., we assign said mutation to the cluster
with the largest eNa=PN
i=1e
ia). Further, in the unlikely event that there is still an
ambiguity, we can try to do more complicated things, or we can simply assign such
a mutation to the cluster with the lowest value of the index a{ typically, there is so
much noise in the system that dwelling on such minutiae simply does not pay o.
However, we still need to tie up a loose end, to wit, our assumption that the
clusters from dierent runs were somehow all aligned. In practice each run produces
Kclusters, but i) they are not the same clusters and there is no foolproof way of
mapping them, especially when we have a large number of runs; and ii) even if the
clusters were the same or similar, they would not be ordered, i.e., the clusters from
one run generally would be in a dierent order than the clusters from another run.
So, we need a way to \\\\match"" clusters from dierent samplings. Again, there is
no magic bullet here either. We can do a lot of complicated and contrived things
with not much to show for it at the end. A simple pragmatic solution is to use
k-means to align the clusters from dierent runs. Each run labeled by r= 1;:::;M ,
among other things, produces a set of cluster centers Yr
as. We can \\\\bootstrap"" them
by row into a ( KM)dmatrixeYeas=Yr
as, where ea=a+ (r",2017-03-02T10:39:59Z, instead sampli what suusters t na furtr iusters so ag articial intellence  eayr  year yr
paper_qf_45.pdf,11,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","a clustering with some K0Kclusters, and this K0varies from aggregation to ag-
gregation. However, what if we take a large number Pof aggregations (each based
onMsamplings)? Typically there will be a relatively large number of dierent
clusterings we get this way. However, assuming some degree of stability in the data,
this number is much smaller than the number of a priori dierent local minima we
would obtain by running the vanilla k-means algorithm. What is even better, the
occurrence counts of aggregated clusterings are not uniform but typically have a
(sharply) peaked distribution around a few (or manageable) number of aggregated
clusterings. In fact, as we will see below, in our empirical genome data we are able
to pinpoint the \\\\ultimate"" clustering! So, to recap, what we have done here is this.
There are myriad clusterings we can get via vanilla k-means with little to no guid-
ance as to which one to pick.23We have reduced this proliferation by aggregating
a large number of such clusterings into our aggregated clusterings. We then further
zoom onto a few or even a unique clustering we consider to be the likely \\\\ultimate""
clustering by examining the occurrence counts of such aggregated clusterings, which
turns out to have a (sharply) peaked distribution. Since vanilla k-means is a rela-
tively fast-converging algorithm, each aggregation is not computationally taxing and
running a large number of aggregations is nowhere as time consuming as running a
similar number (or even a fraction thereof) of NMF computations (see below).
2.4 What to Cluster?
So, now that we know how to make clustering work, we need to decide what to
cluster, i.e., what to take as our matrix Xisin (1). The na ve choice Xis=Gisis
suboptimal for multiple reasons (as discussed in [Kakushadze and Yu, 2016b]).
First, the elements of the matrix Gisare populated by nonnegative occurrence
counts. Nonnegative quantities with large numbers of samples tend to have skewed
distributions with long tails at higher values. I.e., such distributions are not normal
but (in many cases) roughly log-normal. One simple way to deal with this is to
identifyXiswith a (natural) logarithm of Gis(instead ofGisitself). A minor hiccup
here is that some elements of Giscan be 0. We can do a lot of complicated and even
convoluted things to deal with this issue. Here, as in [Kakushadze and Yu, 2016b],
we will follow a pragmatic approach and do something simple instead { there is so
much noise in the data that doing convoluted things simply does not pay o. So, as
the rst cut, we can take
Xis= ln (1 +Gis) (3)
This takes care of the Gis= 0 cases; for Gis1 we haveRisln(Gis), as desired.
Second, the detailed empirical analysis of [Kakushadze and Yu, 2016b] uncovered
what is termed therein the \\\\overall"" mode24unequivocally present in the occurrence
count data. This \\\\overall"" mode is interpreted as somatic mutational noise unrelated
23This is because things are pretty much random and the only \\\\distribution"" at hand is at.
24In nance the analog of this is the so-called \\\\market"" mode (see, e.g., [Bouchaud and Potters,
2011] and references therein) corresponding to the overall movement of the broad market, which
10",2017-03-02T10:39:59Z,usters of sampli typically what iso tre   since what uster so is it is is is u sha yu rst is are nonegative one is with is is itself is ca re u sha yu so is is  is is is is second u sha yu   i chaters
paper_qf_45.pdf,12,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","to (and in fact obscuring) the true underlying cancer signatures and must therefore
be factored out somehow. Here is a simple way to understand the \\\\overall"" mode.
Let the correlation matrix 	 ij= Cor(Xis;Xjs), where Cor(;) is serial correlation.25
I.e., 	ij=Cij=ij, where2
i=Ciiare variances, and the serial covariance matrix26
Cij= Cov(Xis;Xjs) =1
d",2017-03-02T10:39:59Z,re  cor is xj cor ci ii are ci cov is xj
paper_qf_45.pdf,13,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","2.4.1 Normalizing Log-counts
As was discussed in [Kakushadze and Yu, 2016c], clustering Xis(or equivalently X0
is)
would be suboptimal.29The issue is this. Let 0
ibe serial standard deviations, i.e.,
(0
i)2= Cov(X0
is;X0
is), where, as above, Cov( ;) is serial covariance. Here we assume
that samples are aggregated by cancer types, so s= 1;:::;d withd=n= 14.
Now,0
iare not cross-sectionally uniform and vary substantially across mutation
categories. The density of 0
iis depicted in Figure 1 and is skewed (tailed). The
summary of 0
ireads:30Min = 0.2196, 1st Qu. = 0.3409, Median = 0.4596, Mean =
0.4984, 3rd Qu. = 0.6060, Max = 1.0010, SD = 0.1917, MAD = 0.1859, Skewness
= 0.8498. If we simply cluster X0
is, this variability in 0
iwill not be accounted for.
A simple solution is to cluster normalized demeaned log-counts eX0
is=X0
is=0
i
instead of X0
is. This way we factor out the nonuniform (and skewed) standard
deviation out of the log-counts. Note that now de-noising does make a dierence in
clustering. Indeed, if we use eXis=Xis=i(recall that 2
i= Cov(Xis;Xis)) instead
ofeX0
is=X0
is=0
iin (1) and (2), the quantity g(and also clusterings) will be dierent.
2.5 Fixing Cluster Number
Now that we know what to cluster (to wit, eX0
is) and how to get to the \\\\unique""
clustering, we need to gure out how to x the (target) number of clusters K, which
is one of the inputs in our algorithm above.31In [Kakushadze and Yu, 2016b] it
was argued that in the context of cancer signatures their number can be xed by
building a statistical factor model [Kakushadze and Yu, 2017b], i.e., the number
of signatures is simply the number of statistical factors.32So, by the same token,
here we identify the (target) number of clusters in our clustering algorithm with the
number of statistical factors xed via the method of [Kakushadze and Yu, 2017b].
2.5.1 Eective Rank
So, following [Kakushadze and Yu, 2017b] and [Kakushadze and Yu, 2016b], we set33
K= Round(eRank(	)) (6)
29More precisely, the discussion of [Kakushadze and Yu, 2016c] is in the nancial context, to
wit, quantitative trading, which has its own nuances (see below). However, some of that discussion
is quite general and can be adapted to a wide variety of applications.
30Qu. = Quartile, SD = Standard Deviation, MAD = Mean Absolute Deviation.
31A variety of methods for xing the number of clusters have been discussed in other contexts,
e.g., [Rousseeuw, 1987], [Pelleg and Moore, 2000], [Steinbach et al, 2000], [Goutte et al, 2001], [Sugar
and James, 2003], [Hamerly and Elkan, 2004], [Lleit  et al, 2004], [De Amorim and Hennig, 2015].
32In the nancial context, these are known as statistical risk models [Kakushadze and Yu,
2017b]. For a discussion and literature on multifactor risk models, see, e.g., [Grinold and Kahn,
2000], [Kakushadze and Yu, 2016a] and references therein. For prior works on xing the number
of statistical risk factors, see, e.g., [Connor and Korajczyk, 1993] and [Bai and Ng, 2002].
33Here Round() can be replaced by oor( ) =bc.
12",2017-03-02T10:39:59Z,normalizi log as u sha yu is t  cov cov re  t  t miqu mediameaqu max skew ness   note ined is is cov is is xi uster number  iu sha yu u sha yu so u sha yu rank so u sha yu u sha yu round rank  u sha yu qu quartz e standard viatmeaabsolute viatrows  uw lle moore steinbaout te sugar jamhaer ly  alle it  amor tennis iu sha yu for griold khau sha yu for connor rea jc  articial intellence  re round
paper_qf_45.pdf,14,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Here eRank( Z) is the eective rank [Roy and Vetterli, 2007] of a symmetric semi-
positive-denite (which suces for our purposes here) matrix Z. It is dened as
eRank(Z) = exp(H) (7)
H=",2017-03-02T10:39:59Z,re rank roy ve er li it rank
paper_qf_45.pdf,15,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","So, we wish to approximate G0
isvia a product W H . However, with clustering
we haveWiA=wiG(i);A, i.e., we have a block (cluster) structure where for a given
value ofAallWiAare zero except for i2J(A) =fjjG(j) =Ag, i.e., for the
mutation categories labeled by ithat belong to the cluster labeled by A. Therefore,
our matrix factorization of Gisinto a product W H now simplies into a set of K
independent factorizations as follows:
G0
iswiHAs; i2J(A); A = 1:::;K (11)
So, there is no need to run NMF anymore! Indeed, if we can somehow x HAsfor a
given cluster, then within this cluster we can determine the corresponding weights
wi(i2J(A)) via a serial linear regression:
G0
is=""is+wiHAs; i2J(A); A = 1:::;K (12)
where""isare the regression residuals. I.e., for each A2f1;:::;Kg, we regress
thednAmatrix36[(G0)T]si(i2J(A),nA=jJ(A)j) over thed-vectorHAs(s=
1;:::;d ), and the regression coecients are nothing but the nA-vectorwi(i2J(A)),
while the residuals are the dnAmatrix [("")T]si. Note that this regression is run
without the intercept. Now, this all makes sense as (for each i2J(A)) the regression
minimizes the quadratic error termPd
s=1""2
is. Furthermore, if HAsare nonnegative,
then the weights wiareautomatically nonnegative as they are given by:
wi=Pd
s=1G0
isHG(i);sPd
s=1H2
G(i);s(13)
Now, we wish these weights to be normalized:
X
i2J(A)wi= 1 (14)
This can always be achieved by rescaling HAs. Alternatively, we can pick HAs
without worrying about the normalization, compute wivia (13), rescale them so that
they satisfy (14), and simultaneously accordingly rescale HAs. Mission accomplished!
2.6.1 Fixing Exposures
Well, almost... We still need to gure out how to x the exposures HAs. The
simplest way to do this is to note that we can use the matrix 
 iA=G(i);Ato swap
the indexiinG0
isby the index A, i.e., we can take
HAs=ANX
i=1
iAG0
is=eA1
nAX
i2J(A)G0
is (15)
36The superscript Tdenotes matrix transposition.
14",2017-03-02T10:39:59Z,so wi all wi are ag trefore is into as so ined as for as kg matrix as matrix note  pd furtr as are pd pd   as alternatively as as missxi eosurll  as t to as t notes
paper_qf_45.pdf,16,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","That is, up to the normalization constants eA(which are xed via (14)) we simply
take cross-sectional means of G0
isin each cluster. (Recall that nA=J(A).) The so-
denedHAsare automatically positive as all G0
isare positive. Therefore, widened
via (13) are also all positive. This is a good news { vanishing wiwould amount to an
incomplete weights matrix WiA(i.e., some mutations would belong to no cluster.)
So, why does (15) make sense? Looking at (12), we can observe that, if the
residuals""iscross-sectionally, within each cluster labeled by A, are random, then we
expect thatP
i2J(A)""is0. If we had an exact equality here, then we would have
(15) withA= 1 (i.e., eA=nA) assuming the normalization (14). In practice, the
residuals""isare not exactly \\\\random"". First, the number nAof mutation categories
in each cluster is not large. Second, as mentioned above, there is variability in serial
standard deviations across mutation types. This leads us to consider variations.
2.6.2 A Variation
Above we argued that it makes sense to cluster normalized demeaned log-counts
eX0
is=X0
is=0
idue to the cross-sectional variability (and skewness) in the serial
standard deviations 0
i. We may worry about similar eects in G0
iswhen computing
HAsandwias we did above. This can be mitigated by using normalized quantities
eG0
is=G0
is=!i, where!2
i= Cov(G0
is;G0
is) are serial variances. That is, we can dene37
HAs=eA1
AX
i2J(A)eG0
is=eA1
AX
i2J(A)1
!iG0
is (16)
wi=!iPd
s=1eG0
isHG(i);sPd
s=1H2
G(i);s=Pd
s=1G0
isHG(i);sPd
s=1H2
G(i);s(17)
whereA=P
i2J(A)1=!i. So, 1=!iare the weights in the averages over the clusters.
2.6.3 Another Variation
Here one may wonder, considering the skewed roughly log-normal distribution of Gis
and henceforth G0
is, would it make sense to relate the exposures to within-cluster
cross-sectional averages of demeaned log-counts X0
isas opposed to those of G0
is? This
is easily achieved. Thus, we can dene (this ensures positivity of HAs):
ln(HAs) = ln(eA) +1
nAX
i2J(A)X0
is (18)
Exponentiating we get
HAs=eA2
4Y
i2J(A)G0
is3
51=nA
(19)
37I.e., here we assume that ""is=!iare approximately random in (12).
15",2017-03-02T10:39:59Z,that recall t as are trefore  wi so looki  irst of second  variatabove  as and wi as  cov that as pd pd pd pd so anotr variatre is   as as eonent ti as
paper_qf_45.pdf,17,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","I.e., instead of an arithmetic average as in (15), here we have a geometric average.
As above, here too we can introduce nontrivial weights. Note that the form of
(17) is the same as (13), it is only HAsthat is aected by the weights. So, we can
introduce the weights in the geometric means as follows:
ln(HAs) = ln(eA) +1
AX
i2J(A)eX0
is= ln(eA) +1
AX
i2J(A)1
0
iX0
is (20)
whereA=P
i2J(A)1=0
i. Recall that ( 0
i)2= Cov(X0
is;X0
is). Thus, we have:
HAs=eAY
i2J(A)(G0
is)1=A0
i (21)
So, the weights are the exponents 1 =A0
i. Other variations are also possible.
2.7 Implementation
We are now ready to discuss an actual implementation of the above algorithm,
much of the R code for which is already provided in [Kakushadze and Yu, 2016b]
and [Kakushadze and Yu, 2016c]. The R source code is given in Appendix A hereof.
3 Empirical Results
3.1 Data Summary
In our empirical analysis below we use the same genome data (from published sam-
ples only) as in [Kakushadze and Yu, 2016b]. This data is summarized in Table 1
(borrowed from [Kakushadze and Yu, 2016b]), which gives total counts, number of
samples and the data sources, which are as follows: A1 = [Alexandrov et al, 2013b],
A2 = [Love et al, 2012], B1 = [Tirode et al, 2014], C1 = [Zhang et al, 2013], D1
= [Nik-Zainal et al, 2012], E1 = [Puente et al, 2011], E2 = [Puente et al, 2015],
F1 = [Cheng et al, 2016], G1 = [Wang et al, 2014], H1 = [Sung et al, 2012], H2
= [Fujimoto et al, 2016], I1 = [Imielinksi et al, 2012], J1 = [Jones et al, 2012], K1
= [Patch et al, 2015], L1 = [Waddell et al, 2015], M1 = [Gundem et al, 2015], N1
= [Scelo et al, 2014]. Sample IDs with the corresponding publication sources are
given in Appendix A of [Kakushadze and Yu, 2016b]. In our analysis below we
aggregate samples by the 14 cancer types. The resulting data is in Tables 2 and 3.
3.1.1 Structure of Data
The underlying data consists of a matrix { call it Gis{ whose elements are occurrence
counts of mutation types labeled by i= 1;:::;N = 96 in samples labeled by s=
1;:::;d . More precisely, we can work with one matrix Giswhich combines data from
dierent cancer types; or, alternatively, we may choose to work with individual
16",2017-03-02T10:39:59Z,as note as that so as recall cov  as so otr implementat u sha yu u sha yu t  emical results data suary iu sha yu  table u sha yu alexandra love tico    articial intellence nal plenty plenty c wa su fimoto mie links jonpatwadll gum cell sample ds  u sha yu it tablstruure data t is  is which
paper_qf_45.pdf,18,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","matrices [G()]is, where:= 1;:::;n labelsndierent cancer types; as before,
i= 1;:::;N = 96; ands= 1;:::;d (). Hered() is the number of samples for the
cancer type labeled by . The combined matrix Gisis obtained simply by appending
(i.e., bootstrapping) the matrices [ G()]istogether column-wise. In the case of the
data we use here (see above), this \\\\big matrix"" turns out to have 1389 columns.
Generally, individual matrices [ G()]isand, thereby, the \\\\big matrix"", contain a
lot of noise. For some cancer types we can have a relatively small number of samples.
We can also have \\\\sparsely populated"" data, i.e., with many zeros for some mutation
categories. As mentioned above, dierent samples are not necessarily uniformly
normalized. Etc. The bottom line is that the data is noisy. Furthermore, intuitively
it is clear that the larger the matrix we work with, statistically the more \\\\signatures""
(or clusters) we should expect to get with any reasonable algorithm. However, as
mentioned above, a large number of signatures would be essentially useless and
defy the whole purpose of extracting them in the rst place { we have 96 mutation
categories, so it is clear that the number of signatures cannot be more than 96! If
we end up with, say, 50+ signatures, what new or useful does this tell us about the
underlying cancers? The answer is likely nothing other than that most cancers have
not much in common with each other, which would be a disappointing result from
the perspective of therapeutic applications. To mitigate the aforementioned issues,
at least to a certain extent, following [Kakushadze and Yu, 2016b], we can aggregate
samples by cancer types. This way we get an Nnmatrix, which we also refer to
asGis, where the index s= 1;:::;d now takesd=nvalues corresponding to the
cancer types. In the data we use n= 14, the aggregated matrix Gisis much less
noisy than the \\\\big matrix"", and we are ready to apply the above machinery to it.
3.2 Genome Data Results
The 9614 matrixGisgiven in Tables 2 and 3 is what we pass into the function
bio.cl.sigs() in Appendix A as the input matrix x. We use: iter.max = 100 (this
is the maximum number of iterations used in the built-in R function kmeans() { we
note that there was not a single instance in our 150 million runs of kmeans() where
more iterations were required);38num.try = 1000 (this is the number of individual
k-means samplings we aggregate every time); and num.runs = 150000 (which is the
number of aggregated clusterings we use to determine the \\\\ultimate"" { that is, the
most frequently occurring { clustering). So, we ran k-means 150 million times. More
precisely, we ran 15 batches with num.runs = 10000 as a sanity check, to make sure
that the nal result based on 150000 aggregated clusterings was consistent with the
results based on smaller batches, i.e., that it was in-sample stable.39Based on Table
4, we identify Clustering-A as the \\\\ultimate"" clustering (cf. Clustering-B/C/D).
38The R function kmeans() produces a warning if it does not converge within iter.max .
39We ran these 15 batches consecutively, and each batch produced the same top-10 (by occur-
rence counts) clusterings as in Table 4; however, the actual occurrence counts are dierent across
the batches with slight variability in the corresponding rankings. The results are pleasantly stable.
17",2017-03-02T10:39:59Z, red t is is enlly for  as etc t furtr  t to u sha yu  is iis is  data results t is givetabl  so  based table usteri usteri t  table t
paper_qf_45.pdf,19,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","We give the weights for Clustering-A, Clustering-B, Clustering-C and Clustering-
D using unnormalized and normalized regressions with exposures computed based
on arithmetic averages (see Subsection 2.6) in Tables 5, 6, 7, 8, 9, 10, 11, 12, and
Figures 2 through 55. We give the weights for Clustering-A using unnormalized and
normalized regressions with exposures computed based on geometric averages (see
Subsection 2.6) in Tables 13, 14, and Figures 56 through 69. The actual mutation
categories in each cluster for a given clustering can be read o the aforesaid Tables
with the weights (the mutation categories with nonzero weights belong to a given
cluster), or from the horizontal axis labels in the aforesaid Figures. It is evident that
Clustering-A, Clustering-B, Clustering-C and Clustering-D are essentially variations
of each other (Clustering-D has only 6 clusters, while the other 3 have 7 clusters).
3.3 Reconstruction and Correlations
So, based on genome data, we have constructed clusterings and weights. Do they
work? I.e., do they reconstruct the input data well? It is evident from the get-go
that the answer to this question may not be binary in the sense that for some cancer
types we might have a nice clustering structure, while for others we may not. The
aim of the following exercise is to sort this all out. Here come the correlations...
3.3.1 Within-cluster Correlations
We have our de-noised40matrixG0
is. We are approximating this matrix via the
following factorized matrix:
G
is=KX
A=1WiAHAs=wiHG(i);s (22)
We can now compute an nKmatrix  sAofwithin-cluster cross-sectional correla-
tions between G0
isandG
isdened via (xCor( ;) stands for \\\\cross-sectional correla-
tion"" to distinguish it from \\\\serial correlation"" Cor( ;) we use above)41
sA= xCor(G0
is;G
is)ji2J(A)= xCor(G0
is;wi)ji2J(A) (23)
We give this matrix for Clustering-A with weights using normalized regressions
with exposures computed based on arithmetic means (see Subsection 2.6) in Table
15. Let us mention that, with exposures based on arithmetic means, weights using
normalized regressions work a bit better than using unnormalized regressions. Using
exposures based on geometric means changes the weights a bit, which in turn slightly
aects the within-cluster correlations, but does not alter the qualitative picture.
40De-noising per se does not aect cross-sectional correlations. Adding extra 1 in (3) (recall
that we obtain G0
isby cross-sectionally demeaning Xisand then re-exponentiating) has a negligible
eect. So, in the correlations below we can use the original data matrix Gisinstead of G0
is.
41Due to the factorized structure (22), these correlations do not directly depend on HAs.
18",2017-03-02T10:39:59Z, usteri usteri usteri usteri subsetabls  usteri subsetabls t tabls it usteri usteri usteri usteri usteri reconstrucorrelatns so do it t re withicorrelatns   wi as  matrix of withicor cor cor cor  usteri subsetable  usi  addi is and so is instead due as
paper_qf_45.pdf,20,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","3.3.2 Overall Correlations
Another useful metric, which we use as a sanity check, is this. For each value of s
(i.e., for each cancer type), we can run a linear cross-sectional regression (without
the intercept) of G0
isover the matrix WiA. So, we have n= 14 of these regressions.
Each regression produces multiple R2and adjusted R2, which we give in Table 15.
Furthermore, we can compute the tted valuesbG
isbased on these regressions, which
are given by
bG
is=KX
A=1WiAFAs=wiFG(i);s (24)
where (for each value of s)FAsare the regression coecients. We can now compute
the overall cross-sectional correlations (i.e., the index iruns over all N= 96 mutation
categories)
s= xCor(G0
is;bG
is) (25)
These correlations are also given in Table 15 and measure the overall t quality.
3.3.3 Interpretation
Looking at Table 15 a few things become immediately evident. Clustering works well
for 10 out the 14 cancer types we study here. The cancer types for which clustering
does not appear to work all that well are Breast Cancer (labeled by X4 in Table
15), Liver Cancer (X8), Lung Cancer (X9), and Renal Cell Carcinoma (X14). More
precisely, for Breast Cancer we do have a high within-cluster correlation for Cl-5 (and
also Cl-4), but the overall t is not spectacular due to low within-cluster correlations
in other clusters. Also, above 80% within-cluster correlations42arise for 5 clusters,
to wit, Cl-1, Cl-3, Cl-4, Cl-5 and Cl-6, but not for Cl-2 or Cl-7. Furthermore,
remarkably, Cl-1 has high within-cluster correlations for 9 cancer types, and Cl-5
for 6 cancer types. These appear to be the leading clusters. Together they have
high within-cluster correlations in 11 cancer types. So what does all this mean?
Additional insight is provided by looking at the within-cluster correlations be-
tween the 7 cancer signature extracted in [Kakushadze and Yu, 2016b] and the
clusters we nd here. Let Wibe the weights for the 7 cancer signatures from
Tables 13 and 14 of [Kakushadze and Yu, 2016b]. We can compute the following
within-cluster correlations ( = 1;:::; 7 labels the cancer signatures of [Kakushadze
and Yu, 2016b], which we refer to as Sig1 through Sig7):
A= xCor(Wi;WiA)ji2J(A) (26)
These correlations are given in Table 16. High within-cluster correlations arise
for Cl-1 (with Sig1 and Sig7), Cl-5 (with Sig2) and Cl-6 (with Sig4). And this
makes perfect sense. Indeed, looking at Figures 14 through 20 of [Kakushadze and
42The 80% cuto is somewhat arbitrary, but reasonable.
19",2017-03-02T10:39:59Z,ovll correlatns anotr for wi so eatable furtr wi as as are  cor tse table interpatlooki table usteri t breast cancer table licancer lu cancer renal cell carcinoma  breast cancer   also        furtr   tse togetr so additnal u sha yu  wi tablu sha yu  u sha yu s s cor wi wi tse table hh  s s  s  s and ined s u sha t
paper_qf_45.pdf,21,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Yu, 2016b], Sig1, Sig2, Sig4 and Sig7 are precisely the cancer signatures that have
\\\\peaks"" (or \\\\spikes"" { \\\\tall mountain landscapes""), whereas Sig3, Sig5 and Sig6
do not have such \\\\peaks"" (\\\\at"" or \\\\rolling hills landscapes""). No wonder such
signatures do not have high within-cluster correlations { they simply do not have
cluster-like structures. Looking at Figure 21 in [Kakushadze and Yu, 2016b], it
becomes evident why clustering does not work well for Liver Cancer (X8) { it has a
whopping 96% contribution from Sig5! Similarly, Renal Cell Carcinoma (X14) has
a 70% contribution from Sig6. Lung Cancer (X9) is dominated by Sig3, hence no
cluster-like structure. Finally, Breast Cancer (X4) is dominated by Sig2, which has
a high within-cluster correlation with Cl-5, which is why Breast Cancer has a high
within-cluster correlation with Cl-5 (but poor overall correlation in Table 15). So, it
all makes sense. The question is, what does all this tell us about cancer signatures?
Quite a bit! It tells us that cancers such as Liver Cancer, Lung Cancer and Renal
Cell Carcinoma have little in common with other cancers (and each other)! At least
at the level of mutation categories that dominate the genome structure of such can-
cers. On the other hand, 9 cancers, to wit, Bone Cancer (X2), Brain Lower Grade
Glioma (X3), Chronic Lymphocytic Leukemia (X5), Esophageal Cancer (X6), Gas-
tric Cancer (X7), Medulloblastoma (X10), Ovarian Cancer (X11), Pancreatic Cancer
(X12) and Prostate Cancer (X13) apparently all have the Cl-1 cluster structure em-
bedded in them substantially. Similarly, 6 cancers, to wit, B Cell Lymphoma (X1),
Breast Cancer (X4), Esophageal Cancer(X6), Ovarian Cancer (X11), Pancreatic
Cancer (X12) and Prostate Cancer (X13) apparently all have the Cl-5 cluster struc-
ture embedded in them substantially. Furthermore, note the overlap between these
two lists, to wit, Esophageal Cancer(X6), Ovarian Cancer (X11), Pancreatic Cancer
(X12) and Prostate Cancer (X13). We obtained this result purely statistically, with
no biologic input, using our clustering algorithm and other statistical methods such
as linear regression to obtain the actual weights. It is too early to know whether
this insight will aid any therapeutic applications, but that is the hope { similari-
ties in the underlying genomic structures of dierent cancer types raise hope that
therapeutics for one cancer type could perhaps be applicable to other cancer types.
On the other hand, our ndings above relating to Liver Cancer, Lung Cancer and
Renal Cell Carcinoma (and possibly also Breast Cancer, albeit the latter does ap-
pear to have a not-so-insignicant overlap with Cl-5, which dierentiates it from the
aforesaid 3 cancer types) suggest that these cancer types apparently stand out.
4 Concluding Remarks
Clustering ideas and techniques have been applied in cancer research in various
incarnations and contexts aplenty { for a partial list of works at least to some
extent related to our discussion here, see, e.g, [Chen et al, 2008a], [Chen et al, 2008b],
[Kashuba et al, 2009], [Nik-Zainal et al, 2012], [Roberts et al, 2012], [Alexandrov et al,
2013a], [Alexandrov et al, 2013b], [Burns et al, 2013a], [Burns et al, 2013b], [Lawrence
20",2017-03-02T10:39:59Z,yu s s s s s s s no looki  u sha yu licancer s simarly renal cell carcinoma s lu cancer s nally breast cancer s  breast cancer  table so t quite it licancer lu cancer renal cell carcinoma at obone cancer br articial intellence lor gra lima chronic lymph cy tic leukemia esophageal cancer gas cancer me dull oblast oma ovariacancer paneatic cancer prostate cancer  simarly cell lymphoma breast cancer esophageal cancer ovariacancer paneatic cancer prostate cancer  furtr esophageal cancer ovariacancer paneatic cancer prostate cancer  it olicancer lu cancer renal cell carcinoma breast cancer  conudi remarks usteri ccas hub  articial intellence nal roberts alexandra alexandra burns burns lawrence
paper_qf_45.pdf,22,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","et al, 2013], [Long et al, 2013] [Roberts et al, 2013], [Taylor et al, 2013], [Xuan
et al, 2013], [Alexandrov and Stratton, 2014], [Bacolla et al, 2014], [Bolli et al,
2014], [Caval et al, 2014], [Davis et al, 2014], [Helleday et al, 2014], [Nik-Zainal
et al, 2014], [Poon et al, 2014], [Qian et al, 2014], [Roberts and Gordenin, 2014a],
[Roberts and Gordenin, 2014b], [Roberts and Gordenin, 2014c], [Sima and Gilbert,
2014], [Chan and Gordenin, 2015], [Pettersen et al, 2015] and references therein. As
mentioned above, even in NMF clustering is used at some (perhaps not-so-evident)
layer. What is new in our approach { and hence new results { is that: i) following
[Kakushadze and Yu, 2016b], we apply clustering to aggregated by cancer types and
de-noised data; ii) we use a tried-and-tested in quantitative nance bag of tricks from
[Kakushadze and Yu, 2016c], which improves clustering; and iii) last but not least,
we apply our *K-means algorithm to cancer genome data. As mentioned above, *K-
means, unlike vanilla k-means or its other commonly used variations, is essentially
deterministic, and it achieves determinism statistically , not by \\\\guessing"" initial
centers or as in agglomerative hierarchical clustering, which basically \\\\guesses"" the
initial (e.g., 2-cluster) clustering. Instead, via aggregating a large number of k-means
clusterings and statistical examination of the occurrence counts of such aggregations,
*K-means takes a mess of myriad vanilla k-means clusterings and systematically
reduces randomness and indeterminism without ad hoc initial \\\\guesswork"".
As mentioned above, consistently with the results of [Kakushadze and Yu, 2016b]
obtained via improved NMF techniques, Liver Cancer, Lung Cancer and Renal Cell
Carcinoma do not appear to have clustering (sub)structures. This could be both
good and bad news. It is a good news because we learned something interesting
about these cancer types { and in two complementary ways. However, it could also
be a bad news from the therapeutic standpoint. Since these cancer types appear to
have little in common with others, it is likely that they would require specialized
therapeutics. On the ipside, we should note that it would make sense to exclude
these 3 cancer types when running clustering analysis. However, it would also make
sense to include other cancer types by utilizing the International Cancer Genome
Consortium data, which we leave for future studies. (For comparative reasons, here
we used the same data as in [Kakushadze and Yu, 2016b], which was limited to
data samples published as of the date thereof.) This paper is not intended to be an
exhaustive empirical study but a proof of concept and an opening of a new avenue
for extracting and studying cancer signatures beyond the tools that NMF provides.
And we do nd that 11 out of the 14 cancer types we study here have clustering
structures substantially embedded in them and clustering overall works well for at
least 10 out of these 11 cancer types.43Now, looking at Figure 14 of [Kakushadze
and Yu, 2016b], we see that its \\\\peaks"" are located at ACGT, CCGT, GCGT and
TCGT. The same \\\\peaks"" are present in our cluster Cl-1 (see Figures 2 and 3).
Hence the high within-cluster correlation between Cl-1 and Sig1. On the other
43Breast Cancer possibly being an exception. As mentioned above, it would make sense to
exclude Liver Cancer, Lung Cancer and Renal Cell Carcinoma from the analysis, which may aect
how well clustering works for Breast Cancer and possibly also the other 10 cancer types.
21",2017-03-02T10:39:59Z,lo roberts taylor juaalexandra statba coll bold ce dis ll day  articial intellence nal po oiaroberts gariroberts gariroberts garisima gbert chagaribeer seas what u sha yu u sha yu as instead as u sha yu licancer lu cancer renal cell carcinoma  it since ointernatnal cancer  consortium for u sha yu  and   u sha yu t  s nce  s obreast cancer as licancer lu cancer renal cell carcinoma breast cancer
paper_qf_45.pdf,23,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","hand, Sig1 of [Kakushadze and Yu, 2016b] is essentially the same as the mutational
signature 1 of [Nik-Zainal et al, 2012], [Alexandrov et al, 2013b], which is due to
spontaneous cytosine deamination. So, this is what our cluster Cl-1 describes. Next,
looking at Figure 15 of [Kakushadze and Yu, 2016b], we see that its \\\\peaks"" are
located at TCAG, TCTG, TCAT and TCTT. The rst two of these \\\\peaks"" TCAG
and TCTG are present in our Cl-5 (see Figures 10 and 11), the third \\\\peak"" TCAT
is present in our Cl-1 (see Figures 2 and 3), while the fourth \\\\peak"" TCTT is present
in our Cl-4 (see Figures 8 and 9), which is consistent with the high within-cluster
correlations between Sig2 and Cl-4 and Cl-5, albeit its within-cluster correlation
with Cl-1 is poor. Note that Sig2 of [Kakushadze and Yu, 2016b] is essentially the
same as the mutational signatures 2+13 of [Nik-Zainal et al, 2012], [Alexandrov et al,
2013b], which are due to APOBEC mediated cytosine deamination. In fact, it was
reported as a single signature in [Alexandrov et al, 2013b], however, subsequently, it
was split into 2 distinct signatures, which usually appear in the same samples.44Our
clustering results indicate that grouping TCAG and TCTG into one signature makes
sense as they belong to the same cluster Cl-5. However, grouping TCAT and TCTT
together does not appear to make much sense. Looking at the Figures for Clustering-
A, Clustering-B, Clustering-C and Clustering-D, we see that the TCAT \\\\peak""
invariably appears together with the ACGT, CCGT, GCGT and TCGT \\\\peaks""
as in Cl-1 in Clustering-A, Cl-2 in Clustering-B, Cl-1 in Clustering-C, and Cl-1 in
Clustering-D, but never with TCTT. So, our clustering approach tells us something
new beyond the NMF \\\\intuition"". This may have an important implication for
Breast Cancer, which, as mentioned above, is dominated by Sig2. Thus, based on
our results in Table 15, we see that Breast Cancer has high within-cluster correlations
with Cl-4 and Cl-5, but not with Cl-1. This may imply that clustering simply does
not work well for Breast Cancer, which would appear to put it in the same \\\\stand-
alone"" league as Liver Cancer, Lung Cancer and Renal Cell Carcinoma. In any
event, clustering invariably suggests that the TCAT \\\\peak"" belongs in Cl-1 with
the 4 \\\\peaks"" ACGT, CCGT, GCGT and TCGT related to spontaneous cytosine
deamination, rather than those related to APOBEC mediated cytosine deamination.
Now, let us check the remaining two signatures of [Kakushadze and Yu, 2016b]
with \\\\tall mountain landscapes"" (see above), to wit, Sig4 and Sig7. Looking at
Figure 17 of [Kakushadze and Yu, 2016b], we see that its \\\\peaks"" are at CTTC,
TTTC, CTTG and TTTG. The same peaks appear in our Cl-6 (see Figures 12
and 13). Hence the high within-cluster correlation between Cl-6 and Sig4. Note
that Sig4 is essentially the same as the mutational signature 17 of [Nik-Zainal et al,
2012], [Alexandrov et al, 2013b], and its underlying mutational process is unknown.
Next, looking at Figure 20 of [Kakushadze and Yu, 2016b], we see that its \\\\peaks""
for the C>G mutations are essentially the same as in Cl-1. Hence the high within-
cluster correlation between Cl-7 and Sig1. So, there are no surprises with Sig1, Sig4
and Sig7. However, based on our clustering results, as we discuss above, with Sig2
44For detailed comments, see http://cancer.sanger.ac.uk/cosmic/signatures.
22",2017-03-02T10:39:59Z,s u sha yu  articial intellence nal alexandra so  next  u sha yu t  s  s  s s    note s u sha yu  articial intellence nal alexandra ialexandra our  looki s usteri usteri usteri usteri  usteri  usteri  usteri  usteri so  breast cancer s  table breast cancer     breast cancer licancer lu cancer renal cell carcinoma i  u sha yu s s looki  u sha yu t  s nce  s note s  articial intellence nal alexandra next  u sha yu  nce  s so s s s s for
paper_qf_45.pdf,24,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","we do nd { what we feel is a pleasant { surprise, that splitting it into two signatures
(see above) might be inadequate and the TCAT \\\\peak"" might really belong with
the Sig1 \\\\peaks"" (spontaneous v. APOBEC mediated cytosine deamination). This
is exciting as it might be an indication of the limitations of NMF (or clustering...).45
In Introduction we promised that we would discuss some potential applications
of *K-means in quantitative nance, and so here it is. Let us mention that *K-means
is universal, oblivious to the input data and applicable in a variety of elds. In quan-
titative nance *K-means a priori can be applied everywhere clustering methods are
used with the added bonus of (statistical) determinism.46One evident example is
statistical industry classications discussed in [Kakushadze and Yu, 2016c], where
one uses clustering methods to classify stocks. In fact, *K-means is an extension of
the methods discussed in [Kakushadze and Yu, 2016c]. One thing to keep in mind
is that in *K-means one sifts through a large number Pof aggregations, which can
get computationally costly when clustering 2000+ stocks into 100+ clusters.47An-
other potential application is in the context of combining alphas (trading signals)
{ see, e.g., [Kakushadze and Yu, 2017a]. Yet another application is when we have
a term structure, such as a portfolio of bonds (e.g., U.S. Treasuries or some other
bonds) with varying maturities, or futures (e.g., Eurodollar futures) with varying
deliveries. These cases resemble the genome data more in the sense that the number
Nof instruments is relatively small (typically even fewer than the number of mu-
tation categories). Another example with a relatively small number of instruments
would be a portfolio of various futures for dierent FX (foreign exchange) pairs (even
with the uniform delivery), e.g., USD/EUR, USD/HKD, EUR/AUD, etc., i.e., FX
statistical arbitrage. One approach to optimizing risk in such portfolios is by em-
ploying clustering methods and a stable, essentially deterministic algorithm such as
*K-means can be useful. Hopefully *K-means will prove a valuable tool in cancer
research, quantitative nance as well as various other elds (e.g., image recognition).
A R Source Code
In this appendix we give the R (R Package for Statistical Computing, http://www.r-
project.org) source code for computing the clusterings and weights using the algo-
rithms of Section 2. The code is straightforward and self-explanatory.48The main
function is bio.cl.sigs(x, iter.max = 100, num.try = 1000, num.runs = 10000) .
Here: xis theNdoccurrence counts matrix Gis(whereN= 96 is the number of
45Or both... Alternatively { and that would be truly exciting { perhaps there is a biologic
explanation. In any event, it is too early to tell { yet another possibility is that this is merely an
artifact of the dataset we use. More research and analyses on larger datasets (see above) is needed.
46Albeit with the understanding that it requires additional computational cost.
47This can be mitigated by employing top-down clustering [Kakushadze and Yu, 2016c].
48The source code in Appendix A hereof is not written to be \\\\fancy"" or optimized for speed or
in any other way. Its sole purpose is to illustrate the algorithms described in the main text in a
simple-to-understand fashion. See Appendix B for some important legalese.
23",2017-03-02T10:39:59Z,s  iintrodu ione u sha yu iu sha yu one of au sha yu yet treasureuro dollar tse of anotr one hopefully source co ipackage statistical uti set t re is or alternatively i albeit  u sha yu t  its  
paper_qf_45.pdf,25,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","mutation categories, and dis the number of samples; or d=n, wherenis the number
of cancer types, when the samples are aggregated by cancer types); iter.max is the
maximum number of iterations that are passed into the R built-in function kmeans() ;
num.try is the number Mof aggregated clusterings (see Subsection 2.3.2); num.runs
is the number of runs Pused to determine the most frequently occurring clustering
(the \\\\ultimate"" clustering) obtained via aggregation (see Subsection 2.3.3). The
function bio.erank.pc() is dened in Appendix B of [Kakushadze and Yu, 2016b].
The function qrm.stat.ind.class() is dened in Appendix A of [Kakushadze and
Yu, 2016c]. This function internally calls another function qrm.calc.norm.ret() ,
which we redene here via the function bio.calc.norm.ret() .49The output is a
list, whose elements are as follows: res$ind is anNKbinary matrix 
 iA=G(i);A
(i= 1;:::;N ,A= 1;:::;K , the mapG:f1;:::;Ng7!f 1;:::;Kg{ see Section 2),
which denes the Kclusters in the \\\\ultimate"" clustering;50res$w is anN-vector of
weights obtained via unnormalized regressions using arithmetic means for comput-
ing exposures (i.e., via (13), (14) and (15)); res$v is anN-vector of weights obtained
via normalized regressions using arithmetic means for computing exposures (i.e., via
(17), (14) and (16)); res$w.g is anN-vector of weights obtained via unnormalized
regressions using geometric means for computing exposures (i.e., via (13), (14) and
(19)); res$v.g is anN-vector of weights obtained via normalized regressions using
geometric means for computing exposures (i.e., via (17), (14) and (21)).
bio.calc.norm.ret <- function (ret)
f
s <- apply(ret, 1, sd)
x <- ret / s
return(x)
g
qrm.calc.norm.ret <- bio.calc.norm.ret
bio.cl.sigs <- function(x, iter.max = 100,
num.try = 1000, num.runs = 10000)
f
cl.ix <- function(x) match(1, x)
y <- log(1 + x)
y <- t(t(y) - colMeans(y))
49The denition of qrm.calc.norm.ret() in [Kakushadze and Yu, 2016c] accounts for some
peculiarities and nuances pertinent to quantitative trading, which are not applicable here.
50The code returns the Kclusters ordered such that the number of mutation nA(i.e., the
column sum of 
 iA) in the cluster labeled by Ais in the increasing order. It also orders clusters
with identical nA. We note, however, that (for presentational convenience reasons) the order of
such clusters in the tables and gures below is not necessarily the same as what this code returns.
24",2017-03-02T10:39:59Z,of subseused subset  u sha yu t  u sha yu  t binary  kg seusters means t u sha yu t usters is it 
paper_qf_45.pdf,27,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","s <- apply(log(x), 1, sd)
else
s <- rep(1, nrow(x))
s <- 1 / s / sum(1 / s)
fac <- apply(x^s, 2, prod)
g
else
f
if(use.wts)
s <- apply(x, 1, sd)
else
s <- rep(1, nrow(x))
fac <- colMeans(x / s)
g
w <- coefficients(lm(t(x) -1 + fac))
w <- 100 * w / sum(w)
return(w)
g
n <- nrow(x)
w <- w.g <- v <- v.g <- rep(NA, n)
z <- colSums(ind)
z <- as.numeric(paste(z, ""."", apply(ind, 2, first.ix), sep = """"))
dimnames(ind)[[2]] <- names(z) <- 1:ncol(ind)
z <- sort(z)
z <- names(z)
ind <- ind[, z]
dimnames(ind)[[2]] <- NULL
for(i in 1:ncol(ind))
f
take <- ind[, i] == 1
if(sum(take) == 1)
f
w[take] <- w.g[take] <- 1
v[take] <- v.g[take] <- 1
next
g
w[take] <- calc.wts(x[take, ], F, F)
w.g[take] <- calc.wts(x[take, ], F, T)
v[take] <- calc.wts(x[take, ], T, F)
26",2017-03-02T10:39:59Z,means sums
paper_qf_45.pdf,28,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","v.g[take] <- calc.wts(x[take, ], T, T)
g
res <- new.env()
res$ind <- ind
res$w <- w
res$w.g <- w.g
res$v <- v
res$v.g <- v.g
return(res)
g
B DISCLAIMERS
Wherever the context so requires, the masculine gender includes the feminine and/or
neuter, and the singular form includes the plural and vice versa . The author of this
paper (\\\\Author"") and his aliates including without limitation QuantigicrSolu-
tions LLC (\\\\Author's Aliates"" or \\\\his Aliates"") make no implied or express
warranties or any other representations whatsoever, including without limitation
implied warranties of merchantability and tness for a particular purpose, in con-
nection with or with regard to the content of this paper including without limitation
any code or algorithms contained herein (\\\\Content"").
The reader may use the Content solely at his/her/its own risk and the reader
shall have no claims whatsoever against the Author or his Aliates and the Author
and his Aliates shall have no liability whatsoever to the reader or any third party
whatsoever for any loss, expense, opportunity cost, damages or any other adverse
eects whatsoever relating to or arising from the use of the Content by the reader
including without any limitation whatsoever: any direct, indirect, incidental, spe-
cial, consequential or any other damages incurred by the reader, however caused
and under any theory of liability; any loss of prot (whether incurred directly or
indirectly), any loss of goodwill or reputation, any loss of data suered, cost of pro-
curement of substitute goods or services, or any other tangible or intangible loss;
any reliance placed by the reader on the completeness, accuracy or existence of the
Content or any other eect of using the Content; and any and all other adversities
or negative eects the reader might encounter in using the Content irrespective of
whether the Author or his Aliates is or are or should have been aware of such
adversities or negative eects.
The R code included in Appendix A hereof is part of the copyrighted R code
of QuantigicrSolutions LLC and is provided herein with the express permission of
QuantigicrSolutions LLC. The copyright owner retains all rights, title and interest
in and to its copyrighted source code included in Appendix A hereof and any and
all copyrights therefor.
27",2017-03-02T10:39:59Z,w author quant ic so lu author content t content author author content content content content author t  quant ic solutns quant ic solutns t 
paper_qf_45.pdf,29,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","References
Alexandrov, L.B., Nik-Zainal, S., Wedge, D.C., Campbell, P.J. and Stratton,
M.R. (2013a) Deciphering Signatures of Mutational Processes Operative in Hu-
man Cancer. Cell Reports 3(1): 246-259.
Alexandrov, L.B., Nik-Zainal, S., Wedge, D.C., Aparicio, S.A., Behjati, S.,
Biankin, A.V., Bignell, G.R., Bolli, N., Borg, A., Brresen-Dale, A.L., Boyault,
S., Burkhardt, B., Butler, A.P., Caldas, C., Davies, H.R., Desmedt, C., Eils,
R., Eyfj ord, J.E., Foekens, J.A., Greaves, M., Hosoda, F., Hutter, B., Ilicic,
T., Imbeaud, S., Imielinski, M., J ager, N., Jones, D.T., Jones, D., Knappskog,
S., Kool, M., Lakhani, S.R., L opez-Ot n, C., Martin, S., Munshi, N.C., Naka-
mura, H., Northcott, P.A., Pajic, M., Papaemmanuil, E., Paradiso, A., Pearson,
J.V., Puente, X.S., Raine, K., Ramakrishna, M., Richardson, A.L., Richter, J.,
Rosenstiel, P., Schlesner, M., Schumacher, T.N., Span, P.N., Teague, J.W.,
Totoki, Y., Tutt, A.N., Vald es-Mas, R., van Buuren, M.M., van 't Veer, L.,
Vincent-Salomon, A., Waddell, N., Yates, L.R.; Australian Pancreatic Can-
cer Genome Initiative; ICGC Breast Cancer Consortium; ICGC MMML-Seq
Consortium; ICGC PedBrain, Zucman-Rossi, J., Futreal, P.A., McDermott,
U., Lichter, P., Meyerson, M., Grimmond, S.M., Siebert, R., Campo, E., Shi-
bata, T., Pster, S.M., Campbell, P.J., Stratton, M.R. (2013b) Signatures of
mutational processes in human cancer. Nature 500(7463): 415-421.
Alexandrov, L.B. and Stratton, M.R. (2014) Mutational signatures: the pat-
terns of somatic mutations hidden in cancer genomes. Current Opinion in Ge-
netics & Development 24: 52-60.
Ananthaswamy, H.N. and Pierceall, W.E. (1990) Molecular mechanisms of
ultraviolet radiation carcinogenesis. Photochemistry and Photobiology 52(6):
1119-1136.
Bacolla, A., Cooper, D.N. and Vasquez, K.M. (2014) Mechanisms of base sub-
stitution mutagenesis in cancer genomes. Genes 5(1): 108-146.
Bai, J. and Ng, S. (2002) Determining the number of factors in approximate
factor models. Econometrica 70(1): 191-221.
Bolli, N., Avet-Loiseau, H., Wedge, D.C., Van Loo, P., Alexandrov, L.B., Mar-
tincorena, I., Dawson, K.J., Iorio, F., Nik-Zainal, S., Bignell, G.R., Hinton,
J.W., Li, Y., Tubio, J.M., McLaren, S., O' Meara, S., Butler, A.P., Teague,
J.W., Mudie, L., Anderson, E., Rashid, N., Tai, Y.T., Shammas, M.A., Sper-
ling, A.S., Fulciniti, M., Richardson, P.G., Parmigiani, G., Magrangeas, F.,
Minvielle, S., Moreau, P., Attal, M., Facon, T., Futreal, P.A., Anderson, K.C.,
Campbell, P.J., Munshi, N.C. (2014) Heterogeneity of genomic evolution and
mutational proles in multiple myeloma. Nature Communications 5: 2997.
28",2017-03-02T10:39:59Z,referencalexandra  articial intellence nal dge campbell statcipri snaturmtal processoative hu cancer cell rets alexandra  articial intellence nal dge par c be jai bikib  bold borg dale boy all bulk hard butler calls dis edit  ey fj foe lens grho soda uer  ici im be aud me i jonjonapp s tola khaot martimushi aka northco pa ji papa ea nui dise pearsoplenty articial intellence ne ramakrishna richardsorichter rosetl slner schumacr spaleague to toi bu valid mas but reevincent salmowadll dataustraliapaneatic ca initiative breast cancer consortium seq consortium ped br articial intellence zu cm arossi fu real mc r most liter meters ogri obieber campo shi campbell statsnaturnature alexandra statmtal current opinge velopment samantha swamp pierce all molecular photo cmistry photo blogy ba coll coo squez meisms genarticial intellence  termini econometric bold vet oise dge valoo alexandra mar dawso r  articial intellence nal b  htoli tu b mc aremara butler league ma anrsorapid articial intellence sharma  ful iit richardsoparti gaimag rae as mivle  at tal fac ofu real anrsocampbell mushi terogeneity nature counicatns
paper_qf_45.pdf,30,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Bouchaud, J.-P. and Potters, M. (2011) Financial applications of random matrix
theory: a short review. In: Akemann, G., Baik, J. and Di Francesco, P. (eds.)
The Oxford Handbook of Random Matrix Theory. Oxford, United Kingdom:
Oxford University Press.
Burns, M.B., Lackey, L., Carpenter, M.A., Rathore, A., Land, A.M., Leonard,
B., Refsland, E.W., Kotandeniya, D., Tretyakova, N., Nikas, J.B., Yee, D.,
Temiz, N.A., Donohue, D.E., McDougle, R.M., Brown, W.L., Law, E.K., Harris,
R.S. (2013a) APOBEC3B is an enzymatic source of mutation in breast cancer.
Nature 494(7437): 366-370.
Burns, M.B., Temiz, N.A. and Harris, R.S. (2013b) Evidence for APOBEC3B
mutagenesis in multiple human cancers. Nature Genetics 45(9): 977-983.
Caval, V., Susp ene, R., Shapira, M., Vartanian, J.P. and Wain-Hobson, S.
(2014) A prevalent cancer susceptibility APOBEC3A hybrid allele bearing
APOBEC3B 30UTR enhances chromosomal DNA damage. Nature Communi-
cations 5: 5129.
Campbell, L.L. (1960) Minimum coecient rate for stationary random pro-
cesses. Information and Control 3(4): 360-371.
Chan, K. and Gordenin, D.A. (2015) Clusters of Multiple Mutations: Incidence
and Molecular Mechanisms. Annual Review of Genetics 49: 243-627
Chen, Z., Feng, J., Buzin, C.H. and Sommer, S.S. (2008a) Epidemiology of
doublet/multiplet mutations in lung cancers: evidence that a subset arises by
chronocoordinate events. PloS One 3(11): e3714.
Chen, Z., Feng, J., Saldivar, J.S., Gu, D., Bockholt, A., and Sommer, S.S.
(2008b) EGFR somatic doublets in lung cancer are frequent and generally arise
from a pair of driver mutations uncommonly seen as singlet mutations: one-
third of doublets occur at ve pairs of amino acids. Oncogene 27(31): 4336-4343.
Cheng, C., Zhou, Y., Li, H., Xiong, T., Li, S., Bi, Y., Kong, P., Wang, F.,
Cui, H., Li, Y., Fang, X., Yan, T., Li, Y., Wang, J., Yang, B., Zhang, L., Jia,
Z., Song, B., Hu, X., Yang, J., Qiu, H., Zhang, G., Liu, J., Xu, E., Shi, R.,
Zhang, Y., Liu, H., He, C., Zhao, Z., Qian, Y., Rong, R., Han, Z., Zhang, Y.,
Luo, W., Wang, J., Peng, S., Yang, X., Li, X., Li, L., Fang, H., Liu, X., Ma,
L., Chen, Y., Guo, S., Chen, X., Xi, Y., Li, G., Liang, J., Yang, X., Guo, J.,
Jia, J., Li, Q., Cheng, X., Zhan, Q., Cui, Y. (2016) Whole-Genome Sequencing
Reveals Diverse Models of Structural Variations in Esophageal Squamous Cell
Carcinoma. The American Journal of Human Genetics 98(2): 256-274.
Connor, G. and Korajczyk, R.A. (1993) A Test for the Number of Factors in
an Approximate Factor Model. The Journal of Finance 48(4): 1263-1291.
29",2017-03-02T10:39:59Z, chaters nancial ike manarticial intellence di france t oxford handbook random matrix tory oxford united kidom oxford   burns lacked carpenter math ore land leonard refs land t and eni ya tre v  as yee te  donohue mc doug le browlaw harris nature burns te  harris evince nature genetics ce usp sha ira tartaiaarticial intellence poisonature com muni campbell minimum informatcontrchagariusters multiple mtns incince molecular meisms annual review genetics cfe but isuer epimlogy plo one cfe saliva gu book holt suer onene c hou li so li bi  wa cui li fa yali wa ya  jia so hu ya qi   xu shi    hao iaso ha lu wa pe ya li li fa  ma cguo cxi li lia ya guo jia li c zh acui whole  sequenci reveals diverse mols struural variatns esophageal quad us cell carcinoma t journal humagenetics connor rea jc  test number faors approximate faor mt journal nance
paper_qf_45.pdf,31,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Davis, C.F., Ricketts, C.J., Wang, M., Yang, L., Cherniack, A.D., Shen, H.,
Buhay, C., Kang, H., Kim, S.C., Fahey, C.C., Hacker, K.E., Bhanot, G., Gor-
denin, D.A., Chu, A., Gunaratne, P.H., Biehl, M., Seth, S., Kaipparettu, B.A.,
Bristow, C.A., Donehower, L.A., Wallen, E.M., Smith, A.B., Tickoo, S.K., Tam-
boli, P., Reuter, V., Schmidt, L.S., Hsieh, J.J., Choueiri, T.K., Hakimi, A.A.;
Cancer Genome Atlas Research Network, Chin, L., Meyerson, M., Kucherlap-
ati, R., Park, W.Y., Robertson, A.G., Laird, P.W., Henske, E.P., Kwiatkowski,
D.J., Park, P.J., Morgan, M., Shuch, B., Muzny, D., Wheeler, D.A., Line-
han, W.M., Gibbs, R.A., Rathmell, W.K., Creighton, C.J. (2014) The somatic
genomic landscape of chromophobe renal cell carcinoma. Cancer Cell 26(3):
319-330.
De Amorim, R.C. and Hennig, C. (2015) Recovering the number of clusters
in data sets with noise features using feature rescaling factors. Information
Sciences 324: 126-145.
Forgy, E.W. (1965) Cluster analysis of multivariate data: eciency versus in-
terpretability of classications. Biometrics 21(3): 768-769.
Fujimoto, A., Furuta, M., Totoki, Y., Tsunoda, T., Kato, M., Shiraishi, Y.,
Tanaka, H., Taniguchi, H., Kawakami, Y., Ueno, M., Gotoh, K., Ariizumi, S.,
Wardell, C.P., Hayami, S., Nakamura, T., Aikata, H., Arihiro, K., Boroevich,
K.A., Abe, T., Nakano, K., Maejima, K., Sasaki-Oku, A., Ohsawa, A., Shibuya,
T., Nakamura, H., Hama, H., Hosoda, F., Arai, Y., Ohashi, S., Urushidate, T.,
Nagae, G., Yamamoto, S., Ueda, H., Tatsuno, K., Ojima, H., Hiraoka, N.,
Okusaka, T., Kubo, M., Marubashi, S., Yamada, T., Hirano, S., Yamamoto,
M., Ohdan, H., Shimada, K., Ishikawa, O., Yamaue, H., Chayama, K., Miyano,
S., Aburatani, H., Shibata, T., Nakagawa, H. (2016) Whole-genome mutational
landscape and characterization of noncoding and structural mutations in liver
cancer. Nature Genetics 48(5): 500-509.
Goodman, M.F. and Fygenson, K.D. (1998) DNA polymerase delity: from
genetics toward a biochemical understanding. Genetics 148(4): 1475-1482.
Goutte, C., Hansen, L.K., Liptrot, M.G. and Rostrup, E. (2001) Feature-Space
Clustering for fMRI Meta-Analysis. Human Brain Mapping 13(3): 165-183.
Grinold, R.C. and Kahn, R.N. (2000) Active Portfolio Management. New York,
NY: McGraw-Hill.
Gundem, G., Van Loo, P., Kremeyer, B., Alexandrov, L.B., Tubio, J.M., Pa-
paemmanuil, E., Brewer, D.S., Kallio, H.M., H ogn as, G., Annala, M., Kiv-
inummi, K., Goody, V., Latimer, C., O'Meara, S., Dawson, K.J., Isaacs,
W., Emmert-Buck, M.R., Nykter, M., Foster, C., Kote-Jarai, Z., Easton, D.,
Whitaker, H.C.; ICGC Prostate UK Group, Neal, D.E., Cooper, C.S., Eeles,
30",2017-03-02T10:39:59Z,dis rick e wa ya rnia ck wbu hay ka kim ty hacker bha not gor chu guart ne bie hl seth articial intellence pp are tu bristdone how er fallesmith tick oo tam reuters schmidt hsiehou air him cancer  atlas researnetwork chimeters oku cr lap park robertsoarticial intellence rd ns ke kwiatw park morgashu uz ny ler line gibbs math ll brhtot cancer cell  amor tennis recoveri informatsciencfor gy uster bmetric fimoto futura to toi tsu no kate ship articial intellence shi tank tanuchi kawakami xeno got oh ari zum war ll hay mi naka aik at ari ro borribe nano mae jima asahi oku oh saw shibuya naka ham ho soda ar articial intellence hash urls hi date naga moto uefa tats no jima hiraoka oku saka kubo area shi ha hira no moto oh dashima ishikawa yam are cha  mano bur tank shima ta nakagawa whole nature genetics goodmafy gens ogenetics out te hanselip trot most rup feature space usteri meta analysis humabr articial intellence mappi griold khaaive tfmanagement new york mc raw hl gum valoo free yer alexandra tu b pa  all anla iv good latimmara dawsoisaac meme rt buck ny ter foster note  astowhitaker prostate group neal coo eel es
paper_qf_45.pdf,32,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","R.A., Visakorpi, T., Campbell, P.J., McDermott, U., Wedge, D.C., Bova, G.S.
(2015) The evolutionary history of lethal metastatic prostate cancer. Nature
520(7547): 353-357.
Hamerly, G. and Elkan, C. (2004) Learning the k in k-means. In: Thrun, S. (ed.)
Advances of the Neural Information Processing Systems , Vol. 16. Campridge,
MA: MIT Press, pp. 281-289.
Hartigan, J.A. (1975) Clustering algorithms. New York, NY: John Wiley &
Sons, Inc.
Hartigan, J.A. and Wong, M.A. (1979) Algorithm AS 136: A K-Means Clus-
tering Algorithm. Journal of the Royal Statistical Society, Series C (Applied
Statistics) 28(1): 100-108.
Helleday, T., Eshtad, S. and Nik-Zainal, S. (2014) Mechanisms underlying mu-
tational signatures in human cancers. Nature Reviews Genetics 15(9): 585-598.
Imielinski, M., Berger, A.H., Hammerman, P.S., Hernandez, B., Pugh, T.J.,
Hodis, E., Cho, J., Suh, J., Capelletti, M., Sivachenko, A., Sougnez, C., Au-
clair, D., Lawrence, M.S., Stojanov, P., Cibulskis, K., Choi, K., de Waal, L.,
Sharifnia, T., Brooks, A., Greulich, H., Banerji, S., Zander, T., Seidel, D.,
Leenders, F., Ans en, S., Ludwig, C., Engel-Riedel, W., Stoelben, E., Wolf, J.,
Goparju, C., Thompson, K., Winckler, W., Kwiatkowski, D., Johnson, B.E.,
J anne, P.A., Miller, V.A., Pao, W., Travis, W.D., Pass, H.I., Gabriel, S.B., Lan-
der, E.S., Thomas, R.K., Garraway, L.A., Getz, G., Meyerson, M. (2012) Map-
ping the hallmarks of lung adenocarcinoma with massively parallel sequencing.
Cell150(6): 1107-1120.
Jones, D.T., J ager, N., Kool, M., Zichner, T., Hutter, B., Sultan, M., Cho, Y.J.,
Pugh, T.J., Hovestadt, V., St utz, A.M., Rausch, T., Warnatz, H.J., Ryzhova,
M., Bender, S., Sturm, D., Pleier, S., Cin, H., Pfa, E., Sieber, L., Wittmann,
A., Remke, M., Witt, H., Hutter, S., Tzaridis, T., Weischenfeldt, J., Raeder, B.,
Avci, M., Amstislavskiy, V., Zapatka, M., Weber, U.D., Wang, Q., Lasitschka,
B., Bartholomae, C.C., Schmidt, M., von Kalle, C., Ast, V., Lawerenz, C., Eils,
J., Kabbe, R., Benes, V., van Sluis, P., Koster, J., Volckmann, R., Shih, D.,
Betts, M.J., Russell, R.B., Coco, S., Tonini, G.P., Sch uller, U., Hans, V., Graf,
N., Kim, Y.J., Monoranu, C., Roggendorf, W., Unterberg, A., Herold-Mende,
C., Milde, T., Kulozik, A.E., von Deimling, A., Witt, O., Maass, E., R ossler,
J., Ebinger, M., Schuhmann, M.U., Fr uhwald, M.C., Hasselblatt, M., Jabado,
N., Rutkowski, S., von Bueren, A.O., Williamson, D., Cliord, S.C., McCabe,
M.G., Collins, V.P., Wolf, S., Wiemann, S., Lehrach, H., Brors, B., Scheurlen,
W., Felsberg, J., Reifenberger, G., Northcott, P.A., Taylor, M.D., Meyerson,
M., Pomeroy, S.L., Yaspo, M.L., Korbel, J.O., Korshunov, A., Eils, R., Pster,
31",2017-03-02T10:39:59Z,visa  ri campbell mc r most dge ova t nature haer ly  alearni ith ruadvancneural informatprocessi tems vcamp ridge  partisausteri new york  rey sons inc partisawo algorithm means ub algorithm journal royal statistical society seriapplied statistics ll day tad  articial intellence nal meisms nature reviews genetics me i berger haer marnanz push had is cho uh capable  sica c so ug nez au lawrence to jaov ci bul s choi wall sharia nia brooks gre libane ji zar sei l le unr aludw el rie l to else wolf go par ju thompsowick ler kwiatw somler pao tris pass gabriel lathomas rarr away get meters omap cell jontozi cr uer sultacho push hove stadt st rau swar ry zh ova benr storm plea er ipfa sie ber with manrem ke with uer tz arid is  is c nefeld rear vc mstisl  zap aka ber wa las it ska bartholom schmidt alle st la re nz  kab be be nluis  ster vck manship beats russell coco toishans graf kim moor anu rogue orf unter berg r old momd ku loi  im li with maps bei er smanfr has sel blast jab do rut   bu rewliamsoi mc ca be collins wolf womaler abr or sc url efelt berg rei rnberg er northco taylor meters opoverty yas po r bel r shu nov 
paper_qf_45.pdf,33,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","S.M., Lichter, P. (2012) Dissecting the genomic complexity underlying medul-
loblastoma. Nature 488(7409): 100-105.
Kakushadze, Z. and Yu, W. (2016a) Multifactor Risk Models and Heterotic
CAPM. The Journal of Investment Strategies 5(4): 1-49.
Available online: http://ssrn.com/abstract=2722093.
Kakushadze, Z. and Yu, W. (2016b) Factor Models for Cancer Signatures. Phys-
ica A 462: 527-559. Available online: http://ssrn.com/abstract=2772458.
Kakushadze, Z. and Yu, W. (2016c) Statistical Industry Classication. Journal
of Risk & Control 3(1): 17-65.
Available online: http://ssrn.com/abstract=2802753.
Kakushadze, Z. and Yu, W. (2017a) How to Combine a Billion Alphas. Journal
of Asset Management 18(1): 64-80.
Available online: http://ssrn.com/abstract=2739219.
Kakushadze, Z. and Yu, W. (2017b) Statistical Risk Models. The Journal of
Investment Strategies 6(2): 1-40.
Available online: http://ssrn.com/abstract=2732453.
Kashuba, V.I., Pavlova, T.V., Grigorieva, E.V., Kutsenko, A., Yenamandra,
S.P., Li, J., Wang, F., Protopopov, A.I., Zabarovska, V.I., Senchenko, V., Har-
aldson, K., Eshchenko, T., Kobliakova, J., Vorontsova, O., Kuzmin, I., Braga,
E., Blinov, V.M., Kisselev, L.L., Zeng, Y.-X., Ernberg, I., Lerman, M.I., Klein,
G. and Zabarovsky, E.R. (2009) High mutability of the tumor suppressor genes
RASSF1 and RBSP3 (CTDSPL) in cancer. PloS One 4(5): e5231.
Lawrence, M.S., Stojanov, P., Polak, P., Kryukov, G.V., Cibulskis, K.,
Sivachenko, A., Carter, S.L., Stewart, C., Mermel, C.H., Roberts, S.A., Kiezun,
A., Hammerman, P.S., McKenna, A., Drier, Y., Zou, L., Ramos, A.H., Pugh,
T.J., Stransky, N., Helman, E., Kim, J., Sougnez, C., Ambrogio, L., Nicker-
son, E., Sheer, E., Cort es, M.L., Auclair, D., Saksena, G., Voet, D., Noble,
M., DiCara, D., Lin, P., Lichtenstein, L., Heiman, D.I., Fennell, T., Imielinski,
M., Hernandez, B., Hodis, E., Baca, S., Dulak, A.M., Lohr, J., Landau, D.A.,
Wu, C.J., Melendez-Zajgla, J., Hidalgo-Miranda, A., Koren, A., McCarroll,
S.A., Mora, J., Lee, R.S., Crompton, B., Onofrio, R., Parkin, M., Winckler,
W., Ardlie, K., Gabriel, S.B., Roberts C.W., Biegel, J.A., Stegmaier, K., Bass,
A.J., Garraway, L.A., Meyerson, M., Golub, T.R., Gordenin, D.A., Sunyaev,
S., Lander, E.S., Getz, G. (2013) Mutational heterogeneity in cancer and the
search for new cancer-associated genes. Nature 499(7457): 214-208.
Lee, D.D. and Seung, H.S. (1999) Learning the parts of objects by non-negative
matrix factorization. Nature 401(6755): 788-791.
32",2017-03-02T10:39:59Z,liter dissenti nature u sha yu multi faor risk mols  erotic t journal investment strategi articial intellence  u sha yu faor mols cancer snaturph ys  articial intellence  u sha yu statistical industry ass journal risk contr articial intellence  u sha yu how combine blalpha journal asset management  articial intellence  u sha yu statistical risk mols t journal investment strategi articial intellence  as hub plova gregory eva ku t yeam and ra li wa proto pov za bar ovsk se nc har suc   lia v vor ots ova ku zm iraga bl iov kiss lev e rnberg germakleiza bar ov  hh plo one lawrence to jaov poll ryu v ci bul s sica c carter stewart mer mel roberts ki ruhaer mamc kenya drier zo ramos push str a lm akim so ug nez abbr cgi nick er s sort au  articial intellence sak sent vo et noble di cara lilichtenstei mafunnel me i rnanz had is bac du lak loh land wu melenz za gla hidalgo miranda  mc carroll mora  toontar park iwick ler ard lie gabriel roberts bie gel st eg articial intellence er bass rarr away meters ogo sub garisuev lanr get mtal nature  seu learni nature
paper_qf_45.pdf,34,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Lindahl, T. (1993) Instability and decay of the primary structure of DNA.
Nature 362(6422): 709-715.
Llet , R, Ortiz, M.C., Sarabia, L.A. and S anchez, M.S. (2004) Selecting Vari-
ables for k-Means Cluster Analysis by Using a Genetic Algorithm that Opti-
mises the Silhouettes. Analytica Chimica Acta 515(1): 87-100.
Lloyd, S.P. (1957) Least square quantization in PCM. Working Paper. Bell
Telephone Laboratories, Murray Hill, NJ.
Lloyd, S.P. (1982) Least square quantization in PCM. IEEE Transactions on
Information Theory 28(2): 129-137.
Loeb, L.A. and Harris, C.C. (2008) Advances in chemical carcinogenesis: a
historical review and perspective. Cancer Research 68(17): 6863-6872.
Long, J., Delahanty, R.J., Li, G., Gao, Y.T., Lu, W., Cai, Q., Xiang, Y.B.,
Li, C., Ji, B.T., Zheng, Y., Ali, S., Shu, X.O., Zheng, W. (2013) A common
deletion in the APOBEC3 genes and breast cancer risk. Journal of National
Cancer Institute 105(8): 573-579.
Love, C., Sun, Z., Jima, D., Li, G., Zhang, J., Miles, R., Richards, K.L., Dun-
phy, C.H., Choi, W.W., Srivastava, G., Lugar, P.L., Rizzieri, D.A., Lagoo, A.S.,
Bernal-Mizrachi, L., Mann, K.P., Flowers, C.R., Naresh, K.N., Evens, A.M.,
Chadburn, A., Gordon, L.I., Czader, M.B., Gill, J.I., Hsi, E.D., Greenough,
A., Mott, A.B., McKinney, M., Banerjee, A., Grubor, V., Levy, S., Dun-
son, D.B., Dave, S.S. (2012) The genetic landscape of mutations in Burkitt
lymphoma. Nature Genetics 44(12): 1321-1325.
MacQueen, J.B. (1967) Some Methods for classication and Analysis of Multi-
variate Observations. In: LeCam, L. and Neyman, J. (eds.) Proceedings of the
5th Berkeley Symposium on Mathematical Statistics and Probability. Berkeley,
CA: University of California Press, pp. 281-297.
Murtagh, F. and Contreras, P. (2011) Algorithms for hierarchical clustering:
An overview. Wiley Interdisciplinary Reviews: Data Mining and Knowledge
Discovery 2(1): 86-97.
Nik-Zainal, S., Alexandrov, L.B., Wedge, D.C., Van Loo, P., Greenman, C.D.,
Raine, K., Jones, D., Hinton, J., Marshall, J., Stebbings, L.A., Menzies, A.,
Martin, S., Leung, K., Chen, L., Leroy, C., Ramakrishna, M., Rance, R.,
Lau, K.W., Mudie, L.J., Varela, I., McBride, D.J., Bignell, G.R., Cooke, S.L.,
Shlien, A., Gamble, J., Whitmore, I., Maddison, M., Tarpey, P.S., Davies,
H.R., Papaemmanuil, E., Stephens, P.J., McLaren, S., Butler, A.P., Teague,
J.W., J onsson, G., Garber, J.E., Silver, D., Miron, P., Fatima, A., Boyault,
S., Langerd, A., Tutt, A., Martens, J.W., Aparicio, S.A., Borg, A., Salomon,
33",2017-03-02T10:39:59Z,linda hl instabity nature  ortiz arabia selei var means uster analysis usi genetic algorithm opt shouee analytic chi mica a lloyd least worki pa bell telephone laboratorimurray hl lloyd least transans informattory lobe harris advanccancer researlo l thaty li gao lu articial intellence lia li ji c ali shu c journal natnal cancer institute love sima li  mrichards duchoi sri vast a lugar ii eri la goo serial  ra chi manflors are sh evechad burgordocz r gl hsi gre enough mo mc kidney banerjee grab or levy dut bur kit nature genetics mac queesome methods analysis multi observatns ile cam ney maproceedis berkeley symposium matmatical statistics probabity berkeley  calornia  murtagh contrs algorithms arey interdisciplinary reviews data mini kledge divery  articial intellence nal alexandra dge valoo greemaarticial intellence ne jonhtomarshall st bbi nimartilu cvery ramakrishna race lau ma are la mc bri b  cooke sh liegamble whit addittar  dipapa ea nui stepns mc arebutler league garner sirotima boy all laer bu martins par c borg salmon
paper_qf_45.pdf,35,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","A.V., Thomas, G., Brresen-Dale, A.L., Richardson, A.L., Neuberger, M.S.,
Futreal, P.A., Campbell, P.J., Stratton, M.R.; Breast Cancer Working Group
of the International Cancer Genome Consortium. (2012) Mutational processes
molding the genomes of 21 breast cancers. Cell149(5): 979-993.
Nik-Zainal, S., Wedge, D.C., Alexandrov, L.B., Petljak, M., Butler, A.P., Bolli,
N., Davies, H.R., Knappskog, S., Martin, S., Papaemmanuil, E., Ramakrishna,
M., Shlien, A., Simonic, I., Xue, Y., Tyler-Smith, C., Campbell, P.J., Strat-
ton, M.R. (2014) Association of a germline copy number polymorphism of
APOBEC3A and APOBEC3B with burden of putative APOBEC-dependent
mutations in breast cancer. Nature Genetics 46(5): 487-491.
Paatero, P. and Tapper, U. (1994) Positive matrix factorization: A non-negative
factor model with optimal utilization of error. Environmetrics 5(1): 111-126.
Patch, A.M., Christie, E.L., Etemadmoghadam, D., Garsed, D.W., George,
J., Fereday, S., Nones, K., Cowin, P., Alsop, K., Bailey, P.J., Kassahn, K.S.,
Newell, F., Quinn, M.C., Kazako, S., Quek, K., Wilhelm-Benartzi, C., Curry,
E., Leong, H.S.; Australian Ovarian Cancer Study Group, Hamilton, A.,
Mileshkin, L., Au-Yeung, G., Kennedy, C., Hung, J., Chiew, Y.E., Harnett, P.,
Friedlander, M., Quinn, M., Pyman, J., Cordner, S., O'Brien, P., Leditschke,
J., Young, G., Strachan, K., Waring, P., Azar, W., Mitchell, C., Tracante, N.,
Hendley, J., Thorne, H., Shackleton, M., Miller, D.K., Arnau, G.M., Tothill,
R.W., Holloway, T.P., Semple, T., Harliwong, I., Nourse, C., Nourbakhsh, E.,
Manning, S., Idrisoglu, S., Bruxner, T.J., Christ, A.N., Poudel, B., Holmes,
O., Anderson, M., Leonard, C., Lonie, A., Hall, N., Wood, S., Taylor, D.F.,
Xu, Q., Fink, J.L., Waddell, N., Drapkin, R., Stronach, E., Gabra, H., Brown,
R., Jewell, A., Nagaraj, S.H., Markham, E., Wilson, P.J., Ellul, J., McNally,
O., Doyle, M.A., Vedururu, R., Stewart, C., Lengyel, E., Pearson, J.V., Wad-
dell, N., deFazio, A., Grimmond, S.M., Bowtell, D.D. (2015) Whole-genome
characterization of chemoresistant ovarian cancer. Nature 521(7553): 489-494.
Pelleg, D. and Moore, A.W. (2000) X-means: Extending K-means with Ecient
Estimation of the Number of Clusters. In: Langley, P. (ed.) Proceedings of
the 17th International Conference on Machine Learning . San Francisco, CA:
Morgan Kaufman: pp. 727-734.
Pettersen, H.S., Galashevskaya, A., Doseth, B., Sousa, M.M., Sarno, A., Visnes,
T., Aas, P.A., Liabakk, N.B., Slupphaug, G., Strom, P., Kavli, B., Krokan,
H.E. (2015) AID expression in B-cell lymphomas causes accumulation of ge-
nomic uracil and a distinct AID mutational signature. DNA Repair 25: 60-71.
Poon, S., McPherson, J., Tan, P., Teh, B. and Rozen, S. (2014) Mutation sig-
natures of carcinogen exposure: genome-wide detection and new opportunities
for cancer prevention. Genome Medicine 6(3): 24.
34",2017-03-02T10:39:59Z,thomas dale richardsoneu berger fu real campbell statbreast cancer worki group internatnal cancer  consortium mtal cell  articial intellence nal dge alexandra pet jak butler bold diapp s martipapa ea nui ramakrishna sh liesimodue tyler smith campbell start associatnature genetics pa at ero tap  positive eirometrics patchristie te mad moth adam gar sed george fer day none cow ialso articial intellence ley pass hll quinkaz  que wlm beart zi curry lo australiaovariacancer study group hamtomhk iau e kennedy hu chi ew barne fried lanr quinpy macord ner brief led it ske you stra chawar i c mitcll tra  nd ley throne shacmler area to hl hollow sem le har li wo course our baths manni iris og lu bru ner christ po l holmanrsoleonard lo hall wood taylor xu link wadll draw kistomaaba browjel nagar marshal wsoelul mc ally doyle ve du ru stewart la ypearsowad fa z gri obow tell whole nature lle moore extendi estimatnumber usters ilale proceedis internatnal conference machine learni safranci morgakaufmabeer segala s vs kaya do seth sous ano vi snas lia bak slu pp aug ka li rok arep articial intellence po omc r sotateh role mt medicine
paper_qf_45.pdf,36,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Puente, X.S., Pinyol, M., Quesada, V., Conde, L., Ord o~ nez, G.R., Villamor,
N., Escaramis, G., Jares, P., Be a, S., Gonz alez-D az, M., Bassaganyas, L.,
Baumann, T., Juan, M., L opez-Guerra, M., Colomer, D., Tub o, J.M., L opez,
C., Navarro, A., Tornador, C., Aymerich, M., Rozman, M., Hern andez, J.M.,
Puente, D.A., Freije, J.M., Velasco, G., Guti errez-Fern andez, A., Costa, D.,
Carri o, A., Guijarro, S., Enjuanes, A., Hern andez, L., Yag ue, J., Nicol as,
P., Romeo-Casabona, C.M., Himmelbauer, H., Castillo, E., Dohm, J.C., de
Sanjos e, S., Piris, M.A., de Alava, E., San Miguel, J., Royo, R., Gelp , J.L.,
Torrents, D., Orozco, M., Pisano, D.G., Valencia, A., Guig o, R., Bay es, M.,
Heath, S., Gut, M., Klatt, P., Marshall, J., Raine, K., Stebbings, L.A., Futreal,
P.A., Stratton, M.R., Campbell, P.J., Gut, I., L opez-Guillermo, A., Estivill,
X., Montserrat, E., L opez-Ot n, C., Campo, E. (2011) Whole-genome sequenc-
ing identies recurrent mutations in chronic lymphocytic leukaemia. Nature
475(7354): 101-105.
Puente, X.S., Be a, S., Vald es-Mas, R., Villamor, N., Guti errez-Abril, J.,
Mart n-Subero, J.I., Munar, M., Rubio-P erez, C., Jares, P., Aymerich, M.,
Baumann, T., Beekman, R., Belver, L., Carrio, A., Castellano, G., Clot, G.,
Colado, E., Colomer, D., Costa, D., Delgado, J., Enjuanes, A., Estivill, X.,
Ferrando, A.A., Gelp , J.L., Gonz alez, B., Gonz alez, S., Gonz alez, M., Gut,
M., Hern andez-Rivas, J.M., L opez-Guerra, M., Mart n-Garc a, D., Navarro,
A., Nicol as, P., Orozco, M., Payer, A.R., Pinyol, M., Pisano, D.G., Puente,
D.A., Queir os, A.C., Quesada, V., Romeo-Casabona, C.M., Royo, C., Royo,
R., Rozman, M., Russi~ nol, N., Salaverr a, I., Stamatopoulos, K., Stunnenberg,
H.G., Tamborero, D., Terol, M.J., Valencia, A., L opez-Bigas, N., Torrents, D.,
Gut, I., L opez-Guillermo, A., L opez-Ot n, C., Campo, E. (2015) Non-coding
recurrent mutations in chronic lymphocytic leukaemia. Nature 526(7574): 519-
524.
Qian, J., Wang, Q., Dose, M., Pruett, N., Kieer-Kwon, K.R., Resch, W.,
Liang, G., Tang, Z., Math e, E., Benner, C., Dubois, W., Nelson, S., Vian, L.,
Oliveira, T.Y., Jankovic, M., Hakim, O., Gazumyan, A., Pavri, R., Awasthi,
P., Song, B., Liu, G., Chen, L., Zhu, S., Feigenbaum, L., Staudt, L., Murre, C.,
Ruan, Y., Robbiani, D.F., Pan-Hammarstr om, Q., Nussenzweig, M.C., Casel-
las, R. (2014) B Cell Super-Enhancers and Regulatory Clusters Recruit AID
Tumorigenic Activity. Cell159(7): 1524-1537.
Roberts, S.A., Sterling, J., Thompson, C., Harris, S., Mav, D., Shah, R., Klim-
czak, L.J., Kryukov, G.V., Malc, E., Mieczkowski, P.A., Resnick, M.A., Gor-
denin, D.A. (2012) Clustered mutations in yeast and in human cancers can arise
from damaged long single-strand DNA regions. Molecular Cell 46(4): 424-435.
Roberts, S.A., Lawrence, M.S., Klimczak, L.J., Grimm, S.A., Fargo, D., Sto-
janov, P., Kiezun, A., Kryukov, G.V., Carter, S.L., Saksena, G., Harris, S.,
35",2017-03-02T10:39:59Z,plenty piyo que sad coor vla mor scar amis are be gone bass ag any as batmajuaguerra cotub narro tornado am rioz mar plenty frei je va gut  costa carry gui jar ro er ya nico romeo casa bona himself bauer castlo do hm banjo  is la samuel roy lp torrent or oz co piano valencia gui bay ath gut last marshall articial intellence ne st bbi fu real statcampbell gut gulermo est iv l montserrat ot campo whole nature plenty be valid mas vla mor gut abr  mart sub ero muar rub are am ribatmabeekmabel car r castellano ot cola do cocosta lgado eest iv l  lp gone gone gone gut r rival guerra mart arc narro nico or oz co payer piyo piano plenty que ir que sad romeo casa bona roy roy oz maruss later st math poll os sturnberg tam borer ter valencia b as torrent gut gulermo ot campo nonature iawa dose true  ki kworealia ta math banner dubus nelsovi aolivi him ga zum yapay ri was thi so  czhu genbaum st aud murr ru arobbie ni paham mars tr nuns ezi case cell su enhancregulatory usters reuit tumor e nic aivity cell roberts sterli thompsoharris ma shah li ryu v mal mie ck ow  rnick gor ustered molecular cell roberts lawrence li mc zak gri cargo to ki ruryu v carter sak sent harris
paper_qf_45.pdf,37,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Shah, R.R., Resnick, M.A., Getz, G. and Gordenin, D.A. (2013) An APOBEC
cytidine deaminase mutagenesis pattern is widespread in human cancers. Nature
Genetics 45(9): 970-976.
Roberts, S.A. and Gordenin, D.A. (2014a) Clustered Mutations in Human Can-
cer. In: eLS (Genetics & Disease) . Chichester, UK: John Wiley & Sons, Ltd.
Roberts S.A and Gordenin D.A. (2014b) Clustered and genome-wide transient
mutagenesis in human cancers: Hypermutation without permanent mutators
or loss of tness. BioEsseys 36(4): 382-393.
Roberts S.A and Gordenin D.A. (2014c) Hypermutation in human cancer
genomes: footprints and mechanisms. Nature Reviews Cancer 14(12): 786-800.
Rousseeuw, P.J. (1987) Silhouettes: a Graphical Aid to the Interpretation and
Validation of Cluster Analysis. Journal of Computational and Applied Mathe-
matics 20(1): 53-65.
Roy, O. and Vetterli, M. (2007) The eective rank: A measure of eective di-
mensionality. In: European Signal Processing Conference (EUSIPCO). Pozna n,
Poland (September 3-7), pp. 606-610.
Scelo, G., Riazalhosseini, Y., Greger, L., Letourneau, L., Gonz alez-Porta,
M., Wozniak, M.B., Bourgey, M., Harnden, P., Egevad, L., Jackson, S.M.,
Karimzadeh, M., Arseneault, M., Lepage, P., How-Kit, A., Daunay, A., Re-
nault, V., Blanch e, H., Tubacher, E., Sehmoun, J., Viksna, J., Celms, E.,
Opmanis, M., Zarins, A., Vasudev, N.S., Seywright, M., Abedi-Ardekani, B.,
Carreira, C., Selby, P.J., Cartledge, J.J., Byrnes, G., Zavadil, J., Su, J., Holca-
tova, I., Brisuda, A., Zaridze, D., Moukeria, A., Foretova, L., Navratilova, M.,
Mates, D., Jinga, V., Artemov, A., Nedoluzhko, A., Mazur, A., Rastorguev, S.,
Boulygina, E., Heath, S., Gut, M., Bihoreau, M.T., Lechner, D., Foglio, M.,
Gut, I.G., Skryabin, K., Prokhortchouk, E., Cambon-Thomsen, A., Rung, J.,
Bourque, G., Brennan, P., Tost, J., Banks, R.E., Brazma, A., Lathrop, G.M.
(2014) Variation in genomic landscape of clear cell renal cell carcinoma across
Europe. Nature Communications 5: 5135.
Sibson, R. (1973) SLINK: an optimally ecient algorithm for the single-link
cluster method. The Computer Journal (British Computer Society) 16(1): 30-
34.
Sima, J. and Gilbert, D.M. (2014) Complex correlations: replication timing and
mutational landscapes during cancer and genome evolution. Current Opinion
in Genetics & Developmen 25: 93-100.
Steinbach, M., Karypis, G. and Kumar, V. (2000) A comparison of document
clustering techniques. KDD Workshop on Text Mining 400(1): 525-526.
36",2017-03-02T10:39:59Z,shah rnick get garianature genetics roberts gariustered mtns humacaenetics disease chicster  rey sons ltd roberts gariustered  mtb else ys roberts gari mtnature reviews cancer rows  uw shouee graphical aid interpatvalidatuster analysis journal tnal applied ma t roy ve er li t iasnal processi conference point poland september cell ria al hosseini refer ourneau gone tal wozniak borg ey har neg eva jacksofarm za  areall le page how kit any re brantuba cr eh mou vi ks na elm mais  ivas v  wrht be di r ani care ira self cart ledge byrne a d su hca bri soda arid ze mou ker for et ova nratova matji arte ov ne do luz  major ra st argue  ly gina ath gut bihar eau le cr fog l gut sk rya bipro thor tc hour came othomsoru torque brennato st banks bra zm lathrvariat nature counicatns sib sot uter journal british uter society sima gbert lex current opingenetics velmesteinbavary is kumar workshtext mini
paper_qf_45.pdf,38,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Steinhaus, H. (1957) Sur la division des corps mat eriels en parties. Bull. Acad.
Polon. Sci. 4(12): 801-804.
Sugar, C.A. and James, G.M. (2003) Finding the number of clusters in a data
set: An information theoretic approach. Journal of the American Statistical
Association 98(463): 750-763.
Sung, W.K., Zheng, H., Li, S., Chen, R., Liu, X., Li, Y., Lee, N.P., Lee, W.H.,
Ariyaratne, P.N., Tennakoon, C., Mulawadi, F.H., Wong, K.F., Liu, A.M.,
Poon, R.T., Fan, S.T., Chan, K.L., Gong, Z., Hu, Y., Lin, Z., Wang, G., Zhang,
Q., Barber, T.D., Chou, W.C., Aggarwal, A., Hao, K., Zhou, W., Zhang, C.,
Hardwick, J., Buser, C., Xu, J., Kan, Z., Dai, H., Mao, M., Reinhard, C.,
Wang, J., Luk, J.M. (2012) Genome-wide survey of recurrent HBV integration
in hepatocellular carcinoma. Nature Genetics 44(7): 765-769.
Taylor, B.J.M., Nik-Zainal, S., Wu, Y.L., Stebbings, L.A., Raine, K., Campbell,
P.J., Rada, C., Stratton, M.R., Neuberger, M.S. (2013) DNA deaminases induce
break-associated mutation showers with implication of APOBEC3B and 3A in
breast cancer kataegis. eLife 2: e00534.
Tirode, F., Surdez, D., Ma, X., Parker, M., Le Deley, M.C., Bahrami, A.,
Zhang, Z., Lapouble, E., Grosset^ ete-Lalami, S., Rusch, M., Reynaud, S., Rio-
Frio, T., Hedlund, E., Wu, G., Chen, X., Pierron, G., Oberlin, O., Zaidi, S.,
Lemmon, G., Gupta, P., Vadodaria, B., Easton, J., Gut, M., Ding, L., Mardis,
E.R., Wilson, R.K., Shurtle, S., Laurence, V., Michon, J., Marec-B erard, P.,
Gut, I., Downing, J., Dyer, M., Zhang, J., Delattre, O.; St. Jude Children's
Research Hospital { Washington University Pediatric Cancer Genome Project
and the International Cancer Genome Consortium. (2014) Genomic Landscape
of Ewing Sarcoma denes an aggressive subtype with co-association of STAG2
and TP53 mutations. Cancer Discovery 4(11): 1342-1353.
Waddell, N., Pajic, M., Patch, A.M., Chang, D.K., Kassahn, K.S., Bailey, P.,
Johns, A.L., Miller, D., Nones, K., Quek, K., Quinn, M.C., Robertson, A.J.,
Fadlullah, M.Z., Bruxner, T.J., Christ, A.N., Harliwong, I., Idrisoglu, S., Man-
ning, S., Nourse, C., Nourbakhsh, E., Wani, S., Wilson, P.J., Markham, E.,
Cloonan, N., Anderson, M.J., Fink, J.L., Holmes, O., Kazako, S.H., Leonard,
C., Newell, F., Poudel, B., Song, S., Taylor, D., Waddell, N., Wood, S.,
Xu, Q., Wu, J., Pinese, M., Cowley, M.J., Lee, H.C., Jones, M.D., Nagrial,
A.M., Humphris, J., Chantrill, L.A., Chin, V., Steinmann, A.M., Mawson, A.,
Humphrey, E.S., Colvin, E.K., Chou, A., Scarlett, C.J., Pinho, A.V., Giry-
Laterriere, M., Rooman, I., Samra, J.S., Kench, J.G., Pettitt, J.A., Merrett,
N.D., Toon, C., Epari, K., Nguyen, N.Q., Barbour, A., Zeps, N., Jamieson,
N.B., Graham, J.S., Niclou, S.P., Bjerkvig, R., Gr utzmann, R., Aust, D.,
Hruban, R.H., Maitra, A., Iacobuzio-Donahue, C.A., Wolfgang, C.L., Mor-
gan, R.A., Lawlor, R.T., Corbo, V., Bassi, C., Falconi, M., Zamboni, G., Tor-
37",2017-03-02T10:39:59Z,steihas sur bull acad posci sugar jamndi ajournal statistical associatsu c li c li   ari yar at ne tesoomla wa wo  po ofachago hu liwa  barber hou ag gar wal hao hou  hardback user xu kaarticial intellence mao richard wa uk  nature genetics taylor  articial intellence nal wu st bbi articial intellence ne campbell rada statneu berger le tico  sur z ma parker le  ley m  lap ble gross laga mi rus reina ud r r  land wu cpier roberliarticial intellence lemogupta va do aria astogut di mark is wsoturtle lawrence miomare gut downi r   laer st ju chdreresearhospital washito pediatric cancer  proje internatnal cancer  consortium genomic landscape wi sarcoma cancer divery wadll pa ji pat pass harticial intellence ley s mler none que quinrobertsofall alla bru ner christ har li wo iris og lu macourse our baths wawsomarshal coaanrsolink holmkaz  leonard ll po l so taylor wadll wood xu wu pincole  jonagri al hump hr is chatrl chisteinmamasohumphrey colihou scar piho try later rie re room asam ra kepetit mer re  to oep ari uyeharr ep jamie sograham nic lou jerk via gr just rub aarticial intellence tra co but  donate wolfga mor law lor cor bo basis falcocameo ni tor
paper_qf_45.pdf,39,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","tora, G., Tempero, M.A.; Australian Pancreatic Cancer Genome Initiative, Gill,
A.J., Eshleman, J.R., Pilarsky, C., Scarpa, A., Musgrove, E.A., Pearson, J.V.,
Biankin, A.V., Grimmond, S.M. (2015) Whole genomes redene the mutational
landscape of pancreatic cancer. Nature 518(7540): 495-501.
Wang, K., Yuen, S.T., Xu, J., Lee, S.P., Yan, H.H., Shi, S.T., Siu, H.C., Deng,
S., Chu, K.M., Law, S., Chan, K.H., Chan, A.S., Tsui, W.Y., Ho, S.L., Chan,
A.K., Man, J.L., Foglizzo, V., Ng, M.K., Chan, A.S., Ching, Y.P., Cheng, G.H.,
Xie, T., Fernandez, J., Li, V.S., Clevers, H., Rejto, P.A., Mao, M., Leung,
S.Y. (2014) Whole-genome sequencing and comprehensive molecular proling
identify new driver mutations in gastric cancer. Nature Genetics 46(6): 573-582.
Xuan, D., Li, G., Cai, Q., Deming-Halverson, S., Shrubsole, M.J., Shu, X.O.,
Kelley, M.C., Zheng, W., Long, J. (2013) APOBEC3 deletion polymorphism is
associated with breast cancer risk among women of European ancestry. Car-
cinogenesis 34(10): 2240-2243.
Yang, W., Gibson, J.D. and He, T. (2005) Coecient rate and lossy source
coding. IEEE Transactions on Information Theory 51(1): 381-386.
Zhang, J., Wu, G., Miller, C.P., Tatevossian, R.G., Dalton, J.D., Tang, B.,
Orisme, W., Punchihewa, C., Parker, M., Qaddoumi, I., Boop, F.A., Lu, C.,
Kandoth, C., Ding, L., Lee, R., Huether, R., Chen, X., Hedlund, E., Naga-
hawatte, P., Rusch, M., Boggs, K., Cheng, J., Becksfort, J., Ma, J., Song, G.,
Li, Y., Wei, L., Wang, J., Shurtle, S., Easton, J., Zhao, D., Fulton, R.S.,
Fulton, L.L., Dooling, D.J., Vadodaria, B., Mulder, H.L., Tang, C., Ochoa,
K., Mullighan, C.G., Gajjar, A., Kriwacki, R., Sheer, D., Gilbertson, R.J.,
Mardis, E.R., Wilson, R.K., Downing, J.R., Baker, S.J., Ellison, D.W.; St. Jude
Children's Research Hospital-Washington University Pediatric Cancer Genome
Project. (2013) Whole-genome sequencing identies genetic alterations in pe-
diatric low-grade gliomas. Nature Genetics 45(6): 602-612.
38",2017-03-02T10:39:59Z,tem australiapaneatic cancer  initiative gl hle mapolar  sharp marove pearsobikri owhole nature wa yexu  yashi siu e chu law chachasui ho chamafog liz zo  chachi c die  li ere to mao lu whole nature genetics juali articial intellence emi hal vers oshrubs ole shu kelly c lo acar ya gibso coe transans informattory  wu mler tate daytota or is me pun wa parker ad do umi boo lu kaboth di  hue t c land naga rus bogus c backs fort ma so li i wa turtle astohao buobuodoo li va do aria murr ta omul l haga jar kri was ki ser gbert somark is wsodowni baker alost ju chdreresearhospital washito pediatric cancer  proje whole nature genetics
paper_qf_45.pdf,40,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Table 1: Genome data summary. All Brain Lower Glioma samples are Pilocytic
Astrocytoma samples. See Subsection 3.1 for the data source denitions.
Cancer Type Total Counts # of Samples Source
B Cell Lymphoma 43626 24 A1-2
Bone Cancer 36374 98 B1
Brain Lower Grade Glioma 3572 101 A1, C1
Breast Cancer 254381 119 A1, D1
Chronic Lymphocytic Leukemia 19489 134 A1, E1-2
Esophageal Cancer 1064 17 F1
Gastric Cancer 1996615 100 G1
Liver Cancer 3017487 389 H1-2
Lung Cancer 449527 24 I1
Medulloblastoma 44689 100 J1
Ovarian Cancer 668918 84 K1
Pancreatic Cancer 5087 100 L1
Prostate Cancer 29142 5 M1
Renal Cell Carcinoma 483329 94 N1
All Cancer Types 7053300 1389 Above
39",2017-03-02T10:39:59Z,table  all br articial intellence lor lima pi loc tic astrocytoma  subsecancer  total counts samplsource cell lymphoma bone cancer br articial intellence lor gra lima breast cancer chronic lymph cy tic leukemia esophageal cancer gastric cancer licancer lu cancer me dull oblast oma ovariacancer paneatic cancer prostate cancer renal cell carcinoma all cancer s above
paper_qf_45.pdf,41,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Table 2: Occurrence counts for the rst 48 mutation categories for the genome data
summarized in Table 1 aggregated by cancer types. X1 = B Cell Lymphoma, X2 =
Bone Cancer, X3 = Brain Lower Grade Glioma, X4 = Breast Cancer, X5 = Chronic
Lymphocytic Leukemia, X6 = Esophageal Cancer, X7 = Gastric Cancer, X8 = Liver
Cancer, X9 = Lung Cancer, X10 = Medulloblastoma, X11 = Ovarian Cancer, X12
= Pancreatic Cancer, X13 = Prostate Cancer, X14 = Renal Cell Carcinoma. The
mutations are encoded as follows: XYZW = Y >W: XYZ.
Mutation X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 X13 X14
ACAA 466 716 59 3024 286 11 24884 51929 15865 801 13831 36 577 10734
ACCA 355 528 41 2600 249 3 14361 15227 9217 681 11192 42 422 4952
ACGA 43 112 6 446 55 4 2253 51156 3567 115 1804 9 59 1231
ACTA 309 577 38 2201 195 12 15833 33385 10038 557 10508 20 380 5573
CCAA 420 538 29 2814 238 11 21980 9507 19569 579 10943 59 459 7899
CCCA 245 361 30 2149 147 6 19624 26869 16889 424 9720 45 336 6037
CCGA 77 75 22 470 35 7 4034 8695 4995 105 1644 29 67 1069
CCTA 418 502 26 2380 211 10 43418 2967 15930 544 10037 80 388 6825
GCAA 452 624 41 2144 244 16 26007 42508 11084 532 6835 33 419 4795
GCCA 234 346 18 1677 149 6 10594 32252 9087 337 6423 33 247 3563
GCGA 67 66 7 375 29 5 2649 17426 3607 94 1295 17 67 853
GCTA 351 463 13 1684 197 11 21205 33670 7502 345 6543 30 332 3637
TCAA 584 628 38 5873 233 11 24288 33919 13358 766 9816 54 582 8973
TCCA 313 472 36 3735 145 19 17372 139576 13140 567 9697 53 479 7388
TCGA 66 57 7 543 28 4 3168 11569 2385 128 1334 17 87 1097
TCTA 916 849 54 4938 376 25 43364 20943 14028 1111 12202 56 817 8611
ACAG 232 260 19 2124 162 1 10057 42216 4369 445 8653 18 197 4551
ACCG 143 148 11 1307 92 4 4818 6202 2088 256 4506 21 188 2687
ACGG 48 82 10 641 32 2 1606 27957 953 121 2172 5 46 837
ACTG 290 240 25 2278 113 8 8670 90212 3091 394 8460 23 237 4457
CCAG 153 160 13 1726 78 7 4942 8975 3233 175 5559 30 125 2474
CCCG 131 154 14 1391 75 2 4117 27928 2902 163 4857 18 142 2601
CCGG 31 65 14 597 20 0 1176 87975 1210 122 1769 11 55 532
CCTG 213 227 19 2425 92 11 6840 14272 3348 238 7943 28 204 3846
GCAG 106 120 8 1128 52 2 3844 53180 2546 196 3621 12 95 1847
GCCG 147 148 7 1142 80 3 4174 11783 2406 160 3363 16 112 1630
GCGG 25 42 2 525 33 1 776 52658 1113 52 918 5 27 423
GCTG 225 146 9 1650 81 5 5116 43761 2187 177 5418 14 135 2045
TCAG 391 266 22 16099 79 46 12809 12172 8882 265 13033 49 359 3968
TCCG 279 185 18 4896 95 14 7208 41020 4566 244 7977 30 264 4441
TCGG 30 52 11 794 13 2 1294 9554 992 64 1423 11 33 553
TCTG 660 444 44 21847 202 53 21720 3519 10841 449 20292 75 563 7668
ACAT 950 931 97 3557 440 14 46364 28569 6530 1137 13367 66 542 8872
ACCT 482 482 49 1875 265 14 16724 47067 3418 590 6543 100 285 4786
ACGT 1085 2373 289 4978 792 70 78681 17833 3830 3694 17475 585 1603 9034
ACTT 603 628 57 2570 263 11 21545 63195 4179 827 10578 54 380 6567
CCAT 729 542 74 3344 294 16 19611 23743 7430 801 8689 79 428 7106
CCCT 607 545 78 2443 337 10 18553 52051 6155 810 7554 70 325 6038
CCGT 845 1095 180 3489 483 70 53496 7282 4362 1983 11157 498 888 4824
CCTT 784 774 124 3468 380 13 22137 29223 7932 854 11714 69 430 8606
GCAT 615 531 65 2815 285 16 31548 50399 4166 711 8492 107 420 6069
GCCT 583 484 64 2256 295 14 34372 10858 3715 825 7784 120 402 5474
GCGT 930 1382 152 3870 546 60 75814 35864 3544 2428 13447 629 1294 5232
GCTT 660 585 78 2254 316 15 26554 49322 3836 741 8329 93 378 5765
TCAT 1531 761 86 25251 268 69 30161 11611 13414 780 13880 131 681 9723
TCCT 1172 685 73 7268 319 25 23799 31779 7136 859 10287 112 544 8833
TCGT 628 903 127 4510 281 58 33759 53275 2998 1553 8055 300 708 3772
TCTT 1466 813 57 13615 313 43 29570 21922 11432 859 12692 79 662 8933
40",2017-03-02T10:39:59Z,table occurrence table cell lymphoma bone cancer br articial intellence lor gra lima breast cancer chronic lymph cy tic leukemia esophageal cancer gastric cancer licancer lu cancer me dull oblast oma ovariacancer paneatic cancer prostate cancer renal cell carcinoma t mtn
paper_qf_45.pdf,42,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Table 3: Table 2 continued: occurrence counts (aggregated by cancer types) for the
next 48 mutation categories.
Mutation X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 X13 X14
ATAA 385 382 22 1375 248 2 9613 35715 3957 315 5776 7 313 9726
ATCA 241 274 13 1388 191 6 9419 7570 1917 235 4439 18 214 2973
ATGA 232 344 21 1459 211 4 7666 48117 3962 228 5983 18 197 7703
ATTA 580 516 29 2262 301 5 30067 32078 2928 590 9131 16 523 5833
CTAA 185 217 9 1009 98 3 5551 9978 4331 151 4473 10 175 12963
CTCA 211 256 17 1535 118 6 14584 41504 3430 224 6343 21 184 6465
CTGA 219 227 26 1406 146 12 8654 8508 7905 210 6473 28 207 14356
CTTA 379 408 27 1716 162 6 24273 2400 4255 303 8578 24 221 8077
GTAA 180 162 10 797 82 1 4092 36890 3147 104 3254 5 138 5769
GTCA 174 147 8 791 80 1 5919 25736 1399 113 3026 10 79 1954
GTGA 156 145 9 948 102 10 6717 9623 4021 122 4233 8 135 3924
GTTA 181 272 14 1168 129 3 10065 43279 2112 200 5741 10 156 2579
TTAA 576 454 29 2192 256 2 20211 13356 4086 510 6936 16 499 11629
TTCA 158 202 17 1095 110 3 7296 75515 1766 222 4859 12 142 4966
TTGA 179 156 4 774 144 3 5147 5013 2912 140 3493 10 129 7897
TTTA 434 467 37 2298 289 3 27521 12174 3117 481 10786 8 417 10311
ATAC 910 792 84 3342 519 10 34145 35102 7884 865 14628 36 552 11060
ATCC 382 255 32 1439 192 7 14365 4444 1952 354 6071 22 206 2821
ATGC 572 526 51 2354 329 14 28630 16789 5325 534 10017 48 331 6561
ATTC 792 734 94 3340 457 11 22018 48320 4344 769 13211 38 503 6714
CTAC 456 283 30 1378 187 1 21942 9143 4198 301 5972 21 202 4855
CTCC 427 295 31 1670 197 5 31474 13443 2361 331 7202 48 163 3092
CTGC 531 328 29 1659 222 6 38742 59589 3845 360 6553 60 206 4024
CTTC 795 328 48 1828 316 12 73708 10381 3566 383 8088 48 239 4982
GTAC 452 378 30 1533 215 4 25980 46065 3605 462 6804 48 345 4411
GTCC 404 332 25 981 176 10 15268 17372 1619 299 4123 20 195 1823
GTGC 370 284 33 1233 189 8 24550 54718 2912 332 5851 48 211 2962
GTTC 511 447 46 1680 269 7 22803 46663 2547 532 7513 36 367 3575
TTAC 541 428 48 1819 297 2 22540 16834 3732 497 7741 25 372 6232
TTCC 606 383 44 1610 211 2 25505 42903 1979 473 7086 24 280 3847
TTGC 306 242 22 1079 185 9 17328 8016 2277 323 4419 24 210 2892
TTTC 818 437 60 2187 415 4 48109 2717 3140 617 10866 28 427 13766
ATAG 462 133 9 862 229 2 5956 20568 1055 164 3114 5 158 2747
ATCG 134 69 3 451 51 0 5214 50363 476 92 1335 10 93 1949
ATGG 196 136 11 941 76 3 5505 35331 1134 165 3935 5 116 2726
ATTG 580 155 17 1030 166 2 31252 58405 1010 177 3455 11 221 4941
CTAG 416 110 4 601 134 3 5035 29104 779 85 1932 3 87 1381
CTCG 219 91 16 759 71 4 14376 86003 695 96 2754 15 96 1628
CTGG 342 137 7 1165 140 4 12511 13036 1684 142 4488 19 142 2080
CTTG 1780 202 21 1560 344 5 142206 27191 1801 187 4424 38 171 2944
GTAG 224 82 3 617 78 0 2552 45734 746 87 1792 3 45 896
GTCG 132 53 4 667 43 0 3804 10333 542 53 1290 10 47 628
GTGG 183 102 12 2989 116 2 5256 21041 2874 171 5467 16 58 1313
GTTG 1090 113 14 1194 142 3 37454 46981 1106 174 3439 13 122 1423
TTAG 654 199 19 1021 260 1 7674 11096 1235 187 3327 14 158 2799
TTCG 167 96 13 725 79 2 8315 32124 740 130 2507 12 125 3312
TTGG 265 132 16 1124 121 7 11452 62446 1693 161 4480 8 142 2974
TTTG 1349 296 43 2144 403 4 77262 28801 2361 379 7679 39 353 11415
41",2017-03-02T10:39:59Z,table table mtn
paper_qf_45.pdf,43,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Table 4: Top 10 clusterings by occurrence counts (second column) in 150,000 runs
(performed as 15 consecutive batches of 10,000 runs in each batch). Each run is based
on 1,000 samplings (i.e., num.try = 1000 in the R function qrm.stat.ind.class() ;
also, the target number of clusters is k = 7 ; see Appendix A for details). The
columns \\\\Cl-1"" through \\\\Cl-7"" give the numbers of mutations in each cluster (the
total number of mutations in each clustering is 96). The entries \\\\{"" correspond to
clusterings with fewer than 7 clusters. Note that Clustering-C and Clustering-H have
the same numbers of mutations in their 7 clusters; however, these two clusterings
are dierent, i.e., equally-sized clusters contain dierent mutations. While there was
slight variability in the placement (by occurrence counts) of the top 10 clusterings
within the aforesaid 15 batches of 10,000 runs, Clustering-A through Clustering-J
invariably were the top 10 in each batch. The weights are given in Tables 5 through
12 (for Clustering-A through Clustering-D) and 13 and 14 (for Clustering-A).
Name Count Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7
Clustering-A 12085 8 8 10 15 16 18 21
Clustering-B 10962 7 8 8 12 15 17 29
Clustering-C 10788 8 8 11 15 16 17 21
Clustering-D 10328 8 8 15 16 18 31 {
Clustering-E 6499 7 8 8 12 15 18 28
Clustering-F 5451 8 8 15 17 17 31 {
Clustering-G 5421 8 8 10 15 17 17 21
Clustering-H 5302 8 8 11 15 16 17 21
Clustering-I 4602 8 8 10 15 21 34 {
Clustering-J 3698 8 8 15 31 34 { {
42",2017-03-02T10:39:59Z,table tea t   t note usteri usteri w usteri usteri t tablusteri usteri usteri name count        usteri usteri usteri usteri usteri usteri usteri usteri usteri usteri
paper_qf_45.pdf,44,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Table 5: Weights (in the units of 1%, rounded to 2 digits) for the rst 48 mutation
categories (this Table 5 is continued in Table 6 with the next 48 mutation categories)
for the 7 clusters in Clustering-A (see Table 4) based on unnormalized (columns 2-
8) and normalized (columns 9-15) regressions (see Subsection 2.6 for details). Each
cluster is dened as containing the mutations with nonzero weights. For instance,
cluster Cl-2 contains 8 mutations GCGA, TCGA, ACGG, GCCG, GCGG, TCGG,
GTCA, GTCG. In each cluster the weights are normalized to add up to 100% (up
to 2 digits due to the aforesaid rounding). In Tables 5 through 12 \\\\weights based
on unnormalized regressions"" are given by (13), (14) and (15), while \\\\weights based
on normalized regressions"" are given by (17), (14) and (16), i.e., the exposures are
calculated based on arithmetic averages (see Subsection 2.6 for details).
Mutation Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7 Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7
ACAA 0.00 0.00 0.00 6.55 0.00 0.00 0.00 0.00 0.00 0.00 6.55 0.00 0.00 0.00
ACCA 0.00 0.00 0.00 0.00 5.83 0.00 0.00 0.00 0.00 0.00 0.00 6.08 0.00 0.00
ACGA 0.00 0.00 0.00 0.00 0.00 0.00 4.06 0.00 0.00 0.00 0.00 0.00 0.00 4.00
ACTA 0.00 0.00 0.00 0.00 6.16 0.00 0.00 0.00 0.00 0.00 0.00 6.38 0.00 0.00
CCAA 0.00 0.00 0.00 0.00 7.91 0.00 0.00 0.00 0.00 0.00 0.00 8.10 0.00 0.00
CCCA 0.00 0.00 0.00 0.00 6.46 0.00 0.00 0.00 0.00 0.00 0.00 6.68 0.00 0.00
CCGA 0.00 0.00 7.21 0.00 0.00 0.00 0.00 0.00 0.00 7.23 0.00 0.00 0.00 0.00
CCTA 0.00 0.00 0.00 0.00 0.00 6.75 0.00 0.00 0.00 0.00 0.00 0.00 6.79 0.00
GCAA 4.05 0.00 0.00 0.00 0.00 0.00 0.00 4.65 0.00 0.00 0.00 0.00 0.00 0.00
GCCA 0.00 0.00 0.00 0.00 4.56 0.00 0.00 0.00 0.00 0.00 0.00 4.73 0.00 0.00
GCGA 0.00 13.81 0.00 0.00 0.00 0.00 0.00 0.00 13.89 0.00 0.00 0.00 0.00 0.00
GCTA 0.00 0.00 0.00 0.00 5.02 0.00 0.00 0.00 0.00 0.00 0.00 5.20 0.00 0.00
TCAA 0.00 0.00 0.00 6.26 0.00 0.00 0.00 0.00 0.00 0.00 6.21 0.00 0.00 0.00
TCCA 0.00 0.00 0.00 0.00 8.94 0.00 0.00 0.00 0.00 0.00 0.00 9.29 0.00 0.00
TCGA 0.00 11.87 0.00 0.00 0.00 0.00 0.00 0.00 12.24 0.00 0.00 0.00 0.00 0.00
TCTA 0.00 0.00 0.00 8.05 0.00 0.00 0.00 0.00 0.00 0.00 8.00 0.00 0.00 0.00
ACAG 0.00 0.00 0.00 0.00 3.96 0.00 0.00 0.00 0.00 0.00 0.00 4.18 0.00 0.00
ACCG 0.00 0.00 8.07 0.00 0.00 0.00 0.00 0.00 0.00 8.17 0.00 0.00 0.00 0.00
ACGG 0.00 12.62 0.00 0.00 0.00 0.00 0.00 0.00 12.22 0.00 0.00 0.00 0.00 0.00
ACTG 0.00 0.00 0.00 0.00 4.77 0.00 0.00 0.00 0.00 0.00 0.00 5.03 0.00 0.00
CCAG 0.00 0.00 9.26 0.00 0.00 0.00 0.00 0.00 0.00 9.35 0.00 0.00 0.00 0.00
CCCG 0.00 0.00 0.00 0.00 0.00 0.00 3.91 0.00 0.00 0.00 0.00 0.00 0.00 4.02
CCGG 0.00 0.00 0.00 0.00 0.00 0.00 5.37 0.00 0.00 0.00 0.00 0.00 0.00 5.12
CCTG 0.00 0.00 12.46 0.00 0.00 0.00 0.00 0.00 0.00 12.58 0.00 0.00 0.00 0.00
GCAG 0.00 0.00 0.00 0.00 0.00 0.00 4.61 0.00 0.00 0.00 0.00 0.00 0.00 4.57
GCCG 0.00 14.79 0.00 0.00 0.00 0.00 0.00 0.00 15.62 0.00 0.00 0.00 0.00 0.00
GCGG 0.00 15.50 0.00 0.00 0.00 0.00 0.00 0.00 13.92 0.00 0.00 0.00 0.00 0.00
GCTG 0.00 0.00 0.00 0.00 0.00 0.00 4.86 0.00 0.00 0.00 0.00 0.00 0.00 4.92
TCAG 0.00 0.00 0.00 0.00 10.31 0.00 0.00 0.00 0.00 0.00 0.00 9.03 0.00 0.00
TCCG 0.00 0.00 0.00 0.00 5.10 0.00 0.00 0.00 0.00 0.00 0.00 4.95 0.00 0.00
TCGG 0.00 8.40 0.00 0.00 0.00 0.00 0.00 0.00 8.65 0.00 0.00 0.00 0.00 0.00
TCTG 0.00 0.00 0.00 0.00 14.10 0.00 0.00 0.00 0.00 0.00 0.00 12.53 0.00 0.00
ACAT 0.00 0.00 0.00 7.67 0.00 0.00 0.00 0.00 0.00 0.00 7.71 0.00 0.00 0.00
ACCT 4.78 0.00 0.00 0.00 0.00 0.00 0.00 5.02 0.00 0.00 0.00 0.00 0.00 0.00
ACGT 23.47 0.00 0.00 0.00 0.00 0.00 0.00 23.18 0.00 0.00 0.00 0.00 0.00 0.00
ACTT 0.00 0.00 0.00 5.43 0.00 0.00 0.00 0.00 0.00 0.00 5.47 0.00 0.00 0.00
CCAT 0.00 0.00 0.00 6.02 0.00 0.00 0.00 0.00 0.00 0.00 6.02 0.00 0.00 0.00
CCCT 0.00 0.00 0.00 5.59 0.00 0.00 0.00 0.00 0.00 0.00 5.63 0.00 0.00 0.00
CCGT 17.66 0.00 0.00 0.00 0.00 0.00 0.00 17.12 0.00 0.00 0.00 0.00 0.00 0.00
CCTT 0.00 0.00 0.00 7.01 0.00 0.00 0.00 0.00 0.00 0.00 7.04 0.00 0.00 0.00
GCAT 0.00 0.00 0.00 5.98 0.00 0.00 0.00 0.00 0.00 0.00 6.01 0.00 0.00 0.00
GCCT 5.74 0.00 0.00 0.00 0.00 0.00 0.00 5.93 0.00 0.00 0.00 0.00 0.00 0.00
GCGT 20.46 0.00 0.00 0.00 0.00 0.00 0.00 19.80 0.00 0.00 0.00 0.00 0.00 0.00
GCTT 0.00 0.00 0.00 5.88 0.00 0.00 0.00 0.00 0.00 0.00 5.93 0.00 0.00 0.00
TCAT 11.42 0.00 0.00 0.00 0.00 0.00 0.00 12.00 0.00 0.00 0.00 0.00 0.00 0.00
TCCT 0.00 0.00 0.00 7.81 0.00 0.00 0.00 0.00 0.00 0.00 7.76 0.00 0.00 0.00
TCGT 12.42 0.00 0.00 0.00 0.00 0.00 0.00 12.30 0.00 0.00 0.00 0.00 0.00 0.00
TCTT 0.00 0.00 0.00 9.47 0.00 0.00 0.00 0.00 0.00 0.00 9.29 0.00 0.00 0.00
43",2017-03-02T10:39:59Z,table hts table table usteri table subseeafor  iitablsubsemt             
paper_qf_45.pdf,45,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Table 6: Table 5 continued: weights for the next 48 mutation categories.
Mutation Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7 Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7
ATAA 0.00 0.00 0.00 0.00 4.18 0.00 0.00 0.00 0.00 0.00 0.00 4.52 0.00 0.00
ATCA 0.00 0.00 10.00 0.00 0.00 0.00 0.00 0.00 0.00 10.15 0.00 0.00 0.00 0.00
ATGA 0.00 0.00 0.00 0.00 4.02 0.00 0.00 0.00 0.00 0.00 0.00 4.30 0.00 0.00
ATTA 0.00 0.00 0.00 0.00 0.00 5.54 0.00 0.00 0.00 0.00 0.00 0.00 5.66 0.00
CTAA 0.00 0.00 11.74 0.00 0.00 0.00 0.00 0.00 0.00 11.16 0.00 0.00 0.00 0.00
CTCA 0.00 0.00 0.00 0.00 3.79 0.00 0.00 0.00 0.00 0.00 0.00 3.98 0.00 0.00
CTGA 0.00 0.00 0.00 0.00 4.88 0.00 0.00 0.00 0.00 0.00 0.00 5.02 0.00 0.00
CTTA 0.00 0.00 0.00 0.00 0.00 4.28 0.00 0.00 0.00 0.00 0.00 0.00 4.33 0.00
GTAA 0.00 0.00 0.00 0.00 0.00 0.00 4.30 0.00 0.00 0.00 0.00 0.00 0.00 4.35
GTCA 0.00 15.20 0.00 0.00 0.00 0.00 0.00 0.00 15.36 0.00 0.00 0.00 0.00 0.00
GTGA 0.00 0.00 9.28 0.00 0.00 0.00 0.00 0.00 0.00 9.21 0.00 0.00 0.00 0.00
GTTA 0.00 0.00 0.00 0.00 0.00 0.00 5.13 0.00 0.00 0.00 0.00 0.00 0.00 5.19
TTAA 0.00 0.00 0.00 0.00 0.00 5.13 0.00 0.00 0.00 0.00 0.00 0.00 5.26 0.00
TTCA 0.00 0.00 0.00 0.00 0.00 0.00 6.64 0.00 0.00 0.00 0.00 0.00 0.00 6.58
TTGA 0.00 0.00 8.84 0.00 0.00 0.00 0.00 0.00 0.00 8.55 0.00 0.00 0.00 0.00
TTTA 0.00 0.00 0.00 0.00 0.00 5.27 0.00 0.00 0.00 0.00 0.00 0.00 5.38 0.00
ATAC 0.00 0.00 0.00 7.03 0.00 0.00 0.00 0.00 0.00 0.00 7.06 0.00 0.00 0.00
ATCC 0.00 0.00 0.00 0.00 0.00 3.30 0.00 0.00 0.00 0.00 0.00 0.00 3.39 0.00
ATGC 0.00 0.00 0.00 4.97 0.00 0.00 0.00 0.00 0.00 0.00 4.98 0.00 0.00 0.00
ATTC 0.00 0.00 0.00 6.30 0.00 0.00 0.00 0.00 0.00 0.00 6.34 0.00 0.00 0.00
CTAC 0.00 0.00 0.00 0.00 0.00 3.78 0.00 0.00 0.00 0.00 0.00 0.00 3.81 0.00
CTCC 0.00 0.00 0.00 0.00 0.00 4.30 0.00 0.00 0.00 0.00 0.00 0.00 4.31 0.00
CTGC 0.00 0.00 0.00 0.00 0.00 5.37 0.00 0.00 0.00 0.00 0.00 0.00 5.41 0.00
CTTC 0.00 0.00 0.00 0.00 0.00 7.14 0.00 0.00 0.00 0.00 0.00 0.00 6.92 0.00
GTAC 0.00 0.00 0.00 0.00 0.00 4.84 0.00 0.00 0.00 0.00 0.00 0.00 4.96 0.00
GTCC 0.00 0.00 11.51 0.00 0.00 0.00 0.00 0.00 0.00 11.78 0.00 0.00 0.00 0.00
GTGC 0.00 0.00 0.00 0.00 0.00 4.32 0.00 0.00 0.00 0.00 0.00 0.00 4.43 0.00
GTTC 0.00 0.00 0.00 0.00 0.00 5.05 0.00 0.00 0.00 0.00 0.00 0.00 5.23 0.00
TTAC 0.00 0.00 0.00 0.00 0.00 4.97 0.00 0.00 0.00 0.00 0.00 0.00 5.10 0.00
TTCC 0.00 0.00 0.00 0.00 0.00 4.69 0.00 0.00 0.00 0.00 0.00 0.00 4.79 0.00
TTGC 0.00 0.00 11.62 0.00 0.00 0.00 0.00 0.00 0.00 11.82 0.00 0.00 0.00 0.00
TTTC 0.00 0.00 0.00 0.00 0.00 7.29 0.00 0.00 0.00 0.00 0.00 0.00 7.28 0.00
ATAG 0.00 0.00 0.00 0.00 0.00 0.00 3.98 0.00 0.00 0.00 0.00 0.00 0.00 4.09
ATCG 0.00 0.00 0.00 0.00 0.00 0.00 3.81 0.00 0.00 0.00 0.00 0.00 0.00 3.70
ATGG 0.00 0.00 0.00 0.00 0.00 0.00 3.97 0.00 0.00 0.00 0.00 0.00 0.00 3.99
ATTG 0.00 0.00 0.00 0.00 0.00 0.00 7.13 0.00 0.00 0.00 0.00 0.00 0.00 7.08
CTAG 0.00 0.00 0.00 0.00 0.00 0.00 3.55 0.00 0.00 0.00 0.00 0.00 0.00 3.56
CTCG 0.00 0.00 0.00 0.00 0.00 0.00 6.52 0.00 0.00 0.00 0.00 0.00 0.00 6.31
CTGG 0.00 0.00 0.00 0.00 0.00 0.00 3.67 0.00 0.00 0.00 0.00 0.00 0.00 3.83
CTTG 0.00 0.00 0.00 0.00 0.00 9.67 0.00 0.00 0.00 0.00 0.00 0.00 8.89 0.00
GTAG 0.00 0.00 0.00 0.00 0.00 0.00 3.58 0.00 0.00 0.00 0.00 0.00 0.00 3.49
GTCG 0.00 7.80 0.00 0.00 0.00 0.00 0.00 0.00 8.11 0.00 0.00 0.00 0.00 0.00
GTGG 0.00 0.00 0.00 0.00 0.00 0.00 3.82 0.00 0.00 0.00 0.00 0.00 0.00 3.98
GTTG 0.00 0.00 0.00 0.00 0.00 0.00 7.02 0.00 0.00 0.00 0.00 0.00 0.00 6.97
TTAG 0.00 0.00 0.00 0.00 0.00 0.00 4.24 0.00 0.00 0.00 0.00 0.00 0.00 4.43
TTCG 0.00 0.00 0.00 0.00 0.00 0.00 3.73 0.00 0.00 0.00 0.00 0.00 0.00 3.75
TTGG 0.00 0.00 0.00 0.00 0.00 0.00 6.10 0.00 0.00 0.00 0.00 0.00 0.00 6.06
TTTG 0.00 0.00 0.00 0.00 0.00 8.31 0.00 0.00 0.00 0.00 0.00 0.00 8.05 0.00
44",2017-03-02T10:39:59Z,table table mt             
paper_qf_45.pdf,46,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Table 7: Weights for the rst 48 mutation categories for the 7 clusters in Clustering-
B (see Table 4). The conventions are the same as in Table 5.
Mutation Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7 Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7
ACAA 0.00 0.00 0.00 0.00 6.55 0.00 0.00 0.00 0.00 0.00 0.00 6.55 0.00 0.00
ACCA 0.00 0.00 0.00 6.66 0.00 0.00 0.00 0.00 0.00 0.00 7.03 0.00 0.00 0.00
ACGA 0.00 0.00 0.00 0.00 0.00 0.00 3.07 0.00 0.00 0.00 0.00 0.00 0.00 2.91
ACTA 0.00 0.00 0.00 6.99 0.00 0.00 0.00 0.00 0.00 0.00 7.33 0.00 0.00 0.00
CCAA 0.00 0.00 0.00 9.30 0.00 0.00 0.00 0.00 0.00 0.00 9.73 0.00 0.00 0.00
CCCA 0.00 0.00 0.00 7.52 0.00 0.00 0.00 0.00 0.00 0.00 7.93 0.00 0.00 0.00
CCGA 0.00 0.00 0.00 0.00 0.00 0.00 2.41 0.00 0.00 0.00 0.00 0.00 0.00 2.51
CCTA 0.00 0.00 0.00 8.69 0.00 0.00 0.00 0.00 0.00 0.00 9.10 0.00 0.00 0.00
GCAA 0.00 4.05 0.00 0.00 0.00 0.00 0.00 0.00 4.65 0.00 0.00 0.00 0.00 0.00
GCCA 0.00 0.00 0.00 5.20 0.00 0.00 0.00 0.00 0.00 0.00 5.46 0.00 0.00 0.00
GCGA 0.00 0.00 13.81 0.00 0.00 0.00 0.00 0.00 0.00 13.89 0.00 0.00 0.00 0.00
GCTA 0.00 0.00 0.00 5.70 0.00 0.00 0.00 0.00 0.00 0.00 5.97 0.00 0.00 0.00
TCAA 0.00 0.00 0.00 0.00 6.26 0.00 0.00 0.00 0.00 0.00 0.00 6.21 0.00 0.00
TCCA 0.00 0.00 0.00 9.80 0.00 0.00 0.00 0.00 0.00 0.00 10.21 0.00 0.00 0.00
TCGA 0.00 0.00 11.87 0.00 0.00 0.00 0.00 0.00 0.00 12.24 0.00 0.00 0.00 0.00
TCTA 0.00 0.00 0.00 0.00 8.05 0.00 0.00 0.00 0.00 0.00 0.00 8.00 0.00 0.00
ACAG 13.14 0.00 0.00 0.00 0.00 0.00 0.00 13.38 0.00 0.00 0.00 0.00 0.00 0.00
ACCG 0.00 0.00 0.00 0.00 0.00 0.00 2.76 0.00 0.00 0.00 0.00 0.00 0.00 2.91
ACGG 0.00 0.00 12.62 0.00 0.00 0.00 0.00 0.00 0.00 12.22 0.00 0.00 0.00 0.00
ACTG 17.06 0.00 0.00 0.00 0.00 0.00 0.00 17.11 0.00 0.00 0.00 0.00 0.00 0.00
CCAG 0.00 0.00 0.00 0.00 0.00 0.00 3.16 0.00 0.00 0.00 0.00 0.00 0.00 3.32
CCCG 0.00 0.00 0.00 0.00 0.00 0.00 3.22 0.00 0.00 0.00 0.00 0.00 0.00 3.24
CCGG 0.00 0.00 0.00 0.00 0.00 0.00 3.64 0.00 0.00 0.00 0.00 0.00 0.00 3.27
CCTG 0.00 0.00 0.00 0.00 0.00 0.00 4.32 0.00 0.00 0.00 0.00 0.00 0.00 4.51
GCAG 0.00 0.00 0.00 0.00 0.00 0.00 3.48 0.00 0.00 0.00 0.00 0.00 0.00 3.34
GCCG 0.00 0.00 14.79 0.00 0.00 0.00 0.00 0.00 0.00 15.62 0.00 0.00 0.00 0.00
GCGG 0.00 0.00 15.50 0.00 0.00 0.00 0.00 0.00 0.00 13.92 0.00 0.00 0.00 0.00
GCTG 0.00 0.00 0.00 0.00 0.00 0.00 3.82 0.00 0.00 0.00 0.00 0.00 0.00 3.78
TCAG 0.00 0.00 0.00 12.20 0.00 0.00 0.00 0.00 0.00 0.00 10.90 0.00 0.00 0.00
TCCG 0.00 0.00 0.00 5.76 0.00 0.00 0.00 0.00 0.00 0.00 5.60 0.00 0.00 0.00
TCGG 0.00 0.00 8.40 0.00 0.00 0.00 0.00 0.00 0.00 8.65 0.00 0.00 0.00 0.00
TCTG 0.00 0.00 0.00 16.63 0.00 0.00 0.00 0.00 0.00 0.00 15.02 0.00 0.00 0.00
ACAT 0.00 0.00 0.00 0.00 7.67 0.00 0.00 0.00 0.00 0.00 0.00 7.71 0.00 0.00
ACCT 0.00 4.78 0.00 0.00 0.00 0.00 0.00 0.00 5.02 0.00 0.00 0.00 0.00 0.00
ACGT 0.00 23.47 0.00 0.00 0.00 0.00 0.00 0.00 23.18 0.00 0.00 0.00 0.00 0.00
ACTT 0.00 0.00 0.00 0.00 5.43 0.00 0.00 0.00 0.00 0.00 0.00 5.47 0.00 0.00
CCAT 0.00 0.00 0.00 0.00 6.02 0.00 0.00 0.00 0.00 0.00 0.00 6.02 0.00 0.00
CCCT 0.00 0.00 0.00 0.00 5.59 0.00 0.00 0.00 0.00 0.00 0.00 5.63 0.00 0.00
CCGT 0.00 17.66 0.00 0.00 0.00 0.00 0.00 0.00 17.12 0.00 0.00 0.00 0.00 0.00
CCTT 0.00 0.00 0.00 0.00 7.01 0.00 0.00 0.00 0.00 0.00 0.00 7.04 0.00 0.00
GCAT 0.00 0.00 0.00 0.00 5.98 0.00 0.00 0.00 0.00 0.00 0.00 6.01 0.00 0.00
GCCT 0.00 5.74 0.00 0.00 0.00 0.00 0.00 0.00 5.93 0.00 0.00 0.00 0.00 0.00
GCGT 0.00 20.46 0.00 0.00 0.00 0.00 0.00 0.00 19.80 0.00 0.00 0.00 0.00 0.00
GCTT 0.00 0.00 0.00 0.00 5.88 0.00 0.00 0.00 0.00 0.00 0.00 5.93 0.00 0.00
TCAT 0.00 11.42 0.00 0.00 0.00 0.00 0.00 0.00 12.00 0.00 0.00 0.00 0.00 0.00
TCCT 0.00 0.00 0.00 0.00 7.81 0.00 0.00 0.00 0.00 0.00 0.00 7.76 0.00 0.00
TCGT 0.00 12.42 0.00 0.00 0.00 0.00 0.00 0.00 12.30 0.00 0.00 0.00 0.00 0.00
TCTT 0.00 0.00 0.00 0.00 9.47 0.00 0.00 0.00 0.00 0.00 0.00 9.29 0.00 0.00
45",2017-03-02T10:39:59Z,table hts usteri table t table mt             
paper_qf_45.pdf,47,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Table 8: Table 7 continued: weights for the next 48 mutation categories.
Mutation Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7 Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7
ATAA 14.61 0.00 0.00 0.00 0.00 0.00 0.00 14.91 0.00 0.00 0.00 0.00 0.00 0.00
ATCA 0.00 0.00 0.00 0.00 0.00 0.00 3.46 0.00 0.00 0.00 0.00 0.00 0.00 3.65
ATGA 13.85 0.00 0.00 0.00 0.00 0.00 0.00 14.13 0.00 0.00 0.00 0.00 0.00 0.00
ATTA 0.00 0.00 0.00 0.00 0.00 5.95 0.00 0.00 0.00 0.00 0.00 0.00 6.08 0.00
CTAA 0.00 0.00 0.00 0.00 0.00 0.00 3.93 0.00 0.00 0.00 0.00 0.00 0.00 4.01
CTCA 12.75 0.00 0.00 0.00 0.00 0.00 0.00 12.93 0.00 0.00 0.00 0.00 0.00 0.00
CTGA 0.00 0.00 0.00 5.54 0.00 0.00 0.00 0.00 0.00 0.00 5.73 0.00 0.00 0.00
CTTA 0.00 0.00 0.00 0.00 0.00 4.55 0.00 0.00 0.00 0.00 0.00 0.00 4.62 0.00
GTAA 0.00 0.00 0.00 0.00 0.00 0.00 3.44 0.00 0.00 0.00 0.00 0.00 0.00 3.36
GTCA 0.00 0.00 15.20 0.00 0.00 0.00 0.00 0.00 0.00 15.36 0.00 0.00 0.00 0.00
GTGA 0.00 0.00 0.00 0.00 0.00 0.00 3.13 0.00 0.00 0.00 0.00 0.00 0.00 3.26
GTTA 0.00 0.00 0.00 0.00 0.00 0.00 4.04 0.00 0.00 0.00 0.00 0.00 0.00 4.00
TTAA 0.00 0.00 0.00 0.00 0.00 5.49 0.00 0.00 0.00 0.00 0.00 0.00 5.64 0.00
TTCA 0.00 0.00 0.00 0.00 0.00 0.00 4.99 0.00 0.00 0.00 0.00 0.00 0.00 4.77
TTGA 0.00 0.00 0.00 0.00 0.00 0.00 2.94 0.00 0.00 0.00 0.00 0.00 0.00 3.05
TTTA 0.00 0.00 0.00 0.00 0.00 5.65 0.00 0.00 0.00 0.00 0.00 0.00 5.77 0.00
ATAC 0.00 0.00 0.00 0.00 7.03 0.00 0.00 0.00 0.00 0.00 0.00 7.06 0.00 0.00
ATCC 0.00 0.00 0.00 0.00 0.00 3.53 0.00 0.00 0.00 0.00 0.00 0.00 3.64 0.00
ATGC 0.00 0.00 0.00 0.00 4.97 0.00 0.00 0.00 0.00 0.00 0.00 4.98 0.00 0.00
ATTC 0.00 0.00 0.00 0.00 6.30 0.00 0.00 0.00 0.00 0.00 0.00 6.34 0.00 0.00
CTAC 0.00 0.00 0.00 0.00 0.00 4.03 0.00 0.00 0.00 0.00 0.00 0.00 4.08 0.00
CTCC 0.00 0.00 0.00 0.00 0.00 4.59 0.00 0.00 0.00 0.00 0.00 0.00 4.61 0.00
CTGC 0.00 0.00 0.00 0.00 0.00 5.75 0.00 0.00 0.00 0.00 0.00 0.00 5.79 0.00
CTTC 0.00 0.00 0.00 0.00 0.00 7.65 0.00 0.00 0.00 0.00 0.00 0.00 7.41 0.00
GTAC 0.00 0.00 0.00 0.00 0.00 5.18 0.00 0.00 0.00 0.00 0.00 0.00 5.32 0.00
GTCC 0.00 0.00 0.00 0.00 0.00 0.00 4.18 0.00 0.00 0.00 0.00 0.00 0.00 4.36
GTGC 0.00 0.00 0.00 0.00 0.00 4.63 0.00 0.00 0.00 0.00 0.00 0.00 4.76 0.00
GTTC 0.00 0.00 0.00 0.00 0.00 5.42 0.00 0.00 0.00 0.00 0.00 0.00 5.62 0.00
TTAC 0.00 0.00 0.00 0.00 0.00 5.32 0.00 0.00 0.00 0.00 0.00 0.00 5.47 0.00
TTCC 0.00 0.00 0.00 0.00 0.00 5.05 0.00 0.00 0.00 0.00 0.00 0.00 5.16 0.00
TTGC 0.00 0.00 0.00 0.00 0.00 0.00 3.98 0.00 0.00 0.00 0.00 0.00 0.00 4.20
TTTC 0.00 0.00 0.00 0.00 0.00 7.82 0.00 0.00 0.00 0.00 0.00 0.00 7.82 0.00
ATAG 0.00 0.00 0.00 0.00 0.00 0.00 3.18 0.00 0.00 0.00 0.00 0.00 0.00 3.24
ATCG 0.00 0.00 0.00 0.00 0.00 0.00 2.70 0.00 0.00 0.00 0.00 0.00 0.00 2.52
ATGG 0.00 0.00 0.00 0.00 0.00 0.00 3.09 0.00 0.00 0.00 0.00 0.00 0.00 3.04
ATTG 14.44 0.00 0.00 0.00 0.00 0.00 0.00 14.09 0.00 0.00 0.00 0.00 0.00 0.00
CTAG 0.00 0.00 0.00 0.00 0.00 0.00 2.68 0.00 0.00 0.00 0.00 0.00 0.00 2.66
CTCG 0.00 0.00 0.00 0.00 0.00 0.00 4.59 0.00 0.00 0.00 0.00 0.00 0.00 4.30
CTGG 0.00 0.00 0.00 0.00 0.00 0.00 3.07 0.00 0.00 0.00 0.00 0.00 0.00 3.19
CTTG 0.00 0.00 0.00 0.00 0.00 10.44 0.00 0.00 0.00 0.00 0.00 0.00 9.55 0.00
GTAG 0.00 0.00 0.00 0.00 0.00 0.00 2.52 0.00 0.00 0.00 0.00 0.00 0.00 2.36
GTCG 0.00 0.00 7.80 0.00 0.00 0.00 0.00 0.00 0.00 8.11 0.00 0.00 0.00 0.00
GTGG 0.00 0.00 0.00 0.00 0.00 0.00 3.18 0.00 0.00 0.00 0.00 0.00 0.00 3.25
GTTG 14.14 0.00 0.00 0.00 0.00 0.00 0.00 13.44 0.00 0.00 0.00 0.00 0.00 0.00
TTAG 0.00 0.00 0.00 0.00 0.00 0.00 3.48 0.00 0.00 0.00 0.00 0.00 0.00 3.62
TTCG 0.00 0.00 0.00 0.00 0.00 0.00 2.91 0.00 0.00 0.00 0.00 0.00 0.00 2.87
TTGG 0.00 0.00 0.00 0.00 0.00 0.00 4.62 0.00 0.00 0.00 0.00 0.00 0.00 4.49
TTTG 0.00 0.00 0.00 0.00 0.00 8.95 0.00 0.00 0.00 0.00 0.00 0.00 8.65 0.00
46",2017-03-02T10:39:59Z,table table mt             
paper_qf_45.pdf,48,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Table 9: Weights for the rst 48 mutation categories for the 7 clusters in Clustering-
C (see Table 4). The conventions are the same as in Table 5.
Mutation Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7 Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7
ACAA 0.00 0.00 0.00 6.55 0.00 0.00 0.00 0.00 0.00 0.00 6.55 0.00 0.00 0.00
ACCA 0.00 0.00 0.00 0.00 5.83 0.00 0.00 0.00 0.00 0.00 0.00 6.08 0.00 0.00
ACGA 0.00 0.00 0.00 0.00 0.00 0.00 4.06 0.00 0.00 0.00 0.00 0.00 0.00 4.00
ACTA 0.00 0.00 0.00 0.00 6.16 0.00 0.00 0.00 0.00 0.00 0.00 6.38 0.00 0.00
CCAA 0.00 0.00 0.00 0.00 7.91 0.00 0.00 0.00 0.00 0.00 0.00 8.10 0.00 0.00
CCCA 0.00 0.00 0.00 0.00 6.46 0.00 0.00 0.00 0.00 0.00 0.00 6.68 0.00 0.00
CCGA 0.00 0.00 6.40 0.00 0.00 0.00 0.00 0.00 0.00 6.40 0.00 0.00 0.00 0.00
CCTA 0.00 0.00 0.00 0.00 0.00 6.97 0.00 0.00 0.00 0.00 0.00 0.00 7.02 0.00
GCAA 4.05 0.00 0.00 0.00 0.00 0.00 0.00 4.65 0.00 0.00 0.00 0.00 0.00 0.00
GCCA 0.00 0.00 0.00 0.00 4.56 0.00 0.00 0.00 0.00 0.00 0.00 4.73 0.00 0.00
GCGA 0.00 13.81 0.00 0.00 0.00 0.00 0.00 0.00 13.89 0.00 0.00 0.00 0.00 0.00
GCTA 0.00 0.00 0.00 0.00 5.02 0.00 0.00 0.00 0.00 0.00 0.00 5.20 0.00 0.00
TCAA 0.00 0.00 0.00 6.26 0.00 0.00 0.00 0.00 0.00 0.00 6.21 0.00 0.00 0.00
TCCA 0.00 0.00 0.00 0.00 8.94 0.00 0.00 0.00 0.00 0.00 0.00 9.29 0.00 0.00
TCGA 0.00 11.87 0.00 0.00 0.00 0.00 0.00 0.00 12.24 0.00 0.00 0.00 0.00 0.00
TCTA 0.00 0.00 0.00 8.05 0.00 0.00 0.00 0.00 0.00 0.00 8.00 0.00 0.00 0.00
ACAG 0.00 0.00 0.00 0.00 3.96 0.00 0.00 0.00 0.00 0.00 0.00 4.18 0.00 0.00
ACCG 0.00 0.00 7.23 0.00 0.00 0.00 0.00 0.00 0.00 7.29 0.00 0.00 0.00 0.00
ACGG 0.00 12.62 0.00 0.00 0.00 0.00 0.00 0.00 12.22 0.00 0.00 0.00 0.00 0.00
ACTG 0.00 0.00 0.00 0.00 4.77 0.00 0.00 0.00 0.00 0.00 0.00 5.03 0.00 0.00
CCAG 0.00 0.00 8.26 0.00 0.00 0.00 0.00 0.00 0.00 8.31 0.00 0.00 0.00 0.00
CCCG 0.00 0.00 0.00 0.00 0.00 0.00 3.91 0.00 0.00 0.00 0.00 0.00 0.00 4.02
CCGG 0.00 0.00 0.00 0.00 0.00 0.00 5.37 0.00 0.00 0.00 0.00 0.00 0.00 5.12
CCTG 0.00 0.00 11.12 0.00 0.00 0.00 0.00 0.00 0.00 11.19 0.00 0.00 0.00 0.00
GCAG 0.00 0.00 0.00 0.00 0.00 0.00 4.61 0.00 0.00 0.00 0.00 0.00 0.00 4.57
GCCG 0.00 14.79 0.00 0.00 0.00 0.00 0.00 0.00 15.62 0.00 0.00 0.00 0.00 0.00
GCGG 0.00 15.50 0.00 0.00 0.00 0.00 0.00 0.00 13.92 0.00 0.00 0.00 0.00 0.00
GCTG 0.00 0.00 0.00 0.00 0.00 0.00 4.86 0.00 0.00 0.00 0.00 0.00 0.00 4.92
TCAG 0.00 0.00 0.00 0.00 10.31 0.00 0.00 0.00 0.00 0.00 0.00 9.03 0.00 0.00
TCCG 0.00 0.00 0.00 0.00 5.10 0.00 0.00 0.00 0.00 0.00 0.00 4.95 0.00 0.00
TCGG 0.00 8.40 0.00 0.00 0.00 0.00 0.00 0.00 8.65 0.00 0.00 0.00 0.00 0.00
TCTG 0.00 0.00 0.00 0.00 14.10 0.00 0.00 0.00 0.00 0.00 0.00 12.53 0.00 0.00
ACAT 0.00 0.00 0.00 7.67 0.00 0.00 0.00 0.00 0.00 0.00 7.71 0.00 0.00 0.00
ACCT 4.78 0.00 0.00 0.00 0.00 0.00 0.00 5.02 0.00 0.00 0.00 0.00 0.00 0.00
ACGT 23.47 0.00 0.00 0.00 0.00 0.00 0.00 23.18 0.00 0.00 0.00 0.00 0.00 0.00
ACTT 0.00 0.00 0.00 5.43 0.00 0.00 0.00 0.00 0.00 0.00 5.47 0.00 0.00 0.00
CCAT 0.00 0.00 0.00 6.02 0.00 0.00 0.00 0.00 0.00 0.00 6.02 0.00 0.00 0.00
CCCT 0.00 0.00 0.00 5.59 0.00 0.00 0.00 0.00 0.00 0.00 5.63 0.00 0.00 0.00
CCGT 17.66 0.00 0.00 0.00 0.00 0.00 0.00 17.12 0.00 0.00 0.00 0.00 0.00 0.00
CCTT 0.00 0.00 0.00 7.01 0.00 0.00 0.00 0.00 0.00 0.00 7.04 0.00 0.00 0.00
GCAT 0.00 0.00 0.00 5.98 0.00 0.00 0.00 0.00 0.00 0.00 6.01 0.00 0.00 0.00
GCCT 5.74 0.00 0.00 0.00 0.00 0.00 0.00 5.93 0.00 0.00 0.00 0.00 0.00 0.00
GCGT 20.46 0.00 0.00 0.00 0.00 0.00 0.00 19.80 0.00 0.00 0.00 0.00 0.00 0.00
GCTT 0.00 0.00 0.00 5.88 0.00 0.00 0.00 0.00 0.00 0.00 5.93 0.00 0.00 0.00
TCAT 11.42 0.00 0.00 0.00 0.00 0.00 0.00 12.00 0.00 0.00 0.00 0.00 0.00 0.00
TCCT 0.00 0.00 0.00 7.81 0.00 0.00 0.00 0.00 0.00 0.00 7.76 0.00 0.00 0.00
TCGT 12.42 0.00 0.00 0.00 0.00 0.00 0.00 12.30 0.00 0.00 0.00 0.00 0.00 0.00
TCTT 0.00 0.00 0.00 9.47 0.00 0.00 0.00 0.00 0.00 0.00 9.29 0.00 0.00 0.00
47",2017-03-02T10:39:59Z,table hts usteri table t table mt             
paper_qf_45.pdf,49,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Table 10: Table 9 continued: weights for the next 48 mutation categories.
Mutation Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7 Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7
ATAA 0.00 0.00 0.00 0.00 4.18 0.00 0.00 0.00 0.00 0.00 0.00 4.52 0.00 0.00
ATCA 0.00 0.00 8.99 0.00 0.00 0.00 0.00 0.00 0.00 9.10 0.00 0.00 0.00 0.00
ATGA 0.00 0.00 0.00 0.00 4.02 0.00 0.00 0.00 0.00 0.00 0.00 4.30 0.00 0.00
ATTA 0.00 0.00 0.00 0.00 0.00 5.72 0.00 0.00 0.00 0.00 0.00 0.00 5.85 0.00
CTAA 0.00 0.00 10.32 0.00 0.00 0.00 0.00 0.00 0.00 9.83 0.00 0.00 0.00 0.00
CTCA 0.00 0.00 0.00 0.00 3.79 0.00 0.00 0.00 0.00 0.00 0.00 3.98 0.00 0.00
CTGA 0.00 0.00 0.00 0.00 4.88 0.00 0.00 0.00 0.00 0.00 0.00 5.02 0.00 0.00
CTTA 0.00 0.00 0.00 0.00 0.00 4.42 0.00 0.00 0.00 0.00 0.00 0.00 4.47 0.00
GTAA 0.00 0.00 0.00 0.00 0.00 0.00 4.30 0.00 0.00 0.00 0.00 0.00 0.00 4.35
GTCA 0.00 15.20 0.00 0.00 0.00 0.00 0.00 0.00 15.36 0.00 0.00 0.00 0.00 0.00
GTGA 0.00 0.00 8.23 0.00 0.00 0.00 0.00 0.00 0.00 8.16 0.00 0.00 0.00 0.00
GTTA 0.00 0.00 0.00 0.00 0.00 0.00 5.13 0.00 0.00 0.00 0.00 0.00 0.00 5.19
TTAA 0.00 0.00 0.00 0.00 0.00 5.30 0.00 0.00 0.00 0.00 0.00 0.00 5.43 0.00
TTCA 0.00 0.00 0.00 0.00 0.00 0.00 6.64 0.00 0.00 0.00 0.00 0.00 0.00 6.58
TTGA 0.00 0.00 7.82 0.00 0.00 0.00 0.00 0.00 0.00 7.57 0.00 0.00 0.00 0.00
TTTA 0.00 0.00 0.00 0.00 0.00 5.44 0.00 0.00 0.00 0.00 0.00 0.00 5.55 0.00
ATAC 0.00 0.00 0.00 7.03 0.00 0.00 0.00 0.00 0.00 0.00 7.06 0.00 0.00 0.00
ATCC 0.00 0.00 10.75 0.00 0.00 0.00 0.00 0.00 0.00 10.92 0.00 0.00 0.00 0.00
ATGC 0.00 0.00 0.00 4.97 0.00 0.00 0.00 0.00 0.00 0.00 4.98 0.00 0.00 0.00
ATTC 0.00 0.00 0.00 6.30 0.00 0.00 0.00 0.00 0.00 0.00 6.34 0.00 0.00 0.00
CTAC 0.00 0.00 0.00 0.00 0.00 3.91 0.00 0.00 0.00 0.00 0.00 0.00 3.94 0.00
CTCC 0.00 0.00 0.00 0.00 0.00 4.44 0.00 0.00 0.00 0.00 0.00 0.00 4.45 0.00
CTGC 0.00 0.00 0.00 0.00 0.00 5.56 0.00 0.00 0.00 0.00 0.00 0.00 5.61 0.00
CTTC 0.00 0.00 0.00 0.00 0.00 7.39 0.00 0.00 0.00 0.00 0.00 0.00 7.16 0.00
GTAC 0.00 0.00 0.00 0.00 0.00 5.00 0.00 0.00 0.00 0.00 0.00 0.00 5.14 0.00
GTCC 0.00 0.00 10.41 0.00 0.00 0.00 0.00 0.00 0.00 10.61 0.00 0.00 0.00 0.00
GTGC 0.00 0.00 0.00 0.00 0.00 4.47 0.00 0.00 0.00 0.00 0.00 0.00 4.59 0.00
GTTC 0.00 0.00 0.00 0.00 0.00 5.21 0.00 0.00 0.00 0.00 0.00 0.00 5.40 0.00
TTAC 0.00 0.00 0.00 0.00 0.00 5.13 0.00 0.00 0.00 0.00 0.00 0.00 5.26 0.00
TTCC 0.00 0.00 0.00 0.00 0.00 4.84 0.00 0.00 0.00 0.00 0.00 0.00 4.96 0.00
TTGC 0.00 0.00 10.48 0.00 0.00 0.00 0.00 0.00 0.00 10.62 0.00 0.00 0.00 0.00
TTTC 0.00 0.00 0.00 0.00 0.00 7.53 0.00 0.00 0.00 0.00 0.00 0.00 7.52 0.00
ATAG 0.00 0.00 0.00 0.00 0.00 0.00 3.98 0.00 0.00 0.00 0.00 0.00 0.00 4.09
ATCG 0.00 0.00 0.00 0.00 0.00 0.00 3.81 0.00 0.00 0.00 0.00 0.00 0.00 3.70
ATGG 0.00 0.00 0.00 0.00 0.00 0.00 3.97 0.00 0.00 0.00 0.00 0.00 0.00 3.99
ATTG 0.00 0.00 0.00 0.00 0.00 0.00 7.13 0.00 0.00 0.00 0.00 0.00 0.00 7.08
CTAG 0.00 0.00 0.00 0.00 0.00 0.00 3.55 0.00 0.00 0.00 0.00 0.00 0.00 3.56
CTCG 0.00 0.00 0.00 0.00 0.00 0.00 6.52 0.00 0.00 0.00 0.00 0.00 0.00 6.31
CTGG 0.00 0.00 0.00 0.00 0.00 0.00 3.67 0.00 0.00 0.00 0.00 0.00 0.00 3.83
CTTG 0.00 0.00 0.00 0.00 0.00 10.06 0.00 0.00 0.00 0.00 0.00 0.00 9.27 0.00
GTAG 0.00 0.00 0.00 0.00 0.00 0.00 3.58 0.00 0.00 0.00 0.00 0.00 0.00 3.49
GTCG 0.00 7.80 0.00 0.00 0.00 0.00 0.00 0.00 8.11 0.00 0.00 0.00 0.00 0.00
GTGG 0.00 0.00 0.00 0.00 0.00 0.00 3.82 0.00 0.00 0.00 0.00 0.00 0.00 3.98
GTTG 0.00 0.00 0.00 0.00 0.00 0.00 7.02 0.00 0.00 0.00 0.00 0.00 0.00 6.97
TTAG 0.00 0.00 0.00 0.00 0.00 0.00 4.24 0.00 0.00 0.00 0.00 0.00 0.00 4.43
TTCG 0.00 0.00 0.00 0.00 0.00 0.00 3.73 0.00 0.00 0.00 0.00 0.00 0.00 3.75
TTGG 0.00 0.00 0.00 0.00 0.00 0.00 6.10 0.00 0.00 0.00 0.00 0.00 0.00 6.06
TTTG 0.00 0.00 0.00 0.00 0.00 8.61 0.00 0.00 0.00 0.00 0.00 0.00 8.36 0.00
48",2017-03-02T10:39:59Z,table table mt             
paper_qf_45.pdf,50,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Table 11: Weights for the rst 48 mutation categories for the 6 clusters in Clustering-
D (see Table 4). The conventions are the same as in Table 5.
Mutation Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6
ACAA 0.00 0.00 6.55 0.00 0.00 0.00 0.00 0.00 6.55 0.00 0.00 0.00
ACCA 0.00 0.00 0.00 5.83 0.00 0.00 0.00 0.00 0.00 6.08 0.00 0.00
ACGA 0.00 0.00 0.00 0.00 0.00 2.75 0.00 0.00 0.00 0.00 0.00 2.63
ACTA 0.00 0.00 0.00 6.16 0.00 0.00 0.00 0.00 0.00 6.38 0.00 0.00
CCAA 0.00 0.00 0.00 7.91 0.00 0.00 0.00 0.00 0.00 8.10 0.00 0.00
CCCA 0.00 0.00 0.00 6.46 0.00 0.00 0.00 0.00 0.00 6.68 0.00 0.00
CCGA 0.00 0.00 0.00 0.00 0.00 2.13 0.00 0.00 0.00 0.00 0.00 2.25
CCTA 0.00 0.00 0.00 0.00 6.75 0.00 0.00 0.00 0.00 0.00 6.79 0.00
GCAA 4.05 0.00 0.00 0.00 0.00 0.00 4.65 0.00 0.00 0.00 0.00 0.00
GCCA 0.00 0.00 0.00 4.56 0.00 0.00 0.00 0.00 0.00 4.73 0.00 0.00
GCGA 0.00 13.81 0.00 0.00 0.00 0.00 0.00 13.89 0.00 0.00 0.00 0.00
GCTA 0.00 0.00 0.00 5.02 0.00 0.00 0.00 0.00 0.00 5.20 0.00 0.00
TCAA 0.00 0.00 6.26 0.00 0.00 0.00 0.00 0.00 6.21 0.00 0.00 0.00
TCCA 0.00 0.00 0.00 8.94 0.00 0.00 0.00 0.00 0.00 9.29 0.00 0.00
TCGA 0.00 11.87 0.00 0.00 0.00 0.00 0.00 12.24 0.00 0.00 0.00 0.00
TCTA 0.00 0.00 8.05 0.00 0.00 0.00 0.00 0.00 8.00 0.00 0.00 0.00
ACAG 0.00 0.00 0.00 3.96 0.00 0.00 0.00 0.00 0.00 4.18 0.00 0.00
ACCG 0.00 0.00 0.00 0.00 0.00 2.46 0.00 0.00 0.00 0.00 0.00 2.61
ACGG 0.00 12.62 0.00 0.00 0.00 0.00 0.00 12.22 0.00 0.00 0.00 0.00
ACTG 0.00 0.00 0.00 4.77 0.00 0.00 0.00 0.00 0.00 5.03 0.00 0.00
CCAG 0.00 0.00 0.00 0.00 0.00 2.81 0.00 0.00 0.00 0.00 0.00 2.98
CCCG 0.00 0.00 0.00 0.00 0.00 2.88 0.00 0.00 0.00 0.00 0.00 2.92
CCGG 0.00 0.00 0.00 0.00 0.00 3.29 0.00 0.00 0.00 0.00 0.00 2.97
CCTG 0.00 0.00 0.00 0.00 0.00 3.84 0.00 0.00 0.00 0.00 0.00 4.05
GCAG 0.00 0.00 0.00 0.00 0.00 3.13 0.00 0.00 0.00 0.00 0.00 3.02
GCCG 0.00 14.79 0.00 0.00 0.00 0.00 0.00 15.62 0.00 0.00 0.00 0.00
GCGG 0.00 15.50 0.00 0.00 0.00 0.00 0.00 13.92 0.00 0.00 0.00 0.00
GCTG 0.00 0.00 0.00 0.00 0.00 3.44 0.00 0.00 0.00 0.00 0.00 3.42
TCAG 0.00 0.00 0.00 10.31 0.00 0.00 0.00 0.00 0.00 9.03 0.00 0.00
TCCG 0.00 0.00 0.00 5.10 0.00 0.00 0.00 0.00 0.00 4.95 0.00 0.00
TCGG 0.00 8.40 0.00 0.00 0.00 0.00 0.00 8.65 0.00 0.00 0.00 0.00
TCTG 0.00 0.00 0.00 14.10 0.00 0.00 0.00 0.00 0.00 12.53 0.00 0.00
ACAT 0.00 0.00 7.67 0.00 0.00 0.00 0.00 0.00 7.71 0.00 0.00 0.00
ACCT 4.78 0.00 0.00 0.00 0.00 0.00 5.02 0.00 0.00 0.00 0.00 0.00
ACGT 23.47 0.00 0.00 0.00 0.00 0.00 23.18 0.00 0.00 0.00 0.00 0.00
ACTT 0.00 0.00 5.43 0.00 0.00 0.00 0.00 0.00 5.47 0.00 0.00 0.00
CCAT 0.00 0.00 6.02 0.00 0.00 0.00 0.00 0.00 6.02 0.00 0.00 0.00
CCCT 0.00 0.00 5.59 0.00 0.00 0.00 0.00 0.00 5.63 0.00 0.00 0.00
CCGT 17.66 0.00 0.00 0.00 0.00 0.00 17.12 0.00 0.00 0.00 0.00 0.00
CCTT 0.00 0.00 7.01 0.00 0.00 0.00 0.00 0.00 7.04 0.00 0.00 0.00
GCAT 0.00 0.00 5.98 0.00 0.00 0.00 0.00 0.00 6.01 0.00 0.00 0.00
GCCT 5.74 0.00 0.00 0.00 0.00 0.00 5.93 0.00 0.00 0.00 0.00 0.00
GCGT 20.46 0.00 0.00 0.00 0.00 0.00 19.80 0.00 0.00 0.00 0.00 0.00
GCTT 0.00 0.00 5.88 0.00 0.00 0.00 0.00 0.00 5.93 0.00 0.00 0.00
TCAT 11.42 0.00 0.00 0.00 0.00 0.00 12.00 0.00 0.00 0.00 0.00 0.00
TCCT 0.00 0.00 7.81 0.00 0.00 0.00 0.00 0.00 7.76 0.00 0.00 0.00
TCGT 12.42 0.00 0.00 0.00 0.00 0.00 12.30 0.00 0.00 0.00 0.00 0.00
TCTT 0.00 0.00 9.47 0.00 0.00 0.00 0.00 0.00 9.29 0.00 0.00 0.00
49",2017-03-02T10:39:59Z,table hts usteri table t table mt           
paper_qf_45.pdf,51,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Table 12: Table 11 continued: weights for the next 48 mutation categories.
Mutation Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6
ATAA 0.00 0.00 0.00 4.18 0.00 0.00 0.00 0.00 0.00 4.52 0.00 0.00
ATCA 0.00 0.00 0.00 0.00 0.00 3.11 0.00 0.00 0.00 0.00 0.00 3.29
ATGA 0.00 0.00 0.00 4.02 0.00 0.00 0.00 0.00 0.00 4.30 0.00 0.00
ATTA 0.00 0.00 0.00 0.00 5.54 0.00 0.00 0.00 0.00 0.00 5.66 0.00
CTAA 0.00 0.00 0.00 0.00 0.00 3.49 0.00 0.00 0.00 0.00 0.00 3.60
CTCA 0.00 0.00 0.00 3.79 0.00 0.00 0.00 0.00 0.00 3.98 0.00 0.00
CTGA 0.00 0.00 0.00 4.88 0.00 0.00 0.00 0.00 0.00 5.02 0.00 0.00
CTTA 0.00 0.00 0.00 0.00 4.28 0.00 0.00 0.00 0.00 0.00 4.33 0.00
GTAA 0.00 0.00 0.00 0.00 0.00 3.09 0.00 0.00 0.00 0.00 0.00 3.04
GTCA 0.00 15.20 0.00 0.00 0.00 0.00 0.00 15.36 0.00 0.00 0.00 0.00
GTGA 0.00 0.00 0.00 0.00 0.00 2.79 0.00 0.00 0.00 0.00 0.00 2.93
GTTA 0.00 0.00 0.00 0.00 0.00 3.65 0.00 0.00 0.00 0.00 0.00 3.63
TTAA 0.00 0.00 0.00 0.00 5.13 0.00 0.00 0.00 0.00 0.00 5.26 0.00
TTCA 0.00 0.00 0.00 0.00 0.00 4.50 0.00 0.00 0.00 0.00 0.00 4.33
TTGA 0.00 0.00 0.00 0.00 0.00 2.63 0.00 0.00 0.00 0.00 0.00 2.74
TTTA 0.00 0.00 0.00 0.00 5.27 0.00 0.00 0.00 0.00 0.00 5.38 0.00
ATAC 0.00 0.00 7.03 0.00 0.00 0.00 0.00 0.00 7.06 0.00 0.00 0.00
ATCC 0.00 0.00 0.00 0.00 3.30 0.00 0.00 0.00 0.00 0.00 3.39 0.00
ATGC 0.00 0.00 4.97 0.00 0.00 0.00 0.00 0.00 4.98 0.00 0.00 0.00
ATTC 0.00 0.00 6.30 0.00 0.00 0.00 0.00 0.00 6.34 0.00 0.00 0.00
CTAC 0.00 0.00 0.00 0.00 3.78 0.00 0.00 0.00 0.00 0.00 3.81 0.00
CTCC 0.00 0.00 0.00 0.00 4.30 0.00 0.00 0.00 0.00 0.00 4.31 0.00
CTGC 0.00 0.00 0.00 0.00 5.37 0.00 0.00 0.00 0.00 0.00 5.41 0.00
CTTC 0.00 0.00 0.00 0.00 7.14 0.00 0.00 0.00 0.00 0.00 6.92 0.00
GTAC 0.00 0.00 0.00 0.00 4.84 0.00 0.00 0.00 0.00 0.00 4.96 0.00
GTCC 0.00 0.00 0.00 0.00 0.00 3.81 0.00 0.00 0.00 0.00 0.00 3.95
GTGC 0.00 0.00 0.00 0.00 4.32 0.00 0.00 0.00 0.00 0.00 4.43 0.00
GTTC 0.00 0.00 0.00 0.00 5.05 0.00 0.00 0.00 0.00 0.00 5.23 0.00
TTAC 0.00 0.00 0.00 0.00 4.97 0.00 0.00 0.00 0.00 0.00 5.10 0.00
TTCC 0.00 0.00 0.00 0.00 4.69 0.00 0.00 0.00 0.00 0.00 4.79 0.00
TTGC 0.00 0.00 0.00 0.00 0.00 3.61 0.00 0.00 0.00 0.00 0.00 3.80
TTTC 0.00 0.00 0.00 0.00 7.29 0.00 0.00 0.00 0.00 0.00 7.28 0.00
ATAG 0.00 0.00 0.00 0.00 0.00 2.91 0.00 0.00 0.00 0.00 0.00 2.95
ATCG 0.00 0.00 0.00 0.00 0.00 2.46 0.00 0.00 0.00 0.00 0.00 2.30
ATGG 0.00 0.00 0.00 0.00 0.00 2.80 0.00 0.00 0.00 0.00 0.00 2.76
ATTG 0.00 0.00 0.00 0.00 0.00 4.93 0.00 0.00 0.00 0.00 0.00 4.79
CTAG 0.00 0.00 0.00 0.00 0.00 2.47 0.00 0.00 0.00 0.00 0.00 2.43
CTCG 0.00 0.00 0.00 0.00 0.00 4.21 0.00 0.00 0.00 0.00 0.00 3.93
CTGG 0.00 0.00 0.00 0.00 0.00 2.80 0.00 0.00 0.00 0.00 0.00 2.90
CTTG 0.00 0.00 0.00 0.00 9.67 0.00 0.00 0.00 0.00 0.00 8.89 0.00
GTAG 0.00 0.00 0.00 0.00 0.00 2.30 0.00 0.00 0.00 0.00 0.00 2.16
GTCG 0.00 7.80 0.00 0.00 0.00 0.00 0.00 8.11 0.00 0.00 0.00 0.00
GTGG 0.00 0.00 0.00 0.00 0.00 2.85 0.00 0.00 0.00 0.00 0.00 2.93
GTTG 0.00 0.00 0.00 0.00 0.00 4.83 0.00 0.00 0.00 0.00 0.00 4.73
TTAG 0.00 0.00 0.00 0.00 0.00 3.20 0.00 0.00 0.00 0.00 0.00 3.30
TTCG 0.00 0.00 0.00 0.00 0.00 2.64 0.00 0.00 0.00 0.00 0.00 2.60
TTGG 0.00 0.00 0.00 0.00 0.00 4.20 0.00 0.00 0.00 0.00 0.00 4.08
TTTG 0.00 0.00 0.00 0.00 8.31 0.00 0.00 0.00 0.00 0.00 8.05 0.00
50",2017-03-02T10:39:59Z,table table mt           
paper_qf_45.pdf,52,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Table 13: Weights (in the units of 1%, rounded to 2 digits) for the rst 48 mutation
categories for the 7 clusters in Clustering-A (see Table 4) based on unnormalized
(columns 2-8) and normalized (columns 9-15) regressions with the exposures com-
puted via geometric means (see Subsection 2.6 for details). Here \\\\weights based on
unnormalized regressions"" are given by (13), (14) and (19), while \\\\weights based on
normalized regressions"" are given by (17), (14) and (21). Other conventions are the
same as in Table 5.
Mutation Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7 Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7
ACAA 0.00 0.00 0.00 6.54 0.00 0.00 0.00 0.00 0.00 0.00 6.54 0.00 0.00 0.00
ACCA 0.00 0.00 0.00 0.00 6.16 0.00 0.00 0.00 0.00 0.00 0.00 6.20 0.00 0.00
ACGA 0.00 0.00 0.00 0.00 0.00 0.00 4.12 0.00 0.00 0.00 0.00 0.00 0.00 4.05
ACTA 0.00 0.00 0.00 0.00 6.38 0.00 0.00 0.00 0.00 0.00 0.00 6.44 0.00 0.00
CCAA 0.00 0.00 0.00 0.00 8.27 0.00 0.00 0.00 0.00 0.00 0.00 8.27 0.00 0.00
CCCA 0.00 0.00 0.00 0.00 6.73 0.00 0.00 0.00 0.00 0.00 0.00 6.77 0.00 0.00
CCGA 0.00 0.00 7.32 0.00 0.00 0.00 0.00 0.00 0.00 7.24 0.00 0.00 0.00 0.00
CCTA 0.00 0.00 0.00 0.00 0.00 6.77 0.00 0.00 0.00 0.00 0.00 0.00 6.76 0.00
GCAA 4.31 0.00 0.00 0.00 0.00 0.00 0.00 4.68 0.00 0.00 0.00 0.00 0.00 0.00
GCCA 0.00 0.00 0.00 0.00 4.70 0.00 0.00 0.00 0.00 0.00 0.00 4.75 0.00 0.00
GCGA 0.00 13.79 0.00 0.00 0.00 0.00 0.00 0.00 13.76 0.00 0.00 0.00 0.00 0.00
GCTA 0.00 0.00 0.00 0.00 5.16 0.00 0.00 0.00 0.00 0.00 0.00 5.22 0.00 0.00
TCAA 0.00 0.00 0.00 6.22 0.00 0.00 0.00 0.00 0.00 0.00 6.20 0.00 0.00 0.00
TCCA 0.00 0.00 0.00 0.00 8.86 0.00 0.00 0.00 0.00 0.00 0.00 9.08 0.00 0.00
TCGA 0.00 11.96 0.00 0.00 0.00 0.00 0.00 0.00 12.13 0.00 0.00 0.00 0.00 0.00
TCTA 0.00 0.00 0.00 8.04 0.00 0.00 0.00 0.00 0.00 0.00 8.01 0.00 0.00 0.00
ACAG 0.00 0.00 0.00 0.00 4.08 0.00 0.00 0.00 0.00 0.00 0.00 4.16 0.00 0.00
ACCG 0.00 0.00 8.12 0.00 0.00 0.00 0.00 0.00 0.00 8.17 0.00 0.00 0.00 0.00
ACGG 0.00 12.58 0.00 0.00 0.00 0.00 0.00 0.00 12.32 0.00 0.00 0.00 0.00 0.00
ACTG 0.00 0.00 0.00 0.00 4.73 0.00 0.00 0.00 0.00 0.00 0.00 4.88 0.00 0.00
CCAG 0.00 0.00 9.34 0.00 0.00 0.00 0.00 0.00 0.00 9.36 0.00 0.00 0.00 0.00
CCCG 0.00 0.00 0.00 0.00 0.00 0.00 3.97 0.00 0.00 0.00 0.00 0.00 0.00 4.04
CCGG 0.00 0.00 0.00 0.00 0.00 0.00 5.47 0.00 0.00 0.00 0.00 0.00 0.00 5.24
CCTG 0.00 0.00 12.56 0.00 0.00 0.00 0.00 0.00 0.00 12.61 0.00 0.00 0.00 0.00
GCAG 0.00 0.00 0.00 0.00 0.00 0.00 4.68 0.00 0.00 0.00 0.00 0.00 0.00 4.63
GCCG 0.00 14.96 0.00 0.00 0.00 0.00 0.00 0.00 15.53 0.00 0.00 0.00 0.00 0.00
GCGG 0.00 15.17 0.00 0.00 0.00 0.00 0.00 0.00 14.18 0.00 0.00 0.00 0.00 0.00
GCTG 0.00 0.00 0.00 0.00 0.00 0.00 4.92 0.00 0.00 0.00 0.00 0.00 0.00 4.94
TCAG 0.00 0.00 0.00 0.00 9.40 0.00 0.00 0.00 0.00 0.00 0.00 8.99 0.00 0.00
TCCG 0.00 0.00 0.00 0.00 4.93 0.00 0.00 0.00 0.00 0.00 0.00 4.90 0.00 0.00
TCGG 0.00 8.53 0.00 0.00 0.00 0.00 0.00 0.00 8.60 0.00 0.00 0.00 0.00 0.00
TCTG 0.00 0.00 0.00 0.00 13.10 0.00 0.00 0.00 0.00 0.00 0.00 12.56 0.00 0.00
ACAT 0.00 0.00 0.00 7.72 0.00 0.00 0.00 0.00 0.00 0.00 7.73 0.00 0.00 0.00
ACCT 4.86 0.00 0.00 0.00 0.00 0.00 0.00 5.01 0.00 0.00 0.00 0.00 0.00 0.00
ACGT 23.50 0.00 0.00 0.00 0.00 0.00 0.00 23.33 0.00 0.00 0.00 0.00 0.00 0.00
ACTT 0.00 0.00 0.00 5.45 0.00 0.00 0.00 0.00 0.00 0.00 5.47 0.00 0.00 0.00
CCAT 0.00 0.00 0.00 6.02 0.00 0.00 0.00 0.00 0.00 0.00 6.02 0.00 0.00 0.00
CCCT 0.00 0.00 0.00 5.60 0.00 0.00 0.00 0.00 0.00 0.00 5.62 0.00 0.00 0.00
CCGT 17.45 0.00 0.00 0.00 0.00 0.00 0.00 17.08 0.00 0.00 0.00 0.00 0.00 0.00
CCTT 0.00 0.00 0.00 7.03 0.00 0.00 0.00 0.00 0.00 0.00 7.05 0.00 0.00 0.00
GCAT 0.00 0.00 0.00 5.98 0.00 0.00 0.00 0.00 0.00 0.00 6.00 0.00 0.00 0.00
GCCT 5.85 0.00 0.00 0.00 0.00 0.00 0.00 5.97 0.00 0.00 0.00 0.00 0.00 0.00
GCGT 20.08 0.00 0.00 0.00 0.00 0.00 0.00 19.63 0.00 0.00 0.00 0.00 0.00 0.00
GCTT 0.00 0.00 0.00 5.90 0.00 0.00 0.00 0.00 0.00 0.00 5.92 0.00 0.00 0.00
TCAT 11.55 0.00 0.00 0.00 0.00 0.00 0.00 12.00 0.00 0.00 0.00 0.00 0.00 0.00
TCCT 0.00 0.00 0.00 7.77 0.00 0.00 0.00 0.00 0.00 0.00 7.75 0.00 0.00 0.00
TCGT 12.39 0.00 0.00 0.00 0.00 0.00 0.00 12.30 0.00 0.00 0.00 0.00 0.00 0.00
TCTT 0.00 0.00 0.00 9.35 0.00 0.00 0.00 0.00 0.00 0.00 9.27 0.00 0.00 0.00
51",2017-03-02T10:39:59Z,table hts usteri table subsere otr table mt             
paper_qf_45.pdf,53,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Table 14: Table 13 continued: weights for the next 48 mutation categories.
Mutation Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7 Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7
ATAA 0.00 0.00 0.00 0.00 4.41 0.00 0.00 0.00 0.00 0.00 0.00 4.51 0.00 0.00
ATCA 0.00 0.00 10.06 0.00 0.00 0.00 0.00 0.00 0.00 10.15 0.00 0.00 0.00 0.00
ATGA 0.00 0.00 0.00 0.00 4.15 0.00 0.00 0.00 0.00 0.00 0.00 4.25 0.00 0.00
ATTA 0.00 0.00 0.00 0.00 0.00 5.59 0.00 0.00 0.00 0.00 0.00 0.00 5.64 0.00
CTAA 0.00 0.00 11.34 0.00 0.00 0.00 0.00 0.00 0.00 11.10 0.00 0.00 0.00 0.00
CTCA 0.00 0.00 0.00 0.00 3.87 0.00 0.00 0.00 0.00 0.00 0.00 3.94 0.00 0.00
CTGA 0.00 0.00 0.00 0.00 5.08 0.00 0.00 0.00 0.00 0.00 0.00 5.07 0.00 0.00
CTTA 0.00 0.00 0.00 0.00 0.00 4.33 0.00 0.00 0.00 0.00 0.00 0.00 4.31 0.00
GTAA 0.00 0.00 0.00 0.00 0.00 0.00 4.33 0.00 0.00 0.00 0.00 0.00 0.00 4.36
GTCA 0.00 15.17 0.00 0.00 0.00 0.00 0.00 0.00 15.40 0.00 0.00 0.00 0.00 0.00
GTGA 0.00 0.00 9.30 0.00 0.00 0.00 0.00 0.00 0.00 9.24 0.00 0.00 0.00 0.00
GTTA 0.00 0.00 0.00 0.00 0.00 0.00 5.18 0.00 0.00 0.00 0.00 0.00 0.00 5.22
TTAA 0.00 0.00 0.00 0.00 0.00 5.21 0.00 0.00 0.00 0.00 0.00 0.00 5.21 0.00
TTCA 0.00 0.00 0.00 0.00 0.00 0.00 6.73 0.00 0.00 0.00 0.00 0.00 0.00 6.66
TTGA 0.00 0.00 8.62 0.00 0.00 0.00 0.00 0.00 0.00 8.51 0.00 0.00 0.00 0.00
TTTA 0.00 0.00 0.00 0.00 0.00 5.36 0.00 0.00 0.00 0.00 0.00 0.00 5.35 0.00
ATAC 0.00 0.00 0.00 7.07 0.00 0.00 0.00 0.00 0.00 0.00 7.08 0.00 0.00 0.00
ATCC 0.00 0.00 0.00 0.00 0.00 3.38 0.00 0.00 0.00 0.00 0.00 0.00 3.40 0.00
ATGC 0.00 0.00 0.00 4.99 0.00 0.00 0.00 0.00 0.00 0.00 4.99 0.00 0.00 0.00
ATTC 0.00 0.00 0.00 6.34 0.00 0.00 0.00 0.00 0.00 0.00 6.36 0.00 0.00 0.00
CTAC 0.00 0.00 0.00 0.00 0.00 3.82 0.00 0.00 0.00 0.00 0.00 0.00 3.81 0.00
CTCC 0.00 0.00 0.00 0.00 0.00 4.31 0.00 0.00 0.00 0.00 0.00 0.00 4.32 0.00
CTGC 0.00 0.00 0.00 0.00 0.00 5.27 0.00 0.00 0.00 0.00 0.00 0.00 5.35 0.00
CTTC 0.00 0.00 0.00 0.00 0.00 7.09 0.00 0.00 0.00 0.00 0.00 0.00 7.01 0.00
GTAC 0.00 0.00 0.00 0.00 0.00 4.82 0.00 0.00 0.00 0.00 0.00 0.00 4.90 0.00
GTCC 0.00 0.00 11.65 0.00 0.00 0.00 0.00 0.00 0.00 11.80 0.00 0.00 0.00 0.00
GTGC 0.00 0.00 0.00 0.00 0.00 4.26 0.00 0.00 0.00 0.00 0.00 0.00 4.36 0.00
GTTC 0.00 0.00 0.00 0.00 0.00 5.08 0.00 0.00 0.00 0.00 0.00 0.00 5.18 0.00
TTAC 0.00 0.00 0.00 0.00 0.00 5.06 0.00 0.00 0.00 0.00 0.00 0.00 5.09 0.00
TTCC 0.00 0.00 0.00 0.00 0.00 4.69 0.00 0.00 0.00 0.00 0.00 0.00 4.76 0.00
TTGC 0.00 0.00 11.69 0.00 0.00 0.00 0.00 0.00 0.00 11.81 0.00 0.00 0.00 0.00
TTTC 0.00 0.00 0.00 0.00 0.00 7.37 0.00 0.00 0.00 0.00 0.00 0.00 7.31 0.00
ATAG 0.00 0.00 0.00 0.00 0.00 0.00 3.94 0.00 0.00 0.00 0.00 0.00 0.00 4.03
ATCG 0.00 0.00 0.00 0.00 0.00 0.00 3.83 0.00 0.00 0.00 0.00 0.00 0.00 3.74
ATGG 0.00 0.00 0.00 0.00 0.00 0.00 4.00 0.00 0.00 0.00 0.00 0.00 0.00 4.01
ATTG 0.00 0.00 0.00 0.00 0.00 0.00 6.98 0.00 0.00 0.00 0.00 0.00 0.00 7.00
CTAG 0.00 0.00 0.00 0.00 0.00 0.00 3.50 0.00 0.00 0.00 0.00 0.00 0.00 3.52
CTCG 0.00 0.00 0.00 0.00 0.00 0.00 6.53 0.00 0.00 0.00 0.00 0.00 0.00 6.37
CTGG 0.00 0.00 0.00 0.00 0.00 0.00 3.63 0.00 0.00 0.00 0.00 0.00 0.00 3.76
CTTG 0.00 0.00 0.00 0.00 0.00 9.36 0.00 0.00 0.00 0.00 0.00 0.00 9.13 0.00
GTAG 0.00 0.00 0.00 0.00 0.00 0.00 3.59 0.00 0.00 0.00 0.00 0.00 0.00 3.51
GTCG 0.00 7.84 0.00 0.00 0.00 0.00 0.00 0.00 8.08 0.00 0.00 0.00 0.00 0.00
GTGG 0.00 0.00 0.00 0.00 0.00 0.00 3.87 0.00 0.00 0.00 0.00 0.00 0.00 3.97
GTTG 0.00 0.00 0.00 0.00 0.00 0.00 6.71 0.00 0.00 0.00 0.00 0.00 0.00 6.77
TTAG 0.00 0.00 0.00 0.00 0.00 0.00 4.17 0.00 0.00 0.00 0.00 0.00 0.00 4.32
TTCG 0.00 0.00 0.00 0.00 0.00 0.00 3.74 0.00 0.00 0.00 0.00 0.00 0.00 3.76
TTGG 0.00 0.00 0.00 0.00 0.00 0.00 6.11 0.00 0.00 0.00 0.00 0.00 0.00 6.09
TTTG 0.00 0.00 0.00 0.00 0.00 8.22 0.00 0.00 0.00 0.00 0.00 0.00 8.12 0.00
52",2017-03-02T10:39:59Z,table table mt             
paper_qf_45.pdf,54,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Table 15: The within-cluster cross-sectional correlations  sA(columns 2-8), the
overall correlations  s(column 11) based on the overall cross-sectional regressions,
and multiple R2and adjusted R2of these regressions (columns 9 and 10). See
Subsection 3.3 for details. Cancer types are labeled by X1 through X14 as in Table
2. All quantities are in the units of 1% rounded to 2 digits. The values above 80%
are given in bold font.
Cancer Type Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7 r.sq adj.r.sq Overall Cor
X1 57.66 31.8 75.04 88.43 81.27 84.82 41.7 89.05 88.19 83.84
X2 90.57 66.35 81.97 79.64 41.42 -2.87 25.43 94.77 94.35 93.82
X3 93.29 -12.6 39.19 12.59 68.65 17.06 68.74 93.86 93.38 94.19
X4 9.88 16.97 52.94 79.11 81.85 46.74 7.34 58.18 54.9 61.53
X5 89.52 63.31 50.79 28.58 5.12 80.88 13.66 93.26 92.73 88.62
X6 86.53 34.07 48.92 76.77 85.01 19.59 34.54 89.57 88.75 91.28
X7 92.78 34.69 64.65 48.79 63.79 86.55 72.56 86.72 85.67 86.04
X8 -31.6 39.99 65.56 -46.21 -6.95 -3.36 61.8 69.52 67.12 41.88
X9 -28.63 53.86 -34.26 46.93 59.88 13.59 -12.39 77.76 76.02 70.18
X10 93.97 61.59 63.06 67.15 41.13 4.11 43.87 95.17 94.79 95.47
X11 88.16 56.6 66.76 55.12 90.27 16.33 26.3 95.02 94.63 89.62
X12 94.75 17.48 5.1 16.5 90 27.74 21.63 94.04 93.57 96.11
X13 97.05 58.21 75.77 78.67 88.42 20.28 44.07 96.31 96.02 95.35
X14 38.93 65.92 17.23 58.54 4.73 35.72 31.27 82.52 81.14 65.4
53",2017-03-02T10:39:59Z,table t  subsecancer table all t cancer         ovll cor
paper_qf_45.pdf,55,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","Table 16: The within-cluster cross-sectional correlations  Abetween the weights
for 7 cancer signatures Sig1 through Sig7 of [Kakushadze and Yu, 2016b] and the
weights (using normalized regressions with exposures based on arithmetic averages)
for 7 clusters in Clustering A (see Subsection 3.3 for details). All quantities are in
the units of 1% rounded to 2 digits. The values above 80% are given in bold font.
Signature Cl-1 Cl-2 Cl-3 Cl-4 Cl-5 Cl-6 Cl-7
Sig1 92.05 10.29 -6.42 -8.33 51.12 29.06 20.61
Sig2 -0.37 1.75 42.13 75.58 80.12 -27.92 -3.34
Sig3 -51.53 54.4 -37.16 28.19 32.98 12.37 -17.7
Sig4 31.56 11.97 54.43 56.83 -1.17 84.25 60.41
Sig5 -42.53 40.31 62.96 -47.62 -8.34 -8.39 61.61
Sig6 47.79 40.62 17.8 27.45 -27.96 16.87 16.97
Sig7 80.94 19.87 55.03 33.4 13.89 -29.59 13.93
54",2017-03-02T10:39:59Z,table t betes s u sha yu usteri subseall t snature        s s s s s s s
paper_qf_45.pdf,56,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0.0 0.2 0.4 0.6 0.8 1.0 1.20.0 0.5 1.0 1.5 2.0
Variability Accross Cancer TypesDensityFigure 1: Horizontal axis: serial standard deviation 0
iforN= 96 mutation cat-
egories (i= 1;:::;N ) of cross-sectionally demeaned log-counts X0
isacrossn= 14
cancer types (for samples aggregated by cancer types, so s= 1;:::;d ,d=n).
Vertical axis: density using R function density() . See Subsection 2.4.1 for details.
55",2017-03-02T10:39:59Z,variabity aoss cancer s nsity  horizontal vertical  subsen
paper_qf_45.pdf,57,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15 20
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 2: Cluster Cl-1 in Clustering-A with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 5, 6. Here and in all
Figures below, for comparison and visualization convenience, we show all 96 channels
on the horizontal axis even though the weights are nonzero only for the mutation
categories belonging to a given cluster. Thus, in this cluster, only 8 weights are
nonzero, to wit, for GCAA, ACCT, ACGT, CCGT, GCCT, GCGT, TCAT, TCGT.
56",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tablre s 
paper_qf_45.pdf,58,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15 20
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 3: Cluster Cl-1 in Clustering-A with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 5, 6.
57",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,59,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 4: Cluster Cl-2 in Clustering-A with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 5, 6.
58",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,60,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 5: Cluster Cl-2 in Clustering-A with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 5, 6.
59",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,61,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8 10 12
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 6: Cluster Cl-3 in Clustering-A with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 5, 6.
60",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,62,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8 10 12
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 7: Cluster Cl-3 in Clustering-A with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 5, 6.
61",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,63,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 8: Cluster Cl-4 in Clustering-A with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 5, 6.
62",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,64,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 9: Cluster Cl-4 in Clustering-A with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 5, 6.
63",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,65,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8 10 12 14
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 10: Cluster Cl-5 in Clustering-A with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 5, 6.
64",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,66,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8 10 12
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 11: Cluster Cl-5 in Clustering-A with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 5, 6.
65",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,67,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8 10
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 12: Cluster Cl-6 in Clustering-A with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 5, 6.
66",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,68,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 13: Cluster Cl-6 in Clustering-A with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 5, 6.
67",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,69,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 1 2 3 4 5 6 7
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 14: Cluster Cl-7 in Clustering-A with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 5, 6.
68",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,70,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 1 2 3 4 5 6 7
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 15: Cluster Cl-7 in Clustering-A with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 5, 6.
69",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,71,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 16: Cluster Cl-1 in Clustering-B with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 7, 8.
70",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,72,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 17: Cluster Cl-1 in Clustering-B with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 7, 8.
71",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,73,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15 20
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 18: Cluster Cl-2 in Clustering-B with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 7, 8.
72",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,74,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15 20
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 19: Cluster Cl-2 in Clustering-B with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 7, 8.
73",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,75,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 20: Cluster Cl-3 in Clustering-B with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 7, 8.
74",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,76,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 21: Cluster Cl-3 in Clustering-B with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 7, 8.
75",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,77,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 22: Cluster Cl-4 in Clustering-B with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 7, 8.
76",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,78,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 23: Cluster Cl-4 in Clustering-B with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 7, 8.
77",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,79,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 24: Cluster Cl-5 in Clustering-B with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 7, 8.
78",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,80,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 25: Cluster Cl-5 in Clustering-B with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 7, 8.
79",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,81,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8 10
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 26: Cluster Cl-6 in Clustering-B with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 7, 8.
80",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,82,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 27: Cluster Cl-6 in Clustering-B with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 7, 8.
81",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,83,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 1 2 3 4 5
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 28: Cluster Cl-7 in Clustering-B with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 7, 8.
82",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,84,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 1 2 3 4
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 29: Cluster Cl-7 in Clustering-B with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 7, 8.
83",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,85,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15 20
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 30: Cluster Cl-1 in Clustering-C with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 9, 10.
84",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,86,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15 20
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 31: Cluster Cl-1 in Clustering-C with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 9, 10.
85",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,87,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 32: Cluster Cl-2 in Clustering-C with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 9, 10.
86",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,88,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 33: Cluster Cl-2 in Clustering-C with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 9, 10.
87",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,89,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8 10
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 34: Cluster Cl-3 in Clustering-C with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 9, 10.
88",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,90,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8 10
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 35: Cluster Cl-3 in Clustering-C with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 9, 10.
89",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,91,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 36: Cluster Cl-4 in Clustering-C with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 9, 10.
90",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,92,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 37: Cluster Cl-4 in Clustering-C with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 9, 10.
91",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,93,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8 10 12 14
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 38: Cluster Cl-5 in Clustering-C with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 9, 10.
92",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,94,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8 10 12
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 39: Cluster Cl-5 in Clustering-C with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 9, 10.
93",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,95,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8 10
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 40: Cluster Cl-6 in Clustering-C with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 9, 10.
94",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,96,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 41: Cluster Cl-6 in Clustering-C with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 9, 10.
95",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,97,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 1 2 3 4 5 6 7
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 42: Cluster Cl-7 in Clustering-C with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 9, 10.
96",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,98,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 1 2 3 4 5 6 7
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 43: Cluster Cl-7 in Clustering-C with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 9, 10.
97",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,99,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15 20
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 44: Cluster Cl-1 in Clustering-D with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 11, 12.
98",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,100,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15 20
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 45: Cluster Cl-1 in Clustering-D with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 11, 12.
99",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,101,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 46: Cluster Cl-2 in Clustering-D with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 11, 12.
100",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,102,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 47: Cluster Cl-2 in Clustering-D with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 11, 12.
101",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,103,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 48: Cluster Cl-3 in Clustering-D with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 11, 12.
102",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,104,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 49: Cluster Cl-3 in Clustering-D with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 11, 12.
103",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,105,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8 10 12 14
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 50: Cluster Cl-4 in Clustering-D with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 11, 12.
104",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,106,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8 10 12
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 51: Cluster Cl-4 in Clustering-D with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 11, 12.
105",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,107,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8 10
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 52: Cluster Cl-5 in Clustering-D with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 11, 12.
106",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,108,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 53: Cluster Cl-5 in Clustering-D with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 11, 12.
107",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,109,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 1 2 3 4 5
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 54: Cluster Cl-6 in Clustering-D with weights based on unnormalized regres-
sions with arithmetic means (see Subsection 2.6). See Tables 4, 11, 12.
108",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,110,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 1 2 3 4
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 55: Cluster Cl-6 in Clustering-D with weights based on normalized regressions
with arithmetic means (see Subsection 2.6). See Tables 4, 11, 12.
109",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,111,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15 20
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 56: Cluster Cl-1 in Clustering-A with weights based on unnormalized regres-
sions with geometric means (see Subsection 2.6). See Tables 4, 13, 14.
110",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,112,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15 20
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 57: Cluster Cl-1 in Clustering-A with weights based on normalized regressions
with geometric means (see Subsection 2.6). See Tables 4, 13, 14.
111",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,113,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 58: Cluster Cl-2 in Clustering-A with weights based on unnormalized regres-
sions with geometric means (see Subsection 2.6). See Tables 4, 13, 14.
112",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,114,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 5 10 15
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 59: Cluster Cl-2 in Clustering-A with weights based on normalized regressions
with geometric means (see Subsection 2.6). See Tables 4, 13, 14.
113",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,115,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8 10 12
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 60: Cluster Cl-3 in Clustering-A with weights based on unnormalized regres-
sions with geometric means (see Subsection 2.6). See Tables 4, 13, 14.
114",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,116,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8 10 12
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 61: Cluster Cl-3 in Clustering-A with weights based on normalized regressions
with geometric means (see Subsection 2.6). See Tables 4, 13, 14.
115",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,117,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 62: Cluster Cl-4 in Clustering-A with weights based on unnormalized regres-
sions with geometric means (see Subsection 2.6). See Tables 4, 13, 14.
116",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,118,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 63: Cluster Cl-4 in Clustering-A with weights based on normalized regressions
with geometric means (see Subsection 2.6). See Tables 4, 13, 14.
117",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,119,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8 10 12
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 64: Cluster Cl-5 in Clustering-A with weights based on unnormalized regres-
sions with geometric means (see Subsection 2.6). See Tables 4, 13, 14.
118",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,120,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8 10 12
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 65: Cluster Cl-5 in Clustering-A with weights based on normalized regressions
with geometric means (see Subsection 2.6). See Tables 4, 13, 14.
119",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,121,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 66: Cluster Cl-6 in Clustering-A with weights based on unnormalized regres-
sions with geometric means (see Subsection 2.6). See Tables 4, 13, 14.
120",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,122,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 2 4 6 8
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 67: Cluster Cl-6 in Clustering-A with weights based on normalized regressions
with geometric means (see Subsection 2.6). See Tables 4, 13, 14.
121",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,123,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 1 2 3 4 5 6 7
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 68: Cluster Cl-7 in Clustering-A with weights based on unnormalized regres-
sions with geometric means (see Subsection 2.6). See Tables 4, 13, 14.
122",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_45.pdf,124,*K-means and Cluster Models for Cancer Signatures,"  We present *K-means clustering algorithm and source code by expanding
statistical clustering methods applied in https://ssrn.com/abstract=2802753 to
quantitative finance. *K-means is statistically deterministic without
specifying initial centers, etc. We apply *K-means to extracting cancer
signatures from genome data without using nonnegative matrix factorization
(NMF). *K-means' computational cost is a fraction of NMF's. Using 1,389
published samples for 14 cancer types, we find that 3 cancers (liver cancer,
lung cancer and renal cell carcinoma) stand out and do not have cluster-like
structures. Two clusters have especially high within-cluster correlations with
11 other cancers indicating common underlying structures. Our approach opens a
novel avenue for studying such structures. *K-means is universal and can be
applied in other fields. We discuss some potential applications in quantitative
finance.
","0 1 2 3 4 5 6 7
MutationsWeights
ACAA
ACCA
ACGA
ACTA
CCAA
CCCA
CCGA
CCTA
GCAA
GCCA
GCGA
GCTA
TCAA
TCCA
TCGA
TCTA
ACAG
ACCG
ACGG
ACTG
CCAG
CCCG
CCGG
CCTG
GCAG
GCCG
GCGG
GCTG
TCAG
TCCG
TCGG
TCTG
ACAT
ACCT
ACGT
ACTT
CCAT
CCCT
CCGT
CCTT
GCAT
GCCT
GCGT
GCTT
TCAT
TCCT
TCGT
TCTT
ATAA
ATCA
ATGA
ATTA
CTAA
CTCA
CTGA
CTTA
GTAA
GTCA
GTGA
GTTA
TTAA
TTCA
TTGA
TTTA
ATAC
ATCC
ATGC
ATTC
CTAC
CTCC
CTGC
CTTC
GTAC
GTCC
GTGC
GTTC
TTAC
TTCC
TTGC
TTTC
ATAG
ATCG
ATGG
ATTG
CTAG
CTCG
CTGG
CTTG
GTAG
GTCG
GTGG
GTTG
TTAG
TTCG
TTGG
TTTGFigure 69: Cluster Cl-7 in Clustering-A with weights based on normalized regressions
with geometric means (see Subsection 2.6). See Tables 4, 13, 14.
123",2017-03-02T10:39:59Z,mtns hts  uster  usteri subse tables
paper_qf_46.pdf,1,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
To appear in Quantitative Finance , Vol. 08, No. 15, August 2020, 1{22
Neural Network-based
Automatic Factor Construction
Jie Fangy, Jianwu Liny, Shutao Xia , Zhikang Xia , Shenglei Hu,
Xiang Liu and Yong Jiangy
yTsinghua-Berkeley Shenzhen Institute, Tsinghua University, Shenzhen, China
Tsinghua Shenzhen International Graduate School, Shenzhen, China
ySino-UK Blockchain Industry Institute, Guangxi University, Guangxi, China
(Received 01 June 2020; in nal form 05 August 2020 )
Instead of conducting manual factor construction based on traditional and behavioural nance analy-
sis, academic researchers and quantitative investment managers have leveraged Genetic Programming
(GP) as an automatic feature construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development of deep learning, more
powerful feature extraction tools are available. This paper proposes Neural Network-based Automatic
Factor Construction (NNAFC), a tailored neural network framework that can automatically construct
diversied nancial factors based on nancial domain knowledge and a variety of neural network struc-
tures. The experiment results show that NNAFC can construct more informative and diversied factors
than GP, to eectively enrich the current factor pool. For the current market, both fully connected and
recurrent neural network structures are better at extracting information from nancial time series than
convolution neural network structures. Moreover, new factors constructed by NNAFC can always improve
the return, Sharpe ratio, and the max draw-down of a multi-factor quantitative investment strategy due
to their introducing more information and diversication to the existing factor pool.
Keywords : Factor Construction, Deep Learning, Neural Network, Quantitative Investment
JEL Classication : C23, C33, C45, C53, C63
1. Introduction
1.1. Background
In quantitative trading, predicting future returns of stocks is one of the most important and chal-
lenging tasks. Various factors can be used to predict future returns based on nancial time series,
such as price, volume, and the company's accounting data. Usually, researchers dene the factors
that are constructed from price and volume data as technical factors, and the other factors, which
are constructed from the company's accounting data, are dened as fundamental factors. Typically,
researchers conduct manual factor selection based on traditional and behavioural nance analysis.
After Sharpe (1964) proposed the single-factor model for stock returns, Fama and French (1993)
proposed the Fama-French Three-factor Model, which selected three important factors that could
provide the most important major information to explain the stock return. Fama (2015) proposed
the Fama-French Five-factor Model, which added two more factors into their Three-factor Model.
First author. Email: fangx18@mails.tsinghua.edu.cn
Corresponding author. Email: lin.jianwu@sz.tsinghua.edu.cnarXiv:2008.06225v3  [q-fin.ST]  13 Oct 2020",2020-08-14T07:44:49Z,to quantitative nance vno august neural network automatic faor construjie fa jawu lishut via zhi ka via s lei hu lia  yo lia tsihua berkeley snzinstitute tsihua  snz tsihua snzinternatnal graduate schosnz sino block articial intellence industry institute guaxi  guaxi  received june august instead genetic prograi  neural network automatic faor construt for oshare keywords faor construep learni neural network quantitative investment ass introdubackground ivarus usually typically after share fam frenfam frenthree mfam fam frenve mthree mrst em articial intellence correspondi em articial intellence  
paper_qf_46.pdf,2,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
Currently, some commercial multi-factor risk models, such as the Barra and Axioma model, man-
ually select factors that are updated periodically by human researchers. However, there are two
limitations in manual factor selection. First, it is very time-consuming to update factors man-
ually. Second, it is not easy to construct certain nonlinear factors from high-dimensional data.
Thus, to address data from a rapidly developing nancial market, both academic researchers and
quantitative investment managers have paid more and more attention to automatic nancial fac-
tor construction tools. In the computer science eld, this task is dened as Automatic Feature
Construction, Krawiec (2002).
Feature construction in computer science is a process that discovers missing relationships between
features and outcomes and augments the space of the features by inferring or creating new features.
In this sense, a nancial factor is a special case among general features in computer science.
During this process, new features can be generated from a combination of existing features. A more
straightforward description is that the algorithms use operators, hyper-parameters and existing
features to construct a new feature. Sometimes, both feature construction and feature selection
occur in the same procedure. Dash (1997) summarized these methods, which consist of wrapping,
ltering, and embedding. Filtering utilizes only some criteria to choose a feature, and sometimes
it can help us to monitor the feature construction process. It is easy to conduct but achieves
poor performance. Wrapping performs well by directly applying the model's results as an objective
function. Thus, it can treat an individually trained model as a newly constructed feature. However,
a considerable amount of computational resources and time is required. Embedding is a method
that uses generalized features and a pruning technique to select or combine features, which serves
as a middle choice between ltering and wrapping.
Statistical learning is a common factor construction tool that is used for nancial factor con-
struction, which is similar to embedding in feature construction. Harvey et al. (2016) proposed a
new method for estimating latent asset pricing factors that t the time-series and cross-section
returns by using Principal Component Analysis (PCA), as proposed by Wold (1987). Feng et al.
(2020) proposed model selection procedures to select among many asset pricing factors by using
cross-sectional LASSO regression, which was introduced by Tibshirani (1996). However, this statis-
tical learning mainly focuses on factor selection from existing factor pools but does not construct
brand-new factors.
1.2. Literature Review
The most well-known and frequently employed method for automatic brand-new factor construction
is Genetic Programming (GP), which is a type of wrapping method in feature construction that uses
reverse polish expressions to construct new factors by an evolutionary process. However, dierent
domains require dierent objective functions, and the input data structure can dier. Thus, it
is very important to design this task within a specic domain. This method has been proved to
work well in many industries, such as object detection, according to Lillywhite (2013), and in the
educational industry, according to Romero (2004). However, its drawback is that the constructed
formulas are very similar and have highly correlated outputs. The nancial factor construction task
uses GP to conduct the evolutionary process of formulaic factors, according to Thomas (1999) and
Ravisankar (2011). WorldQuant published 101 formulaic alpha factors, which are also constructed
by using this method, Kakushadze (2016). However, it only constructs a large number of similar
factors, which do not contain much new information.
With the development of deep learning, more and more researchers have begun to use neural
networks to extract information from raw data, and then, they add a fully connected layer to
reshape the output. Some researchers have designed some specic loss functions according to specic
domain knowledge. Examples are Zhang (2019) and Zhang (2020), who designed a vector-error
function and derived an implicit-dynamic equation with a time-varying parameter in a neural
network structure to solve the disturbed time-varying inversion problem. Yang Zhong deployed
",2020-08-14T07:44:49Z,currently barra axm rst second  iautomatic feature construraw iec feature i sometimdash lteri it pi  embeddi statistical harvey principal onent analysis would fe tib shi rani re review t genetic prograi   sly white romero t thomas ri shankar world quant u sha with some s   ya ho
paper_qf_46.pdf,3,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
better accuracy than its benchmark. Many researchers directly treat a trained neural network as a
newly constructed feature. There are many representative studies in this eld. For example, Shan
(2017) conducted experiments on this task and deployed a deeper and wider CNN. Hidasi (2016)
used a Recurrent Neural Network (RNN) to pre-locate the factor-rich region and construct more
puried features. In a text classication task, Botsis (2011) leveraged the RNN to build a hierarchy
classier for text data, in which each classier represents a part of the text. Lai (2015) proposed a
network structure that uses both an RNN and a CNN to extract text information. The produced
features contain more information than the previous work. With the help of a neural network's
strong tting ability, highly informative factors can be produced by tailoring the network structure
for dierent industries. There is no conclusion on whether using a neural network to represent a new
feature can achieve better prediction performance. However, using an individual neural network
to represent a new feature can add more interpretability, which is highly valued in the nance,
medical, and educational sectors.
Some researchers have begun to use neural networks to give an embedding representation of a
nancial time series. More specically, Feng (2019) leveraged LSTM to embed various stock time
series and then used adversarial training to make a binary classication on the stock's return.
Sidra (2019) adopted a well-designed LSTM to extract factors from unstructured news data and,
then, formed a continuous embedding. The experiment result shows that these unstructured data
can provide much information and they are very helpful for event-driven trading. Most of these
research studies focus on predicting single-stock returns by neural networks using their own time
series. However, on nancial factor construction tasks, we do not nd literature that provides a full
solution for automatic factor construction by using neutral networks.
1.3. Contributions
In this paper, a novel network structure called Neural Network-based Automatic Factor Con-
struction (NNAFC) is proposed, which can use deep neural networks to automatically construct
nancial factors. Dierent from previous research, we make three contributions in this paper for
nancial factor construction. (1) We create a novel loss function that diers from the accuracy of
the stock's return by using the Rank Information Coecient (Rank IC) between stock factor values
and stock returns. (2) We dene a new derivable correlation formula for the Rank IC calculation to
make back preparation training of neural networks possible. (3) We adopt pre-training and model
pruning to add up enough diversity into the constructed factors, which helps to produce more
diversied factors. NNAFC has outperformed many benchmarks in all aspects. Furthermore, dif-
ferent pre-training networks for prior knowledge are equipped to further improve their performance
in real-world situations.
2. Denition of the Factor Construction Task
2.1. Denition of an alpha factor
The alpha factor is a raw forecasting vector that has a certain correlation with the nancial assets'
future returns. Richard (1999) pointed out that the informed expected return E(rjg) can be dened
as the expected return conditional on an alpha factor gat timeT. In formula (1) and (2), r
represents the asset return in the future, E(r) represents the consensus return, E(g) represents
the expected forecast, Cov(r;g) means the covariance between randg, andVar(g) represents the
variance of g.
E(rjg) =E(r) +Cov(r;g)
Var(g)(g",2020-08-14T07:44:49Z,many tre for shahid as recurrent neural network ibots is articial intellence t with tre some  fe sid ra t most contributns ineural network automatic faor codi  rank informatcoe rank  rank  furtr  faor construtask  t richard icov var cov var
paper_qf_46.pdf,5,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
where^ICcombine is the IC vector for a set of sub-factors called Factor combine , which consists of
an existing factor set Factor oldand a newly constructed factor set Factor combine . Here, ^ICcombineis the IC co-variance matrix of this set of sub-factors.
Factor construction (FC) is dened as a process for constructing new factors from all nancial
data setsFDand an existing factor set Factor newin a set of mathematical presentations MP to
maximize the objective function in formula (7). The new factor set can be written as
Factor combine =FC(FD;Factor old;MP ) (8)
Based on this denition, manual factor construction stands for a factor construction process to
search potential factors in closed-form mathematical presentations by leveraging human experts'
knowledge in economics and behavioural nance research. The whole process is not completely
automatic and quantitative. GP stands for a factor construction process to search for potential
factors in closed-form mathematical presentations created from a set of mathematical operations
based on certain genetic programming algorithms. The whole process becomes completely auto-
matic and quantitative, and it imitates how a human constructs a mathematical formula from a set
of operations. However, the set of mathematical presentations is still limited by all mathematical
presentations that can be constructed by the set of mathematical operations. If the new factor must
be represented by a higher order of non-linearity or fractal factions, it is dicult to accomplish by
GP. NNAFC stands for a factor construction process to search for potential factors in all parame-
ters of a neural network structure based on a network optimization algorithm to maximize IC and
reduce the correlation with existing factors. The whole process becomes completely automatic and
quantitative and can cover all mathematical presentations. According to Leshno (1993), the deep
neural network is proved to be able to represent all types of mathematical formulas if the deep
neural network has sucient depth and width. Thus, the MP of NNAFC is a super set of manual
factor construction and GP. In the following sections, we discuss the details of GP and NNAFC.
3. Framework Introduction
3.1. Baseline method
Before we introduce our NNAFC, we must introduce its baseline method of Genetic Programming
(GP) as a benchmark. GP leverages reverse polish expression to represent factors. Each tree is an
explicit formula. We use GP to conduct its evolutionary process, such as adding, deleting, merging,
evolving and selecting.
In each round, the samples are selected by a pre-dened objective function. This objective func-
tion is the Spearman Coecient between the factor value and factor return, which is exactly the
same as the objective function in NNAFC. GP adds diversity into the constructed factors by
changing certain parts of a reverse polish expression. In an explicit formula, a small change in the
operators can make the meaning of the factor totally dierent. Thus, in GP, the parent and child
samples can have nothing in common. As a result, the child sample might not successfully inherent
some good characteristics from its parent samples, such as a good ability to predict a future stock
return. Thus, genetic programming is not a good method for constructing new factors, due to its
inecient evolutionary process on this task.
GP adds diversity into the constructed factors by changing certain parts of a reverse polish
expression. For example, we use a binary tree to present initial factor 1 shown in formula (9) and
then conduct an evolution process via GP. In the evolutionary process, GP can make a small change
",2020-08-14T07:44:49Z,combine faor faor faor re combine is faor and faor t faor faor based t t  t accordi lno  iframework introdubaseline before genetic prograi ea i sumacoe i as  for in
paper_qf_46.pdf,6,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
Factor 1 =highprice",2020-08-14T07:44:49Z,faor
paper_qf_46.pdf,7,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
nancial descriptors called Prior Knowledge (PK). They serve as initial seeds, which can improve
the diversity of the newly constructed factors.
In NNAFC, we pre-train the neural network with an existing nancial factor each time, which
performs like a seed that can provide diversity to the system. According to the neural network's uni-
versal approximation theorem, in Leshno (1993), technically, the neural network can approximate
with arbitrary precision any measurable function from one nite dimensional space to another. By
pre-training, the neural network can inherit good characteristics from its parent factor. With the
guidance of the objective function, the new factor should have better IC than the old factor. Thus,
we believe that the neural network can provide us with a more ecient and intuitive evolution
process than GP on this task. The NNAFC framework is shown in Figure 2.
Figure 2. Neural Network-based Automatic Factor Construction's framework
We design the batch sampling rules in which all of the stocks in the same trading day are
grouped into one batch, because NNAFC focuses on predicting the relative return of the stocks
on the same trading day, rather than its absolute return. More specically, for a daily multi-factor
trading strategy, we care only about the relative cross-sectional performance. Additionally, we long
the relatively strong stocks and short the relatively weak stocks in each trading day to make a
prot. The input tensor's shape is ( n;5;m), because there are nstocks in each trading day, and 5
types of time series, which are the open price, high price, low price, close price and volume. The
input length of each time series is m. In Figure 2, the output tensor of this network is called a
feature list and consists of factor values with the shape ( n;1). Here, we assume that all of the
selected networks in Figure 2 are Multi-layer Perceptrons (MLPs), for which it is easy to give a
general mathematical description. In a later section, the experimental results are based on more
complicated and diversied networks. Wiis the kernel matrix in the ithlayer,biis the bias matrix
in theithlayer, andaiis the activation function in the ithlayer. The initial layer starts from
1, and there are players in total. The factor return tensor, with the shape ( n;1), consists of the
returns with which we can earn from the assets if we hold the asset for a period of time. The length
of the holding time is  t, and the current state is t. According to this principle, the factor value
xand factor return ycan be written as in formula (11) and formula (12).
x=lp=ap(WpTlp",2020-08-14T07:44:49Z,prr kledge ty iaccordi lno by with  t   neural network automatic faor constru  additnally t t i re  multi peace ptr ops iwi is t t t accordi wp lp
paper_qf_46.pdf,8,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
y=FactorReturn =closeprice t+t
closeprice t",2020-08-14T07:44:49Z,faor urn
paper_qf_46.pdf,9,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
Shown in Figure 3, if the distribution's deviation is very small, there is no need to use g(x), but
usingg(x) will not make things worse. If the distribution's deviation is very large, then g(x) can
perform almost the same function as the operator rank (), and they can bring positive impacts.
With the kernel function g(x), we dene the nal objective function in formula (14), where E(x)
represents the expected value of x, and xrepresents the average value of x. In each batch, we
calculate the average correlation in qtrading days, which can make the optimization process more
robust.
IC(x;y) =E(g(x)",2020-08-14T07:44:49Z,show  with in
paper_qf_46.pdf,10,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
each layer should be decided by the length of the input time series, where wis the kernel matrix,
bis the bias matrix, and ais the activation function. Then, we use this embedded layer to mimic
the prior knowledge. In this part, we use the min(mean squared error) as the objective function,
which is shown in formula (15).
min
a;b;w1
NNX
i=1(yi",2020-08-14T07:44:49Z,tin
paper_qf_46.pdf,11,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
Shown in Figure 5, we leverage convolution kernels to calculate the data that belongs to dierent
stocks. Since one kernel's size is truly small, it cannot calculate the samples at a far distance. How-
ever, if we repeat the convolution layer several times, all of the stock's data can have the opportunity
to obtain the relevant data. Actually, this structure can also be used to learn time series technical
indicators. In addition, this structure is not tailored for only time series. The previous structure
can better mimic the performance and costs less training time. In the following experiment, these
two types of network structures are both called Fully Connected Networks (FCNs).
For a deep neural network, almost all of the technical factors can be easily learned. Here, MSE
or MAE cannot represent the real pre-training performance, because all of the factor values are
truly small, which makes all MSE values very small. To have a better measure of the performance,
1
NPN
i=1abs(yi",2020-08-14T07:44:49Z,show since how aually it ifully conneed networks ns for re to
paper_qf_46.pdf,12,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
the knowledge is sucient to serve as a source of diversity. However, why is pre-training with
prior knowledge needed? Zhang (2016) pointed out that according to the concept of multi-task
learning, pre-training can permanently keep some parts of the domain knowledge in the network.
The dierent domain knowledge can increase the diversity. Pruning can lter out noise from the
neural network and retain the prior knowledge that we need. In addition, the pruning rate should
be controlled. A larger pruning rate will bring too much diculty for pre-training to converge to
the nal optimization direction, but a smaller pruning rate could lose the diversity. Frankle and
Carbin (2018) pointed out that the ideal pruning rate should be approximately 0.2-0.5, and the
mask matrix is composed of only 0 and 1. All of the settings are the same as those in the paper
of Ding (2015). After embedding the data as f(x) in formula (16), we obtain its parameter matrix
W. Then, we create a mask matrix to prune the parameters. For example, xijin the parameter
matrix is relatively small, which means that some of the input data is useless. Then, Mij=0 is set to
permanently mask this value. If the xijis not useless, then we set Mij=1. This method can retain
the diversity in the network. Furthermore, it can focus on improving the current situation, without
heading into an unknown local minimum. The pruning process by using matrix Mis shown in
formula (16):
f(x) = (WM)Tx+b (16)
After pre-training and pruning the network, we use the objective function shown in formula (14)
for the NNAFC's training. We simply reshape the input data into a graph, and then, we use the
Saliency Map proposed by Simonyan (2014) to look at how the raw data contributes to the nal
constructed factor. The Saliency Map's calculation method is shown in formula (17). If we assume
that the input pixel is I, then the Saliency Value can be approximated by the First Order Taylor
Expansion, S(Ip)wTIp+b. After calculating the derivative on the variable I, we can obtain the
contribution rate of pixel Ip.
WIp=@S(Ip)
@Ip(17)
Figure 6. Factor construction process of one stock series for illustrating the evolution process of NNAFC. However,
in our factor construction process, we do use cross-sectional data as inputs.
The training process is illustrated in Figure 6. The y-axis is [open price, high price, low price,
",2020-08-14T07:44:49Z, t pruni ifrank le car iall di after tfor tmi  mi  furtr t mis tx after  aliecy map simoat aliecy map  aliecy value rst orr taylor eansip ip after ip ip ip ip  faor t  t
paper_qf_46.pdf,13,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
positive contribution, and the blue means a negative contribution. The colour's depth has a positive
correlation with the contribution rate. From Figure 6, we can obtain further understanding of the
construction process. First, there is a reverse technical factor, according to the raw data's negative
contributions at the latest time. Second, we can know that this factor is mainly constructed by the
price information, but not volume information. Third, the components of this factor are discrete.
The factors constructed by human experts are usually continuous. This dierence makes the factors
constructed by NNAFC uncorrelated to manually constructed factors by human experts. Because
the trading opportunities are limited, if we all use similar trading signals, the trading will be
more and more crowded. Prior knowledge can be viewed as seeds for new factors, which can bring
diversied and robust initialization. Starting from this prior knowledge would be a very safe choice.
Furthermore, as mentioned in section 3.1, one of GP's shortcomings is that the child factor might
not inherit the good characteristics of its parent factor. However, for NNAFC, all of the updates in
the parameters are dominated by back-propagation. The mechanism of back-propagation is gradient
descent, which ensures that every optimization step is continuous. These continuous characteristics
can ensure that the parent factor and child factor have a lot in common. Thus, GP is more similar
to an advanced searching algorithm, but NNAFC is more like an advanced evolutionary algorithm.
We also conduct experiments on dierent feature extractors (in the computer science eld, dier-
ent feature extractors mean dierent neural network structures) to have a better understanding of
the pre-training on dierent nancial time series. There are two motivations for conducting experi-
ments on dierent feature extractors. First, dierent feature extractors require dierent input data
structures. We explored many dierent methods for organizing the input data. We conducted exper-
iments on many basic networks and SOTA (state-of-the-art) networks, including Fully connected
network (FCN), Le-net, Resnet-50, Long Short Term Memory Network (LSTM), Transformer (be-
cause Transformer is too large, we used only the self-attention encoder) and Temporal Convolution
Network (TCN). FCN has 3-5 hidden layers, with 64-128 neutrals in each layer. We used the tanh
and relu function to serve as the activation function, and the output layer should not be activated.
For Le-net, Resnet-50, Transformer, and TCN, we did not change their structures, but we employed
only a smaller stride because the input data has a low dimension compared with the real picture.
The second motivation is that dierent feature extractors have their own advantages and dis-
advantages. Some of them aim at extracting temporal information, but the others aim at spatial
information. Some of them are designed for a long term series, but others are designed for quick
training. We believe that they can make our factor pool even more diversied.
3.4. Summarizing the pros and cons
Until now, we have introduced all of the network settings of NNAFC, and we have shown the
characteristics of NNAFC compared with GP. Here, we summarize their pros and cons. First,
GP conducts a very low eciency evolution process, and its performance is more similar to a
searching process. As a result, the factors constructed from GP are very similar. However, NNAFC
can conduct an ecient evolutionary process, and it fully inherits the diversity from the prior
knowledge. Thus, the factors constructed by NNAFC are more diversied.
Second, the factors constructed by GP are formulaic factors, which is easier to explain. However,
for NNAFC, although we attempt to explain it in section 3.4, its explainable ability is still limited.
Third, for some simple factors, GP is truly easier to explain. However, the simple factors cannot
provide much information. In other words, there is no room for us to improve both the quality
and quantity together when the factors are very simple. Only when the formula is very complex,
we have the possibility to reproduce many dierent and useful factors based on it. However, if the
formula is very complex, we cannot understand and explain the factors constructed by GP, either.
In this situation, both methods cannot be easily explained. The non-formulaic factors constructed
by NNAFC could be better. At least, this approach has the ability to retain more information. As
",2020-08-14T07:44:49Z,t from  rst second third t  because prr starti furtr t tse   tre rst   fully le rnet lo short term memory network transformer transformer temal consolatnetwork  for le rnet transformer t some some  suarizi unt re rst as  second third ionly it at as
paper_qf_46.pdf,14,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
4. Numerical Experiments
4.1. Experiment setting
In the experiment part, we performed two types of experiment. First, we conducted experiments
to compare the performance of NNAFC and GP from the perspective of IC and diversity. Second,
we conducted experiments to measure their contribution in real trading strategies. We compared
a strategy that uses only the prior knowledge, a strategy that uses both prior knowledge and
the factors constructed by GP, and a strategy that uses both prior knowledge and the factors
constructed by NNAFC. This comparison can show how much contribution our method can bring
to the current multi-factor strategy.
We used daily trading data in the Chinese A-share stock market (in the following, we call it the
A-share market data), including the daily open price, high price, low price, close price and trading
volume over the past 30 trading days. The raw data is standardized by using its time-series mean
and standard deviation. Both the mean and standard deviation are calculated from the training
set. We attempted to use these inputs to predict the stock return in the next 5 trading days, which
stands for the weekly frequency and is a common period used in short-term technical indicators.
In the A-share market, we cannot short single stocks easily because of the current limitations in
stock lending, but we can short some important index futures.
We performed some research on selecting some reasonable hyper-parameters. During the model
training process, we calculated the average value of IC over 20 randomly selected trading days. For
each experiment, 250 trading days served as the training set (no technical factor can work well for
a long period of time), and the following 30 trading days served as the validation set, while the
following 90 trading days served as the testing set. These hyper-parameters are common settings in
industrial practices. To make a fair comparison, the same setting is deployed for the GP algorithm.
In this paper, we analyse the construed factors' performance from dierent perspectives. Based
on the denition of alpha factors, we use the information coecient (IC), shown in formula (6),
to measure how much information is carried by a factor. For diversity, each factor value should be
normalized at rst. The softmax function in formula (18) can eliminate the eect from the scale
without losing the factors' rank information.
Softmax (xi) =exp(xi)Pexp(xi)(18)
Then, the cross-entropy is used to measure the distance between two dierent factors' distribu-
tions on the same trading day.
Distance (f1;f2) =X
softmax (f1)log1
softmax (f2)(19)
In formula (19), f1andf2refer to dierent factors' distributions in the same trading day. K-means
is used to cluster the distance matrix of the relative distance between two factors. The average
distance between each cluster centre refers to the diversity of factors on this trading day. We have
shown in formula (6) that a higher optimal IC for an investment strategy is related to a higher sub
IC and a larger factor diversity. In addition to measuring the IC and diversity, the performance
of a trading strategy based on the constructed factors is also measured, such as absolute return,
max-drawdown, and sharp-ratio.
To put all in a nutshell, we summarize the construction procedure mentioned in section 3. And
then, we illustrate the entire process of constructing a new factor via NNAFC. First, we pre-train
a neural network with a technical indicator MA as prior knowledge. The pre-training performance
",2020-08-14T07:44:49Z,numerical eximents eximent irst second    chinese t both  i  for tse to ibased for t soft max e tdistance it  ito and rst t
paper_qf_46.pdf,15,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
Second, we train the neural network by maximizing IC dened in formula (14). During the back-
propagation, the neural network has been changed, and the output factor value changes also. This
process has been shown in Figure 6. Because we use the IC to serve as an objective function and
the mechanism of back-propagation is gradient descent, the newly constructed factors will have
higher IC than the initialized factor. We also use the following experiment results to prove this
assumption. At last, the trained neural network is a newly constructed factor. As mentioned in
the section 3.4, it has pros and cons compared with the benchmark. We denitely cannot know its
closed-form formula; however, we can know what raw data contributes to it and how dierent it is
compared with the initialized factor, shown in Figure 6.
4.2. Beating the state-of-the-art technique
NNAFC can be equipped with dierent neural networks. In this test case, NNAFC is equipped
with 4 layers in a fully connected neural network (FCN). The experiment shows that NNAFC can
beat the GP. We propose three schemes to help illustrate NNAFC's contribution and to show how
it beat the GP. The only GP means only using GP, Only NNAFC means only using NNAFC to
construct factors, and GP and NNAFC means using the GP's value to initialize NNAFC and then
construct factors. The out-of-sample results of the experiments are summarized in Table 4.
Table 4. The performance of dierent schemes
Object Information Coecient Diversity
Only GP 0.094 17.21
GP and NNAFC 0.122 25.44
Only NNAFC 0.107 21.65
Only NNAFC is better than Only GP , which means that NNAFC outperforms GP on this task.
We also nd that GP and NNAFC is the best, which means that our method can even improve
on the performance of GP. However, in real practice, we should leverage the constructed factors to
form a multi-factor strategy and compare its performance with GP. The specic strategy setting
is the same as in section 3.4, and we have repeated this experiment for dierent periods of time.
The long-term back-testing result is shown in Table 5.
Table 5. Strategy's absolute return for each scheme.
Time Only GP GP and NNAFC Only NNAFC ZZ500
Train:2015.01-2015.12
Test: 2016.02-2016.03+2.59% +5.74% +4.52% +1.67%
Train:2016.01-2016.12
Test: 2017.02-2017.03+5.40% +10.26% +8.33% +2.53%
Train:2017.01-2017.12
Test: 2018.02-2018.03-5.27% -4.95% -4.16% -6.98%
Train:2018.01-2018.12
Test: 2019.02-2019.03+13.00% +15.62% +15.41% +13.75%
As shown in Table 5, the Only NNAFC always has better performance than the Only GP during
the long-term backtest. The results show that our method has also beaten the SOTA in real practice.
However, will there be more powerful feature extractors to discover knowledge from nancial time
series? And what shall be the suitable input data structure for dierent nancial time series? We
",2020-08-14T07:44:49Z,second    because  at as   beati it  t only t table table t obje informatcoe diversity only only only only  t t table table strategy time only only tr articial intellence test tr articial intellence test tr articial intellence test tr articial intellence test as table only only t and 
paper_qf_46.pdf,16,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
4.3. Comparing dierent feature extractors
For the hardware equipment, we used 20 g GPU (NVIDIA 1080Ti) and 786 g CPU (Intel Xeon
E5-2680 v2, 10 cores). Based on this setting, we show the amount of time that we need to construct
50 factors. Moreover, the time to restore 50 trained networks and obtain their factor values will be
substantially faster than traditional factors. Because some of the traditional factors are constructed
with complicated explicit formulas, these formulas are not suitable for matrix computing. Using
neural networks to represent factors in matrix computing, which have faster testing speeds. For
the overhead of this framework, the time complexity is acceptable. With 20G GPU, it costs only
several hours to construct 50 factors. However, the GPU memory is a large problem. In each back-
propagation, we need to store more than 3000 stocks' time series, each stock has at least 5 time
series, and each time series contains more than 100 points. Thus, this framework requires at least
20G GPU in resources for the Chinese stock market's backtest, and more GPU resources will be
better.
Table 6. The higher the information coecient (IC) and diversity are, the better is their performance. Normally, a
good factor's long-term IC should be higher than 0.05.
Type Network IC Diversity Time
Baseline GP 0.072 17.532 0.215 hours
Vanilla FCN 0.124 22.151 0.785 hours
SpatialLe-net 0.123 20.194 1.365 hours
Resnet-50 0.108 21.403 3.450 hours
TemporalLSTM 0.170 24.469 1.300 hours
TCN 0.105 21.139 2.725 hours
Transformer 0.111 25.257 4.151 hours
Shown in Table 6, Type means the category of neural networks. For example, both Le-net and
Resnet are designed for extracting spatial information. Allen (1999) pointed out that all neural
networks can produce more diversied factors than using GP. For Le-net and Resnet, they do not
provide us with more informative factors, but for more diversied factors, there is LeCun (1999)
and He (2016). Temporal extractors are especially better at producing diversied factors, such as
LSTM and Transformer, Hochreiter (1997) and Vaswani (2017). For TCN, Dea (2018) proves its
ability to capture the temporal rules buried in data. However, they have enormous dierences. TCN
relies on the CNN, but LSTM and Transformer still contain an RNN. Normally, the transformer
uses an RNN to embed the input data. The existence of a recurrent neural network structure
can contribute to more diversity. All of the neural networks mentioned above can produce more
informative and diversied factors than GP. For the same types of networks, the results suggest
that the simple network structure performs relatively better than the sophisticated networks.
4.4. Real-world use case test
In the real-world use case test, we use the factor constructed via NNAFC. At the same time, we
also use the factors constructed by human experts. This approach should give a fair setting for a
comparison, and we want to see whether the NNAFC can bring marginal benets for the traditional
and mature investment system. Table 7 shows the back-testing results of our strategy.
In the training set, the stocks whose returns rank in the top 30% in each trading day are labelled
as 1, and the stocks whose return ranked in the last 30% of each trading day are labelled as 0.
We abandon the remaining stocks in the training set, according to Fama (1993). After training
these factors with Chen (2015)'s XGBoost using binary logistics mode, we can obtain a trained
model that can make binary predictions. The prediction result reects the odds as to whether the
stock's return will be larger than 0 in the following several trading days. It denes the 50 factors
constructed by human experts as PK 50 , and the 50 factors constructed by NNAFC as New 50",2020-08-14T07:44:49Z,ari for ti intel xeno based obecause usi for with i chinese table t normally  network diversity time baseline vanla spatial le rnet temal transformer shoable  for le rnet allefor le rnet le u temal transformer ocenter  for  transformer normally t all for real iat  table i fam after cboost t it new
paper_qf_46.pdf,17,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
another case, in the training set, we use XGBoost to train 100 factors, which are composed of PK
50andNew 50 . Then, we select 50 factors whose factor weights are relatively large among these
100 factors. The weights can be calculated by the frequency of using the factor. For example, in a
tree-based algorithm, the important features can obtain more opportunity to serve as a splitting
feature. By using this approach, we can use XGBoost to select 50 important factors, which consist
of both traditional factors and neural network-based factors. We want to stress the point again
that the feature selection process occurred in the training set. We do not conduct feature selection
in the validation set or testing set. Furthermore, during each backtesting, this factor selection
process should only be conducted once. Last, we dene the selected 50 factors as Combine 50 ,
which represents the practical use of our factors.
Table 7. The investment target is all Chinese A-share stocks, except for the stocks that cannot be traded during
this period of time. The strategy's commission fee is 0.3%.
Type Target Group Revenue MD SR
BaselineZZ500 Stock Index 19.60% 13,50% 1.982
HS300 Stock Index 18.60% 20.30% 1.606
PK PK 50 24.70% 18.90% 2.314
GPGP 50 17.60% 25.30% 1.435
Combine 50 25.40% 14.80% 2.672
Vanilla FCN Combine 50 29.60% 15.70% 3.167
SpatialLe-net Combine 50 27.50% 16.40% 2.921
Resnet-50 Combine 50 29.30% 17.20% 2.787
TemporalLSTM Combine 50 29.90% 15.00% 3.289
TCN Combine 50 26.90% 16.80% 2.729
Transformer Combine 50 27.20% 15.10% 2.806
As shown in Table 7, HS300 andZZ500 are important stock indices in the A-share stock market.
The strategy for the excess return is the annualized excess return of the long portfolio vs. the index.
The max drawdown is the worst loss of the excess return from its peak. The Sharpe ratio is the
annually adjusted excess return divided by a certain level of risk. These indicators can show the
strategy's performance from the perspective of both return and risk.
In a multi-factor strategy, a higher correlation among the factors will reduce the strategy's
performance, and it is shown in the denition of the factor construction process in formula (7).
The goal of factor construction is not to nd factors with higher performance, but to nd factors
that can improve the overall performance of the combined factors selected from PK 50 andNew
50. Thus, combining the factors from both the new and existing human experts' factors is more
reasonable and suitable for practical use cases. In all cases, our combined 50 is better than PK 50
and GP's Combine 50 , which means that the NNAFC can construct more useful factors than GP
with a reasonable factor selection process.
4.5. Comprehending the results
In section 4, the numerical experiment results show that the LSTM can extract more information
than FCN. We suspect that only the RNN and FCN are helpful for extracting information from
the nancial time series. To verify this idea, we constructed 50 factors by using an FCN, 50 factors
by using a spatial neural network and 50 factors by using a temporal neural network. Then, we
clustered all of these factors into three groups by using k-means. The goal of this process is that
there are mainly three types of neural networks, and we want to nd whether the constructed
factors have a similarity relationship. The denition of the distance has been mentioned in formula
(19) in section 4.1. To visualize this distance matrix, this matrix should be transformed into a 2D
graph. We initialize one of the cluster centres as (0 ;0) and then determine the other two cluster
",2020-08-14T07:44:49Z,boost new tt for by boost   furtr last combine table t chinese t  target group revenue baseline stock inx stock inx combine vanla combine spatial le combine rnet combine temal combine combine transformer combine as table t t t share tse it new  icombine rendi i to tt t to 
paper_qf_46.pdf,18,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
only the outlook of this graph and will not inuence the shared space between the two dierent
clusters. For samples that belong to the same cluster, their location is determined according to the
relative distance between a cluster centre and a randomly generated direction. As a result, we can
obtain the distribution of the constructed factors. The larger the factor's sparsity is, the larger the
contribution of a certain type of network. Sometimes, the factor's sparsity is low, but if it has never
been overlapped by the other network's factors, its distinct contributions are also highly valued.
The experiment results are shown in Figure 7.
Figure 7. Cluster dierent networks (spatial against temporal)
As shown in Figure 7(left), the factors constructed by the LSTM have the sparsest distribution,
which means that the network structure that focuses on temporal information is excellent at ex-
tracting diversied information from the nancial time series. However, a large space is shared by
FCN and Le-net. We can regard Le-net's information as a subset of the FCN. Combined with the
CNN's poor performance in sections 4.2 and 4.3, it looks such as that the CNN structure does not
make a substantial contribution in extracting information from the nancial time series. Figure
7(right) is an extra experiment, whose results support this conclusion as well.
Except for the type of neural network, actually, the complexity of the neural networks also
inuences the result. We provide extra experiments in Figure 8. Normally, the model's complexity
should meet the dataset's complexity. Most of the nancial decisions are still made by humans, and
thus, the signals are mostly linear and simple because this way is how the human brain processes
information. A very complicated network structure will bring in extra risk of over-tting.
Figure 8. More complicated neural networks compared with the networks used in gure 8.",2020-08-14T07:44:49Z,for as t sometimt   uster as  le  le combined  except   normally most  
paper_qf_46.pdf,19,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
In Figure 8, the shared space between Vanilla and Spatial is still larger than the space shared
between Vanilla and Temporal. Thus, it appears that the reason why the LSTM's information
coecient can outperform the other networks is that the recurrent structure and fully connected
structure are truly helpful in extracting information from nancial time series. At the same time,
these two structures focus on a dierent part of the information, which makes their joint eort
more valuable. Using an RNN to obtain an embedding representation of a nancial time series is
the best choice, as shown in our experiments. However, a more complex time-decay structure could
be used, such the TCN, but the Transformer does not perform better than the LSTM. We believe
that the trading signals in the Chinese A-share market are mostly linear, and thus, a very complex
non-linear feature extractor (a neural network structure) is not suitable at present.
However, while the stock market is developing, more and more investors crowd into this game.
We think that the factor crowding phenomenon will become more and more clear. In addition, as
more and more tradings are made by algorithms, the non-linear part in the trading signals will be
larger. Thus, for quantitative trading, we believe that the complicated and tailored neural network
structure will have its supreme moment in the near future.
5. Conclusions and Future Research
In this paper, we propose Neural Network-based Automatic Factor Construction (NNAFC). This
framework can automatically construct diversied and highly informative technical indicators with
the help of prior knowledge and dierent feature extractors. In both numerical experiment and real-
world use case tests, it can perform better than the state-of-the-art in this task, which is genetic
programming. Although dierent network structures perform dierently in this factor construction
task, they can contribute to the diversity of the factor pool. Thus, they are also highly valuable
for the multi-factor quantitative investment strategy. Furthermore, we also conduct experiments
to comprehend their contributions and dierences. For further research, this framework can also
be tested on a company's fundamental and market news data.
References
Allen F, Karjalainen R. Using genetic algorithms to nd technical trading rules[J]. Journal of Financial
Economics, 1999, 51(2): 245-271.
Botsis T, Nguyen M D, Woo E J, et al. Text mining for the Vaccine Adverse Event Reporting
System: medical text classication using informative factor selection[J]. Journal of the American Medical
Informatics Association, 2011, 18(5): 631-638.
Chen T, He T, Benesty M, et al. Xgboost: extreme gradient boosting[J]. R package version 0.4-2,
2015: 1-4.
Dash M, Liu H. Feature selection for classication[J]. Intelligent data analysis, 1997, 1(3): 131-
156.
Ding X, Zhang Y, Liu T, et al. Deep learning for event-driven stock prediction[C]. Twenty-fourth
international joint conference on articial intelligence. 2015.
Edward E. Qian, Ronald H. Hua, Eric H. Sorensen, Quantitative Equity Portfolio Management,
Chapman and Hall/CRC, 2007.
Fama E F, French K R. Common risk factors in the returns on stocks and bonds[J]. Journal of
Financial Economics, 1993.",2020-08-14T07:44:49Z,i vanla spatial vanla temal  at usi transformer  chinese  i conusns future researineural network automatic faor constru ialthough  furtr for referencallekar al articial intellence eusi journal nancial economics bots is uyewoo text vaccine adverse event reti tem journal medical informatics associatc best xg boost dash  feature intellent di   ep tnty edward iaronald hua eric so rensequantitative equity tfmanagement chapmahall fam frencoojournal nancial economics
paper_qf_46.pdf,20,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
Fama E F, French K R. A ve-factor asset pricing model[J]. Journal of nancial economics, 2015,
116(1): 1-22.
Feng G, Giglio S, Xiu D. Taming the factor zoo: A test of new factors[J]. The Journal of Fi-
nance, June 2020, 75(3): 1327-1370.
Fuli Feng, Huimin Chen, Xiangnan He, Ji Ding, Maosong Sun, and Tat-Seng Chua.
Enhancing Stock Movement Prediction with Adversarial Training. Papers.arXiv.org.
https://EconPapers.repec.org/RePEc:arx:papers:1810.09936, 2019.
Frankle, Jonathan, and Michael Carbin. \\\\The lottery ticket hypothesis: Finding sparse, trainable
neural networks."" arXiv preprint arXiv:1803.03635 (2018).
Fischer T, Krauss C. Deep learning with long short-term memory networks for nancial market
predictions[J]. European Journal of Operational Research, 2018, 270(2): 654-669.
Grinold, Richard C. and Kanh, Ronald N.,Active portfolio management: a quantitative approach
for providing superior returns and controlling risk[M], McGraw-Hill, 1999.
Harvey, C.R., Liu, Y. and Zhu, H. The Cross-Section of Expected Returns. The Review of Finan-
cial Studies, 2016, 29, 5-68.
Hidasi B, Quadrana M, Karatzoglou A, et al. Parallel recurrent neural network architectures for
factor-rich session-based recommendations[C]. Proceedings of the 10th ACM conference on recommender
systems. ACM, 2016: 241-248.
He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]. Proceedings of the
IEEE conference on computer vision and pattern recognition. 2016: 770-778.
Hochreiter S, Schmidhuber J. Long short-term memory[J]. Neural computation, 1997, 9(8): 1735-
1780.
Juuti M, Szyller S, Marchal S, et al. PRADA: protecting against DNN model stealing attacks[C].
2019 IEEE European Symposium on Security and Privacy. IEEE, 2019: 512-527.
Fang, J., Xia, Z., Liu, X., Xia, S., Jiang, Y., & Lin, J. (2019). Alpha Discovery Neural Network
based on Prior Knowledge. arXiv preprint arXiv:1912.11761.
J. Wang, Y. Yang, J. Mao, Z. Huang, C. Huang and W. Xu, ""CNN-RNN: A Unied Framework
for Multi-label Image Classication,"" 2016 IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2016, 2285-2294.
Fang, J., & Lin, J. (2020). Prior knowledge distillation based on nancial time series. arXiv preprint
arXiv:2006.09247.
Krawiec K. Genetic programming-based construction of factors for machine learning and knowl-
edge discovery tasks[J]. Genetic Programming and Evolvable Machines, 2002, 3(4): 329-343.
Kakushadze Z. 101 formulaic alphas[J]. Wilmott, 2016, 84: 72-81.
Lillywhite K, Lee D J, Tippetts B, et al. A factor construction method for general object recog-
nition[J]. Pattern Recognition, 2013, 46(12): 3300-3314.
Lai S, Xu L, Liu K, et al. Recurrent convolutional neural networks for text classication[C]. Twenty-ninth
AAAI conference on articial intelligence. 2015.",2020-08-14T07:44:49Z,fam frenjournal fe bibl xi taki t journal  june ful fe hui miclia na ji di mao so sutat so cha enhanci stock movement prediadversarial tr articial intellence ni pas  ec opas re ec frank le jonathamichael car it ndi   scr grass ep ajournal oatnal researgriold richard karonald aive mc raw hl harvey  zhu t oss seeeed urns t review astudihid as squadra na karate zo lou llel proceedis   reep proceedis ocenter schmidt haber lo neural ju uti  ller maral asymposium security privacy fa via  via lia lialpha divery neural network prr kledge   wa ya mao hu hu xu uni framework multi image ass conference uter vispaerrec oni fa liprr   raw iec genetic genetic prograi evo lv able machinu sha pot sly white  tipped paerrenitarticial intellence xu  recurrent tnty
paper_qf_46.pdf,21,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
Leshno M, Lin V Y, Pinkus A, et al. Multilayer feedforward networks with a nonpolynomial acti-
vation function can approximate any function[J]. Neural networks, 1993, 6(6): 861-867.
LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J].
Proceedings of the IEEE, 1998, 86(11): 2278-2324.
Lea C, Flynn M D, Vidal R, et al. Temporal convolutional networks for action segmentation and
detection[C]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017:
156-165.
Mittal S. A survey of techniques for approximate computing[J]. ACM Computing Surveys (CSUR), 2016,
48(4): 1-33.
Romero, et al. Knowledge discovery with genetic programming for providing feedback to course-
ware authors[J]. User Modeling and User-Adapted Interaction, 2004, 14(5):425{464.
Ravisankar P, Ravi V, Rao G R, et al. Detection of nancial statement fraud and factor selection
using data mining techniques[J]. Decision Support Systems, 2011, 50(2): 491-500.
Sharpe W F. Capital asset prices: A theory of market equilibrium under conditions of risk[J].
Journal of nance, 1964, 19(3): 425-442.
Shan K, Guo J, You W, et al. Automatic facial expression recognition based on a deep convolutional-
neural-network structure[C]. 2017 IEEE 15th International Conference on Software Engineering Research,
Management and Applications (SERA). IEEE, 2017: 123-128.
Sidra Mehtab and Jaydip Sen. A Robust Predictive Model for Stock PricePre-
diction Using Deep Learning and Natural Language Processing. Papers.arXiv.org.
https://EconPapers.repec.org/RePEc:arx:papers:1912.07700, 2019.
Simonyan, K., Vedaldi, A., & Zisserman, A. (2014). Deep Inside Convolutional Networks: Visual-
ising Image Classication Models and Saliency Maps. CoRR, abs/1312.6034.
Tibshirani R. Regression shrinkage and selection via the lasso[J]. Journal of the Royal Statistical
Society: Series B (Methodological), 1996, 58(1): 267-288.
Tran B, Xue B, Zhang M. Genetic programming for factor construction and selection in classica-
tion on high-dimensional data[J]. Memetic Computing, 2016, 8(1): 3-15.
Thomas J D, Sycara K. The importance of simplicity and validation in genetic programming for
data mining in nancial data[C]. Proceedings of the joint AAAI-1999 and GECCO-1999 Workshop on
Data Mining with Evolutionary Algorithms. 1999.
Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]. Advances in neural infor-
mation processing systems. 2017: 5998-6008.
Wold S, Esbensen K, Geladi P. Principal component analysis[J]. Chemometrics and intelligent
laboratory systems, 1987, 2(1-3): 37-52.
Yu Zhang, Ying Wei, et.al . Learning to Multitask[C]. NeurlPS 2018.
Z. Zhang, T. Chen, M. Wang and L. Zheng, An Exponential-Type Anti-Noise Varying-Gain Net-
work for Solving Disturbed Time-Varying Inversion System, IEEE Transactions on Neural Networks and
Learning Systems, 2019.
Z. Zhang, L. Zheng, T. Qiu and F. Deng, ""Varying-Parameter Convergent-Dierential Neural So-",2020-08-14T07:44:49Z,lno lipink us multiplayer neural le ubot to be   gradient proceedis lea flynvital temal proceedis conference uter vispaerrenitmi uti surveys romero kledge user moli user adapted intri shankar ri rao tecissupt tems share capital journal shaguo you automatic internatnal conference software eineeri researmanagement applicatns sid ra meta jay dip serobust prediive mstock price pre usi ep learni natural  processi pas  ec opas re ec simoaved ldi is ser maep insi convolutnal networks visual image ass mols aliecy maps co tib shi rani regressjournal royal statistical society serimethodological tr adue  genetic metric uti thomas sy cara t proceedis workshdata mini evolutnary algorithms  haze er parma aentadvancwould besegel di principal c mo metrics yu  yi i learni multi task ne url  cwa c aeonential  anti noise varyi articial intellence net solvi disturbed time varyi iverstem transans neural networks learni tems  c qi e varyi meter convergent di neural so
paper_qf_46.pdf,22,Neural Network-based Automatic Factor Construction,"  Instead of conducting manual factor construction based on traditional and
behavioural finance analysis, academic researchers and quantitative investment
managers have leveraged Genetic Programming (GP) as an automatic feature
construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development
of deep learning, more powerful feature extraction tools are available. This
paper proposes Neural Network-based Automatic Factor Construction (NNAFC), a
tailored neural network framework that can automatically construct diversified
financial factors based on financial domain knowledge and a variety of neural
network structures. The experiment results show that NNAFC can construct more
informative and diversified factors than GP, to effectively enrich the current
factor pool. For the current market, both fully connected and recurrent neural
network structures are better at extracting information from financial time
series than convolution neural network structures. Moreover, new factors
constructed by NNAFC can always improve the return, Sharpe ratio, and the max
draw-down of a multi-factor quantitative investment strategy due to their
introducing more information and diversification to the existing factor pool.
","
lution to Time-Varying Overdetermined System of Linear Equations,"" in IEEE Transactions on
Automatic Control, 2020, 65(2), 874-881.
Zhong Y, Sullivan J, Li H. Face attribute prediction using o-the-shelf cnn factors[C]. 2016 Inter-
national Conference on Biometrics (ICB). IEEE, 2016: 1-7.",2020-08-14T07:44:49Z,time varyi otermined tem linear equatns transans automatic contrho sullivali face inter conference bmetric
paper_qf_47.pdf,1,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","arXiv:0909.1974v2  [q-fin.GN]  21 Jun 2010Econophysics: Empirical facts and agent-based models
Anirban Chakrabortia)and Ioane Muni Tokeb)
Laboratoire de Math´ ematiques Appliqu´ ees aux Syst` emes, Ecole Centrale Paris, 92290 Chˆ atenay-Malabry,
France
Marco Patriarcac)
National Institute of Chemical Physics and Biophysics, R¨ a vala 10, 15042 Tallinn,
Estonia and
Instituto de Fisica Interdisciplinaire y Sistemas Complej os (CSIC-UIB), E-07122 Palma de Mallorca,
Spain
Fr´ ed´ eric Abergeld)
Laboratoire de Math´ ematiques Appliqu´ ees aux Syst` emes, Ecole Centrale Paris, 92290 Chˆ atenay-Malabry,
France
(Dated: 29 May 2018)
This article aim at reviewing recent empirical and theoretical develop ments usually grouped under the term
Econophysics . Since its name was coined in 1995 by merging the words “Economics” a nd “Physics”, this new
interdisciplinary ﬁeld has grown in various directions: theoretical ma croeconomics (wealth distributions),
microstructure of ﬁnancial markets (order book modelling), econ ometrics of ﬁnancial bubbles and crashes,
etc. In the ﬁrst part of the review, we begin with discussions on the interactions between Physics, Math-
ematics, Economics and Finance that led to the emergence of Econo physics. Then we present empirical
studies revealing statistical properties of ﬁnancial time series. We begin the presentation with the widely
acknowledged “stylized facts” which describe the returns of ﬁnan cial assets – fat tails, volatility clustering,
autocorrelation, etc. – and recall that some of these properties are directly linked to the way “time” is taken
into account. We continue with the statistical properties observe d on order books in ﬁnancial markets. For
the sake of illustrating this review, (nearly) all the stated facts ar e reproduced using our own high-frequency
ﬁnancial database. Finally, contributions to the study of correlat ions of assets such as random matrix theory
and graph theory are presented. In the second part of the revie w, we deal with models in Econophysics
through the point of view of agent-based modelling. Amongst a large number of multi-agent-based models,
we have identiﬁed three representative areas. First, using previo us work originally presented in the ﬁelds of
behavioural ﬁnance and market microstructure theory, econop hysicists have developed agent-based models of
order-driven markets that are extensively presented here. Sec ond, kinetic theory models designed to explain
some empirical facts on wealth distribution are reviewed. Third, we b rieﬂy summarize game theory models
by reviewing the now classic minority game and related problems.
Keywords: Econophysics; Stylized facts; Financial time series; Co rrelations; Order book models; Agent-
based models; Wealth distributions; Game Theory; Minority Games; P areto Law; Entropy maximization;
Utility maximization.
PACS Nos.: 05.45.Tp, 02.50.Sk, 05.40.-a, 05.45.Ra, 89.75.Fb
Part I
I. INTRODUCTION
What is Econophysics ? Fifteen years after the word
“Econophysics” was coined by H. E. Stanley by a merg-
ing ofthe words‘Economics’and ‘Physics’, at aninterna-
tional conference on Statistical Physics held in Kolkata
in 1995, this is still a commonly asked question. Many
still wonder how theories aimed at explaining the phys-
ical world in terms of particles could be applied to un-
derstand complex structures, such as those found in the
social and economic behaviour of human beings. In fact,
a)Electronic mail: anirban.chakraborti@ecp.fr
b)Electronic mail: ioane.muni-toke@ecp.fr
c)Electronic mail: marco.patriarca@kbﬁ.ee
d)Electronic mail: frederic.abergel@ecp.frphysics as a natural science is supposed to be precise
or speciﬁc; its predictive powers based on the use of a
few but universal properties of matter which are suﬃ-
cient to explain many physical phenomena. But in social
sciences, are there analogous precise universal properties
known for human beings, who, on the contrary of funda-
mental particles, are certainly not identical to each other
in any respect ? And what little amount of informa-
tion would be suﬃcient to infer some of their complex
behaviours ? There exists a positive strive in answer-
ing these questions. In the 1940’s, Majorana had taken
scientiﬁc interest in ﬁnancial and economic systems. He
wrote a pioneering paper on the essential analogy be-
tween statistical laws in physics and in social sciences
(di Ettore Majorana (1942); Mantegna (2005, 2006)).
However, during the following decades, only few physi-
cists like Kadanoﬀ(1971) or Montroll and Badger (1974)
had an explicit interest for research in social or eco-
nomic systems. It wasnot until the 1990’sthat physicists
started turning to this interdisciplinary subject, and in",2009-09-10T15:25:45Z, jueco no psics emical airb achara bor tia loamuni to b labor to ire math applied t ecole centrale paris mala ry france marco patria rca natnal institute cmical psics bpsics calli estonia institute  sica inter disc ipl iarticial intellence re   le alma mallory sp articial intellence fr aber ld labor to ire math applied t ecole centrale paris mala ry france dated may  eco no psics since economics psics ipsics math economics nance eco no t  for nally ieco no psics amost rst sec third keywords eco no psics stylized nancial co orr agent alth game tory minority gamlaw entropy utity nos tp sk ra fb part what eco no psics fteeeco no psics stanley economics psics statistical psics lkata many ieleronic eleronic eleronic eleronic but and tre imajori  entire majori antenna ada no mont roll badger it
paper_qf_47.pdf,2,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","2
the past years, they have made many successful attempts
to approach problems in various ﬁelds of social sciences
(e.g. de Oliveira et al.(1999); Stauﬀer et al.(2006);
Chakrabarti et al.(2006)). In particular, in Quantita-
tive Economics and Finance, physics research has begun
to be complementary to the most traditional approaches
such as mathematical (stochastic) ﬁnance. These various
investigations, based on methods imported from or also
used in physics, are the subject of the present paper.
A. Bridging Physics and Economics
Economics deals with how societies eﬃciently use
their resources to produce valuable commodities and dis-
tribute them among diﬀerent people or economic agents
(Samuelson (1998); Keynes (1973)). It is a discipline
related to almost everything around us, starting from
the marketplace through the environment to the fate of
nations. At ﬁrst sight this may seem a very diﬀerent
situation from that of physics, whose birth as a well de-
ﬁned scientiﬁctheoryisusuallyassociatedwith the study
of particular mechanical objects moving with negligible
friction, such as falling bodies and planets. However, a
deeper comparison shows many more analogies than dif-
ferences. On a general level, both economics and physics
deal with “everything around us”, despite with diﬀer-
ent perspectives. On a practical level, the goals of both
disciplines can be either purely theoretical in nature or
strongly oriented toward the improvement of the quality
of life. On a more technical side, analogies often become
equivalences. Let us give here some examples.
Statistical mechanics has been deﬁned as the
“branch of physics that combines the prin-
ciples and procedures of statistics with the
laws of both classical and quantum mechan-
ics, particularly with respect to the ﬁeld of
thermodynamics. It aims to predict and ex-
plain the measurable properties of macro-
scopic systems on the basis of the properties
andbehaviourofthemicroscopicconstituents
of those systems.”1
The tools of statistical mechanics or statistical physics
(Reif (1985); Pathria (1996); Landau (1965)), that in-
clude extracting the average properties of a macroscopic
systemfromthemicroscopicdynamicsofthesystems,are
believed to prove useful for an economic system. Indeed,
even though it is diﬃcult or almost impossible to write
down the “microscopic equations of motion” for an eco-
nomic system with all the interacting entities, economic
systemsmaybeinvestigatedatvarioussizescales. There-
fore, the understanding of the global behaviour of eco-
1In Encyclopædia Britannica. Retrieved June 11, 2010, from E n-
cyclopædia Britannica Online.nomic systems seems to need concepts such as stochas-
tic dynamics, correlation eﬀects, self-organization, self-
similarity and scaling, and for their application we do
not have to go into the detailed “microscopic” descrip-
tion of the economic system.
Chaos theory has had some impact in Economics mod-
elling, e.g. in the work by Brock and Hommes (1998) or
Chiarella et al.(2006). The theory of disordered systems
has also played a core role in Econophysics and study of
“complex systems”. The term “complex systems” was
coined to cover the great variety of such systems which
include examples from physics, chemistry, biology and
also social sciences. The concepts and methods of sta-
tistical physics turned out to be extremely useful in ap-
plication to these diverse complex systems including eco-
nomic systems. Many complex systems in natural and
social environments share the characteristics of compe-
tition among interacting agents for resources and their
adaptation to dynamically changing environment (Parisi
(1999); Arthur(1999)). Hence, theconceptofdisordered
systems helps for instance to go beyond the concept of
representative agent, an approach prevailing in much of
(macro)economicsandcriticizedbymanyeconomists(see
e.g. Kirman (1992); Gallegati and Kirman (1999)). Mi-
nority games and their physical formulations have been
exemplary.
Physics models have also helped bringing new theo-
ries explaining older observations in Economics. The
Italian social economist Pareto investigated a century
ago the wealth of individuals in a stable economy
(Pareto (1897a)) by modelling them with the distribu-
tionP(>x)∼x−α, whereP(>x) is the number of peo-
ple having income greater than or equal to xandαis
an exponent (known now as the Pareto exponent) which
he estimated to be 1 .5. To explain such empirical ﬁnd-
ings, physicists have come up with some very elegant
and intriguing kinetic exchange models in recent times,
and we will review these developments in the compan-
ion article. Though the economic activities of the agents
are driven by various considerations like “utility maxi-
mization”, the eventual exchanges of money in any trade
can be simply viewed as money/wealth conserving two-
body scatterings, as in the entropy maximization based
kinetic theory of gases. This qualitative analogy seems
to be quite old and both economists and natural scien-
tistshavealreadynoteditinvariouscontexts(Saha et al.
(1950)). Recently, an equivalence between the two maxi-
mization principles have been quantitatively established
(Chakrabarti and Chakrabarti (2010)).
Let us discuss another example of the similarities ofin-
terests and tools in Physicsand Economics. The friction-
less systems which mark the early history of physics were
soon recognized to be rare cases: not only at microscopic
scale – where they obviously represent an exception due
to the unavoidable interactions with the environment –
but also at the macroscopic scale, where ﬂuctuations of
internal or external origin make a prediction of their
time evolution impossible. Thus equilibrium and non-",2009-09-10T15:25:45Z,olitau chara bart iquant it economics nance tse bridgi psics economics economics samuelsoeyit at ooo statistical it t re  path ria land ined tre iencybritannica rieved june britannica online chaos economics brock hochi are lla t eco no psics t t many parish arthur nce kir magable gat kir mami psics economics t italiaparty party party to though  sha recently chara bart chara bart  psics and economics t 
paper_qf_47.pdf,3,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","3
equilibrium statistical mechanics, the theory of stochas-
tic processes, and the theory of chaos, became main tools
for studying real systems as well as an important part of
the theoretical framework of modern physics. Very inter-
estingly, the same mathematical tools have presided at
the growth of classic modelling in Economics and more
particularly in modern Finance. Following the works of
Mandelbrot, Fama of the 1960s, physicists from 1990 on-
wards have studied the ﬂuctuation of prices and univer-
salities in context of scaling theories, etc. These links
open the way for the use of a physics approach in Fi-
nance, complementary to the widespread mathematical
one.
B. Econophysics and Finance
Mathematical ﬁnance has beneﬁted a lot in the past
thirty years from modern probability theory – Brownian
motion, martingale theory, etc. Financial mathemati-
ciansareoftenproudtorecallthemostwell-knownsource
of the interactions between Mathematics and Finance:
ﬁve years before Einstein’s seminal work, the theory of
the Brownian motion was ﬁrst formulated by the French
mathematician Bachelier in his doctoral thesis (Bachelier
(1900); Boness (1967); Haberman and Sibbett (1995)),
in which he used this model to describe price ﬂuctuations
at the Paris Bourse. Bachelier had even given a course
as a “free professor” at the Sorbonne University with the
title: “Probability calculus with applications to ﬁnan-
cial operations and analogies with certain questions from
physics” (see the historical articles in Courtault et al.
(2000); Taqqu (2001); Forfar (2002)).
Then It¯ o, following the works of Bachelier, Wiener,
and Kolmogorov among many, formulated the presently
known It¯ o calculus (It¯ o and McKean (1996)). The ge-
ometric Brownian motion, belonging to the class of
It¯ o processes, later became an important ingredient
of models in Economics (Osborne (1959); Samuelson
(1965)), and in the well-known theory of option pric-
ing (Black and Scholes (1973); Merton (1973)). In
fact, stochastic calculus of diﬀusion processes combined
with classical hypotheses in Economics led to the devel-
opment of the arbitrage pricing theory (Duﬃe (1996),
Follmer and Schied (2004)). The deregulation of ﬁnan-
cial markets at the end of the 1980’s led to the expo-
nential growth of the ﬁnancial industry. Mathematical
ﬁnance followed the trend: stochastic ﬁnance with diﬀu-
sionprocessesandexponential growthofﬁnancialderiva-
tives have had intertwined developments. Finally, this
relationship was carved in stone when the Nobel prize
was given to M.S. Scholes and R.C. Merton in 1997 (F.
Blackdied in 1995)for their contribution to the theoryof
option pricing and their celebrated “Black-Scholes” for-
mula.
However, this whole theory is closely linked to clas-
sical economics hypotheses and has not been grounded
enough with empirical studies of ﬁnancial time series.The Black-Scholes hypothesis of Gaussian log-returns of
prices is in strong disagreement with empirical evidence.
Mandelbrot (1960, 1963) was one of the ﬁrsts to observe
a clear departure from Gaussian behaviour for these ﬂuc-
tuations. It is true that within the framework of stochas-
tic ﬁnance and martingale modelling, more complex pro-
cesses have been considered in order to take into ac-
count some empirical observations: jump processes (see
e.g. Cont and Tankov (2004) for a textbook treatment)
and stochastic volatility (e.g. Heston (1993); Gatheral
(2006)) in particular. But recent events on ﬁnancial
markets and the succession of ﬁnancial crashes (see e.g.
Kindleberger and Aliber (2005) for a historical perspec-
tive) should lead scientists to re-think basic concepts of
modelling. This is where Econophysics is expected to
come to play. During the past decades, the ﬁnancial
landscape has been dramatically changing: deregulation
of markets, growing complexity of products. On a tech-
nical point of view, the ever rising speed and decreasing
costs of computational power and networks have lead to
the emergence of huge databases that record all trans-
actions and order book movements up to the millisec-
ond. The availability of these data should lead to mod-
els that are better empirically founded. Statistical facts
and empirical models will be reviewed in this article and
its companion paper. The recent turmoil on ﬁnancial
markets and the 2008 crash seem to plead for new mod-
els and approaches. The Econophysics community thus
has an important role to play in future ﬁnancial market
modelling, as suggested by contributions from Bouchaud
(2008), Lux and Westerhoﬀ (2009) or Farmer and Foley
(2009).
C. A growing interdisciplinary ﬁeld
The chronological development of Econophysics has
been well covered in the book of Roehner (2002). Here
it is worth mentioning a few landmarks. The ﬁrst ar-
ticle on analysis of ﬁnance data which appeared in a
physics journal was that of Mantegna (1991). The ﬁrst
conference in Econophysics was held in Budapest in 1997
and has been since followed by numerous schools, work-
shops and the regular series of meetings: APFA (Appli-
cation of Physics to Financial Analysis), WEHIA (Work-
shop on Economic Heterogeneous Interacting Agents),
and Econophys-Kolkata, amongst others. In the recent
years the number of papers has increased dramatically;
the community has grown rapidly and several new direc-
tions of research have opened. By now renowned physics
journals like the Reviews of Modern Physics, Physical
Review Letters, Physical Review E, Physica A, Euro-
physics Letters, European Physical Journal B, Interna-
tional Journal of Modern Physics C, etc. publish papers
in this interdisciplinary area. Economics and mathemat-
ical ﬁnance journals, especially Quantitative Finance, re-
ceive contributions from many physicists. The interested
reader can also follow the developments quite well from",2009-09-10T15:25:45Z,very economics nance followi manlbrot fam tse  eco no psics nance matmatical brownish nancial matmatics nance  brownish frencac licac libonhaber masib be  paris course cac lisorbonne  probabity court all aq qu format tit cac liwinner lmogorov it it mc ke at brownish it economics osborne samuelso schools mortoieconomics du oll mer sc t matmatical nally nobel schools morto died  schools t  schools manlbrot it cont tank ov stogatr al but dly berger liner  eco no psics  ot statistical t t eco no psics  chalux sterho farmer foley t eco no psics roe ner re t antenna t eco no psics budapest app li psics nancial analysis work economic terogeneous inti  ec oys lkata iby reviews morpsics psical review ters psical review psics euro ters apsical journal interjournal morpsics economics quantitative nance t
paper_qf_47.pdf,4,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","4
the preprint server (www.arxiv.org). In fact, recently a
new section called quantitative ﬁnance has been added
to it. One could also visit the web sites of the Econo-
physics Forum (www.unifr.ch/econophysics) and Econo-
physics.Org (www.econophysics.org). The ﬁrst textbook
in Econophysics (Sinha et al.(2010)) is also in press.
D. Organization of the review
Thisarticleaimsatreviewingrecentempiricalandthe-
oretical developments that use tools from Physics in the
ﬁelds of Economics and Finance. In section II of this
paper, empirical studies revealing statistical properties
of ﬁnancial time series are reviewed. We present the
widely acknowledged “stylized facts” describing the dis-
tribution of the returns of ﬁnancial assets. In section III
we continue with the statistical properties observed on
order books in ﬁnancial markets. We reproduce most of
the stated facts using our own high-frequency ﬁnancial
database. In the last part of this article (section IV),
we review contributions on correlation on ﬁnancial mar-
kets, among which the computation of correlations using
high-frequency data, analyses based on random matrix
theory and the use of correlations to build economics
taxonomies. In the companion paper to follow, Econo-
physics models are reviewed through the point of view
of agent-based modelling. Using previous work origi-
nally presented in the ﬁelds of behavioural ﬁnance and
market microstructure theory, econophysicists have de-
veloped agent-based models of order-driven markets that
areextensivelyreviewedthere. Wethenturntomodelsof
wealth distribution where an agent-based approach also
prevails. As mentioned above, Econophysics models help
bringinganewlookonsomeEconomicsobservations,and
advances based on kinetic theory models are presented.
Finally, a detailed review of game theory models and the
now classic minority games composes the ﬁnal part.
II. STATISTICS OF FINANCIAL TIME SERIES: PRICE,
RETURNS, VOLUMES, VOLATILITY
Recording a sequence of prices of commodities or as-
sets produce what is called time series. Analysis of ﬁ-
nancial time series has been of great interest not only
to the practitioners (an empirical discipline) but also to
the theoreticians for making inferences and predictions.
The inherent uncertainty in the ﬁnancial time series and
its theory makes it specially interesting to economists,
statisticians and physicists (Tsay (2005)).
Diﬀerent kinds of ﬁnancial time series have been
recorded and studied for decades, but the scale changed
twenty years ago. The computerization of stock ex-
changes that took place all over the world in the mid
1980’s and early 1990’s has lead to the explosion of the
amount of data recorded. Nowadays, all transactions on
a ﬁnancial market are recorded tick-by-tick , i.e. everyevent on a stock is recorded with a timestamp deﬁned up
to the millisecond, leading to huge amounts of data. For
example, as of today (2010), the Reuters Datascope Tick
History (RDTH) database records roughly 25 gigabytes
of dataevery trading day .
Prior to this improvement in recording market activ-
ity, statistics could be computed with daily data at best.
Now scientists can compute intraday statistics in high-
frequency. This allows to check known properties at new
time scales (see e.g. section IIB below), but also implies
special care in the treatment (see e.g. the computation
of correlation on high-frequency in section IVA below).
It is a formidable task to make an exhaustive review
on this topic but we try to give a ﬂavour of some of the
aspects in this section.
A. “Stylized facts” of ﬁnancial time series
The concept of “stylized facts” was introduced in
macroeconomics around 1960 by Kaldor (1961), who ad-
vocated that a scientist studying a phenomenon “should
be free to start oﬀ with a stylized view of the facts”. In
his work, Kaldor isolated several statistical facts char-
acterizing macroeconomic growth over long periods and
in several countries, and took these robust patterns as a
starting point for theoretical modelling.
This expression has thus been adopted to describe em-
pirical facts that arose in statistical studies of ﬁnancial
time series and that seem to be persistent across various
time periods, places, markets, assets, etc. One can ﬁnd
many diﬀerent lists of these facts in several reviews (e.g.
Bollerslev et al.(1994); Pagan (1996); Guillaume et al.
(1997); Cont(2001)). Wechooseinthisarticletopresent
aminimum setoffactsnowwidelyacknowledged,atleast
for the prices of equities.
1. Fat-tailed empirical distribution of returns
Letptbe the price of a ﬁnancial asset at time t. We
deﬁne its return over a period of time τto be:
rτ(t) =p(t+τ)−p(t)
p(t)≈log(p(t+τ))−log(p(t)) (1)
It has been largely observed – starting with Mandelbrot
(1963), see e.g. Gopikrishnan et al.(1999) for tests on
more recent data – and it is the ﬁrst stylized fact, that
the empirical distributions of ﬁnancial returns and log-
returns are fat-tailed. On ﬁgure 1 we reproduce the em-
pirical density function of normalized log-returns from
Gopikrishnan et al.(1999) computed on the S&P500 in-
dex. In addition, we plot similar distributions for unnor-
malized returns on a liquid French stock (BNP Paribas)
withτ= 5 minutes. This graphis computed by sampling
asetoftick-by-tickdatafrom9:05amtill5:20pmbetween
January 1st, 2007 and May 30th, 2008, i.e. 356 days of",2009-09-10T15:25:45Z,ione eco no forum eco no org t eco no psics sinh organizat artie articial intellence ms at reviewi recent emical and t psics economics nance i i iieco no usi  tturto mols of as eco no psics economics observatns nally recordi analysis t say di t adays for reuters data pe tick history prr   it stylized t kal dor ikal dor  one roller lev pagagulaume cont  choose i artie to present fat  pt be  it manlbrot go pi krishna ogo pi krishna ifrenpa ribs  uary may
paper_qf_47.pdf,5,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","5
10-410-310-210-1
-1.5 -1-0.5  0 0.5  1 1.5Probability density function
Log-returnsBNPP.PA
Gaussian
Student
FIG. 1. (Top) Empirical probability density function of the
normalized 1-minute S&P500 returns between 1984 and 1996.
Reproduced from Gopikrishnan et al.(1999). (Bottom) Em-
pirical probability density function of BNP Paribas unnor-
malized log-returns over a period of time τ= 5 minutes.
trading. Except where mentioned otherwise in captions,
this data set will be used for all empirical graphs in this
section. On ﬁgure 2, cumulative distribution in log-log
scale from Gopikrishnan et al.(1999) is reproduced. We
also show the same distribution in linear-log scale com-
puted on our data for a larger time scale τ= 1 day,
showing similar behaviour.
Many studies obtain similar observations on diﬀerent
sets of data. For example, using two years of data on
more than a thousand US stocks, Gopikrishnan et al.
(1998) ﬁnds that the cumulative distribution of returns
asymptotically follow a power law F(rτ)∼ |r|−αwith
α >2 (α≈2.8−3). Withα >2, the second mo-
ment (the variance) is well-deﬁned, excluding stable laws
with inﬁnite variance. There has been various sugges-
tions for the form of the distribution: Student’s-t, hyper-
bolic, normal inverse Gaussian, exponentially truncated
stable, and others, but no general consensus exists on
the exact form of the tails. Although being the most
widely acknowledged and the most elementary one, this
stylized fact is not easily met by all ﬁnancial modelling.
Gabaixet al.(2006) or Wyart and Bouchaud (2007) re-
10-310-210-1100
 0 0.02  0.04  0.06  0.08  0.1Cumulative distribution
Absolute log-returnsSP500
Gaussian
Student
FIG. 2. Empirical cumulative distributions of S&P 500 daily
returns. (Top) Reproduced from Gopikrishnan et al.(1999),
in log-log scale. (Bottom) Computed using oﬃcial daily clos e
price between January 1st, 1950 and June 15th, 2009, i.e.
14956 values, in linear-log scale.
call that eﬃcient market theory have diﬃculties in ex-
plaining fat tails. Lux and Sornette (2002) have shown
that models known as “rational expectation bubbles”,
popular in economics, produced very fat-tailed distribu-
tions (α<1) that were in disagreement with the statis-
tical evidence.
2. Absence of autocorrelations of returns
On ﬁgure 3, we plot the autocorrelation of log-returns
deﬁned as ρ(T)∼ ∝an}bracketle{trτ(t+T)rτ(t)∝an}bracketri}htwithτ=1 minute
and 5 minutes. We observe here, as it is widely known
(see e.g. Pagan (1996); Cont et al.(1997)), that there
is no evidence of correlation between successive returns,
which is the second “stylized-fact”. The autocorrelation
function decays very rapidly to zero, even for a few lags
of 1 minute.",2009-09-10T15:25:45Z,probabity log stunt temical reproduced go pi krishna boom em pa ribs except ogo pi krishna  many for go pi krishna with tre stunt although ga articial intellence et wy art  chacumulative absolute stunt emical treproduced go pi krishna boom uted uary june lux corvee absence o pagacont t
paper_qf_47.pdf,6,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","6
-0.4-0.2 0 0.2 0.4
 0 10 20 30 40 50 60 70 80 90Autocorrelation
LagBNPP.PA 1-minute return
BNPP.PA 5-minute return
FIG. 3. Autocorrelation function of BNPP.PA returns.
 0 0.2 0.4 0.6 0.8 1
 0 10 20 30 40 50 60 70 80 90Autocorrelation
LagBNPP.PA 1-minute return
BNPP.PA 5-minute return
FIG. 4. Autocorrelation function of BNPP.PA absolute re-
turns.
3. Volatility clustering
The third “stylized-fact” that we present here is of pri-
mary importance. Absence of correlation between re-
turns must no be mistaken for a property of indepen-
dence and identical distribution: price ﬂuctuations are
not identically distributed and the properties of the dis-
tribution change with time.
In particular, absolute returns or squared returns ex-
hibit a long-range slowly decaying auto correlation func-
tion. This phenomena is widely known as “volatility
clustering”, and was formulated by Mandelbrot (1963)
as “large changes tend to be followed by large changes –
of either sign – and small changes tend to be followed by
small changes”.
On ﬁgure 4, the autocorrelation function of absolute
returns is plotted for τ= 1 minute and 5 minutes. The
levelsofautocorrelationsat the ﬁrstlagsvarywildly with
the parameter τ. On our data, it is found to be maxi-10-310-210-1100
 0 1 2 3 4 5 6Empirical cumulative distribution
Normalized returnτ = 1 day
τ = 1 week
τ = 1 month
Gaussian
FIG. 5. Distribution of log-returns of S&P 500 daily, weekly
and monthly returns. Same data set as ﬁgure 2 bottom.
mum (more than 70% at the ﬁrst lag) for a returns sam-
pled every ﬁve minutes. However, whatever the sampling
frequency, autocorrelation is still above 10% after several
hours of trading. On this data, we can grosslyﬁt a power
law decay with exponent 0 .4. Other empirical tests re-
port exponents between 0 .1 and 0.3 (Contet al.(1997);
Liuet al.(1997); Cizeau et al.(1997)).
4. Aggregational normality
It has been observed that as one increases the time
scale over which the returns are calculated, the fat-tail
property becomes less pronounced, and their distribu-
tion approaches the Gaussian form, which is the fourth
“stylized-fact”. This cross-over phenomenon is docu-
mented in Kullmann et al.(1999) where the evolution
of the Pareto exponent of the distribution with the time
scale is studied. On ﬁgure 5, we plot these standardized
distributions for S&P 500 index between January 1st,
1950 and June 15th, 2009. It is clear that the larger the
time scale increases, the more Gaussian the distribution
is. The fact that the shape of the distribution changes
withτmakes it clear that the randomprocess underlying
prices must have non-trivial temporal structure.
B. Getting the right “time”
1. Four ways to measure “time”
In the previous section, all “stylized facts” have been
presented in physical time , orcalendar time , i.e. time
series were indexed, as we expect them to be, in hours,
minutes, seconds, milliseconds. Let us recall here that
tick-by-tick data available on ﬁnancial markets all over
the world is time-stamped up to the millisecond, but the",2009-09-10T15:25:45Z,auto correlatlag auto correlatauto correlatlag auto correlatvolatity t absence i manlbrot ot oemical normalized distributsame ootr conte  et size au aregatal it  ullmanparty ouary june it t gei four i
paper_qf_47.pdf,7,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","7
order of magnitude of the guaranteed precision is much
larger, usually one second or a few hundreds of millisec-
onds.
Calendar time is the time usually used to compute sta-
tistical properties of ﬁnancial time series. This means
that computing these statistics involves sampling, which
might be a delicate thing to do when dealing for example
with several stocks with diﬀerent liquidity. Therefore,
three other ways to keep track of time may be used.
Let us ﬁrst introduce event time . Using this count,
time is increased by one unit each time one order is sub-
mitted to the observed market. This framework is nat-
ural when dealing with the simulation of ﬁnancial mar-
kets, as it will be showed in the companion paper. The
main outcome of event time is its “smoothing” of data.
In event time, intraday seasonality (lunch break) or out-
burst of activity consequent to some news are smoothed
in the time series, since we always have one event per
time unit.
Now, when dealing with time series of prices, another
count of time might be relevant, and we call it trade time
ortransaction time . Using this count, time is increased
by one unit eachtime a transactionhappens. The advan-
tage of this count is that limit orders submitted far away
in the order book, and may thus be of lesser importance
with respect to the price series, do not increase the clock
by one unit.
Finally, going on with focusing on important events to
increasethe clock, wecanuse tick time . Using this count,
time is increased by one unit each time the price changes.
Thus consecutive market orders that progressively “eat”
liquidity until the ﬁrst best limit is removed in an order
book are counted as one unit time.
Let us ﬁnish by noting that with these deﬁnitions,
when dealing with mid prices, or bid and ask prices, a
time series in event time can easily be extracted from a
time series in calendar time. Furthermore, one can al-
ways extract a time series in trade time or in price time
from a time series in event time. However, one cannot
extract a series in price time from a series in trade time,
as the latter ignores limit orders that are submitted in-
side the spread, and thus change mid, bid or ask prices
without any transaction taking place.
2. Revisiting “stylized facts” with a new clock
Now, using the right clock might be of primary impor-
tance when dealing with statistical properties and esti-
mators. For example, Griﬃn and Oomen (2008) investi-
gates the standard realized variance estimator (see sec-
tion IVA) in trade time and tick time. Muni Toke(2010)
also recalls that the diﬀerences observed on a spread dis-
tribution in trade time and physical time are meaning-
ful. In this section we compute some statistics comple-
mentary to the ones we have presented in the previous
section IIA and show the role of the clock in the studied
properties.10-510-410-310-210-1
-4 -2  0  2  4Empirical density function
Normalized returnτ = 17 trades
τ = 1 minute
τ = 1049 trades
τ = 1 hour
Gaussian
FIG. 6. Distribution of log-returns of stock BNPP.PA. This
empirical distribution is computed using data from 2007,
April 1st until 2008, May 31st.
a. Aggregational normality in trade time We have
seenabovethatwhen the samplingsizeincreases, the dis-
tribution of the log-returns tends to be more Gaussian.
This property is much better seen using trade time. On
ﬁgure 6, we plot the distributions of the log-returns for
BNP Paribas stock using 2-month-long data in calendar
time and trade time. Over this period, the average num-
ber of trade per dayis 8562, so that 17 trades (resp. 1049
trades) corresponds to an average calendar time step of
1 minute (resp. 1 hour). We observe that the distribu-
tion of returns sampled every 1049 trades is much more
Gaussian than the one sampled every 17 trades (aggre-
gational normality), and that it is also more Gaussian
that the one sampled every 1 hour (quicker convergence
in trade time).
Note that this property appears to be valid in a mul-
tidimensional setting, see Huth and Abergel (2009).
b. Autocorrelation of trade signs in tick time It is
well-known that the series of the signs of the trades on
a given stock (usual convention: +1 for a transaction
at the ask price, −1 for a transaction at the bid price)
exhibit large autocorrelation. It has been observed in
Lillo and Farmer (2004) for example that the autocorre-
lation function of the signs of trades ( ǫn) was a slowly
decaying function in n−α, withα≈0.5. We compute
this statistics for the trades on BNP Paribas stock from
2007, January 1st until 2008, May 31st. We plot the re-
sult in ﬁgure 7. We ﬁnd that the ﬁrst values for short
lags areabout 0 .3, and that the log-logplot clearlyshows
some power-law decay with roughly α≈0.7.
A very plausible explanation of this phenomenon re-
lies on the execution strategies of some major brokers on
a given markets. These brokers have large transaction
to execute on the account of some clients. In order to
avoid market making move because of an inconsiderably
large order (see below section IIIF on market impact),
they tend to split large orders into small ones. We think",2009-09-10T15:25:45Z,calendar  trefore  usi  t i usi t nally usi   furtr revisiti  for gri oo memuni take iemical normalized distribut apr may aregatal   opa ribs o note hu th aber gel auto correlatit it lle farmer  pa ribs uary may   tse i
paper_qf_47.pdf,8,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","8
10-210-1100
 1  10  100  1000Autocorrelation
LagBNPP.PA trade time
BNPP.PA tick time
FIG. 7. Auto-correlation of trade signs for stock BNPP.PA.
that these strategies explain, at least partly, the large
autocorrelation observed. Using data on markets where
orders are publicly identiﬁed and linked to a given bro-
ker, it can be shown that the autocorrelation function
of the order signs of a given broker , is even higher. See
Bouchaud et al.(2009) for a review of these facts and
some associated theories.
We present here another evidence supporting this ex-
planation. We compute the autocorrelation function of
order signs in tick time , i.e. taking only into account
transactions that make the price change. Results are
plotted on ﬁgure 7. We ﬁnd that the ﬁrst values for short
lags are about 0 .10, which is much smaller than the val-
uesobservedwith the previoustime series. This supports
the ideathat manysmalltransactionsprogressively“eat”
the available liquidity at the best quotes. Note however
that even in tick time, the correlation remains positive
for large lags also.
3. Correlation between volume and volatility
Investigating time series of cotton prices, Clark (1973)
noted that “trading volume and price change vari-
ance seem to have a curvilinear relationship”. Trade
timeallows us to have a better view on this property:
Plerouet al.(2000) and Silva and Yakovenko (2007)
among others, show that the variance of log-returns after
Ntrades, i.e. over a time period of Nin trade time, is
proprtional to N. We conﬁrm this observation by plot-
ting the second moment of the distribution of log-returns
afterNtrades as a function of Nfor our data, as well as
the average number of trades and the average volatility
on a given time interval. The results are shown on ﬁgure
8 and 9.
This results are to be put in relation to the one pre-
sented in Gopikrishnan et al.(2000b), where the statis-
tical properties of the number of shares traded Q∆tfor a10010-510-410-410-4
 0 1000  2000  3000  4000  5000Number of trades
VarianceBNPP.PA
Linear fit
FIG. 8. Second moment of the distribution of returns over N
trades for the stock BNPP.PA.
10-410-310-210-1100
 10  100  1000  10000Number of trades
Time period τAverage variance
Average number of trades
FIG. 9. Average number of trades and average volatility on
a time period τfor the stock BNPP.PA.
given stock in a ﬁxed time interval ∆ tis studied. They
analyzed transaction data for the largest 1000 stocks
for the two-year period 1994-95, using a database that
recorded every transaction for all securities in three ma-
jor US stock markets. They found that the distribution
P(Q∆t) displayed a power-law decay as shown in Fig.
10, and that the time correlations in Q∆tdisplayed long-
range persistence. Further, they investigated the rela-
tion between Q∆tand the number of transactions N∆t
in a time interval ∆ t, and found that the long-range
correlations in Q∆twere largely due to those of N∆t.
Their results are consistent with the interpretation that
thelargeequal-timecorrelationpreviouslyfoundbetween
Q∆tand the absolutevalue ofprice change |G∆t|(related
to volatility) were largely due to N∆t.
Therefore, studying variance of price changer in trade
timesuggests that the number of trade is a good proxy
for the unobserved volatility.",2009-09-10T15:25:45Z,auto correlatlag auto usi   cha  results   note correlatinvestati ark tra le route sva v e tras i tras for t  go pi krishna number variance linear second number time ge ge ge ty ty  furtr tir trefore
paper_qf_47.pdf,9,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","9
10−1100101102103104
Normalized Q∆t10−910−810−710−610−510−410−310−210−1100Probability density  P(Q∆t )
 I
 II
 III
 IV
 V
 VI
1+λ = 2.7(c) 1000 stocks
FIG. 10. Distribution of the number of shares traded Q∆t.
Adapted from Gopikrishnan et al.(2000b).
4. A link with stochastic processes: subordination
Theseempiricalfacts(aggregationalnormalityintrade
time, relationship between volume and volatility) rein-
force the interest for models based on the subordination
of stochastic processes, which had been introduced in ﬁ-
nancial modeling by Clark (1973).
Let us introduce it here. Assuming the proportionality
between the variance ∝an}bracketle{tx∝an}bracketri}ht2
τof the centred returns xand
the number of trades Nτover a time period τ, we can
write:
∝an}bracketle{tx∝an}bracketri}ht2
τ=αNτ. (2)
Therefore, assuming the normality in trade time, we can
write the density function of log-returns after Ntrades
as
fN(x) =e−x2
2αN√
2παN, (3)
Finally, denoting Kτ(N) the probability density function
of havingNtrades in a time period τ, the distribution
of log returns in calendar time can be written
Pτ(x) =/integraldisplay∞
0e−x2
2αN√
2παNKτ(N)dN. (4)
This is the subordination of the Gaussian process xN
using the number of trades Nτas thedirecting process ,
i.e. as the new clock. With this kind of modelization,
it is expected, since PNis gaussian, the observed non-
gaussian behavior will come from Kτ(N). For exam-
ple, some speciﬁc choice of directing processes may leadto a symmetric stable distribution (see Feller (1968)).
Clark (1973) tests empirically a log-normal subordina-
tion with time series of prices of cotton. In a similar
way, Silva and Yakovenko (2007) ﬁnd that an exponen-
tial subordination with a kernel:
Kτ(N) =1
ητe−N
ητ. (5)
is in good agreement with empirical data. If the orders
were submitted to the market in a independent way and
at a constant rate η, then the distribution of the number
of trade per time period τshould be a Poisson process
with intensity ητ. Therefore, the empirical ﬁt of equa-
tion (5) is inconsistent with such a simplistic hypothesis
of distribution of time of arrivals of orders. We will sug-
gest in the next section some possible distributions that
ﬁt our empirical data.
III. STATISTICS OF ORDER BOOKS
The computerization of ﬁnancial markets in the sec-
ond half of the 1980’s provided the empirical scientists
with easier access to extensive data on order books.
Biaiset al.(1995) is an early study of the new data
ﬂows on the newly (at that time) computerized Paris
Bourse. Variables crucial to a ﬁne modeling of order
ﬂows and dynamics of order books are studied: time
of arrival of orders, placement of orders, size of orders,
shape of order book, etc. Many subsequent papers of-
fer complementary empirical ﬁndings and modeling, e.g.
Gopikrishnan et al.(2000a), Challet and Stinchcombe
(2001), Maslov and Mills(2001), Bouchaud et al.(2002),
Potters and Bouchaud (2003). Before going further in
ourreviewofavailablemodels, wetryto summarizesome
of these empirical facts.
For each of the enumerated properties, we present new
empirical plots. We use Reuters tick-by-tick data on the
Paris Bourse. We select four stocks: France Telecom
(FTE.PA) , BNP Paribas (BNPP.PA), Societe G´ en´ erale
(SOGN.PA) and Renault (RENA.PA). For any given
stocks, the data displays time-stamps, traded quantities,
traded prices, the ﬁrst ﬁve best-bid limits and the ﬁrst
ﬁve best-ask limits. From now on, we will denote ai(t)
(resp. (bj(t)) the price of the i-th limit at ask (resp. j-
th limit at bid). Except when mentioned otherwise, all
statistics are computed using all trading days from Oct,
1st 2007 to May, 30th 2008, i.e. 168 trading days. On a
given day, orders submitted between 9:05am and 5:20pm
are taken into account, i.e. ﬁrst and last minutes of each
trading days are removed.
Note that we do not deal in this section with the cor-
relations of the signs of trades, since statistical results on
this fact have alreadybeen treated in section IIB2. Note
alsothatalthoughmostofthesefactsarewidelyacknowl-
edged, we will not describe them as new “stylized facts
for order books” since their ranges of validity are still
to be checked among various products/stocks, markets",2009-09-10T15:25:45Z,normalized probabity distributadapted go pi krishna tse emical fas ark  assumi trefore tras nally tras  with is for seller ark isva v e  poisotrefore  t bia is et paris course variablmany go pi krishna chal  stinchmas lov mls  chaters  chabefore for  reuters paris course  france telecom pa ribs societe renault for from except  may onote note
paper_qf_47.pdf,10,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","10
and epochs, and strong properties need to be properly
extracted and formalized from these observations. How-
ever, we will keep them in mind as we go through the
new trend of “empirical modeling” of order books.
Finally, let us recall that the markets we are dealing
with are electronic order books with no oﬃcial market
maker, in which orders are submitted in a double auc-
tion and executions follow price/time priority. This type
of exchange is now adopted nearly all over the world,
but this was not obvious as long as computerization was
not complete. Diﬀerent market mechanisms have been
widely studied in the microstructure literature, see e.g.
Garman (1976); Kyle (1985); Glosten (1994); O’Hara
(1997); Biais et al.(1997); Hasbrouck (2007). We will
not review this literature here (except Garman (1976) in
our companion paper), as this would be too large a di-
gression. However, such a literature is linked in many
aspects to the problems reviewed in this paper.
A. Time of arrivals of orders
As explained in the previous section, the choice of the
time count might be of prime importance when dealing
with “stylized facts” of empirical ﬁnancial time series.
Whenreviewingthesubordinationofstochasticprocesses
(Clark (1973); Silva and Yakovenko (2007)), we have
seen that the Poisson hypothesis for the arrival times
of orders is not empirically veriﬁed.
We compute the empirical distribution for interarrival
times – or durations– ofmarket orderson the stock BNP
Paribas using our data set described in the previous sec-
tion. The results are plotted in ﬁgures 11 and 12, both in
linear and log scale. It is clearly observed that the expo-
nential ﬁt is not a good one. We check however that the
Weibull distribution ﬁt is potentially a very good one.
Weibull distributions have been suggested for example in
Ivanovet al.(2004). Politi and Scalas (2008) also obtain
good ﬁts with q-exponential distributions.
In the Econometrics literature, these observations
of non-Poissonian arrival times have given rise to a
large trend of modelling of irregular ﬁnancial data.
Engle and Russell (1997) and Engle (2000) have in-
troduced autoregressive condition duration or intensity
models that may help modelling these processes of or-
ders’ submission. See Hautsch (2004) for a textbook
treatment.
Using the same data, we compute the empirical dis-
tribution of the number of transactions in a given time
periodτ. Results are plotted in ﬁgure 13. It seems that
the log-normal and the gamma distributions are both
good candidates, however none of them really describes
the empirical result, suggesting a complex structure of
arrival of orders. A similar result on Russian stocks was
presented in Dremin and Leonidov (2005).10-610-510-410-310-210-1100
 1  10  100Empirical density
Interarrival timeBNPP.PA
Lognormal
Exponential
Weibull
FIG. 11. Distribution ofinterarrival times for stock BNPP. PA
in log-scale.
 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
 0 2 4 6 8 10 12 14Empirical density
Interarrival timeBNPP.PA
Lognormal
Exponential
Weibull
FIG. 12. Distribution ofinterarrival times for stock BNPP. PA
(Main body, linear scale).
B. Volume of orders
Empirical studies show that the unconditional dis-
tribution of order size is very complex to character-
ize. Gopikrishnan et al.(2000a) and Maslov and Mills
(2001) observe a power law decay with an exponent
1+µ≈2.3−2.7 for market orders and 1+ µ≈2.0 for
limit orders. Challet and Stinchcombe (2001) empha-
size on a clustering property: orders tend to have a
“round” size in packages of shares, and clusters are ob-
served around 100’s and 1000’s. As of today, no consen-
sus emerges in proposed models, and it is plausible that
such a distribution varies very wildly with products and
markets.
In ﬁgure 14, we plot the distribution of volume of mar-
ket orders for the four stocks composing our benchmark.
Quantities are normalized by their mean. Power-law co-",2009-09-10T15:25:45Z,how nally  di germakyle gl oftehara bia is harbor uk  germatime as wreviewi t subordinatof stochastic processark sva v e poiso pa ribs t it  i bull i bull ivaet polite scale ieconometrics poisoiaeagle russell eagle  hats usi results it dr milogid ov emical inter arrival log normal eonential i bull distributemical inter arrival log normal eonential i bull distributarticial intellence volume emical go pi krishna mas lov mls chal  stinchas iquantiti
paper_qf_47.pdf,11,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","11
 0 0.01 0.02 0.03 0.04 0.05 0.06
 0 1 2 3 4 5 6Empirical density
Normalized number of tradesτ = 1 minute
τ = 5 minutes
τ = 15 minutes
τ = 30 minutes
FIG. 13. Distribution of the number of trades in a given time
periodτfor stock BNPP.PA. This empirical distribution is
computed using data from 2007, October 1st until 2008, May
31st.
eﬃcient is estimated by a Hill estimator (see e.g. Hill
(1975); de Haan et al.(2000)). Weﬁndapowerlawwith
exponent 1+ µ≈2.7 which conﬁrms studies previously
cited. Figure 15 displays the same distribution for limit
orders (of all available limits). We ﬁnd an average value
of 1+µ≈2.1, consistent with previous studies. How-
ever, we note that the power law is a poorer ﬁt in the
case of limit orders: data normalized by their mean col-
lapse badly on a single curve, and computed coeﬃcients
vary with stocks.
10-610-510-410-310-210-1100
 0.01  0.1  1 10 100 1000 10000Probability functions
Normalized volume for market ordersPower law ∝ x-2.7
Exponential ∝ e-x
BNPP.PA
FTE.PA
RENA.PA
SOGN.PA
FIG. 14. Distribution ofvolumesofmarket orders. Quantiti es
are normalized by their mean.
C. Placement of orders
a. Placement of arriving limit orders10-710-610-510-410-310-210-1100
 0.01  0.1  1 10 100 1000 10000Probability functions
Normalized volume of limit ordersPower law ∝ x-2.1
Exponential ∝ e-x
BNPP.PA
FTE.PA
RENA.PA
SOGN.PA
FIG. 15. Distribution of normalized volumes of limit orders .
Quantities are normalized by their mean.
Bouchaud et al.(2002) observe a broad power-law
placement around the best quotes on French stocks,
conﬁrmed in Potters and Bouchaud (2003) on US
stocks. Observed exponents are quite stable across
stocks, but exchange dependent: 1+ µ≈1.6 on the
Paris Bourse, 1+ µ≈2.0 on the New York Stock
Exchange, 1+ µ≈2.5 on the London Stock Exchange.
Mike and Farmer (2008) propose to ﬁt the empirical
distribution with a Student distribution with 1 .3 degree
of freedom.
We plot the distribution of the following quantity
computed on our data set, i.e. using only the ﬁrst
ﬁve limits of the order book: ∆ p=b0(t−)−b(t) (resp.
a(t)−a0(t−)) if an bid (resp. ask) order arrives at price
b(t) (resp.a(t)), whereb0(t−) (resp.a0(t−)) is the best
bid (resp. ask) before the arrival of this order. Results
are plotted on ﬁgures 16 (in semilog scale) and 17 (in
linear scale). These graphs being computed with in-
complete data (ﬁve best limits), we do not observe a
placement as broad as in Bouchaud et al.(2002). How-
ever, our data makes it clear that fat tails are observed.
We also observe an asymmetry in the empirical distribu-
tion: the left side is less broad than the right side. Since
the left side represent limit orders submitted insidethe
spread, this is expected. Thus, the empirical distribution
of the placement of arriving limit orders is maximum at
zero (same best quote). We then ask the question: How
is it translated in terms of shape of the order book ?
b. Average shape of the order book Contrarytowhat
one might expect, it seems that the maximum of the av-
erage oﬀered volume in an order book is located away
from the best quotes (see e.g. Bouchaud et al.(2002)).
Our data conﬁrms this observation: the averagequantity
oﬀered on the ﬁve best quotes grows with the level. This
result is presented in ﬁgure 18. We also compute the av-
eragepriceoftheselevelsinordertoplotacross-sectional
graph similar to the ones presented in Biais et al.(1995).",2009-09-10T15:25:45Z,emical normalized distribut ober may hl hl has   how probabity normalized  eonential distributquant it placement placement probabity normalized  eonential distributquantiti chafrenters  chaobserved paris course new york stock exe londostock exe mike farmer stunt  results tse  chahow  since   how ge contrary to what  chaour   bia is
paper_qf_47.pdf,12,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","12
10-610-510-410-310-210-1100
-0.6-0.4-0.2  0 0.2 0.4 0.6Probability density function
∆pBNPP.PA
Gaussian
Student
FIG. 16. Placement of limit orders using the same best quote
reference in semilog scale. Data used for this computation
is BNP Paribas order book from September 1st, 2007, until
May 31st, 2008.
 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4
-0.1 -0.05  0  0.05  0.1Probability density function
∆pBNPP.PA
Gaussian
FIG. 17. Placement of limit orders using the same best quote
reference in linear scale. Data used for this computation is
BNP Paribas order book from September 1st, 2007, until May
31st, 2008.
Our result is presented forstock BNP.PAin ﬁgure19 and
displays the expected shape. Results for other stocks are
similar. We ﬁnd that the average gap between two levels
is constant among the ﬁve best bids and asks (less than
one tick for FTE.PA, 1 .5 tick for BNPP.PA, 2 .0 ticks for
SOGN.PA, 2 .5 ticks for RENA.PA). We also ﬁnd that
the average spread is roughly twice as large the aver-
age gap (factor 1.5 for FTE.PA, 2 for BNPP.PA, 2.2 for
SOGN.PA, 2.4 for RENA.PA). 0.7 0.75 0.8 0.85 0.9 0.95 1 1.05 1.1 1.15
-6-4-2 0 2 4 6Normalized numbers of shares
Level of limit orders (<0:bids ; >0:asks)BNPP.PA
FTE.PA
RENA.PA
SOGN.PA
FIG. 18. Average quantity oﬀered in the limit order book.
 580000 600000 620000 640000 660000 680000 700000 720000
 67  67.05  67.1  67.15Numbers of shares
Price of limit ordersBNPP.PA
FIG. 19. Average limit order book: price and depth.
D. Cancelation of orders
Challet and Stinchcombe (2001) show that the dis-
tribution of the average lifetime of limit orders ﬁts
a power law with exponent 1+ µ≈2.1 for cancelled
limit orders, and 1+ µ≈1.5 for executed limit orders.
Mike and Farmer (2008) ﬁnd that in either case the ex-
ponential hypothesis (Poisson process) is not satisﬁed on
the market.
We compute the average lifetime of cancelled and exe-
cuted orders on our dataset. Since our data does not in-
clude a unique identiﬁer of a given order, we reconstruct
life time orders as follows: each time a cancellation is
detected, we go back through the history of limit order
submissionandlookforamatchingorderwithsameprice
andsamequantity. Ifanorderisnotmatched, wediscard
the cancellation from our lifetime data. Results are pre-
sented in ﬁgure 20 and 21. We observea powerlaw decay
with coeﬃcients 1+ µ≈1.3−1.6 for both cancelled and",2009-09-10T15:25:45Z,probabity stunt placement data pa ribs september may probabity placement data pa ribs september may our iresults   normalized level ge numbers price ge cancellatchal  stinchmike farmer poiso since  aorr is not matcd results 
paper_qf_47.pdf,13,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","13
executedlimit orders,withlittle variationsamongstocks.
These results are a bit diﬀerent than the ones presented
in previous studies: similar for executed limit orders, but
our data exhibits a lower decay as for cancelled orders.
Note that the observed cut-oﬀ in the distribution for life-
times above 20000 seconds is due to the fact that we do
not take into account execution or cancellation of orders
submitted on a previous day.
10-710-610-510-410-310-210-1100
 1  20  400  8000Probability functions
Lifetime for cancelled limit ordersPower law ∝ x-1.4
BNPP.PA
FTE.PA
RENA.PA
SOGN.PA
FIG. 20. Distribution of estimated lifetime of cancelled li mit
orders.
10-710-610-510-410-310-210-1100
 1  20  400  8000Probability functions
Lifetime for executed limit ordersPower law ∝ x-1.5
BNPP.PA
FTE.PA
RENA.PA
SOGN.PA
FIG. 21. Distribution of estimated lifetime of executed lim it
orders.
E. Intraday seasonality
Activity on ﬁnancial markets is of course not constant
throughout the day. Figure 22 (resp. 23) plots the (nor-
malized) number of market (resp. limit) orders arriving
in a 5-minute interval. It is clear that a U-shape is ob-
served (an ordinary least-square quadratic ﬁt is plotted):the observed market activity is larger at the beginning
and the end of the day, and more quiet around mid-
day. SuchaU-shapedcurveiswell-known,seeBiais et al.
(1995), for example. On our data, we observe that the
number of orders on a 5-minute interval can vary with a
factor 10 throughout the day.
 0 0.5 1 1.5 2 2.5 3
 30000  35000  40000  45000  50000  55000  60000  65000Number of market orders submitted in ∆t=5 minutes
Time of day (seconds)BNPP.PA
BNPP.PA quadratic fit
FTE.PA
FTE.PA quadratic fit
FIG. 22. Normalized average number of market orders in a
5-minute interval.
 0 0.5 1 1.5 2 2.5 3 3.5
 30000  35000  40000  45000  50000  55000  60000  65000Number of limit orders submitted in ∆t=5 minutes
Time of day (seconds)BNPP.PA
BNPP.PA quadratic fit
FTE.PA
FTE.PA quadratic fit
FIG. 23. Normalized average number of limit orders in a 5-
minute interval.
Challet and Stinchcombe (2001) note that the average
number of orders submitted to the market in a period
∆Tvary wildly during the day. The authors also observe
that these quantities for market orders and limit orders
are highly correlated. Such a type of intraday variation
ofthe globalmarketactivity isa well-knownfact, already
observed in Biais et al.(1995), for example.",2009-09-10T15:25:45Z,tse note probabity ime  distributprobabity ime  distributintra day aivity  it subia is onumber time normalized number time normalized chal  stinchvary t subia is
paper_qf_47.pdf,14,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","14
F. Market impact
The statistics we have presented may help to under-
stand a phenomenon of primary importance for any ﬁ-
nancial market practitioner: the market impact, i.e. the
relationshipbetween the volumetraded and the expected
price shift once the order has been executed. On a ﬁrst
approximation, one understands that it is closely linked
with many items described above: the volume of mar-
ket orders submitted, the shape of the order book (how
much pending limit orders are hit by one large market
orders), the correlation of trade signs (one may assume
that large orders are splitted in order to avoid a large
market impact), etc.
Many empirical studies are available. An empirical
study on the price impact of individual transactions on
1000 stocks on the NYSE is conducted in Lillo et al.
(2003). It is found that proper rescaling make all the
curve collapse onto a single concave master curve. This
function increases as a power that is the order of 1 /2 for
small volumes, but then increases more slowly for large
volumes. They obtain similar results in each year for the
period 1995 to 1998.
We will not review any further the large literature of
market impact, but rather refer the reader to the recent
exhaustive synthesis proposed in Bouchaud et al.(2009),
where diﬀerent types of impacts, as well as some theoret-
ical models are discussed.
IV. CORRELATIONS OF ASSETS
The word “correlation” is deﬁned as “a relation exist-
ing between phenomena or things or between mathemat-
ical or statistical variables which tend to vary, be associ-
ated, oroccurtogetherin awaynotexpected onthebasis
of chance alone”2. When we talk about correlations in
stock prices, what we arereally interested in are relations
between variables such as stock prices, order signs, trans-
action volumes, etc. and more importantly how these
relations aﬀect the nature of the statistical distributions
and laws which govern the price time series. This sec-
tion deals with several topics concerning linear correla-
tion observed in ﬁnancial data. The ﬁrst part deals with
the important issue of computing correlations in high-
frequency. As mentioned earlier, the computerization of
ﬁnancial exchanges has lead to the availability of huge
amount of tick-by-tick data, and computing correlation
using these intraday data raises lots of issues concern-
ing usual estimators. The second and third parts deals
with the use of correlation in order to cluster assets with
potential applications in risk management problems.
2In Merriam-WebsterOnline Dictionary. Retrieved June 14, 2 010,
from http://www.merriam-webster.com/dictionary/corre lationsA. Estimating covariance on high-frequency data
Let us assume that we observe dtime series of
prices or log-prices pi,i= 1,...,d, observed at times
tm,m= 0,...,M. The usual estimator of the covari-
ance of prices iandjis therealized covariance estimator ,
which is computed as:
ˆΣRV
ij(t) =M/summationdisplay
m=1(pi(tm)−pi(tm−1))(pj(tm)−pj(tm−1)).
(6)
The problem is that high-frequency tick-by-tick data
record changes of prices when they happen, i.e. at ran-
dom times. Tick-by-tick data is thus asynchronous, con-
trary to daily close prices for example, that are recorded
at the same time for all the assets on a given exchange.
Using standard estimators without caution, could be
one cause for the “Epps eﬀect”, ﬁrst observed in Epps
(1979), which stated that “[c]orrelations among price
changes in common stocks of companies in one indus-
try are found to decrease with the length of the interval
for which the price changes are measured.” This has
largely been veriﬁed since, e.g. in Bonanno et al.(2001)
or Reno (2003). Hayashi and Yoshida (2005) shows that
non-synchronicityoftick-by-tickdataandnecessarysam-
pling of time seriesin orderto compute the usual realized
covariance estimator partially explain this phenomenon.
We very brieﬂy review here two covariance estimators
that do not need any synchronicity (hence, sampling) in
order to be computed.
1. The Fourier estimator
The Fourier estimator has been introduced by
Malliavin and Mancino (2002). Let us assume that we
havedtime series of log-prices that are observations of
Brownian semi-martingales pi:
dpi=K/summationdisplay
j=1σijdWj+µidt,i= 1,...,d. (7)
The coeﬃcient of the covariance matrix are then writ-
ten Σij(t) =/summationtextK
k=1σik(t)σjk(t). Malliavin and Mancino
(2002)showthattheFouriercoeﬃcientofΣ ij(t)are,with
n0a given integer:
ak(Σij) = lim
N→∞π
N+1−n0N/summationdisplay
s=n01
2[as(dpi)as+k(dpj)
+bs+k(dpi)bs(dpj)], (8)
bk(Σij) = lim
N→∞π
N+1−n0N/summationdisplay
s=n01
2[as(dpi)bs+k(dpj)
−bs(dpi)as+k(dpj)], (9)",2009-09-10T15:25:45Z,market t omany alle it  ty   chat w t as t imerriam bster online dinary rieved june estimati  t t tick usi apps apps  bonanno reno hayashi yeshiva  t courier t courier alla imac  brownish t alla imac courier coe
paper_qf_47.pdf,15,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","15
where the Fourier coeﬃcients ak(dpi) andbk(dpi) ofdpi
can be directly computed on the time series. Indeed,
rescalingthetimewindowon[0 ,2π]andusingintegration
by parts, we have:
ak(dpi) =p(2π)−p(0)
π−k
π/integraldisplay2π
0sin(kt)pi(t)dt.(10)
This last integral can be discretized and approximately
computed using the times ti
mof observations of the pro-
cesspi. Therefore, ﬁxing a suﬃciently large N, one
can compute an estimator ΣF
ijof the covariance of the
processesiandj. See Reno (2003) or Iori and Precup
(2007), for examples of empirical studies using this esti-
mator.
2. The Hayashi-Yoshida estimator
Hayashi and Yoshida (2005) have proposed a simple
estimator in order to compute covariance/correlation
without any need for synchronicity of time series. As
in the Fourier estimator, it is assumed that the observed
process is a Brownian semi-martingale. The time win-
dow of observation is easily partitioned into dfamily of
intervals Πi= (Ui
m),i= 1,...,d, whereti
m= inf{Ui
m+1}
is the time of the m-th observation of the process i. Let
us denote ∆ pi(Ui
m) =pi(ti
m)−pi(ti
m−1). Thecumula-
tive covariance estimator as the authors named it, or the
Hayashi-Yoshida estimator as it has been largely refered
to, is then built as follows:
ˆΣHY
ij(t) =/summationdisplay
m,n∆pi(Ui
m)∆pj(Uj
n)1{Uim∩Uj
n/ne}ationslash=∅}.(11)
There is a large literature in Econometrics that
tackles the new challenges posed by high-frequency
data. We refer the reader, wishing to go be-
yond this brief presentation, to the econometrics re-
views by Barndorﬀ-Nielsen and Shephard (2007) or
McAleer and Medeiros (2008), for example.
B. Correlation matrix and Random Matrix Theory
The stock market data being essentially a multivariate
time seriesdata, weconstructcorrelationmatrixtostudy
its spectra and contrast it with the random multivariate
data from coupled map lattice. It is known from previous
studies that the empirical spectra of correlation matrices
drawn from time series data, for most part, follow ran-
dom matrix theory (RMT, see e.g. Gopikrishnan et al.
(2001)).
1. Correlation matrix and Eigenvalue density
a. Correlation matrix If there are Nassets with
pricePi(t) for assetiat timet, then the logarithmic re-
turn of stock iisri(t) = lnPi(t)−lnPi(t−1), which fora certain consecutive sequence of trading days forms the
returnvector ri. Inordertocharacterizethesynchronous
time evolution of stocks, the equal time correlation coef-
ﬁcients between stocks iandjis deﬁned as
ρij=∝an}bracketle{trirj∝an}bracketri}ht−∝an}bracketle{tri∝an}bracketri}ht∝an}bracketle{trj∝an}bracketri}ht/radicalBig
[∝an}bracketle{tr2
i∝an}bracketri}ht−∝an}bracketle{tri∝an}bracketri}ht2][∝an}bracketle{tr2
j∝an}bracketri}ht−∝an}bracketle{trj∝an}bracketri}ht2],(12)
where∝an}bracketle{t...∝an}bracketri}htindicates a time average over the trading days
included in the return vectors. These correlation coef-
ﬁcients form an N×Nmatrix with −1≤ρij≤1. If
ρij= 1, the stock price changes are completely corre-
lated; ifρij= 0, the stock price changes are uncorre-
lated, and if ρij=−1, then the stock price changes are
completely anti-correlated.
b. Correlation matrix of spatio-temporal series from
coupled map lattices Consider a time series of the form
z′(x,t), wherex= 1,2,...nandt= 1,2....pdenote the
discrete space and time, respectively. In this, the time
series at every spatial point is treated as a diﬀerent vari-
able. We deﬁne the normalised variable as
z(x,t) =z′(x,t)−∝an}bracketle{tz′(x)∝an}bracketri}ht
σ(x), (13)
where the brackets ∝an}bracketle{t.∝an}bracketri}htrepresent temporal averages and
σ(x) the standard deviation of z′at position x. Then,
the equal-time cross-correlation matrix that represents
the spatial correlations can be written as
Sx,x′=∝an}bracketle{tz(x,t)z(x′,t)∝an}bracketri}ht, x,x′= 1,2,...,n. (14)
The correlation matrix is symmetric by construction. In
addition, a large class of processes are translation invari-
ant and the correlation matrix can contain that addi-
tional symmetry too. We will use this property for our
correlation models in the context of coupled map lat-
tice. In time series analysis, the averages ∝an}bracketle{t.∝an}bracketri}hthave to
be replaced by estimates obtained from ﬁnite samples.
As usual, we will use the maximum likelihood estimates,
∝an}bracketle{ta(t)∝an}bracketri}ht ≈1
p/summationtextp
t=1a(t). These estimates contain statisti-
cal uncertainties, which disappears for p→ ∞. Ideally,
one requires p≫nto have reasonably correct correla-
tion estimates. See Chakraborti et al.(2007) for details
of parameters.
c. Eigenvalue Density The interpretation of the
spectra of empirical correlation matrices should be done
carefully if one wants to be able to distinguish between
system speciﬁc signatures and universal features. The
former express themselves in the smoothed level den-
sity, whereas the latter usually are represented by the
ﬂuctuations on top of this smooth curve. In time series
analysis, the matrix elements are not only prone to un-
certainty such as measurement noise on the time series
data, but also statistical ﬂuctuations due to ﬁnite sam-
pleeﬀects. When characterizingtime seriesdatainterms
of random matrix theory, one is not interested in these
trivial sources of ﬂuctuations which are present on every",2009-09-10T15:25:45Z,courier ined  trefore  reno ori pre cup t hayashi yeshiva hayashi yeshiva as courier brownish t ui ui  ui t cu mla hayashi yeshiva ui  im  tre econometrics  bardor nielsesprd mc peer ma ros correlatrandom matrix tory t it go pi krishna correlateenvalue correlat assets pi pi pi iorr to charaerize t synchronous b tse matrix  correlatconsir i tsx t i ias tse ially  chara bor ti eenvalue nsity t t iwn
paper_qf_47.pdf,16,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","16
dataset, but onewouldliketoidentify thesigniﬁcantfea-
tureswhichwouldbeshared,inprinciple,byan“inﬁnite”
amount of data without measurement noise. The eigen-
functions of the correlation matrices constructed from
such empirical time series carry the information con-
tained in the originaltime seriesdata in a “graded”man-
nerand they alsoprovideacompactrepresentationforit.
Thus, by applying an approach based on random matrix
theory, one tries to identify non-random components of
the correlationmatrixspectra asdeviations fromrandom
matrix theory predictions (Gopikrishnan et al.(2001)).
We will look at the eigenvalue density that has been
studied in the context of applying random matrix the-
ory methods to time series correlations. Let N(λ) be the
integrated eigenvalue density which gives the number of
eigenvalues less than a given value λ. Then, the eigen-
value or level density is given by ρ(λ) =dN(λ)
dλ. This can
be obtained assuming random correlation matrix and is
foundtobeingoodagreementwiththeempiricaltimese-
ries data from stock market ﬂuctuations. From Random
Matrix Theory considerations, the eigenvalue density for
random correlations is given by
ρrmt(λ) =Q
2πλ/radicalbig
(λmax−λ)(λ−λmin),(15)
whereQ=N/Tis the ratio of the number of variables
to the length of each time series. Here, λmaxandλmin,
representing the maximum and minimum eigenvalues of
the random correlation matrix respectively, are given by
λmax,min= 1+1/Q±2/radicalbig
1/Q. However, due to presence
of correlations in the empirical correlation matrix, this
eigenvalue density is often violated for a certain number
of dominant eigenvalues. They often correspond to sys-
tem speciﬁc information in the data. In Fig. 24 we show
the eigenvalue density for S&P500 data and also for the
chaotic data from coupled map lattice. Clearly, both
curves are qualitatively diﬀerent. Thus, presence or ab-
sence of correlations in data is manifest in the spectrum
of the corresponding correlation matrices.
2. Earlier estimates and studies using Random Matrix
Theory
Lalouxet al.(1999) showed that results from the ran-
dom matrix theory were useful to understand the statis-
tical structure of the empirical correlation matrices ap-
pearing in the study of price ﬂuctuations. The empirical
determination of a correlation matrix is a diﬃcult task.
If one considers Nassets, the correlation matrix con-
tainsN(N−1)/2 mathematically independent elements,
which must be determined from Ntime series of length
T. IfTis not very large compared to N, then gener-
ally the determination of the covariances is noisy, and
therefore the empirical correlation matrix is to a large
extent random. The smallest eigenvalues of the matrix
are the most sensitive to this ‘noise’. But the eigenvec-
tors corresponding to these smallest eigenvalues deter-0.60.8 1 1.2 1.4 1.61.8 2
λ010203040506070ρ(λ)
00.511.522.533.54
 λ00.511.5 ρ(λ)
FIG. 24. The upper panel shows spectral density for multi-
variate spatio-temporal time series drawn from coupled map
lattices. The lower panel shows the eigenvalue density for t he
return time series of the S&P500 stock market data (8938
time steps).
mine the minimum risk portfolios in Markowitz theory.
It is thus important to distinguish “signal” from “noise”
or, in other words, to extract the eigenvectors and eigen-
values of the correlation matrix containing real informa-
tion (those important for risk control), from those which
donot contain anyuseful informationand areunstable in
time. It is useful to compare the properties of an empiri-
cal correlation matrix to a “null hypothesis”— a random
matrix which arises for example from a ﬁnite time se-
ries of strictly uncorrelated assets. Deviations from the
random matrix case might then suggest the presence of
true information. The main result of their study was the
remarkable agreement between the theoretical prediction
(based on the assumption that the correlation matrix is
random) and empirical data concerning the density of
eigenvalues (shown in Fig. 25) associated to the time
series of the diﬀerent stocks of the S&P 500 (or other
stock markets). Cross-correlations in ﬁnancial data were
also studied by Plerou et al.(1999, 2002). They anal-
ysed cross-correlations between price ﬂuctuations of dif-
ferent stocks using methods of RMT. Using two large
databases, they calculated cross-correlation matrices of
returns constructed from (i) 30-min returns of 1000 US
stocks for the 2-yr period 1994–95, (ii) 30-min returns
of 881 US stocks for the 2-yr period 1996–97, and (iii)
1-day returns of 422 US stocks for the 35-yrperiod 1962–
96. They also tested the statistics of the eigenvalues
λiof cross-correlation matrices against a “null hypoth-",2009-09-10T15:25:45Z,t  go pi krishna   t from random matrix tory tis re ty i   earlier random matrix tory halo ux et t  assets time  tis t but t t marwitz it it viatns t  oss le rou ty usi ty
paper_qf_47.pdf,17,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","17
FIG. 25. Eigenvalue spectrum of the correlation matrices.
Adapted from Laloux et al.(1999).
esis”. They found that a majority of the eigenvalues
of the cross-correlation matrices were within the RMT
bounds [λmin,λmax], as deﬁned above, for the eigenval-
ues of random correlation matrices. They also tested the
eigenvalues of the cross-correlation matrices within the
RMT bounds for universalproperties ofrandommatrices
and found good agreement with the results for the Gaus-
sian orthogonal ensemble (GOE) of random matrices —
implying a large degree of randomness in the measured
cross-correlation coeﬃcients. Furthermore, they found
that the distribution of eigenvector components for the
eigenvectorscorresponding to the eigenvalues outside the
RMT bounds displayed systematic deviations from the
RMT prediction and that these “deviating eigenvectors”
werestableintime. Theyanalysedthecomponentsofthe
deviating eigenvectors and found that the largest eigen-
value corresponded to an inﬂuence common to all stocks.
Their analysis of the remaining deviating eigenvectors
showed distinct groups, whose identities corresponded to
conventionally-identiﬁed business sectors.
C. Analyses of correlations and economic taxonomy
1. Models and theoretical studies of ﬁnancial correlations
Podobnik et al.(2000)studiedhowthepresenceofcor-
relations in physical variables contributes to the form of
probability distributions. They investigated a process
with correlations in the variance generated by a Gaus-
sian or a truncated Levy distribution. For both Gaus-sian and truncated Levy distributions, they found that
due to the correlations in the variance, the process “dy-
namically”generatedpower-lawtailsinthedistributions,
whoseexponents couldbe controlledthroughthe waythe
correlations in the variance were introduced. For a trun-
cated Levy distribution, the process could extend a trun-
cated distribution beyond the truncation cutoﬀ , leading
to a crossoverbetween a Levy stable power law and their
“dynamically-generated” power law. It was also shown
that the process could explain the crossoverbehavior ob-
served in the S&P 500 stock index.
Noh (2000) proposed a model for correlations in stock
markets in which the markets were composed of several
groups, within which the stock price ﬂuctuations were
correlated. The spectral properties of empirical correla-
tion matrices (Plerou et al.(1999); Laloux et al.(1999))
were studied in relation to this model and the connection
between the spectral properties of the empirical corre-
lation matrix and the structure of correlations in stock
markets was established.
Thecorrelationstructureofextremestockreturnswere
studied by Cizeau et al.(2001). It has been commonly
believed that the correlations between stock returns in-
creased in high volatility periods. They investigated how
much of these correlations could be explained within a
simple non-Gaussian one-factor description with time in-
dependent correlations. Using surrogate data with the
true market return as the dominant factor, it was shown
that most of these correlations, measured by a variety of
diﬀerent indicators, could be accounted for. In partic-
ular, their one-factor model could explain the level and
asymmetryofempiricalexceeding correlations. However,
moresubtleeﬀectsrequiredanextensionofthe onefactor
model, where the variance and skewness of the residuals
also depended on the market return.
Burdaet al.(2001) provided a statistical analysis of
three S&P 500 covariances with evidence for raw tail
distributions. They studied the stability of these tails
against reshuﬄing for the S&P 500 data and showed that
the covariance with the strongest tails was robust, with
a spectral density in remarkable agreement with random
Levy matrix theory. They also studied the inverse par-
ticipation ratio for the three covariances. The strong
localization observed at both ends of the spectral den-
sity was analogous to the localization exhibited in the
random Levy matrix ensemble. They showed that the
stocks with the largest scattering were the least suscepti-
ble to correlations and were the likely candidates for the
localized states.
2. Analyses using graph theory and economic taxonomy
Mantegna (1999) introduced a method for ﬁnding a hi-
erarchical arrangement of stocks traded in ﬁnancial mar-
ket, through studying the clustering of companies by us-
ing correlations of asset returns. With an appropriate
metric – based on the earlier explained correlation ma-",2009-09-10T15:25:45Z,eenvalue adapted halo ux ty ty ga furtr ty analysed t onents of t tir analysmols poor  ty ga levy for ga levy for levy levy it not t le rou halo ux t correlatstruure of extreme stock urns re size au it ty usi ibur dae ty levy ty t levy ty analysantenna with
paper_qf_47.pdf,18,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","18
trix coeﬃcients ρij’s between all pairs of stocks iand
jof the portfolio, computed in Eq. 12 by considering
the synchronous time evolution of the diﬀerence of the
logarithm of daily stock price – a fully connected graph
was deﬁned in which the nodes are companies, or stocks,
and the “distances” between them were obtained from
the corresponding correlation coeﬃcients. The minimum
spanning tree (MST) was generated from the graph by
selecting the most important correlationsand it wasused
to identify clusters of companies. The hierarchical tree
of the sub-dominant ultrametric space associated with
the graph provided information useful to investigate the
number and nature of the common economic factors af-
fecting the time evolution of logarithm of price of well
deﬁned groups of stocks. Several other attempts have
been made to obtain clustering from the huge correlation
matrix.
Bonanno et al.(2001) studied the high-frequency
cross-correlation existing between pairs of stocks traded
in a ﬁnancial market in a set of 100 stocks traded in US
equity markets. A hierarchical organization of the inves-
tigated stocks was obtained by determining a metric dis-
tance between stocks and by investigating the properties
of the sub-dominant ultrametric associated with it. A
clear modiﬁcation of the hierarchical organization of the
set of stocks investigated was detected when the time
horizon used to determine stock returns was changed.
The hierarchical location of stocks of the energy sector
was investigated as a function of the time horizon. The
hierarchicalstructureexploredbythe minimumspanning
tree also seemed to give information about the inﬂuential
power of the companies.
It also turned out that the hierarchical structure of
the ﬁnancial market could be identiﬁed in accordance
with the results obtained by an independent cluster-
ing method, based on Potts super-paramagnetic transi-
tions as studied by Kullmann et al.(2000), where the
spins correspond to companies and the interactions are
functions of the correlation coeﬃcients determined from
the time dependence of the companies’ individual stock
prices. The method is a generalization of the clus-
tering algorithm by Blatt et al.(1996) to the case of
anti-ferromagnetic interactions corresponding to anti-
correlations. For the Dow Jones Industrial Average, no
anti-correlations were observed in the investigated time
periodandthepreviousresultsobtainedbydiﬀerenttools
were well reproduced. For the S&P 500, where anti-
correlationsoccur, repulsion between stocks modiﬁed the
cluster structure of the N= 443 companies studied, as
shown in Fig. 26. The eﬃciency of the method is repre-
sented by the fact that the ﬁgure matches well with the
corresponding result obtained by the minimal spanning
tree method, including the speciﬁc composition of the
clusters. For example, at the lowest level of the hierarchy
(highest temperature in the super-paramagnetic phase)
the diﬀerent industrial branches can be clearly identi-
ﬁed: Oil, electricity, gold mining, etc. companies build
separate clusters. The network of inﬂuence was investi-180
70855 3 3 2
6 17 2 3 3 5 2 2 2
5 4 6 17 3 2 5 2 2
3 3 4 14 3 5 2 2
4 12 3 5 2 29 2 2 22 3 11 2
2 17 3 11 2 9 .033.027.0175.014.005T 443
FIG. 26. The hierarchical structure of clusters of the S&P
500 companies in the ferromagnetic case. In the boxes the
number of elements of the cluster are indicated. The cluster s
consisting of single companies are not indicated. Adapted
from Kullmann et al.(2000).
gated by means of a time-dependent correlation method
by Kullmann et al.(2000). They studied the correlations
as the function of the time shift between pairs of stock
return time series of tick-by-tick data of the NYSE. They
investigated whether any “pulling eﬀect” between stocks
existed or not, i.e. whether at any given time the re-
turn value of one stock inﬂuenced that of another stock
at a diﬀerent time or not. They found that, in general,
twotypesofmechanismsgeneratedsigniﬁcantcorrelation
between any two given stocks. One was some kind of ex-
ternal eﬀect (say, economic or political news) that inﬂu-
enced both stock prices simultaneously, and the change
for both prices appeared at the same time, such that
the maximum of the correlation was at zero time shift.
The second eﬀect was that, one of the companies had an
inﬂuence on the other company indicating that one com-
pany’soperationdepended onthe other, sothat the price
change of the inﬂuenced stock appeared latter because it
required some time to react on the price change of the
ﬁrst stock displaying a “pulling eﬀect”. A weak but sig-
niﬁcant eﬀect with the real data set was found, showing
that in many cases the maximum correlation was at non-
zero time shift indicating directions of inﬂuence between
the companies, and the characteristic time was of the
order of a few minutes, which was compatible with eﬃ-
cient market hypothesis. In the pulling eﬀect, they found
that in general, more important companies (which were
traded more) pulled the relatively smaller companies.
The time dependent properties of the minimum span-
ning tree (introduced by Mantegna), called a ‘dynamic
asset tree’, were studied by Onnela et al.(2003b). The
nodes of the tree were identiﬁed with stocks and the dis-
tance between them was a unique function of the corre-
sponding element of the correlationmatrix. By using the
concept of a central vertex, chosen as the most strongly
connected node of the tree, the mean occupation layer
was deﬁned, which was an important characteristic of
the tree. During crashes the strong global correlation in
the market manifested itself by a low value of the mean",2009-09-10T15:25:45Z,eq t t sevl bonanno t t it posts ullmant blast for dow jonindustrial ge for  t for o t t it adapted ullmanullmanty ty ty one t it antenna onel t by 
paper_qf_47.pdf,19,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","19
occupation layer. The tree seemed to have a scale free
structure where the scaling exponent of the degree dis-
tribution was diﬀerent for ‘business as usual’ and ‘crash’
periods. The basic structure of the tree topology was
very robust with respect to time. Let us discuss in more
details how the dynamic asset tree was applied to studies
of economic taxonomy.
a. Financial Correlation matrix and constructing As-
set Trees Two diﬀerent sets of ﬁnancial data were used.
The ﬁrst set from the Standard & Poor’s 500 index
(S&P500) of the New York Stock Exchange (NYSE)
from July 2, 1962 to December 31, 1997 contained 8939
daily closing values. The second set recorded the split-
adjusted daily closurepricesfor atotal of N= 477stocks
traded at the New York Stock Exchange (NYSE) over
the period of 20 years, from 02-Jan-1980to 31-Dec-1999.
This amounted a total of 5056 prices per stock, indexed
by time variable τ= 1,2,...,5056. For analysis and
smoothing purposes, the data was divided time-wise into
Mwindowst= 1,2,...,Mof widthT, whereTcorre-
sponded to the number of daily returns included in the
window. Note that several consecutive windows over-
lap with each other, the extent of which is dictated by
the window step length parameter δT, which describes
the displacement of the window and is also measured in
trading days. The choice of window width is a trade-oﬀ
between too noisy and too smoothed data for small and
large window widths, respectively. The results presented
herewerecalculatedfrommonthlysteppedfour-yearwin-
dows, i.e.δT= 250/12≈21 days and T= 1000 days.
A large scale of diﬀerent values for both parameters were
explored,andthecitedvalueswerefoundoptimal(Onnela
(2000)). With these choices, the overall number of win-
dows isM= 195.
The earlier deﬁnition of correlation matrix, given by
Eq. 12 is used. These correlation coeﬃcients form an
N×Ncorrelationmatrix Ct, whichservesasthebasisfor
trees discussed below. An asset tree is then constructed
according to the methodology by Mantegna (1999). For
the purpose of constructing asset trees, a distance is de-
ﬁned betweenapairofstocks. Thisdistanceis associated
with the edge connecting the stocks and it is expected to
reﬂect the level at which the stocks are correlated. A
simple non-linear transformation dt
ij=/radicalBig
2(1−ρt
ij) is
used to obtain distances with the property 2 ≥dij≥0,
forming an N×Nsymmetric distance matrix Dt. So,
ifdij= 0, the stock price changes are completely cor-
related; if dij= 2, the stock price changes are com-
pletely anti-uncorrelated. The trees for diﬀerent time
windows are not independent of each other, but form
a series through time. Consequently, this multitude of
trees is interpreted as a sequence of evolutionary steps
of a single dynamic asset tree . An additional hypothe-
sis is required about the topology of the metric space:
the ultrametricity hypothesis. In practice, it leads to
determining the minimum spanning tree (MST) of the
distances, denoted Tt. The spanning tree is a simply
connected acyclic (no cycles) graph that connects all Nnodes (stocks) with N−1 edges such that the sum of
all edge weights,/summationtext
dt
ij∈Ttdt
ij, is minimum. We refer to
the minimum spanning tree at time tby the notation
Tt= (V,Et), whereVis a set of vertices and Etis a cor-
responding set of unordered pairs of vertices, or edges.
Since the spanning tree criterion requires all Nnodes to
be always present, the set of vertices Vis time indepen-
dent, whichiswhythetime superscripthasbeendropped
from notation. The set of edges Et, however, does de-
pend on time, as it is expected that edge lengths in the
matrixDtevolve over time, and thus diﬀerent edges get
selected in the tree at diﬀerent times.
b. Market characterization We plot the distribution
of (i) distance elements dt
ijcontained in the distance ma-
trixDt(Fig. 27), (ii) distance elements dijcontained
in the asset (minimum spanning) tree Tt(Fig. 28). In
both plots, but most prominently in Fig. 27, there ap-
pears to be a discontinuity in the distribution between
roughly 1986 and 1990. The part that has been cut out,
pushed to the left and made ﬂatter, is a manifestation of
Black Monday (October 19, 1987), and its length along
the time axis is related to the choice of window width
TOnnelaet al.(2003a,b). Also, note that in the dis-
FIG. 27. Distribution of all N(N−1)/2 distance elements dij
contained in the distance matrix Dtas a function of time.
tribution of tree edges in Fig. 28 most edges included in
the tree seem to come from the area to the right of the
value 1.1 in Fig. 27, and the largest distance element is
dmax= 1.3549.
Tree occupation and central vertex Let us focus
on characterizing the spread of nodes on the tree, by
introducing the quantity of mean occupation layer
l(t,vc) =1
NN/summationdisplay
i=1lev(vt
i), (16)",2009-09-10T15:25:45Z,t t  nancial correlatas tretwo t standard poor new york stock exe july cember t new york stock exe jac  for windows of core note t t onel with t eq tse correlatmatrix  aantenna for  distance is b syetric dt so t consequently ai t nos  dt   et vis et is since nos vis t et dt evolve market  dt    i t  monday ober onel et also distributdt as   tree 
paper_qf_47.pdf,20,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","20
where lev(vi) denotes the level of vertex vi. The levels,
not to be confused with the distances dijbetween nodes,
aremeasuredinnaturalnumbersinrelationtothe central
vertexvc, whose level is taken to be zero. Here the mean
occupation layer indicates the layer on which the mass
of the tree, on average, is conceived to be located. The
central vertex is considered to be the parent of all other
nodes in the tree, and is also known as the root of the
tree. It is used as the reference point in the tree, against
which the locations of all other nodes are relative. Thus
all other nodes in the tree are children of the central
vertex. Although there is an arbitrariness in the choice
of the central vertex, it is proposed that the vertex is
central, in the sense that any change in its price strongly
aﬀects the course of events in the market on the whole.
Three alternative deﬁnitions for the central vertex were
proposed in the studies, all yielding similar and, in most
cases, identical outcomes. The idea is to ﬁnd the node
that is most strongly connected to its nearest neighbors.
Forexample, accordingtoonedeﬁnition, thecentralnode
is the one with the highest vertex degree , i.e. the number
of edges which are incident with (neighbor of) the vertex.
Also, one may have either (i) static (ﬁxed at all times) or
(ii) dynamic (updated at each time step) central vertex,
but again the results do not seem to vary signiﬁcantly.
The study of the variation of the topological properties
and nature of the trees, with time were done.
Economic taxonomy Mantegna’s idea of linking
stocks in an ultrametric space was motivated a posteri-
oriby the property of such a space to providea meaning-
fuleconomictaxonomy(Onnela et al.(2002)). Mantegna
examined the meaningfulness of the taxonomy, by com-
paring the grouping of stocks in the tree with a third
FIG. 28. Distributionofthe( N−1) distance elements dijcon-
tained in the asset (minimum spanning) tree Ttas a function
of time.
FIG. 29. Snapshot of a dynamic asset tree connecting the
examined 116 stocks of the S&P 500 index. The tree was
produced using four-year window width and it is centered on
January 1, 1998. Business sectors are indicated according
to Forbes (www.forbes.com). In this tree, General Electric
(GE) was used as the central vertex and eight layers can be
identiﬁed.
party reference grouping of stocks e.g. by their industry
classiﬁcations (Mantegna (1999)). In this case, the ref-
erence was provided by Forbes (www.forbes.com), which
uses its own classiﬁcation system, assigning each stock
with a sector (higher level) and industry (lower level)
category. In order to visualize the grouping of stocks,
a sample asset tree is constructed for a smaller dataset
(shown in Fig. 29), which consists of 116 S&P 500 stocks,
extending from the beginning of 1982 to the end of
2000, resulting in a total of 4787 price quotes per stock
(Onnela et al.(2003b)). The window width was set at
T= 1000, and the shown sample tree is located time-
wise att=t∗, corresponding to 1.1.1998. The stocks in
this dataset fall into 12 sectors, which are Basic Materi-
als, Capital Goods, Conglomerates, Consumer/Cyclical,
Consumer/Non-Cyclical, Energy, Financial, Healthcare,
Services, Technology, Transportation and Utilities. The
sectors are indicated in the tree (see Fig. 29) with diﬀer-
ent markers, while the industry classiﬁcations are omit-
ted for reasons of clarity. The term sector is used ex-
clusively to refer to the given third party classiﬁcation
system of stocks. The term branchrefers to a subset of
the tree, to all the nodes that share the speciﬁed com-
mon parent. In addition to the parent, it is needed to
have a reference point to indicate the generational direc-
tion (i.e. who is who’s parent) in order for a branch to
be well deﬁned. Without this reference there is abso-
lutely no way to determine where one branch ends and
the other begins. In this case, the reference is the central
node. There aresomebranchesin the tree, in whichmost
of the stocks belong to just one sector, indicating that",2009-09-10T15:25:45Z,t re t it  although three t for  also t economic antenna onel antenna distributof t  as snapshot t uary business forbenl eleric antenna iforbi onel t t basic mater capital goods colomtconsumer cyical consumer nocyical energy nancial althcare serviechnology transtatutitit  t t iwithout itre
paper_qf_47.pdf,21,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","21
the branch is fairly homogeneous with respect to busi-
ness sectors. This ﬁnding is in accordance with those of
Mantegna (1999) , although there are branches that are
fairly heterogeneous, such as the one extending directly
downwards from the central vertex (see Fig. 29).
V. PARTIAL CONCLUSION
This ﬁrst part ofourreview hasshown statisticalprop-
erties of ﬁnancial data (time series of prices, order book
structure, assets correlations). Some of these properties,
such as fat tails of returns or volatility clustering, are
widely known and acknowledged as “ﬁnancial stylized
facts”. They are now largely cited in order to compare
ﬁnancial models, and reveal the lacks of many classical
stochastic models of ﬁnancial assets. Some other prop-
erties are newer ﬁndings that are obtained by studying
high-frequency data of the whole order book structure.
Volume of orders, interval time between orders, intra-
day seasonality, etc. are essential phenomenons to be
understood when working in ﬁnancial modelling. The
important role of studies of correlations has been em-
phasized. Beside the technical challenges raised by high-
frequency, many studies based for example on random
matrixtheoryorclusteringalgorithmshelp gettingabet-
ter grasp on some Economics problems. It is our belief
that future modelling in ﬁnance will have to be partly
based on Econophysics work on agent-based models in
order to incorporate these “stylized facts” in a compre-
hensive way. Agent-based reasoning for order book mod-
els, wealth exchange models and game theoretic models
will be reviewed in the following part of the review, to
appear in a following companion paper.
Part II
VI. INTRODUCTION
In the ﬁrst part of the review, empirical developments
in Econophysics have been studied. We have pointed
out that some of these widely known “stylized facts”
are already at the heart of ﬁnancial models. But many
facts, especially the newer statistical properties of or-
der books, are not yet taken into account. As advo-
cated by many during the ﬁnancial crisis in 2007-2008
(see e.g. Bouchaud (2008); Lux and Westerhoﬀ (2009);
Farmer and Foley (2009)), agent-based models should
have a great role to play in future ﬁnancial modelling.
In economic models, there is usually the representative
agent, who is “perfectly rational” and uses the “utility
maximization” principle while taking actions. Instead
the multi-agent models that have originated from sta-
tistical physics considerations have allowed to go beyond
the prototype theories with the “representative”agent in
traditional economics. In this second part of our review,
we present recent developments of agent-based models inEconophysics.
There are, of course, many reviews and
books already published in this areas (see e.g.
Bouchaud et al.(2009), Lux and Westerhoﬀ (2009),
Samanidou et al.(2007), Yakovenko and Rosser (2009),
Chatterjee and Chakrabarti (2007), Challet et al.
(2004), Coolen (2005), etc.). We will present here our
perspectives in three representative areas.
VII. AGENT-BASED MODELLING OF ORDER BOOKS
A. Introduction
Although known, at least partly, for a long time –
Mandelbrot (1963) gives a reference for a paper dealing
with non-normality of price time series in 1915, followed
by several others in the 1920’s – “stylized facts” have
often been left aside when modelling ﬁnancial markets.
They were even often referred to as “anomalous” charac-
teristics, as if observations failed to comply with theory.
Much has been done these past ﬁfteen years in order to
address this challenge and provide new models that can
reproduce these facts. These recent developments have
been built on top of early attempts at modelling mech-
anisms of ﬁnancial markets with agents. For example,
Stigler (1964), investigating some rules of the SEC3, or
Garman(1976), investigatingdouble-auction microstruc-
ture, belong to those historical works. It seems that the
ﬁrst modern attempts at that type of models were made
in the ﬁeld of behavioural ﬁnance. This ﬁeld aims at
improving ﬁnancial modelling based on the psychology
and sociology of the investors. Models are built with
agents who can exchange shares of stocks according to
exogenouslydeﬁned utility functions reﬂecting their pref-
erences and risk aversions. LeBaron (2006b) shows that
this type of modelling oﬀers good ﬂexibility for repro-
ducing some of the stylized facts and LeBaron (2006a)
provides a review of that type of model. However, al-
though achieving some of their goals, these models suf-
fer from many drawbacks: ﬁrst, they are very complex,
and it may be a very diﬃcult task to identify the role of
their numerous parameters and the types of dependence
to these parameters; second, the chosen utility functions
do not necessarily reﬂect what is observed on the mech-
anisms of a ﬁnancial market.
A sensible change in modelling appears with much
simpler models implementing only well-identiﬁed and
presumably realistic “behaviour”: Cont and Bouchaud
(2000) uses noise traders that are subject to “herding”,
i.e. form random clusters of traders sharing the same
view on the market. The idea is used in Raberto et al.
(2001) as well. A complementary approach is to char-
acterize traders as fundamentalists, chartists or noise
3Security Exchange Commission",2009-09-10T15:25:45Z, antenna   some ty some volume t besi economics it eco no psics agent part ieco no psics  but as  chalux sterho farmer foley iinstead ieco no psics tre  chalux sterho satanic ou v e roster terjee chara bart chal  coe introdualthough manlbrot ty mutse for ter germait  mols le barole barocont  chat racer to security exe coissn
paper_qf_47.pdf,22,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","22
traders. Lux and Marchesi (2000) propose an agent-
based model in which these types of traders interact. In
all these models, the price variation directly results from
the excess demand: at each time step, all agents submit
orders and the resulting price is computed. Therefore,
everything is cleared at each time step and there is no
structure of order book to keep track of orders.
One big step is made with models really taking into
account limit orders and keeping them in an order book
once submitted and not executed. Chiarella and Iori
(2002) build an agent-based model where all traders sub-
mit orders depending on the three elements identiﬁed
in Lux and Marchesi (2000): chartists, fundamentalists,
noise. Orders submitted are then stored in a persistent
order book. In fact, one of the ﬁrst simple models with
this feature was proposed in Bak et al.(1997). In this
model, orders are particles moving along a price line, and
each collision is a transaction. Due to numerous caveats
in this model, the authors propose in the same paper an
extension with fundamentalist and noise traders in the
spirit of the models previously evoked. Maslov (2000)
goes further in the modelling of trading mechanisms by
taking into account ﬁxed limit orders and market orders
that trigger transactions, and really simulating the or-
der book. This model was analytically solved using a
mean-ﬁeld approximation by Slanina (2001).
Following this trend of modelling, the more or less
“rational” agents composing models in economics tends
to vanish and be replaced by the notion of ﬂows: or-
ders are not submitted any more by an agent follow-
ing a strategic behaviour, but are viewed as an arriv-
ing ﬂow whose properties are to be determined by em-
pirical observations of market mechanisms. Thus, the
modelling of order books calls for more “stylized facts”,
i.e. empirical properties that could be observed on a
large number of order-drivenmarkets. Biais et al.(1995)
is a thorough empirical study of the order ﬂows in the
Paris Bourse a few years after its complete computer-
ization. Market orders, limit orders, time of arrivals
and placement are studied. Bouchaud et al.(2002) and
Potters and Bouchaud (2003) provide statistical features
on the order book itself. These empirical studies, that
have been reviewed in the ﬁrst part of this review, are
the foundation for “zero-intelligence” models, in which
“stylized facts” are expected to be reproduced by the
properties of the order ﬂows and the structure of order
book itself, without considering exogenous “rationality”.
Challet and Stinchcombe (2001) propose a simple model
of order ﬂows: limit orders are deposited in the order
book and can be removed if not executed, in a simple
deposition-evaporation process. Bouchaud et al.(2002)
use this type of model with empirical distribution as in-
puts. As of today, the most complete empirical model
is to our knowledge Mike and Farmer (2008), where or-
der placement and cancellation models are proposed
and ﬁtted on empirical data. Finally, new challenges
arise as scientists try to identify simple mechanisms
that allow an agent-basedmodel to reproduce non-trivialbehaviours: herding behaviour inCont and Bouchaud
(2000), dynamic price placement in Preis et al.(2007),
threshold behaviour in Cont (2007), etc.
In this part we review some of these models. This sur-
vey is of course far from exhaustive, and we have just
selected models that we feel are representative of a spe-
ciﬁc trend of modelling.
B. Early order-driven market modelling: Market
microstructure and policy issues
The pioneering works in simulation of ﬁnancial mar-
kets were aimed to study market regulations. The very
ﬁrst one, Stigler (1964), tries to investigate the eﬀect of
regulationsof the SEC on American stock markets, using
empirical data from the 20’s and the 50’s. Twenty years
later, atthestartofthe computerizationofﬁnancialmar-
kets, Hakansson et al.(1985) implements a simulator in
order to test the feasibility of automated market making.
Instead of reviewing the huge microstructure literature,
we refer the reader to the well-known books by O’Hara
(1995) or Hasbrouck (2007), for example, for a panorama
of this branch of ﬁnance. However, by presenting a small
selection of early models, we here underline the ground-
ing of recent order book modelling.
1. A pioneer order book model
Toourknowledge,theﬁrstattempttosimulateaﬁnan-
cial market was by Stigler (1964). This paper was a bit-
ing and controversial reaction to the Report of the Spe-
cial Study of the Securities Markets of the SEC (Cohen
(1963a)), whose aim was to “study the adequacy of rules
of the exchange and that the New York stock exchange
undertakes to regulate its members in all of their activ-
ities” (Cohen (1963b)). According to Stigler, this SEC
report lacks rigorous tests when investigating the eﬀects
of regulation on ﬁnancial markets. Stating that “de-
mand and supply are [...] erratic ﬂows with sequences
of bids and asks dependent upon the random circum-
stances of individual traders”, he proposes a simple sim-
ulation model to investigate the evolution of the market.
In this model, constrained by simulation capability in
1964, price is constrained within L= 10 ticks. (Limit)
orders are randomly drawn, in trade time, as follows:
they can be bid or ask orders with equal probability, and
theirpricelevel isuniformlydistributed on the pricegrid.
Each time an order crosses the opposite best quote, it is
a market order. All orders are of size one. Orders not
executedN= 25 time steps after their submission are
cancelled. Thus, Nis the maximum number of orders
available in the order book.
In the original paper, a run of a hundred trades was
manually computed using tables of random numbers.
Of course, no particular results concerning the “styl-
ized facts” of ﬁnancial time series was expected at that",2009-09-10T15:25:45Z,lux marcs itrefore one chi are lla ori lux marcs orrs ibak idue mas lov  spaifollowi  bia is paris course market  chaters  chatse chal  stinch chaas mike farmer nally cont  cha cont i  market t t ter tnty aka nssoinstead hara harbor uk to our kledge ter  ret pe study securitimarkets conew york coaccordi ter stati ilimit eaall orrs  is iof
paper_qf_47.pdf,23,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","23
time. However, in his review of some order book mod-
els, Slanina (2008) makes simulations of a similar model,
with parameters L= 5000 and N= 5000, and shows
that price returns are not Gaussian: their distribution
exhibits power law with exponent 0 .3, far from empirical
data. As expected, the limitation Lis responsible for a
sharp cut-oﬀ of the tails of this distribution.
2. Microstructure of the double auction
Garman (1976) provides an early study of the double
auction market with a point of view that does not ignore
temporal structure, and really deﬁnes order ﬂows. Price
is discrete and constrained to be within {p1,pL}. Buy
and sell orders are assumed to be submitted according to
two Poisson processes of intensities λandµ. Each time
an order crosses the best opposite quote, it is a market
order. All quantities areassumedto be equaltoone. The
aim of the author was to provide an empirical study of
themarketmicrostructure. ThemainresultofitsPoisson
model was to support the idea that negative correlation
of consecutive price changes is linked the microstructure
of the double auction exchange. This paper is very in-
teresting because it can be seen as precursor that clearly
sets the challenges of order book modelling. First, the
mathematical formulation is promising. With its ﬁxed
constrainedprices, Garman(1976) can deﬁne the state of
theorderbookatagiventimeasthevector( ni)i=1,...,Lof
awaitingorders(negativequantityforbidorders,positive
for ask orders). Future analytical models will use similar
vector formulations that can be cast it into known math-
ematical processes in order to extract analytical results
– see e.g. Cont et al.(2008) reviewed below. Second,
the author points out that, although the Poisson model
is simple, analytical solution is hard to work out, and he
providesMonteCarlosimulation. Theneedfornumerical
and empirical developments is a constant in all following
models. Third, the structural question is clearly asked in
the conclusion of the paper: “Does the auction-market
model imply the characteristic leptokurtosis seen in em-
pirical security price changes?”. The computerization of
markets that was about to take place when this research
was published – Toronto’s CATS4opened a year later in
1977 – motivated many following papers on the subject.
As an example, let us cite here Hakansson et al.(1985),
who built a model to choose the right mechanism for set-
ting clearing prices in a multi-securities market.
3. Zero-intelligence
In the models by Stigler (1964) and Garman (1976),
orders are submitted in a purely random way on the
4Computer Assisted Trading Systemgrid of possible prices. Traders do not observe the mar-
ket here and do not act according to a given strat-
egy. Thus, these two contributions clearly belong to
a class of “zero-intelligence” models. To our knowl-
edge, Gode and Sunder (1993) is the ﬁrst paper to in-
troduce the expression “zero-intelligence” in order to de-
scribe non-strategic behaviour of traders. It is applied
to traders that submit random orders in a double auc-
tion market. The expression has since been widely used
in agent-based modelling, sometimes in a slightly diﬀer-
ent meaning (see more recent models described in this
review). In Gode and Sunder (1993), two types of zero-
intelligence traders are studied. The ﬁrst are uncon-
strained zero-intelligence traders. These agents can sub-
mit random order at random prices, within the allowed
price range {1,...,L}. The second are constrained zero-
intelligence traders. These agents submit random or-
ders as well, but with the constraint that they cannot
cross their given reference price pR
i: constrained zero-
intelligence traders are not allowed to buy or sell at loss.
The aim of the authors was to show that double auction
markets exhibit an intrinsic “allocative eﬃciency” (ratio
between the total proﬁt earned by the traders divided by
the maximum possible proﬁt) even with zero-intelligence
traders. An interesting fact is that in this experiment,
price series resulting from actions by zero-intelligence
traders are much more volatile than the ones obtained
with constrained traders. This fact will be conﬁrmed in
future models where “fundamentalists” traders, having
a reference price, are expected to stabilize the market
(see Wyart and Bouchaud (2007) or Lux and Marchesi
(2000) below). Note that the results have been criticized
by Cliﬀ and Bruten (1997), who show that the observed
convergence of the simulated price towards the theoret-
ical equilibrium price may be an artefact of the model.
More precisely, the choice of traders’ demand carry a lot
of constraints that alone explain the observed results.
Modern works in Econophysics owe a lot to these early
models or contributions. Starting in the mid-90’s, physi-
cists have proposed simple order book models directly
inspired from Physics, where the analogy “order ≡par-
ticle” is emphasized. Three main contributions are pre-
sented in the next section.
C. Order-driven market modelling in Econophysics
1. The order book as a reaction-diﬀusion model
A very simple model directly taken from Physics was
presented in Bak et al.(1997). The authors consider a
market with Nnoise traders able to exchange one share
of stock at a time. Price p(t) at timetis constrained to
beaninteger(i.e. priceisquotedinnumberofticks)with
an upper bound ¯ p:∀t, p(t)∈ {0,...,¯p}. Simulation is
initiated at time 0 with half of the agents asking for one",2009-09-10T15:25:45Z,spaias  m struure germaprice buy poisoeaall t t articial intellence result of its poiso rst with germaof future cont second poiso  simulatt need for numerical third dot toronto as aka nssozero iter germauter assisted tradi tem grid trars  to co unr it t ico unr t tse t tse t a wy art  chalux marcs note i brute  co no psics starti psics three orr eco no psics t psics bak t noise price simulatn
paper_qf_47.pdf,24,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","24
share of stock (buy orders, bid) with price:
pj
b(0)∈ {0,¯p/2}, j= 1,...,N/2,(17)
and the other half oﬀering one share of stock (sell orders,
ask) with price:
pj
s(0)∈ {¯p/2,¯p}, j= 1,...,N/2.(18)
At each time step t, agents revise their oﬀer by exactly
one tick, with equal probability to go up or down. There-
fore, at time t, each seller (resp. buyer) agent chooses his
new price as:
pj
s(t+1) =pj
s(t)±1 (resp. pj
b(t+1) =pj
b(t)±1 ).
(19)
A transaction occurs when there exists ( i,j)∈
{1,...,N/2}2such thatpi
b(t+1) =pj
s(t+1). In such a
case the orders are removed and the transaction price is
recorded as the new price p(t). Once a transaction has
been recorded, two orders are placed at the extreme po-
sitions on the grid: pi
b(t+1) = 0 and pj
s(t+1) = ¯p. As a
consequence, the number of orders in the order book re-
mains constantand equalto the number ofagents. In ﬁg-
ure 30, an illustration of these moving particles is given.
As pointed out by the authors, this process of simula-
tion is similar the reaction-diﬀusion model A+B→ ∅
in Physics. In such a model, two types of particles are
inserted at each side of a pipe of length ¯ pand move ran-
domly with steps of size 1. Each time two particles col-
lide, they’re annihilated and two new particles are in-
serted. The analogy is summarized in table I. Following
TABLE I. Analogy between the A+B→ ∅reaction model
and the order book in Bak et al.(1997).
Physics Baket al.(1997)
Particles Orders
Finite Pipe Order book
Collision Transaction
this analogy, it thus can be showed that the variation
∆p(t) of the price p(t) veriﬁes :
∆p(t)∼t1/4(ln(t
t0))1/2. (20)
FIG. 30. Illustration of the Bak, Paczuski and Shubik model:
white particles (buy orders, bid) moving from the left, blac k
particles (sell orders, ask) moving from the right. Reprodu ced
from Bak et al.(1997).
FIG. 31. Snapshot of the limit order book in the Bak,
Paczuski and Shubik model. Reproduced from Bak et al.
(1997).
Thus, at long time scales, the series of price incre-
ments simulated in this model exhibit a Hurst exponent
H= 1/4. As for the stylized fact H≈0.7, this sub-
diﬀusivebehaviorappearstobeastepinthewrongdirec-
tion compared to the random walk H= 1/2. Moreover,
Slanina (2008) points out that no fat tails are observedin
the distribution of the returns of the model, but rather
ﬁts the empirical distribution with an exponential de-
cay. Other drawbacks of the model could be mentioned.
For example, the reintroduction of orders at each end
of the pipe leads to unrealistic shape of the order book,
as shown on ﬁgure 31. Actually here is the main draw-
back of the model: “moving” orders is highly unrealistic
as for modelling an order book, and since it does not
reproduce any known ﬁnancial exchange mechanism, it
cannot be the base for any larger model. Therefore, at-
tempts by the authors to build several extensions of this
simple framework, in order to reproduce “stylized facts”
by adding fundamental traders, strategies, trends, etc.
are not of interest for us in this review. However, we feel
that the basic model as such is very interesting because
of its simplicity and its “particle” representation of an
order-driven market that has opened the way for more
realistic models.",2009-09-10T15:25:45Z,at tre ionce as ias psics ieat followi analogy bak psics bake partiorrs nite pipe orr coltransalustratbak pac zu  sh ubi re pro du bak snapshot bak pac zu  sh ubi reproduced bak  hurt as ospaiotr for aually trefore ver
paper_qf_47.pdf,25,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","25
FIG. 32. Empirical probability density functions of the pri ce
increments in the Maslov model. In inset, log-log plot of the
positive increments. Reproduced from Maslov (2000).
2. Introducing market orders
Maslov (2000) keeps the zero-intelligence structure of
the Bak et al.(1997) model but adds more realistic fea-
tures in the order placement and evolution of the mar-
ket. First, limit orders are submitted and stored in the
model, without moving. Second, limit orders are sub-
mitted around the best quotes. Third, market orders are
submittedtotriggertransactions. Moreprecisely,ateach
time step, a trader is chosen to perform an action. This
trader can either submit a limit order with probability ql
or submit a market order with probability 1 −ql. Once
this choice is made, the order is a buy or sell order with
equal probability. All orders have a one unit volume.
As usual, we denote p(t) the current price. In case the
submitted order at time step t+ 1 is a limit ask (resp.
bid) order, it is placed in the book at price p(t) + ∆
(resp.p(t)−∆), ∆ being a random variable uniformly
distributed in ]0;∆M= 4]. In case the submitted order
at time step t+ 1 is a market order, one order at the
opposite best quote is removed and the price p(t+1) is
recorded. In order to prevent the number of orders in
the order book from large increase, two mechanisms are
proposed by the author: either keeping a ﬁxed maximum
number of orders (by discarding new limit orders when
this maximum isreached), orremovingthem afteraﬁxed
lifetime if they have not been executed.
Numerical simulations show that this model exhibits
non-Gaussian heavy-tailed distributions of returns. On
ﬁgure 32, the empirical probability density of the price
increments for several time scales are plotted. For a time
scaleδt= 1, the author ﬁt the tails distribution with a
power law with exponent 3 .0, i.e. reasonable compared
to empirical value. However, the Hurst exponent of the
price series is still H= 1/4 with this model. It should
also be noted that Slanina (2001) proposed an analyticalstudyofthemodelusingamean-ﬁeldapproximation(See
below section VIIE).
This model brings very interesting innovations in or-
der book simulation: order book with (ﬁxed) limit or-
ders, market orders, necessity to cancel orders waiting
too long in the order book. These features are of prime
importance in any following order book model.
3. The order book as a deposition-evaporation process
Challet and Stinchcombe (2001) continue the work of
Baket al.(1997) and Maslov (2000), and develop the
analogy between dynamics of an order book and an in-
ﬁnite one dimensional grid, where particles of two types
(ask and bid) are subject to three types of events: de-
position (limit orders), annihilation (market orders) and
evaporation (cancellation). Note that annihilation oc-
curs when a particle is deposited on a site occupied by
a particle of another type. The analogy is summarized
in table II. Hence, the model goes as follows: At each
TABLE II. Analogy between the deposition-evaporation pro-
cess and the order book in Challet and Stinchcombe (2001).
Physics Challet and Stinchcombe (2001)
Particles Orders
Inﬁnite lattice Order book
Deposition Limit orders submission
Evaporation Limit orders cancellation
Annihilation Transaction
time step, a bid (resp. ask) order is deposited with prob-
abilityλat a pricen(t) drawn according to a Gaussian
distribution centred on the best ask a(t) (resp. best bid
b(t)) and with variance depending linearly on the spread
s(t) =a(t)−b(t):σ(t) =Ks(t)+C. Ifn(t)>a(t) (resp.
n(t)<b(t)), then it is a market order: annihilation takes
place and the price is recorded. Otherwise, it is a limit
order and it is stored in the book. Finally, each limit or-
der stored in the book has a probability δto be cancelled
(evaporation).
Figure 33 shows the averagereturn as a function of the
time scale. It appears that the seriesof price returnssim-
ulatedwith thismodel exhibitaHurstexponent H= 1/4
forshorttimescales, andthattendsto H= 1/2forlarger
time scales. This behaviour might be the consequence of
the randomevaporationprocess(which wasnot modelled
in Maslov (2000), where H= 1/4 for large time scales).
Although some modiﬁcations of the process (more than
oneorderpertime step) seem toshortenthesub-diﬀusive
region, it is clear that no over-diﬀusive behaviour is ob-
served.",2009-09-10T15:25:45Z,emical mas lov ireproduced mas lov introduci mas lov bak rst second third  precisely  once all as iiinumerical ofor hurt it spai  tse t chal  stinchbake mas lov note t nce at analogy chal  stinchpsics chal  stinchpartiorrs iorr positlimit evaatlimit annihattransaks  otrwise nally  it hurt eonent  mas lov although
paper_qf_47.pdf,26,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","26
FIG. 33. Average return /angbracketleftr∆t/angbracketrightas a function of ∆ tfor diﬀer-
ent sets of parameters and simultaneous depositions allowe d
in the Challet and Stinchcombe model. Reproduced from
Challet and Stinchcombe (2001).
D. Empirical zero-intelligence models
The three models presented in the previous section
VIIC have successively isolated essential mechanisms
that are to be used when simulating a “realistic” mar-
ket: one order is the smallest entity of the model; the
submission of one order is the time dimension (i.e. event
time is used, not an exogenous time deﬁned by mar-
ket clearing and “tatonnement” on exogenous supply
and demand functions); submission of market orders
(as such in Maslov (2000), as “crossing limit orders” in
Challet and Stinchcombe (2001)) and cancellation of or-
ders are taken into account. On the one hand, one may
try to describe these mechanisms using a small number
of parameters, using Poisson process with constant rates
for orderﬂows, constantvolumes, etc. This might lead to
some analytically tractable models, as will be described
in section VIIE. On the other hand, one may try to
ﬁt more complex empirical distributions to market data
without analytical concern.
This type of modelling is best represented by
Mike and Farmer (2008). It is the ﬁrst model that pro-
poses an advanced calibration on the market data as
for order placement and cancellation methods. As for
volume and time of arrivals, assumptions of previous
models still hold: all orders have the same volume, dis-
crete event time is used for simulation, i.e. one order
(limit or market) is submitted per time step. Following
Challet and Stinchcombe (2001), there is no distinction
between market and limit orders, i.e. market orders are
limit orders that are submitted across the spread s(t).
More precisely, at each time step, one trading order is
simulated: an ask (resp. bid) trading order is randomly
placed atn(t) =a(t) +δa(resp.n(t) =b(t) +δb)
according to a Student distribution with scale and de-
FIG. 34. Lifetime of orders for simulated data in the Mike
and Farmer model, compared to the empirical data used for
ﬁtting. Reproduced from Mike and Farmer (2008).
grees of freedom calibrated on market data. If an ask
(resp. bid) order satisﬁes δa<−s(t) =b(t)−a(t) (resp.
δb>s(t) =a(t)−b(t)), then it is a buy (resp. sell) mar-
ket order and a transaction occurs at price a(t) (resp.
b(t).
During a time step, several cancellations of orders may
occur. The authors propose an empirical distribution for
cancellationbased onthreecomponentsfora givenorder:
•the position in the order book, measured as the
ratioy(t) =∆(t)
∆(0)where ∆(t) is the distance of the
order from the opposite best quote at time t,
•the order book imbalance, measured by the in-
dicatorNimb(t) =Na(t)
Na(t)+Nb(t)(resp.Nimb(t) =
Nb(t)
Na(t)+Nb(t)) for ask (resp. bid) orders, where Na(t)
andNb(t) are the number of orders at ask and bid
in the book at time t,
•the total number N(t) =Na(t)+Nb(t) of orders in
the book.
Their empirical study leads them to assume that the
cancellation probability has an exponential dependance
ony(t), a linear one in Nimband ﬁnally decreases ap-
proximately as 1 /Nt(t) as for the total number of orders.
Thus, the probability P(C|y(t),Nimb(t),Nt(t)) to cancel
an ask order at time tis formally written :
P(C|y(t),Nimb(t),Nt(t)) =A(1−e−y(t))(Nimb(t)+B)1
Nt(t),
(21)
where the constants AandBare to be ﬁtted on mar-
ket data. Figure 34 shows that this empirical formula
provides a quite good ﬁt on market data.
Finally, the authors mimic the observed long memory
of order signs by simulating a fractional Brownian mo-
tion. Theauto-covariancefunctionΓ( t) ofthe increments
of such a process exhibits a slow decay :
Γ(k)∼H(2H−1)t2H−2(22)",2009-09-10T15:25:45Z,ge chal  stinchreproduced chal  stinchemical t mas lov chal  stinchopoiso o mike farmer it as followi chal  stinch stunt ime mike farmer reproduced mike farmer   t nim na na nb nim nb na nb na nb na nb tir nim band nt  nim nt nim nt nim nt and bare  nally brownish t auto
paper_qf_47.pdf,27,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","27
FIG. 35. Cumulative distribution of returns in the Mike and
Farmer model, compared to the empirical data used for ﬁt-
ting. Reproduced from Mike and Farmer (2008).
and it is therefore easy to reproduce exponent βof the
decay of the empirical autocorrelation function of order
signs observed on the market with H= 1−β/2.
The results of this empirical model are quite satisfying
as for return and spread distribution. The distribution
of returns exhibit fat tails which are in agreement with
empirical data, as shown on ﬁgure 35. The spread distri-
bution is also very well reproduced. As their empirical
model has been built on the data of only one stock, the
authors test their model on 24 other data sets of stocks
on the same market and ﬁnd for half of them a good
agreement between empirical and simulated properties.
However, the bad results of the other half suggest that
such a model is still far from being “universal”.
Despite these very nice results, some drawbacks have
to be pointed out. The ﬁrst one is the fact that the sta-
bility of the simulated order book is far from ensured.
Simulations using empirical parameters in the simula-
tions may bring situations where the order book is emp-
tied by large consecutive market orders. Thus, the au-
thors require that there is at least two orders in each
side of the book. This exogenous trick might be impor-
tant, since it is activated precisely in the case of rare
events that inﬂuence the tails of the distributions. Also,
the original model does not focus on volatility clustering.
Gu and Zhou (2009) propose a variant that tackles this
feature. Another important drawbackof the model is the
way order signs are simulated. As noted by the authors,
using an exogenous fractional Brownian motion leads to
correlated price returns, which is in contradiction with
empirical stylized facts. We also ﬁnd that at long time
scales it leads to a dramatic increase of volatility. As we
have seen in the ﬁrst part of the review, the correlation
of trade signs can be at least partly seen as an artefact
of execution strategies. Therefore this element is one of
the numerous that should be taken into account when
“programming” the agents of the model. In order to do
so, we have to leave the (quasi) “zero-intelligence” worldand see how modelling based on heterogeneous agents
might help to reproduce non-trivial behaviours. Prior to
this development below in VIIF, we brieﬂy review some
analytical works on the “zero-intelligence” models.
E. Analytical treatments of zero-intelligence models
In this section we present some analytical results ob-
tained on zero-intelligence models where processes are
kept suﬃciently simple so that a mean-ﬁeld approxima-
tionmaybe derived(Slanina(2001))orprobabilitiescon-
ditionaly to the state of the orderbook may be computed
(Contet al.(2008)). The key assumptions here are such
that the process describing the order book is stationary.
This allows either to write a stable density equation, or
to ﬁt the model into a nice mathematical frameworksuch
as ergodic Markov chains.
1. Mean-ﬁeld theory
Slanina (2001) proposes an analytical treatment of the
model introduced by Maslov (2000) and reviewed above.
Let us brieﬂy described the formalism used. The main
hypothesis is the following: on each side of the current
price level, the density of limit orders is uniform and con-
stant (and ρ+on the ask side, ρ−on the bid side). In
that sense, this is a “mean-ﬁeld”approximationsince the
individual position of a limit order is not taken into ac-
count. Assuming we are in a stable state, the arrival of
a market order of size son the ask (resp. bid) side will
make the price change by x+=s/ρ+(resp.x−=s/ρ−).
It is then observed that the transformationsof the vector
X= (x+,x−) occurring at each event (new limit order,
new buy market order, new sell market order) are linear
transformation that can easily and explicitly be written.
Therefore, an equation satisﬁed by the probability dis-
tributionPof the vector Xof price changes can be ob-
tained. Finally, assuming further simpliﬁcations (such as
ρ+=ρ−), one can solve this equation for a tail exponent
and ﬁnd that the distribution behaves as P(x)≈x−2for
largex. This analytical result is slightly diﬀerent from
the one obtained by simulation in Maslov (2000). How-
ever, the numerous approximationsmake the comparison
diﬃcult. The main point here is that some sort of mean-
ﬁeld approximation is natural if we assume the existence
of a stationary state of the order book, and thus may
help handling order book models.
Smithet al.(2003) also propose some sort of mean-
ﬁeld approximation for zero-intelligence models. In a
similar model (but including a cancellation process),
mean ﬁeld theory and dimensional analysis produces in-
teresting results. For example, it is easy to see that the
book depth (i.e. number of orders) Ne(p) at a price pfar
awayfromthebestquotesisgivenby Ne(p) =λ/δ, where
λis the rate of arrival of limit orders per unit of time and
per unit of price, and δthe probability for an order to",2009-09-10T15:25:45Z,cumulative mike farmer reproduced mike farmer t t t as spite t simulatns   also gu hou anotr as brownish  as trefore iprr analytical ispaiconte t  mark measpaimas lov  t iassumi it trefore of of nally  mas lov how t smith et ifor ne ne
paper_qf_47.pdf,28,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","28
be cancelled per unit of time. Indeed, far from the best
quotes no market orders occurs, so that if a steady-state
exists, the number of limit orders par time step λmust
be balanced by the number of cancellation δNe(p) per
unit of time, hence the result.
2. Explicit computation of probabilities conditionally on
the state of the order book
Contet al.(2008) is an original attempt at analyti-
cal treatments of limit order books. In their model, the
price is contrained to be on a grid {1,...,N}. The state
of the order book can then be described by a vector
X(t) = (X1(t),...,X N(t)) where |Xi(t)|is the quan-
tity oﬀered in the order book at price i. Conventionaly,
Xi(t),i= 1,...,Nis positive on the ask side and neg-
ative on the bid side. As usual, limit orders arrive at
leveliat a constant rate λi, and market orders arrive
at a constant rate µ. Finally, at level i, each order can
be cancelled at a rate θi. Using this setting, Cont et al.
(2008) show that each event (limit order, market order,
cancellation) transforms the vector Xin a simple linear
way. Therefore, it is shown that under reasonable con-
ditions,Xis an ergodic Markov chain, and thus admits
a stationary state. The original idea is then to use this
formalism to compute conditional probabilities on the
processes. More precisely, it is shown that using Laplace
transform, one may explicitly compute the probability of
an increase of the mid price conditionally on the current
state of the order book.
This original contribution could allow explicit evalu-
ation of strategies and open new perspectives in high-
frequency trading. However, it is based on a simple
model that does not reproduce empirical observations
such as volatility clustering. Complex models trying to
include market interactions will not ﬁt into these analyt-
ical frameworks. We review some of these models in the
next section.
F. Towards non-trivial behaviours: modelling market
interactions
In all the models we have reviewed until now, ﬂows
of orders are treated as independent processes. Under
some (strong) modelling constraints, we can see the or-
der book as a Markov chain and look for analytical re-
sults (Cont et al.(2008)). In anycase, even ifthe process
is empirically detailed and not trivial (Mike and Farmer
(2008)), we work with the assumption that orders are in-
dependent and identically distributed. This very strong
(and false) hypothesis is similar to the “representative
agent” hypothesis in Economics: orders being succes-
sively and independently submitted, we may not expect
anything but regular behaviours. Following the work of
economists such as Kirman (1992, 1993, 2002), one has
to translate the heterogeneous property of the marketsinto the agent-based models. Agents are not identical,
and not independent.
In this section we present some toy models imple-
menting mechanisms that aim at bringing heterogeneity:
herding behaviour on markets in Cont and Bouchaud
(2000), trend following behaviour in Lux and Marchesi
(2000) orin Preis et al.(2007), thresholdbehaviourCont
(2007). Most of the models reviewed in this section are
not order book models, since a persistent order book is
not kept during the simulations. They are rather price
models, where the price changes are determined by the
aggregationof excesssupply and demand. However, they
identify essential mechanisms that may clearly explain
some empirical data. Incorporating these mechanisms in
an order book model is not yet achieved but is certainly
a future prospective.
1. Herding behaviour
The model presented in Cont and Bouchaud (2000)
considers a market with Nagents trading a given stock
with price p(t). At each time step, agents choose to
buy or sell one unit of stock, i.e. their demand is
φi(t) =±1,i= 1,...,Nwith probability aor are idle
withprobability1 −2a. Thepricechangeisassumedtobe
linearlylinkedwiththeexcessdemand D(t) =/summationtextN
i=1φi(t)
with a factor λmeasuring the liquidity of the market :
p(t+1) =p(t)+1
λN/summationdisplay
i=1φi(t). (23)
λcan also be interpreted as a market depth, i.e. the ex-
cess demand needed to move the price by one unit. In
order to evaluate the distribution of stock returns from
Eq.(23), we need to know the joint distribution of the
individual demands ( φi(t))1≤i≤N. As pointed out by the
authors, if the distribution of the demand φiis indepen-
dent and identically distributed with ﬁnite variance, then
the Central Limit Theorem stands and the distribution
of the price variation ∆ p(t) =p(t+1)−p(t)will converge
to a Gaussian distribution as Ngoes to inﬁnity.
The idea here is to model the diﬀusion of the informa-
tion among traders by randomly linking their demand
through clusters. At each time step, agents iandjcan
be linked with probability pij=p=c
N,cbeing a param-
eter measuring the degree of clustering among agents.
Therefore, an agent is linked to an average number of
(N−1)pother traders. Once clusters are determined,
the demand are forcedto be identical amongall members
of a given cluster. Denoting nc(t) the number of cluster
at a given time step t,Wkthe size of the k-th cluster,
k= 1,...,n c(t) andφk=±1 its investement decision,
the price variation is then straightforwardly written :
∆p(t) =1
λnc(t)/summationdisplay
k=1Wkφk. (24)",2009-09-10T15:25:45Z,ined ne elicit conte it xi conventnal xi is as nally usi cont itrefore is mark t  place  lex  towards iunr mark cont imike farmer  economics followi kir ma icont  chalux marcs  cont most ty incorati rdi t cont  cha at with t price e is d to be ieq as central limit torem got at trefore once noti t
paper_qf_47.pdf,29,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","29
This modelling is a direct application to the ﬁeld of
ﬁnance of the random graph framework as studied in
Erdos and Renyi (1960). Kirman (1983) previously sug-
gested it in economics. Using these previous theoretical
works,and assumingthat the sizeofacluster Wkandthe
decision taken by its members φk(t) are independent, the
author are able to show that the distribution of the price
variationat time tis the sum of nc(t) independent identi-
cally distributed random variables with heavy-tailed dis-
tributions :
∆p(t) =1
λnc(t)/summationdisplay
k=1Xk, (25)
where the density f(x) ofXk=Wkφkis decaying as :
f(x)∼|x|→∞A
|x|5/2e−(c−1)|x|
W0. (26)
Thus, this simple toy model exhibits fat tails in the dis-
tribution of prices variations, with a decay reasonably
close to empirical data. Therefore, Cont and Bouchaud
(2000) show that taking into account a naive mechanism
of communication between agents (herding behaviour) is
able to drive the model out of the Gaussian convergence
and produce non-trivial shapes of distributions of price
returns.
2. Fundamentalists and trend followers
Lux and Marchesi (2000) proposed a model very much
in line with agent-based models in behavioural ﬁnance,
but where trading rules are kept simple enough so that
they can be identiﬁed with a presumably realistic be-
haviourof agents. This model considersa marketwith N
agents that can be part of two distinct groups of traders:
nftraders are “fundamentalists”, who share an exoge-
nous ideapfof the value of the current price p; andnc
traders are “chartists”(or trend followers), who make as-
sumptions on the price evolution based on the observed
trend (mobile average). The total number of agents is
constant, so that nf+nc=Nat any time. At each time
step, the price can be moved up or down with a ﬁxed
jump size of ±0.01 (a tick). The probability to go up or
downisdirectly linkedtothe excessdemand EDthrough
a coeﬃcient β. The demand of each group of agents is
determined as follows :
•Each fundamentalist trades a volume Vfpropor-
tional, with a coeﬃcient γ, to the deviation of
the current price pfrom the perceived fundamental
valuepf:Vf=γ(pf−p).
•Each chartist trades a constant volume Vc. Denot-
ingn+the number of optimistic (buyer) chartists
andn−the number of pessimistic (seller) chartists,
the excess demand by the whole group of chartists
is written ( n+−n−)Vc.Therefore, assuming that there exists some noise traders
on the market with random demand µ, the global excess
demand is written :
ED= (n+−n−)Vc+nfγ(pf−p)+µ.(27)
The probability that the price goes up (resp. down) is
then deﬁned to be the positive (resp. negative) part of
βED.
As observed in Wyart and Bouchaud (2007), funda-
mentalists are expected to stabilize the market, while
chartists should destabilize it. In addition, following
Cont and Bouchaud (2000), the authors expect non-
trivial features of the price series to results from herding
behaviourand transitionsbetween groupsoftraders. Re-
ferring to Kirman’s work as well, a mimicking behaviour
among chartists is thus proposed. The ncchartists can
changetheirviewonthemarket(optimistic, pessimistic),
their decision being based on a clustering process mod-
elled by an opinion index x=n+−n−
ncrepresenting the
weight of the majority. The probabilities π+andπ−to
switch from one group to another are formally written :
π±=vnc
Ne±U, U=α1x+α2p/v, (28)
wherevis a constant, and α1andα2reﬂect respectively
the weight of the majority’s opinion and the weight of
the observed price in the chartists’ decision. Transi-
tions between fundamentalists and chartists are also al-
lowed, decided by comparison of expected returns (see
Lux and Marchesi (2000) for details).
The authors show that the distribution of returns gen-
erated by their model have excess kurtosis. Using a
Hill estimator, they ﬁt a power law to the fat tails of
the distribution and observe exponents grossly ranging
from 1.9 to 4.6. They also check hints for volatility clus-
tering: absolute returns and squared returns exhibit a
slow decay of autocorrelation, while raw returns do not.
It thus appears that such a model can grossly ﬁt some
“stylized facts”. However, the number of parameters in-
volved, as well as the complicated rules of transition be-
tween agents, make clear identiﬁcation of sources of phe-
nomenons and calibration to market data diﬃcult and
intractable.
Alﬁet al.(2009a,b) provide a somewhat simplifying
view on the Lux-Marchesi model. They clearly identify
thefundamentalistbehaviour,thechartistbehaviour,the
herding eﬀect and the observation of the price by the
agents as four essential eﬀects of an agent-based ﬁnan-
cial model. They show that the number of agents plays
a crucial role in a Lux-Marchesi-type model: more pre-
cisely, the stylized facts are reproduced only with a ﬁnite
number of agents, not when the number of agents grows
asymptotically, in which case the model stays in a fun-
damentalist regime. There is a ﬁnite-size eﬀect that may
prove important for further studies.
The role of the trend following mechanism in produc-
ing non-trivial features in price time series is also studied
in Preiset al.(2007). The startingpoint is anorderbook",2009-09-10T15:25:45Z, ordo re nyi kir mausi and t  trefore cont  chafundamentat lux marcs  t at t through t ea pror  eavc  not vc trefore vc t as wy art  chaicont  chare kir mat t ne trans lux marcs t usi hl ty it al lux marcs ty ty lux marcs tre t  et t
paper_qf_47.pdf,30,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","30
FIG.36. HurstexponentfoundinthePreismodelfor diﬀerent
number of agents when including random demand perturba-
tion and dynamic limit order placement depth. Reproduced
from Preis et al.(2007).
model similar to Challet and Stinchcombe (2001) and
Smithet al.(2003): ateachtime step, liquidityproviders
submit limit orders at rate λand liquidity takers sub-
mit market orders at rate µ. As expected, this zero-
intelligence framework does not produce fat tails in the
distribution of (log-)returns nor an over-diﬀusive Hurst
exponent. Then, a stochastic link between order place-
ment and market trend is added: it is assumed that liq-
uidity providers observing a trend in the market will act
consequently and submit limit orders at a wider depth in
the order book. Although the assumption behind such
a mechanism may not be empirically conﬁrmed (a ques-
tionable symmetry in order placement is assumed) and
should be further discussed, it is interesting enough that
it directly provides fat tails in the log-return distribu-
tions and an over-diﬀusive Hurst exponent H≈0.6−0.7
for medium time-scales, as shown in ﬁgure 36.
3. Threshold behaviour
We ﬁnally review a model focusing primarily on repro-
ducingthestylizedfactofvolatilityclustering,whilemost
of the previous models we have reviewed were mostly fo-
cused on fat tails of log returns. Cont (2007) proposes a
model with a rather simple mechanism to create volatil-
ity clustering. The idea is that volatility clustering char-
acterizes several regimes of volatility (quite periods vs
bursts of activity). Instead of implementing an exoge-
nous change of regime, the author deﬁnes the following
trading rules.
Ateachperiod, anagent i∈ {1,...,N}canissueabuy
or a sell order: φi(t) =±1. Information is represented
by a series of i.i.d Gaussian random variables. ( ǫt). This
public information ǫtis a forecast for the value rt+1of
the return of the stock. Each agent i∈ {1,...,N}de-cides whether to follow this information according to a
thresholdθi>0 representing its sensibility to the public
information:
φi(t) =

1 ifǫi(t)>θi(t)
0 if|ǫi(t)|<θi(t)
−1 ifǫi(t)<−θi(t)(29)
Then, onceeverychoiceismade, thepriceevolvesaccord-
ing to the excess demand D(t) =/summationtextN
i=1φi(t), in a way
similartoCont and Bouchaud(2000). At the endofeach
time stept, threshold are asynchronously updated. Each
agent has a probability sto update its threshold θi(t).
In such a case, the new threshold θi(t+1) is deﬁned to
be the absolute value |rt|of the return just observed. In
short:
θi(t+1) =1{ui(t)<s}|rt|+1{ui(t)>s}θi(t).(30)
The author shows that the time series simulated with
such a model do exhibit some realistic facts on volatility.
In particular, long range correlations of absolute returns
is observed. The strength of this model is that it di-
rectly links the state of the market with the decision of
the trader. Such a feedback mechanism is essential in
order to obtain non trivial characteristics. Of course, the
model presented in Cont (2007) is too simple to be fully
calibrated on empirical data, but its mechanism could be
used in a more elaborate agent-based model in order to
reproduce the empirical evidence of volatility clustering.
G. Remarks
Let us attempt to make some concluding remarks
on these developments of agent-based models for order
books. In table III, we summarize some key features of
some of the order book models reviewed in this section.
Among important elements for future modelling, we may
mention the cancellation of orders, which is the less real-
istic mechanism implemented in existing models ; the or-
der book stability, which is always exogenously enforced
(see our review of Mike and Farmer (2008) above) ; and
the dependence between order ﬂows (see e.g. Muni Toke
(2010) and reference therein). Empirical estimation of
these mechanisms is still challenging.
Emphasis has been put in this section on order book
modelling, a ﬁeld that is at the crossroad of many larger
disciplines (market microstructure, behavioural ﬁnance
and physics). Market microstructure is essential since
it deﬁnes in many ways the goal of the modelling. We
pointed out that it is not a coincidence if the work by
Garman (1976) was published when computerization of
exchanges was about to make the electronic order book
the key of all trading. Regulatory issues that pushed
early studies are still very important today. Realistic
order book models could be a invaluable tool in testing
and evaluating the eﬀects of regulations such as the 2005",2009-09-10T15:25:45Z,hurt eonent found it  mfor reproduced  chal  stinchsmith et as hurt talthough hurt threshold  cont t instead at ead informat eatcont  chaat eaiit it suof cont remarks  iamo mike farmer muni take emical emphasis market  germaregulatory reatic
paper_qf_47.pdf,31,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","31
Regulation NMS5in the USA, or the 2007 MiFID6in
Europe.
VIII. AGENT-BASED MODELLING FOR WEALTH
DISTRIBUTIONS: KINETIC THEORY MODELS
The distributions ofmoney, wealth or income, i.e., how
such quantities are shared among the population of a
given country and among diﬀerent countries, is a topic
which has been studied by economists for a long time.
The relevance of the topic to us is twofold: From the
point of view of the science of Complex Systems, wealth
distributions represent a unique example of a quantita-
tive outcome of a collective behavior which can be di-
rectly compared with the predictions of theoretical mod-
els and numerical experiments. Also, there is a basic
interest in wealth distributions from the social point of
view, in particular in their degree of (in)equality. To this
aim, the Gini coeﬃcient (or the Gini index, if expressed
as a percentage), developed by the Italian statistician
Corrado Gini, represents a concept commonly employed
to measure inequality of wealth distributions or, more
in general, how uneven a given distribution is. For a
cumulative distribution function F(y), that is piecewise
diﬀerentiable, has a ﬁnite mean µ, and is zero for y<0,
the Gini coeﬃcient is deﬁned as
G= 1−1
µ/integraldisplay∞
0dy(1−F(y))2
=1
µ/integraldisplay∞
0dyF(y)(1−F(y)). (31)
It can also be interpreted statistically as half the relative
mean diﬀerence. Thus the Gini coeﬃcient is a number
between 0 and 1, where 0 correspondswith perfect equal-
ity (where everyone has the same income) and 1 corre-
sponds with perfect inequality (where one person has all
the income, and everyone else has zero income). Some
values ofGfor some countries are listed in Table IV.
Let us start by considering the basic economic quanti-
ties: money, wealth and income.
A. Money, wealth and income
A common deﬁnition of moneysuggests that money is
the“[c]ommodityacceptedbygeneralconsentasmedium
of economics exchange”7. In fact, money circulates from
one economic agent (which can represent an individual,
ﬁrm, country, etc.) to another, thus facilitating trade. It
is“somethingwhichallothergoodsorservicesaretraded
5National Market System
6Markets in Financial Instruments Directive
7In Encyclopædia Britannica. Retrieved June 17, 2010, from E n-
cyclopædia Britannica Onlinefor” (for details see Shostak (2000)). Throughout history
various commodities have been used as money, for these
cases termed as “commodity money”, which include for
example rare seashells or beads, and cattle (such as cow
in India). Recently, “commodity money” has been re-
placed by other forms referred to as “ﬁat money”, which
have gradually become the most common ones, such as
metal coins and paper notes. Nowadays, other forms of
money, such as electronic money, have become the most
frequent form used to carry out transactions. In any case
the most relevant points about money employed are its
basic functions, which according to standard economic
theory are
•to serve as a medium of exchange, which is univer-
sally accepted in trade for goods and services;
•to act as a measure of value, making possible the
determination of the prices and the calculation of
costs, or proﬁt and loss;
•to serve as a standard of deferred payments, i.e., a
tool for the payment of debt or the unit in which
loans are made and future transactions are ﬁxed;
•to serve as a means of storing wealth not immedi-
ately required for use.
A related feature relevant for the present investigation is
that money is the medium in which prices or values of all
commodities as well as costs, proﬁts, and transactions
can be determined or expressed. Wealthis usually un-
derstood as things that have economic utility (monetary
value or value of exchange), or material goods or prop-
erty; it also represents the abundance of objects of value
(or riches) and the state of having accumulated these ob-
jects; for our purpose, it is important to bear in mind
that wealth can be measured in terms of money. Also
income, deﬁned in Case and Fair (2008) as “the sum of
all the wages, salaries, proﬁts, interests payments, rents
and other forms of earnings received... in a given period
of time”, is a quantity which can be measured in terms
of money (per unit time).
B. Modelling wealth distributions
It wasﬁrstobservedbyPareto(1897b) that in anecon-
omy the higher end of the distribution of income f(x)
follows a power-law,
f(x)∼x−1−α, (32)
withα, now known as the Pareto exponent, estimated
by him to be α≈3/2. For the last hundred years
the value of α∼3/2 seems to have changed little in
time and across the various capitalist economies (see
Yakovenko and Rosser (2009) and references therein).
Gibrat (1931) clariﬁed that Pareto’s law is valid only
for the high income range, whereas for the middle in-
come range he suggested that the income distribution is",2009-09-10T15:25:45Z,regulatmi  t t from lex tems also to iiitaliatornado ifor iit  isome for table  money iit natnal market tem markets nancial instruments direive iencybritannica rieved june britannica online for host ak throughout india recently adays ialth is also case articial intellence molli it party party for v e roster gi brat party
paper_qf_47.pdf,32,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","32
Model Stigler (1961) Garman (1976) Bak, Paczuski
and Shubik
(1997)Maslov (2000) Challet and
Stinchcombe
(2001)Mike and Farmer
(2008)
Price
rangeFinite grid Finite grid Finite grid Unconstrained Unconstrained Unconstrained
Clock Trade time Physical Time Aggregated time Event time Aggregated time Aggregated time
Flows /
AgentsOne zero-
intelligence agent
/ One ﬂowOne zero-
intelligence
agent / Two ﬂows
(buy/sell)N agents owning
each one limit or-
derOne zero-
intelligence ﬂow
(limit order with
ﬁxed probability,
else market order)One zero-
intelligence agent
/ One ﬂowOne zero-
intelligence agent
/ One ﬂow
Limit
ordersUniform distribu-
tion on the price
gridTwo Poisson pro-
cesses for buy and
sell ordersMoving at each
time step by one
tickUniformly dis-
tributed in a
ﬁnite interval
around last priceNormally dis-
tributed around
best quoteStudent-
distributed
around best
quote
Market
ordersDeﬁned as cross-
ing limit ordersDeﬁned as cross-
ing limit ordersDeﬁned as cross-
ing limit ordersSubmittedas such Deﬁned as cross-
ing limit ordersDeﬁned as cross-
ing limit orders
Cancel-
lation
ordersPending orders
are cancelledafter
a ﬁxed number of
time stepsNone None (constant
number of pend-
ing orders)Pending orders
are cancelledafter
a ﬁxed number of
time stepsPending orders
can be cancelled
with ﬁxed prob-
ability at each
time stepPending orders
can be cancelled
with 3-parameter
conditional prob-
ability at each
time step
Volume Unit Unit Unit Unit Unit Unit
Order
signsIndependent Independent Independent Independent Independent Correlated with a
fractional Brown-
ian motion
Claimed
resultsReturn distribu-
tion is power-law
0.3 / Cut-oﬀ be-
cause ﬁnite gridMicrostructure
is responsible
for negative
correlation of
consecutive price
changesNo fat tails for re-
turns / Hurst ex-
ponent 1/4 for
price incrementsFat tails for distri-
butions of returns
/ Hurst exponent
1/4Hurst exponent
1/4 for short time
scales, tending
to 1/2 for larger
time scalesFat tails distribu-
tions of returns
/ Realistic spread
distribution / Un-
stable order book
TABLE III. Summary of the characteristics of the reviewed li mit order book models.
described by a log-normal probability density
f(x)∼1
x√
2πσ2exp/braceleftbigg
−log2(x/x0)
2σ2/bracerightbigg
,(33)
where log(x0) =∝an}bracketle{tlog(x)∝an}bracketri}htis the mean value of the loga-
rithmic variable and σ2=∝an}bracketle{t[log(x)−log(x0)]2∝an}bracketri}htthe cor-
responding variance. The factor β= 1/√
2σ2, also know
an as Gibrat index, measures the equality of the distri-
bution.
More recent empirical studies on income distribu-
tion have been carried out by physicists, e.g. those
by Dragulescu and Yakovenko (2001b,a) for UK and
US, by Fujiwara et al.(2003) for Japan, and by
Nirei and Souma (2007) for US and Japan. For an
overview see Yakovenko and Rosser (2009). The distri-
butions obtained have been shown to follow either the
log-normal (Gamma like) or power-law types, depending
on the range of wealth, as shown in Fig. 37.
One of the current challenges is to write down the
“microscopic equation” which governs the dynamics of
the evolution of wealth distributions, possibly predict-
ing the observed shape of wealth distributions, in-
cluding the exponential law at intermediate values of
wealth as well as the century-old Pareto law. To this
aim, several studies have been made to investigate the
FIG. 37. Income distributions in the US (left)and
Japan (right). Reproduced and adapted from
Chakrabarti and Chatterjee (2003), available at
arXiv:cond-mat/0302147 .
characteristics of the real income distribution and pro-
vide theoretical models or explanations (see e.g. re-
viewsbyLux(2005), Chatterjee and Chakrabarti(2007),
Yakovenko and Rosser (2009)).
The model of Gibrat (1931) and other models",2009-09-10T15:25:45Z,mter germabak pac zu  sh ubi mas lov chal  stinchmike farmer price nite nite nite uconst articial intellence ned uconst articial intellence ned uconst articial intellence ned ock tra psical time aregated event aregated aregated flows  one one one two one one one one one limit unorm two poisomovi unormly normally stunt market    submied as   cancel pendi none none pendi pendi pendi volume unit unit unit unit unit unit orr inpennt inpennt inpennt inpennt inpennt correlated brow articial intellence med urcut m struure no hurt fat hurt hurt fat reatic usuary t gi brat  drag ul escu v e fiwara  ni rei so uma  for v e roster t gaa  one party to income  reproduced chara bart terjee  lux terjee chara bart v e roster t gi brat
paper_qf_47.pdf,33,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","33
TABLE IV. Gini indices (in percent) of some countries
(fromHuman Development Indicators of the United Na-
tions Human Development Report 2004, pp.50-53, available at
http://hdr.undp.org/en/reports/global/hdr2004 . More
recent data are also available from their website.)
Denmark 24.7
Japan 24.9
Sweden 25.0
Norway 25.8
Germany 28.3
India 32.5
France 32.7
Australia 35.2
UK 36.0
USA 40.8
Hong Kong 43.4
China 44.7
Russia 45.6
Mexico 54.6
Chile 57.1
Brazil 59.1
South Africa 59.3
Botswana 63.0
Namibia 70.7
formulated in terms of a Langevin equation for a
single wealth variable, subjected to multiplicative
noise (Mandelbrot (1960); Levy and Solomon (1996);
Sornette (1998); Burda et al.(2003)), can lead to
equilibrium wealth distributions with a power law tail,
since they converge toward a log-normal distribution.
However, the ﬁt of real wealth distributions does not
turn out to be as good as that obtained using e.g. a
Γ- or aβ-distribution, in particular due to too large
asymptotic variances (Angle (1986)). Other models use
a diﬀerent approach and describe the wealth dynamics
as a wealth ﬂow due to exchanges between (pairs of)
basic units. In this respect, such models are basically
diﬀerent from the class of models formulated in terms
of a Langevin equation for a single wealth variable.
For example, Solomon and Levy (1996) studied the
generalized Lotka-Volterra equations in relation to
power-law wealth distribution. Ispolatov et al.(1998)
studied random exchange models of wealth distri-
butions. Other models describing wealth exchange
have been formulated using matrix theory (Gupta
(2006)), the master equation (Bouchaud and Mezard
(2000); Dragulescu and Yakovenko (2000);
Ferrero (2004)), the Boltzmann equation approach
(Dragulescu and Yakovenko (2000); Slanina (2004);
Repetowicz et al.(2005); Cordier et al.(2005);
Matthes and Toscani (2007); D¨ uring and Toscani
(2007); D¨ uring et al.(2008)), or Markov chains
(Scalaset al.(2006, 2007); Garibaldi et al.(2007)).It should be mentioned that one of the earliest mod-
elling eﬀorts were made by Champernowne (1953).
Since then many economists, Gabaix (1999) and
Benhabib and Bisin (2009) amongst others, have also
studied mechanisms for power laws, and distributions of
wealth.
In the two following sections we consider in greater
detail a class of models usually referred to as ki-
netic wealth exchange models (KWEM), formulated
throughﬁnite time diﬀerencestochasticequations(Angle
(1986, 2002, 2006); Chakraborti and Chakrabarti
(2000); Dragulescu and Yakovenko(2000); Chakraborti
(2002); Hayes (2002); Chatterjee et al.(2003);
Das and Yarlagadda (2003); Scafetta et al.(2004);
Iglesiaset al.(2003, 2004); Ausloos and Pekalski
(2007)). From the studies carried out using wealth-
exchange models, it emerges that it is possible to use
them to generate power law distributions.
C. Homogeneous kinetic wealth exchange models
Here and in the next section we consider KWEMs,
which are statistical models of closed economy. Their
goal, rather then describing the market dynamics in
terms of intelligent agents, is to predict the time evo-
lution of the distribution of some main quantity, such
as wealth, by studying the corresponding ﬂow process
among individuals. The underlying idea is that however
complicated the detailed rules of wealth exchanges can
be, their average behaviour can be described in a rel-
atively more simple way and will share some universal
properties with other transport processes, due to general
conservationconstraintsand the eﬀect ofthe ﬂuctuations
duetotheenvironmentorassociatedtotheindividualbe-
haviour. In this, there is a clear analogy with the general
theory of transport phenomena (e.g. of energy).
Inthesemodelsthestatesofagentsaredeﬁnedinterms
of the wealth variables {xn}, n= 1,2,...,N. The evo-
lution of the system is carried out according to a trading
rulebetween agentswhich, forobtainingthe ﬁnalequilib-
rium distribution, can be interpreted as the actual time
evolution of the agent states as well as a Monte Carlo
optimization. The algorithm is based on a simple update
rule performed at each time step t, when two agents i
andjare extracted randomly and an amount of wealth
∆xis exchanged,
x′
i=xi−∆x,
x′
j=xj+∆x. (34)
Notice that the quantity xis conserved during single
transactions, x′
i+x′
j=xi+xj, wherexi=xi(t)
andxj=xj(t) are the agent wealth before, whereas
x′
i=xi(t+ 1) andx′
j=xj(t+ 1) are the ﬁnal ones
after the transaction. Several rules have been studied
for the model deﬁned by Eqs. (34). It is noteworthy,
that though this theory has been originally derived from",2009-09-10T15:25:45Z,ihumavelopment indicators united na humavelopment ret  nmark  snorway germany india france australia ho    me c braz south africa botswana namibia aelic manlbrot levy solomocorvee bur da ale otr iaelic for solomolevy lot ka volterra is poll to otr gupta  chame hard drag ul escu v e  bolt mandrag ul escu v e spaire pet owicz cord er mat t to cato camark scale set garibaldi it ham  ne since ga articial intellence behabit is iiale chara bor ti chara bart drag ul escu v e chara bor ti hayterjee das yar lag add caf eta lesia set us loss pek al  from homogeneous re ms tir t iitse mols t statof  are  t   t notice sevl it
paper_qf_47.pdf,34,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","34
the entropymaximization principle ofstatisticalmechan-
ics, it has recently been shown that the same could be
derived from the utility maximization principle as well,
following a standard exchange-modelwith Cobb-Douglas
utility function (as explained later), which bridge physics
and economics together.
1. Exchange models without saving
In a simple version of KWEM considered in the
works by Bennati (1988a,b, 1993) and also studied by
Dragulescu and Yakovenko (2000) the money diﬀerence
∆xin Eqs. (34) is assumed to have a constant value,
∆x= ∆x0. Together with the constraint that transac-
tions can take place only if x′
i>0 andx′
j>0, this leads
to an equilibrium exponential distribution, see the curve
forλ= 0 in Fig. 38.
Various other trading rules were studied by
Dragulescu and Yakovenko (2000), choosing ∆ xas
a random fraction of the average money between the
two agents, ∆ x=ǫ(xi+xj)/2, corresponding to a
∆x= (1−ǫ)xi−ǫxjin (34), or of the average money of
the whole system, ∆ x=ǫ∝an}bracketle{tx∝an}bracketri}ht.
The models mentioned, as well as more complicated
ones (Dragulescu and Yakovenko (2000)), lead to an
equilibrium wealth distribution with an exponential tail
f(x)∼βexp(−βx), (35)
with the eﬀective temperature 1 /βof the order of the
average wealth, β−1=∝an}bracketle{tx∝an}bracketri}ht. This result is largely inde-
pendent of the details of the models, e.g. the multi-agent
nature of the interaction, the initial conditions, and the
randomorconsecutiveorderofextractionofthe interact-
ing agents. The Boltzmann distribution is characterized
by a majority of poor agents and a few rich agents (due
to the exponential tail), and has a Gini coeﬃcient of 0 .5.
2. Exchange models with saving
As a generalization and more realistic version of the
basic exchange models, a saving criterion can be intro-
duced. Angle (1983), motivated by the surplus theory,
introduced a unidirectional model of wealth exchange, in
which only a fraction of wealth smaller than one can pass
from one agent to the other, with a ∆ x=ǫxior (−ωxj),
wherethe direction ofthe ﬂowis determined bythe agent
wealth (Angle (1983, 1986)). Later Angle introduced the
One-Parameter Inequality Process (OPIP) where a con-
stant fraction 1 −ωis savedbefore the transaction(Angle
(2002)) by the agent whose wealth decreases, deﬁned by
an exchanged wealth amount ∆ x=ωxior−ωxj, again
with the direction of the transaction determined by the
relative diﬀerence between the agent wealth.
A “saving parameter” 0 < λ < 1 representing the
fraction of wealth saved, was introduced in the model 0 0.5 1 1.5 2 2.5
 0  0.5  1  1.5  2  2.5  3  3.5f(x)
xλ = 0
λ = 0.2
λ = 0.5
λ = 0.7
λ = 0.9
 1e-07 1e-06 1e-05 0.0001 0.001 0.01 0.1 1 10
 0 1 2 3 4 5 6 7 8 9 10f(x)
xλ = 0
λ = 0.2
λ = 0.5
λ = 0.7
λ = 0.9
FIG. 38. Probability density for wealth x. The curve for
λ= 0is theBoltzmann function f(x) =/angbracketleftx/angbracketright−1exp(−x//angbracketleftx/angbracketright)for
the basic model of Sec. VIIIC1. The other curves correspond
to a global saving propensity λ >0, see Sec. VIIIC2.
by Chakraborti and Chakrabarti (2000). In this model
(CC) wealth ﬂows simultaneously toward and from each
agent during a single transaction, the dynamics being
deﬁned by the equations
x′
i=λxi+ǫ(1−λ)(xi+xj),
x′
j=λxj+(1−ǫ)(1−λ)(xi+xj),(36)
or, equivalently, by a ∆ xin (34) given by
∆x= (1−λ)[(1−ǫ)xi−ǫxj]. (37)
These models, apart from the OPIP model of Angle
which has the remarkable property of leading to a power
law in a suitable range of ω, can be well ﬁtted by a Γ-
distribution. The Γ-distribution is characterized by a
modexm>0, in agreement with real data of wealth
and income distributions (Dragulescu and Yakovenko
(2001a); Ferrero (2004); Silva and Yakovenko (2005);
Sala-i Martin and Mohapatra (2002); Sala-i Martin
(2002); Aoyama et al.(2003)). Furthermore, the limit",2009-09-10T15:25:45Z,cbb douglas exe ibeeat drag ul escu v e togetr  varus drag ul escu v e t drag ul escu v e  t bolt maniexe as ale ale later ale one meter inequality process ale probabity t bolt mansec t sec chara bor ti chara bart itse ale t drag ul escu v e  sva v e sala martimoh  sala martiobama furtr
paper_qf_47.pdf,35,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","35
TABLE V. Analogy between kinetic the theory of gases and
the kinetic exchange model of wealth
Kinetic model Economy model
variable K(kinetic energy) x(wealth)
units Nparticles Nagents
interaction collisions trades
dimension integer D real number Dλ
temperature deﬁnition kBT= 2/angbracketleftK/angbracketright/D T λ= 2/angbracketleftx/angbracketright/Dλ
reduced variable ξ=K/kBT ξ =x/Tλ
equilibrium distribution f(ξ) =γD/2(ξ)f(ξ) =γDλ/2(ξ)
for smallxis zero, i.e. P(x→0)→0, see the exam-
ple in Fig. 38. In the particular case of the model by
Chakraborti and Chakrabarti (2000), the explicit distri-
bution is well ﬁtted by
f(x) =n∝an}bracketle{tx∝an}bracketri}ht−1γn(nx/∝an}bracketle{tx∝an}bracketri}ht)
=1
Γ(n)n
∝an}bracketle{tx∝an}bracketri}ht/parenleftbiggnx
∝an}bracketle{tx∝an}bracketri}ht/parenrightbiggn−1
exp/parenleftbigg
−nx
∝an}bracketle{tx∝an}bracketri}ht/parenrightbigg
,(38)
n(λ)≡Dλ
2= 1+3λ
1−λ. (39)
whereγn(ξ) is the standard Γ-distribution. This par-
ticular functional form has been conjectured on the base
of the excellent ﬁtting provided to numerical data (Angle
(1983, 1986); Patriarca et al.(2004b,a, 2009)). Formore
information and a comparison of similar ﬁttings for dif-
ferent models see Patriarca et al.(2010). Very recently,
Lallouache et al.(2010) have shown using the distribu-
tional form of the equation and moment calculations
that strictly speaking the Gamma distribution is not
the solution of Eq. (36), conﬁrming the earlier results
of Repetowicz et al.(2005). However, the Gamma dis-
tribution is a very very good approximation.
The ubiquitous presenceof Γ-functions in the solutions
of kinetic models (see also below heterogeneous models)
suggests a close analogy with kinetic theory of gases. In
fact, interpreting Dλ= 2nas an eﬀective dimension,
the variable xas kinetic energy, and introducing the ef-
fective temperature β−1≡Tλ=∝an}bracketle{tx∝an}bracketri}ht/2Dλaccording to
the equipartition theorem, Eqs. (38) and (39) deﬁne the
canonical distribution βγn(βx) for the kinetic energy of
a gas inDλ= 2ndimensions, see Patriarca et al.(2004a)
for details. The analogy is illustrated in Table V and the
dependences of Dλ= 2nand ofβ−1=Tλon the saving
parameterλare shown in Fig. 39.
The exponential distribution is recovered as a special
case, forn= 1. In the limit λ→1, i.e. forn→ ∞, the
distribution f(x) above tends to a Dirac δ-function, as
shown in Patriarca et al.(2004a) and qualitatively illus-
trated by the curves in Fig. 38. This shows that a large
saving criterion leads to a ﬁnal state in which economic
agents tend to have similar amounts of money and, in
the limit of λ→1, exactly the same amount ∝an}bracketle{tx∝an}bracketri}ht.
The equivalence between a kinetic wealth-exchange
model with saving propensity λ≥0 and anN-particle 1 10 100
 0.01  0.1  1Dλ
λ
 0 0.2 0.4 0.6 0.8 1
 0  0.2  0.4  0.6  0.8  1Tλ/〈 x 〉
λ
FIG. 39. Eﬀective dimension Dλand temperature Tas a
function of the saving parameter λ.
system in a space with dimension Dλ≥2 is suggested by
simple considerations about the kinetics of collision pro-
cessesbetweentwomolecules. Inonedimension,particles
undergo head-on collisions in which the whole amount
of kinetic energy can be exchanged. In a larger num-
ber of dimensions the two particles will not travel in
general exactly along the same line, in opposite verses,
and only a fraction of the energy can be exchanged.
It can be shown that during a binary elastic collision
inDdimensions only a fraction 1 /Dof the total ki-
netic energy is exchanged on average for kinematic rea-
sons, see Chakraborti and Patriarca (2008) for details.
The same 1 /Ddependence is in fact obtained inverting
Eq. (39), which provides for the fraction of exchanged
wealth 1 −λ= 6/(Dλ+4).
Not all homogeneous models lead to distributions with
an exponential tail. For instance, in the model studied in
Chakraborti(2002)anagent icanloseallhiswealth,thus
becomingunabletotradeagain: afterasuﬃcientnumber
of transactions, only one trader survives in the market
and owns the entire wealth. The equilibrium distribution
has a very diﬀerent shape, as explained below:",2009-09-10T15:25:45Z,analogy etic economy parti  ichara bor ti chara bart  ale patria rca for  patria rca very lal lou ae gaa eq re pet owicz gaa t ipatria rca t table  t idfrac patria rca   t as ione dimensiit dimensns of chara bor ti patria rca t pennce eq not for chara bor ti t
paper_qf_47.pdf,36,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","36
In the toy model it is assumed that both the economic
agentsiandjinvest the same amount xmin, which is
taken as the minimum wealth between the two agents,
xmin= min{xi,xj}. The wealth after the trade are x′
i=
xi+ ∆xandx′
j=xj−∆x, where ∆x= (2ǫ−1)xmin.
We note that once an agent has lost all his wealth, he
is unable to trade because xminhas become zero. Thus,
a trader is eﬀectively driven out of the market once he
loses all his wealth. In this way, after a suﬃcient number
of transactions only one trader survives in the market
with the entire amount of wealth, whereas the rest of the
traders have zero wealth. In this toy model, only one
agent has the entire money of the market and the rest
of the traders have zero money, which corresponds to a
distribution with Gini coeﬃcient equal to unity.
Now, a situation is said to be Pareto-optimal “if by
reallocation you cannot make someone better oﬀ without
making someone else worse oﬀ”. In Pareto’s own words:
“We will say that the members of a collectiv-
ity enjoy maximum ophelimity in a certain
position when it is impossible to ﬁnd a way
of moving from that position very slightly in
such a manner that the ophelimity enjoyed
by each of the individuals of that collectiv-
ity increases or decreases. That is to say, any
smalldisplacementindepartingfromthatpo-
sition necessarily has the eﬀect of increasing
the ophelimity which certain individuals en-
joy, and decreasing that which others enjoy,
of being agreeable to some, and disagreeable
to others.”
— Vilfredo Pareto, Manual of Political Econ-
omy (1906), p.261.
However, as Sen (1971) notes, an economy can be Pareto-
optimal, yet still “perfectly disgusting” by any ethi-
cal standards . It is important to note that Pareto-
optimality, is merely a descriptive term, a property of an
“allocation”, and there are no ethical propositions about
the desirability of such allocations inherent within that
notion. Thus, in other words there is nothing inherent in
Pareto-optimalitythat impliesthe maximizationofsocial
welfare.
This simple toy model thus also produces a Pareto-
optimal state (it will be impossible to raisethe well-being
of anyone except the winner, i.e., the agent with all the
money, and vice versa ) but the situation is economically
undesirable as far as social welfare is concerned!
Note also, as mentioned above, the OPIP model of
Angle (2006, 2002), for example, depending on the model
parameters, can also produce a power law tail. Another
generalwaytoproduceapowerlawtailintheequilibrium
distribution seems to diversify the agents, i.e. to consider
heterogeneous models, discussed below.
FIG. 40. Results for randomly assigned saving parameters.
Reproduced and adapted from Chakrabarti and Chatterjee
(2003), available at arXiv:cond-mat/0302147 .
D. Heterogeneous kinetic wealth exchange models
1. Random saving propensities
The models considered above assume the all agents
have the same statistical properties. The corresponding
equilibrium wealth distribution has in most of the cases
an exponential tail, a form which well interpolates real
data at small and intermediate values of wealth. How-
ever, it is possible to conceive generalized models which
lead to even more realistic equilibrium wealth distribu-
tions. This is the case when agents are diversiﬁed by
assigning diﬀerent values of the saving parameter. For
instance, Angle (2002) studied a model with a trading
rule where diversiﬁed parameters {ωi}occur,
∆x=ωiǫxior−ωjǫxj, (40)
with the direction of wealth ﬂow determined by the
wealth of agents iandj. Diversiﬁed saving parame-
ters were independently introduced by Chatterjee et al.
(2003, 2004) by generalizing the model introduced in
Chakraborti and Chakrabarti (2000):
x′
i=λixi+ǫ[(1−λi)xi+(1−λj)xj],
x′
j=λxj+(1−ǫ)[(1−λi)xi+(1−λj)xj],(41)
corresponding to a
∆x= (1−ǫ)(1−λi)xi−ǫ(1−λj)xj.(42)
The surprising result is that if the parameters {λi}are
suitably diversiﬁed, a power law appears in the equilib-
rium wealth distribution, see Fig. 40. In particular if the",2009-09-10T15:25:45Z,it   iii party iparty  that redo party manual political ec oseparty it party  party  party note ale anotr results reproduced chara bart terjee  terogeneous random t t how  for ale divers terjee chara bor ti chara bart t  in
paper_qf_47.pdf,37,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","37
λiare uniformly distributed in (0 ,1) the wealth distribu-
tion exhibits a robust power-law tail,
f(x)∝x−α−1, (43)
with the Pareto exponent α= 1 largely independent of
the details of the λ-distribution. It may be noted that
the exponent value unity is strictly for the tail end of
the distribution and not for small values of the income
or wealth (where the distribution remains exponential).
Also, for ﬁnite number Nof agents, there is always an
exponential (in N) cut oﬀ at the tail end of the distri-
bution. This result is supported by independent theoret-
ical considerations based on diﬀerent approaches, such
as a mean ﬁeld theory approach (Mohanty (2006), see
below for further details) or the Boltzmann equation
(Das and Yarlagadda (2003, 2005); Repetowicz et al.
(2005); Chatterjee et al.(2005a)). For derivation of the
Pareto law from variational principles, using the KWEM
context, see Chakraborti and Patriarca (2009).
2. Power-law distribution as an overlap of Gamma
distributions
A remarkable feature of the equilibrium wealth dis-
tribution obtained from heterogeneous models, noticed
in Chatterjee et al.(2004), is that the individual wealth
distribution fi(x)ofthegeneric i-thagentwithsavingpa-
rameterλihas a well deﬁned mode and exponential tail,
in spite of the resulting power-law tail of the marginal
distribution f(x) =/summationtext
ifi(x). In fact, Patriarca et al.
(2005) found by numerical simulation that the marginal
distribution f(x) canbe resolvedasanoverlapofindivid-
ual Gamma distributions with λ-dependent parameters;
furthermore,themodeandtheaveragevalueofthedistri-
butionsfi(x) bothdivergefor λ→1as∝an}bracketle{tx(λ)∝an}bracketri}ht ∼1/(1−λ)
(Chatterjee et al.(2004); Patriarca et al.(2005)). This
fact was justiﬁed theoretically by Mohanty (2006). Con-
sider the evolution equations (41). In the mean ﬁeld ap-
proximation one can consider that each agents ihas an
(average) wealth ∝an}bracketle{txi∝an}bracketri}ht=yiand replace the random num-
berǫwith its average value ∝an}bracketle{tǫ∝an}bracketri}ht= 1/2. Indicating with
yijthe new wealth of agent i, due to the interaction with
agentj, from Eqs. (41) one obtains
yij= (1/2)(1+λi)yi+(1/2)(1−λj)yj.(44)
At equilibrium, for consistency, average over all the in-
teraction must give back yi,
yi=/summationdisplay
jyij/N . (45)
Then summing Eq. (44) over jand dividing by the num-
ber of agents N, one has
(1−λi)yi=∝an}bracketle{t(1−λ)y∝an}bracketri}ht, (46)
where∝an}bracketle{t(1−λ)y∝an}bracketri}ht=/summationtext
j(1−λj)yj/N. Since the right
hand side is independent of iand this relation holds for10  
1  
10-1
10-2
10-3
10-4
10-5
10210110-110-210-3f (x)
x10  
1  
10-1
10-2
10-3
10-4
10-5
5.02.01.00.50.20.1f (x)
x
10-1
10-2
10-3
10-4
10-5
502010521f (x)
x10-3
10-4
10-5
100 50 20 10f (x)
x
FIG. 41. Wealth distribution in a system of 1000 agents
with saving propensities uniformly distributed in the inte r-
val 0< λ <1. Top left: marginal distribution. Top right:
marginal distribution(dottedline) anddistributions ofw ealth
of agents with λ∈(j∆λ,(j+1)∆λ), ∆λ= 0.1,j= 0,...,9
(continuous lines). Bottom-left: the distribution of weal th of
agents with λ∈(0.9,1) has been further resolved into contri-
butions from subintervals λ∈(0.9 +j∆λ,0.9 + (j+ 1)∆λ),
∆λ= 0.01. Bottom-right: the partial distribution of wealth
of agents with λ∈(0.99,1) has been further resolved into
those from subintervals λ∈(0.99 +j∆λ,0.99 + (j+ 1)∆λ),
∆λ= 0.001.
arbitrary distributions of λi, the solution is
yi=C
1−λi, (47)
whereCis a constant. Besides proving the dependence
ofyi=∝an}bracketle{txi∝an}bracketri}htonλi, this relation also demonstrates the
existence of a power law tail in the equilibrium distribu-
tion. If, in the continuous limit, λis distributed in (0 ,1)
with a density φ(λ),(0≤λ <1), then using (47) the
(average) wealth distribution is given
f(y) =φ(λ)dλ
dy=φ(1−C/x)C
y2.(48)
Figure 41 illustrates the phenomenon for a system of
N= 1000 agents with random saving propensities uni-
formly distributed between 0 and 1. The ﬁgure conﬁrms
the importance of agents with λclose to 1 for producing
a power-law probability distribution (Chatterjee et al.
(2004); Patriarca et al.(2009)).
However, when considering values of λclose enough to
1, the power law can break down at least for two reasons.
The ﬁrst one, illustrated in Fig. 41-bottom right, is that
the power-law can be resolved into almost disjoint con-
tributions representing the wealth distributions of single
agents. Thisfollowsfromtheﬁnitenumberofagentsused
and the fact that the distance between the average val-
ues of the distributions corresponding to two consecutive
values ofλgrows faster than the corresponding widths
(Patriarca et al.(2005); Chatterjee et al.(2005b)). The
second reason is due to the ﬁnite cutoﬀ λM, always",2009-09-10T15:25:45Z,party it also of  mohanty bolt mandas yar lag add re pet owicz terjee for party chara bor ti patria rca  gaa terjee ipatria rca gaa terjee patria rca  mohanty coiindicati at teq since alth ttboom boom is besis   t terjee patria rca t   follows from t patria rca terjee t
paper_qf_47.pdf,38,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","38
1  
10-1
10-2
10-3
10-4
10-5
10-6
10-7
 1  10  100  1000f (x)
xλM = 0.91    
λM = 0.95    
λM = 0.98    
λM = 0.99    
λM = 0.993  
λM = 0.997  
λM = 0.999  
λM = 0.9993
λM = 0.9995
λM = 0.9997
FIG. 42. Wealth distribution obtained for the uniform savin g
propensity distributions of 105agents in the interval (0 ,λM).
present in a numerical simulation. However, to study
this eﬀect, one has to consider a system with a number
ofagentslargeenoughthatitisnotpossibletoresolvethe
wealth distributions of single agents for the sub-intervals
ofλconsidered. This was done in by Patriarca et al.
(2006) using a system with N= 105agents with sav-
ing parameters distributed uniformly between 0 and λM.
Results are shown in Fig. 42, in which curves from left
to right correspond to increasing values of the cutoﬀ λM
from 0.9 to 0.9997. The transition from an exponential
to a power-lawtail takes place continuouslyas the cut-oﬀ
λMis increased beyond a critical value λM≈0.9 toward
λM= 1, through the enlargement of the x-interval in
which the power-law is observed.
3. Relaxation process
Relaxation in systems with constant λhad already
been studied by Chakraborti and Chakrabarti (2000),
where a systematic increase of the relaxation time with
λ, and eventually a divergence for λ→1, was found.
In fact, for λ= 1 no exchanges occurs and the system
is frozen. The relaxation time scale of a heterogeneous
system had been studied by Patriarca et al.(2007). The
system is observed to relax toward the same equilibrium
wealth distribution from any given arbitrary initial dis-
tribution of wealth. If time is measured by the number of
transactions nt, thetimescaleisproportionaltothenum-
ber ofagents N, i.e. deﬁning time tas the ratio t=nt/N
between the number of trades and the total number of
agentsN(correspondingto one Monte Carlo cycle or one
sweep in molecular dynamics simulations) the dynam-
ics and the relaxation process become independent of N.
The existence of a natural time scale independent of the
system size provides a foundation for using simulations
of systems with ﬁnite Nin order to infer properties ofsystems with continuous saving propensity distributions
andN→ ∞.
In a system with uniformly distributed λ, the wealth
distributions of each agent iwith saving parameter λi
relaxes toward diﬀerent states with characteristic shapes
fi(x) (Patriarca et al.(2005); Chatterjee et al.(2005b);
Patriarca et al.(2006)) with diﬀerent relaxation times
τi(Patriarca et al.(2007)). The diﬀerences in the re-
laxation process can be related to the diﬀerent relative
wealth exchange rates, that by direct inspection of the
evolution equations appear to be proportional to 1 −λi.
Thus, in general, higher saving propensities are expected
to be associated to slower relaxation processes with a
relaxation time ∝1/(1−λ).
It is also possible to obtain the relaxation time distri-
bution. If the saving parameters are distributed in (0 ,1)
with a density φ(λ), it follows from probability conser-
vation that ˜f(¯x)d¯x=φ(λ)dλ, where ¯x≡ ∝an}bracketle{tx∝an}bracketri}htλand˜f(¯x)
the corresponding density of average wealth values. In
the case of uniformly distributed saving propensities, one
obtains
˜f(¯x) =φ(λ)dλ(¯x)
d¯x=φ/parenleftbigg
1−k
¯x/parenrightbiggk
¯x2,(49)
showing that a uniform saving propensity distribution
leads to a power law ˜f(¯x)∼1/¯x2in the (average) wealth
distribution. In a similar way it is possible to obtain the
associated distribution of relaxation times ψ(τ) for the
globalrelaxationprocessfromtherelation τi∝1/(1−λi),
ψ(τ) =φ(λ)dλ(τ)
dτ∝φ/parenleftbigg
1−τ′
τ/parenrightbiggτ′
τ2,(50)
whereτ′is a proportionality factor. Therefore ψ(τ) and
˜f(¯x) are characterized by power law tails in τand ¯xre-
spectively with the same Pareto exponent .
In conclusion, the role of the λ-cut-oﬀ is also related to
the relaxation process. This means that the slowest con-
vergencerateisdeterminedbythecut-oﬀandis ∝1−λM.
In numerical simulations of heterogeneous KWEMs, as
wellasinrealwealthdistributions, thecut-oﬀisnecessar-
ily ﬁnite, so that the convergence is fast (Gupta (2008)).
On the other hand, if considering a hypothetical wealth
distribution with a power law extending to inﬁnite values
ofx, one cannot ﬁnd a fast relaxation, due to the inﬁnite
time scale of the system, due to the agents with λ= 1.
E. Microeconomic formulation of Kinetic theory models
Very recently, Chakrabarti and Chakrabarti (2009)
havestudied the frameworkbased on microeconomicthe-
ory from which the kinetic theory market models could
be addressed. They derived the moments of the model
by Chakraborti and Chakrabarti (2000) and reproduced
the exchange equations used in the model (with ﬁxed
savings parameter). In the framework considered, the",2009-09-10T15:25:45Z,alth  patria rca results  t mis relaxatrelaxatchara bor ti chara bart it patria rca t    t iipatria rca terjee patria rca patria rca t  it  iitrefore party i ims gupta om economic etic very chara bart chara bart ty chara bor ti chara bart in
paper_qf_47.pdf,39,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","39
utility function deals with the behaviour of the agents in
an exchange economy.
They start by considering two exchange economy,
where each agent produces a single perishable commod-
ity. Each of these goods is diﬀerent and money exists
in the economy to simply facilitate transactions. Each of
theseagentsareendowedwithaninitialamountofmoney
M1=m1(t) andM2=m2(t). Let agent 1 produce Q1
amount of commodity 1 only, and agent 2 produce Q2
amount of commodity 2 only. At each time step t, two
agents meet randomly to carry out transactions accord-
ing to their utility maximization principle.
The utility functions as deﬁned as follows: For
agent 1,U1(x1,x2,m1) =xα1
1xα2
2mαm
1and for agent 2,
U2(y1,y2,m2) =yα1
1yα2
2mαm
2where the arguments in
both of the utility functions are consumption of the ﬁrst
(i.e.x1andy1) and second good (i.e. x2andy2) and
amount of money in their possession respectively. For
simplicity, they assume that the utility functions are of
the aboveCobb-Douglasformwith the sum ofthe powers
normalized to 1 i.e. α1+α2+αm= 1.
Let the commodityprices to be determined in the mar-
ket be denoted by p1andp2. Now, the budget con-
straints are as follows: For agent 1 the budget constraint
isp1x1+p2x2+m1≤M1+p1Q1and similarly, for agent
2 the constraint is p1y1+p2y2+m2≤M2+p2Q2, which
mean that the amount that agent 1 can spend for con-
sumingx1andx2added to the amount of money that he
holds after trading at time t+1 (i.e.m1) cannot exceed
the amount of money that he has at time t(i.e.M1)
added to what he earns by selling the good he produces
(i.e.Q1), and the same is true for agent 2.
Then the basic idea is that both of the agents try to
maximize their respective utility subject to their respec-
tive budget constraintsand the invisible hand of the mar-
ket that is the pricemechanism worksto clearthe market
for both goods (i.e. total demand equals total supply for
both goods at the equilibrium prices), which means that
agent 1’s problem is to maximize his utility subject to
his budget constraint i.e. maximize U1(x1,x2,m1) sub-
ject top1.x1+p2.x2+m1=M1+p1.Q1. Similarly
for agent 2, the problem is to maximize U1(y1,y2,m2)
subject to p1.y1+p2.y2+m2=M2+p2.Q2. Solv-
ing those two maximization exercises by Lagrange multi-
plier and applying the condition that the market remains
in equilibrium, the competitive price vector (ˆ p1,ˆp2) as
ˆpi= (αi/αm)(M1+M2)/Qifori= 1, 2 is found
(Chakrabarti and Chakrabarti (2009)).
The outcomes of such a trading process are then:
1. At optimal prices (ˆ p1,ˆp2),m1(t)+m2(t) =m1(t+
1)+m2(t+1), i.e., demand matches supply in all
market at the market-determined price in equilib-
rium. Since money is also treated as a commod-
ity in this framework, its demand (i.e. the total
amount of money held by the two persons after
trade) must be equal to what was supplied (i.e. the
total amount of money held by them before trade).2. If a restrictive assumption is made such that α1
in the utility function can vary randomly over time
withαmremainingconstant. It readilyfollowsthat
α2also varies randomly over time with the restric-
tionthatthesumof α1andα2isaconstant(1- αm).
Then inthe moneydemandequationsderived, ifwe
supposeαmisλandα1/(α1+α2) isǫ, it is found
that money evolution equations become
m1(t+1) =λm1(t)+ǫ(1−λ)(m1(t)+m2(t))
m2(t+1) =λm2(t)+(1−ǫ)(1−λ)(m1(t)+m2(t)).
For a ﬁxed value of λ, ifα1(orα2) is a ran-
dom variable with uniform distribution over the
domain [0,1−λ], thenǫis also uniformly dis-
tributed over the domain [0 ,1]. This limit corre-
sponds to the Chakraborti and Chakrabarti (2000)
model, discussed earlier.
3. For the limiting value of αmin the utility function
(i.e.αm→0 which implies λ→0), the money
transfer equation describing the random sharing
of money without saving is obtained, which was
studied by Dragulescu and Yakovenko (2000) men-
tioned earlier.
This actually demonstrates the equivalence of the two
maximizationsprinciples ofentropy(in physics) and util-
ity (in economics), and is certainly noteworthy.
IX. AGENT-BASED MODELLING BASED ON GAMES
A. Minority Game models
1. El Farol Bar Problem
Arthur(1994)introducedthe ‘ElFarolBar’problemas
a paradigm of complex economic systems. In this prob-
lem, a population of agents have to decide whether to go
to the bar opposite Santa Fe, every Thursday night. Due
to a limited number of seats, the bar cannot entertain
more thanX% of the population. If less than X% of the
population go to the bar, the time spent in the bar is
considered to be satisfying and it is better to attend the
bar rather than staying at home. But if more than X%
of the population go to the bar, then it is too crowded
and people in the bar have an unsatisfying time. In this
second case, staying at home is considered to be better
choice than attending the bar. So, in order to optimise
its own utility, each agent has to predict what everybody
else will do.
In particular Arthur was also interested in agents who
have bounds on “rationality”, i.e. agents who:
•do not have perfect information about their envi-
ronment, in general they will only acquire infor-
mation through interaction with the dynamically
changing environment;",2009-09-10T15:25:45Z,ty eaea at t for for cbb douglas form with   for tsimarly so lv arrae qi for chara bart chara bart t at since  it tfor  chara bor ti chara bart for drag ul escu v e  minority game el arbar problem arthur el arbar isanta fe thursday due  but iso iarthur
paper_qf_47.pdf,40,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","40
•do not have a perfect model of their environment;
•have limited computational power, so they can’t
workoutallthelogicalconsequencesoftheirknowl-
edge;
•have other resource limitations (e.g. memory).
Inordertotaketheselimitationsintoaccount,eachagent
israndomlygivenaﬁxedmenuofmodelspotentiallysuit-
able to predict the number of people who will go the bar
given past data (e.g. the same as two weeks ago, the av-
erage of the past few weeks, etc.). Each week, each agent
evaluates these models against the past data. He chooses
the one that was the best predictor on this data and then
uses it to predict the number of people who will go to the
bar this time. If this prediction is less than X, then the
agent decides to go to the bar as well. If its prediction
is more than X, the agent stays home. Thus, in order to
make decisions on whether to attend the bar, all the indi-
vidualsareequipped with certainnumber of“strategies”,
which provide them the predictions of the attendance in
the bar next week, based on the attendance in the past
few weeks. As a result the number who go to the bar
oscillates in an apparently random manner around the
criticalX% mark.
Thiswasoneofthe ﬁrstmodelsthatled awaydiﬀerent
from traditional economics.
2. Basic Minority game
The Minority Games (abbreviated MGs)
(Challet et al.(2004)) refer to the multi-agent models
of ﬁnancial markets with the original formulation
introduced by Challet and Zhang (1997), and all other
variants (Coolen (2005); Lamper et al.(2002)), most
of which share the principal features that the models
are repeated games and agents are inductive in nature.
The original formulation of the Minority Game by
Challet and Zhang (1997) is sometimes referred as
the “Original Minority Game” or the “Basic Minority
Game”.
The basic minority game consists of N(odd natural
number) agents, who choose between one of the two de-
cisions at each round of the game, using their own sim-
ple inductive strategies. The two decisions could be, for
example, “buying” or “selling” commodities/assets, de-
noted by 0 or 1, at a given time t. An agent wins the
game if it is one of the members of the minority group,
and thus at each round, the minority group of agents
win the game and rewards are given to those strategies
that predict the winning side. All the agents have ac-
cess to ﬁnite amount of public information, which is a
common bit-string “memory” of the Mmost recent out-
comes, composed of the winning sides in the past few
rounds. Thus the agents with ﬁnite memory are said to
exhibit “bounded rationality” (Arthur (1994)).0100020003000400050000100200300400500600700800
TA1(t)
010002000300040005000−300−200−1000100200300400
TPerformancea b 
FIG. 43. Attendance ﬂuctuation and performances of players
in Basic Minority Game. Plots of (a) attendance and (b)
performance oftheplayers(ﬁvecurvesare: thebest, thewor st
and three randomly chosen) for the basic minority game with
N= 801;M= 6;k= 10 and T= 5000. Reproduced from
Sysi-Aho et al.(2003b).
Consider for example, memory M= 2; then there are
P= 2M= 4 possible “history” bit strings: 00, 01, 10
and 11. A “strategy” consists of a response, i.e., 0 or 1,
to each possible history bit strings; therefore, there are
G= 2P= 22M= 16 possible strategies which consti-
tute the “strategy space”. At the beginning of the game,
each agent randomly picks kstrategies, and after the
game, assigns one “virtual” point to a strategy which
would have predicted the correct outcome. The actual
performance rof the player is measured by the number
of times the player wins, and the strategy, using which
the player wins, gets a “real” point. A record of the
number of agents who have chosen a particular action,
say, “selling” denoted by 1 , A1(t) as a function of time
is kept (see Fig. 43). The ﬂuctuations in the behaviour
ofA1(t) actually indicate the system’s total utility. For
example, we can have a situation where only one player
is in the minority and all the other players lose. The
other extreme case is when ( N−1)/2 players are in the
minority and ( N+ 1)/2 players lose. The total utility
of the system is obviously greater for the latter case and
from this perspective, the latter situation is more desir-
able. Therefore, the system is more eﬃcient when there
are smaller ﬂuctuations around the mean than when the
ﬂuctuations are larger.
As in the El Farol bar problem, unlike most traditional
economics models which assume agents are “deductive”
in nature, here too a “trial-and-error” inductive thinking
approachisimplicitlyimplementedinprocessofdecision-
making when agents make their choices in the games.",2009-09-10T15:25:45Z,iorr to take tse limitatns into account ea    as  was one of t basic minority t minority gamgs chal  chal   coelam  t minority game chal   orinal minority game basic minority game t t aall most  arthur formance aendance basic minority game plots reproduced sy si ho consir at t  t for t t trefore as el arol
paper_qf_47.pdf,41,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","41
0.0 20000.0 40000.0 60000.0 80000.0 100000.0
t0.0100.0200.0300.0400.0500.0600.0700.0800.0900.01000.0A
FIG. 44. Temporal attendance of Afor the genetic ap-
proach showing a learning process. Reproduced from
Challet and Zhang (1997)
3. Evolutionary minority games
Challet generalized the basic minority game (see
Challet and Zhang (1997, 1998)) mentioned above to in-
clude the Darwinian selection: the worst player is re-
placedbyanewoneaftersometime steps, the newplayer
is a “clone” of the best player, i.e. it inherits all the
strategies but with corresponding virtual capitals reset
to zero (analogous to a new born baby, though having
all the predispositions from the parents, does not inherit
their knowledge). To keep a certain diversity they intro-
duced a mutation possibility in cloning. They allowed
one of the strategies of the best player to be replaced by
a new one. Since strategies are not just recycled among
the players any more, the whole strategy phase space is
available for selection. They expected this population to
be capable of “learning” since bad players are weeded
out with time, and ﬁghting is among the so-to-speak the
“best” players. Indeed in Fig. 44, they observed that the
learning emerged in time. Fluctuations are reduced and
saturated, this implies the average gain for everybody is
improved but never reaches the ideal limit.
Liet al.(2000a,b) also studied the minority game in
the presence of “evolution”. In particular, they exam-
ined the behaviour in games in which the dimension of
the strategy space, m, is the same for all agents and ﬁxed
for all time. They found that for all values of m, not too
large, evolution results in a substantial improvement in
overall system performance. They also showed that after
evolution, results obeyed a scaling relation among games
playedwithdiﬀerent valuesof manddiﬀerentnumbersof
agents, analogous to that found in the non-evolutionary,
adaptive games (see remarks on section IXA5). Bestsystem performance still occurred, for a given number of
agents, atmc, the same value of the dimension of the
strategy space as in the non-evolutionary case, but sys-
tem performance was nearly an order of magnitude bet-
ter than the non-evolutionary result. For m < m c, the
system evolved to states in which average agent wealth
was better than in the random choice game. As mbe-
camelarge, overallsystems performanceapproachedthat
of the random choice game.
Liet al.(2000a,b) continued the study of evolution in
minoritygamesbyexamininggamesin whichagentswith
poorly performing strategies can trade in their strategies
fornew onesfromadiﬀerent strategyspace, whichmeant
allowingforstrategiesthat useinformationfromdiﬀerent
numbers of time lags, m. They found, in all the games,
that after evolution, wealth per agent is high for agents
with strategies drawn from small strategy spaces (small
m), and low for agents with strategies drawn from large
strategy spaces (large m). In the game played with N
agents, wealth per agent as a function of mwas very
nearly a step function. The transition was found to be
atm=mt, wheremt≃mc−1, andmcis the critical
value ofmat whichNagents playing the game with
a ﬁxed strategy space (ﬁxed m) have the best emer-
gent coordination and the best utilization of resources.
They also found that overall system-wide utilization of
resources is independent of N. Furthermore, although
overall system-wide utilization of resources after evolu-
tionvariedsomewhatdepending onsomeotheraspectsof
the evolutionary dynamics, in the best cases, utilization
of resources was on the order of the best results achieved
in evolutionary games with ﬁxed strategy spaces.
4. Adaptive minority games
Sysi-Aho et al.(2003a,c,b, 2004) presented a simple
modiﬁcation of the basic minority game where the play-
ers modify their strategies periodically after every time
intervalτ, depending on their performances: if a player
ﬁnds that he is among the fraction n(where 0<n<1)
who are the worst performing players, he adapts him-
self and modiﬁes his strategies. They proposed that the
agents use hybridized one-point genetic crossovermecha-
nism (as shown in Fig. 45), inspired by genetic evolution
in biology, to modify the strategies and replace the bad
strategies. They studied the performances of the agents
underdiﬀerentconditionsandinvestigatehowtheyadapt
themselves in order to survive or be the best, by ﬁnd-
ing new strategies using the highly eﬀective mechanism.
They also studied the measure of total utility of the sys-
temU(xt), which isthe numberofplayersinthe minority
group;thetotalutilityofthesystemismaximum Umaxas
the highest number of players win is equal to ( N−1)/2.
The system is more eﬃcient when the deviations from
the maximum total utility Umaxare smaller, or in other
words, the ﬂuctuations in A1(t) aroundthe mean become
smaller.",2009-09-10T15:25:45Z,temal for reproduced chal   evolutnary chal  chal   darwiniato ty since ty ined  fluuatns t ity ty best tem for as t ty it  ty furtr adaptive sy si ho ty  ty ty max as t max are
paper_qf_47.pdf,42,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","42
0
1
1
1
1
11
1
1
0
0
11
1
1
1
1
10
1
1
0
0
1
1
11
11
11
1breaking
point
parents childrens s s si j k l
one−point crossover
FIG. 45. Schematic diagram to illustrate the mechanism of
one-point genetic crossover for producing new strategies. The
strategies siandsjare the parents. We choose the breaking
point randomly and through this one-point genetic crossove r,
the children skandslare produced and substitute the par-
ents. Reproduced from Sysi-Aho et al.(2003b).
01000 2000 3000 400002004006008001000
TimeA1(t)
01000 2000 3000 400002004006008001000
TimeA1(t)a b 
FIG. 46. Plot to show the time variations of the number of
playersA1who choose action 1, with the parameters N=
1001,m= 5,s= 10 and t= 4000 for (a) basic minority
game and (b) adaptive game, where τ= 25 and n= 0.6.
Reproduced from Sysi-Aho et al.(2003b).
Interestingly, the ﬂuctuations disappear totally and
the system stabilizes to a state where the total utility
of the system is at maximum, since at each time step the
highest number of players win the game (see Fig. 46).
As expected, the behaviour depends on the parameter
values for the system (see Sysi-Aho et al.(2003b, 2004)).
They used the utility function to study the eﬃciency and
dynamics of the game as shown in Fig. 47. If the par-
ents arechosen randomlyfrom the pool ofstrategiesthen
themechanismrepresentsa“one-pointgeneticcrossover”
and if the parents are the best strategies then the mech-
anism represents a “hybridized genetic crossover”. The
children may replace parents or two worst strategies and
accordingly four diﬀerent interesting cases arise: (a) one-
point genetic crossover with parents “killed”, i.e. par-
ents are replaced by the children, (b) one-point genetic
crossoverwith parents “saved”, i.e. the two worst strate-
gies are replaced by the children but the parents are
retained, (c) hybridized genetic crossover with parents0 10 20 30 40 50 60 70 80 90 100300320340360380400420440460480500520
Scaled time (t/50)Average total utility
FIG. 47. Plot to show the variation of total utility of the
system with time for the basic minority game for N= 1001,
m= 5,s= 10,t= 5000, and adaptive game, for the same
parameters but diﬀerent values of τandn. Each point rep-
resents a time average of the total utility for separate bins of
size 50 time-steps of the game. The maximum total utility
(= (N−1)/2) is shown as a dashed line. The data for the
basic minority game is shown in circles. The plus signs are
forτ= 10 and n= 0.6; the asterisk marks are for τ= 50 an
n= 0.6; the cross marks for τ= 10 and n= 0.2 and trian-
gles forτ= 50 and n= 0.2. The ensemble average over 70
diﬀerent samples was taken in each case. Reproduced from
Sysi-Aho et al.(2003b).
“killed” and (d) hybridized genetic crossover with par-
ents “saved”.
In order to determine which mechanism is the most
eﬃcient, we have made a comparative study of the four
cases, mentioned above. We plot the attendance as a
function of time for the diﬀerent mechanisms in Fig. 48.
In Fig. 49 we show the total utility of the system in each
of the cases (a)-(d), where we have plotted results of the
average over 100 runs and each point in the utility curve
represents a time average taken over a bin of length 50
time-steps. The simulation time is doubled from those
in Fig. 48, in order to expose the asymptotic behaviour
better. On the basis of Figs. 48 and 49, we ﬁnd that
the case (d) is the most eﬃcient. In order to investi-
gate what happens in the level of an individual agent,
we created a competitive surrounding– “test” situation
where after T= 3120 time-steps, six players begin to
adapt and modify their strategies such that three are us-
inghybridizedgeneticcrossovermechanismandtheother
three one point genetic crossover, where children replace
the parents. The rest of the players play the basic mi-
nority game. In this case it turns out that in the end
the best players are those who use the hybridized mech-
anism, second best are those using the one-point mecha-
nism, and the bad players those who do not adapt at all.
Inadditionit turnsoutthat thecompetition amongstthe
players who adapt using the hybridized genetic crossover",2009-09-10T15:25:45Z,scmatic t  reproduced sy si ho time time plot reproduced sy si ho interesti  as sy si ho ty   t scaled ge plot eat t t t reproduced sy si ho i  i t  os it iiadditit
paper_qf_47.pdf,43,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","43
02000400060008000100000200400600800
TimeAttendance
0200040006000800010000100200300400500600700800
TimeAttendance
02000400060008000100000200400600800
TimeAttendance
02000400060008000100000200400600800
TimeAttendancea b 
c d 
FIG. 48. Plots of the attendances by choosing parents ran-
domly (a) and (b), and using the best parents in a player’s
pool (c) and (d). In (a) and (c) case parents are replaced
by children and in (b) and (d) case children replace the two
worst strategies. Simulations have been done with N= 801,
M= 6,k= 16,t= 40,n= 0.4 andT= 10000.
FIG. 49. Plots of the scaled utilities of the four diﬀerent
mechanisms in comparison with that of the basic minority
game. Each curve represents an ensemble average over 100
runs and each point in a curve is a time average over a bin
of length 50 time-steps. In the inset, the quantity (1 −U) is
plotted against scaled time in the double logarithmic scale .
Simulations are done with N= 801,M= 6,k= 16,t= 40,
n= 0.4 andT= 20000. Reproduced from Sysi-Aho et al.
(2003b).mechanism is severe.
It should be noted that the mechanism of evolution of
strategies is considerably diﬀerent from earlier attempts
such as Challet and Zhang (1997) or Li et al.(2000a,b).
This is because in this mechanism the strategies are
changed by the agents themselves and even though the
strategy space evolves continuously, its size and dimen-
sionality remain the same.
Due to the simplicity of these models (Sysi-Aho et al.
(2003a,c,b, 2004)), a lot of freedom is found in modi-
fying the models to make the situations more realistic
and applicable to many real dynamical systems, and not
only ﬁnancial markets. Many details in the model can
be ﬁne-tuned to imitate the real markets or behaviour of
other complex systems. Many other sophisticated mod-
els based on these games can be setup and implemented,
which show a great potential overthe commonly adopted
statistical techniques in analyses of ﬁnancial markets.
5. Remarks
For modelling purposes, the minority game mod-
els were meant to serve as a class of simple models
which could produce some macroscopicfeatures observed
in the real ﬁnancial markets, which included the fat-
tail price return distribution and volatility clustering
(Challet et al.(2004); Coolen (2005)). Despite the hec-
tic activity (Challet and Zhang (1998); Challet et al.
(2000)) they have failed to capture or reproduce most
important stylized facts of the real markets. However, in
the physicists’ community, they have become an interest-
ing and established class of models where the physics of
disorderedsystems(Cavagna et al.(1999); Challet et al.
(2000)), lending a large amount of physical insights
(Savitet al.(1999); Martino et al.(2004)). Since in the
BMGmodel aHamiltonianfunction couldbe deﬁnedand
analytic solutions could be developed in some regimes of
the model, the model was viewed with a more physical
picture. In fact, it is characterized by a clear two-phase
structure with very diﬀerent collective behaviours in the
two phases, as in many known conventional physical sys-
tems (Savit et al.(1999); Cavagna et al.(1999)).
Savitet al.(1999) ﬁrst found that the macroscopic be-
haviour of the system does not depend independently on
the parameters NandM, but instead depends on the
ratio
α≡2M
N=P
N(51)
which serves as the most important control parameter
in the game. The variance in the attendance (see also
Sysi-Aho et al.(2003c)) or volatility σ2/N, for diﬀerent
values ofNandMdepend only on the ratio α. Fig. 50
shows a plot of σ2/Nagainst the control parameter α,
where the data collapse of σ2/Nfor diﬀerent values of
NandMis clearly evident. The dotted line in Fig. 50
corresponds to the “coin-toss” limit (random choice or",2009-09-10T15:25:45Z,time aendance time aendance time aendance time aendance plots isimulatns plots eaisimulatns reproduced sy si ho it chal   li  due sy si ho many many remarks for chal  coespite chal   chal  ce gchal  se tet martisince mhamtoniafunis it ce gse tet and t sy si ho and pend  ag articial intellence nst for and mis t 
paper_qf_47.pdf,44,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","44
0.001 0.01 0.1 1 10 100α0.1110100 σ2/NN = 51
N = 101
N = 251
N = 501
N = 1001
Worse-than-random
Better-than-randomSymmetric Phase Asymmetric Phase
FIG. 50. The simulation results of the variance in attendanc e
σ2/Nas a function of the control parameter α= 2M/Nfor
games with k= 2strategies for each agent, ensemble averaged
over 100 sample runs. Dottedline shows thevalue of volatili ty
in random choice limit. Solid line shows the critical value
ofα=αc≈0.3374. Reproduced from Yeung and Zhang
arxiv:0811.1479 .
pure chance limit), in which agents play by simply mak-
ing random decisions (by coin-tossing) at every rounds
of the game. This value of σ2/Nin coin-toss limit can
be obtained by simply assuming a binomial distribution
of the agents’ binary actions, with probability 0 .5, such
thatσ2/N= 0.5(1−0.5)·4 = 1. When αis small, the
value ofσ2/Nof the game is larger than the coin-toss
limit whichimplies the collectivebehavioursofagentsare
worse than the random choices. In the early literature, it
was popularly called as the worse-than-random regime.
Whenαincreases, the value of σ2/Ndecreases and en-
ters a region where agents are performing better than
the random choices, which was popularly called as the
better-than-random regime. The value of σ2/Nreaches
a minimum value which is substantially smaller than the
coin-toss limit. When αfurther increases, the value of
σ2/Nincreases again and approaches the coin-toss limit.
This allowed one to identify two phases in the Minority
Game, as separated by the minimum value of σ2/Nin
the graph. The value of αwhere the rescaled volatility
attended its minimum was denoted by αc, which repre-
sented the phase transition point; αchas been shown to
have a value of 0 .3374...(fork= 2) by analytical calcu-
lations Challet et al.(2000).
Besides these collective behaviours, physicists became
also interested in the dynamics of the games such as
crowdvsanti-crowdmovementofagents,periodic attrac-
tors, etc. (Johnson et al.(1999b,a); Hart et al.(2001)).
In this way, the Minority Games serve as a useful tool
and provide a new direction for physicists in viewing and
analysing the underlying dynamics of complex evolving
systems such as the ﬁnancial markets.B. The Kolkata Paise Restaurant (KPR) problem
The KPR problem (Chakrabarti et al.(2009);
Ghosh and Chakrabarti (2009); Ghosh et al.(2010a,b))
is a repeated game, played between a large number N
of agents having no interaction amongst themselves. In
KPR problem, prospective customers (agents) choose
fromNrestaurants each evening simultaneously (in
parallel decision mode); Nis ﬁxed. Each restaurant
has the same price for a meal but a diﬀerent rank
(agreed upon by all customers) and can serve only
one customer any evening. Information regarding the
customer distributions for earlier evenings is available
to everyone. Each customer’s objective is to go to the
restaurant with the highest possible rank while avoiding
the crowd so as to be able to get dinner there. If more
than one customer arrives at any restaurant on any
evening, one of them is randomly chosen (each of them
are anonymously treated) and is served. The rest do not
get dinner that evening.
In Kolkata,there wereverycheapand ﬁxedrate“Paise
Restaurants” that were popular among the daily labour-
ers in the city. During lunch hours, the labourers used to
walk (to save the transport costs) to one of these restau-
rants and would miss lunch if they got to a restaurant
where there were too many customers. Walking down to
the next restaurant would mean failing to report back to
work on time! Paise is the smallest Indian coin and there
were indeed some well-known rankings of these restau-
rants,assomeofthem wouldoﬀertastieritemscompared
to the others. A more general example of such a problem
would be when the society provides hospitals (and beds)
in every locality but the local patients go to hospitals
of better rank (commonly perceived) elsewhere, thereby
competing with the local patients of those hospitals. Un-
availability of treatment in time may be considered as
lack of the service for those people and consequently as
(social) wastageof service by those unattended hospitals.
Adictator’s solutionto the KPRproblem is the follow-
ing: the dictator asks everyone to form a queue and then
assigns each one a restaurant with rank matching the se-
quence of the person in the queue on the ﬁrst evening.
Then each person is told to go to the next ranked restau-
rant in the following evening (for the person in the last
ranked restaurant this means going to the ﬁrst ranked
restaurant). This shift proceeds then continuously for
successive evenings. This is clearly one of the most eﬃ-
cient solution (with utilization fraction ¯fof the services
by the restaurants equal to unity) and the system arrives
at this this solution immediately (from the ﬁrst evening
itself). However, in reality this cannot be the true solu-
tionofthe KPRproblem, whereeachagentdecidesonhis
own (in parallel or democratically) every evening, based
oncompleteinformationaboutpastevents. Inthis game,
the customers try to evolve a learning strategy to even-
tually get dinners at the best possible ranked restaurant,
avoiding the crowd. It is seen, the evolution these strate-
gies takeconsiderabletime to convergeand even then the",2009-09-10T15:25:45Z,worse beer syetric phase asyetric phase t nas for doed line solid reproduced e   iwof iast reacs wineas minority game it chal  besis sohart iminority gamt lkata articial intellence se restaurant t chara bart ghost chara bart ghost irestaurants is eainformatea t ilkata articial intellence se restaurants  walki articial intellence se indiaudiator problem t  problem i it
paper_qf_47.pdf,45,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","45
eventual utilization fraction ¯fis far below unity.
Let the symmetric stochastic strategy chosen by each
agent be such that at any time t, the probability pk(t) to
arrive at the k-th ranked restaurant is given by
pk(t) =1
z/bracketleftbigg
kαexp/parenleftbigg
−nk(t−1)
T/parenrightbigg/bracketrightbigg
,
z=N/summationdisplay
k=1/bracketleftbigg
kαexp/parenleftbigg
−nk(t−1)
T/parenrightbigg/bracketrightbigg
,(52)
wherenk(t) denotes the number of agents arriving at the
k-th ranked restaurant in period t,T >0 is a scaling
factor andα≥0 is an exponent.
For any natural number αandT→ ∞, an agent goes
to thek-th ranked restaurant with probability pk(t) =
kα//summationtextkα; which means in the limit T→ ∞in (52) gives
pk(t) =kα//summationtextkα.
If an agent selects any restaurant with equal probabil-
itypthen probability that a single restaurant is chosen
bymagents is given by
∆(m) =/parenleftBigg
N
m/parenrightBigg
pm(1−p)N−m. (53)
Therefore, the probability that a restaurant with rank k
is not chosen by any of the agents will be given by
∆k(m= 0) =/parenleftBigg
N
0/parenrightBigg
(1−pk)N;pk=kα
/summationtextkα
≃exp/parenleftbigg−kαN
/tildewideN/parenrightbigg
asN→ ∞,(54)
where/tildewideN=/summationtextN
k=1kα≃/integraltextN
0kαdk=Nα+1
(α+1).Hence
∆k(m= 0) = exp/parenleftbigg
−kα(α+1)
Nα/parenrightbigg
.(55)
Therefore the averagefraction of agents getting dinner in
thek-th ranked restaurant is given by
¯fk= 1−∆k(m= 0). (56)
Naturally for α= 0, the problem corresponding to
random choice ¯fk= 1−e−1, giving ¯f=/summationtext¯fk/N≃0.63
and forα= 1,¯fk= 1−e−2k/Ngiving¯f=/summationtext¯fk/N≃
0.58.
In summary, in the KPR problem where the decision
made by each agent in each evening tis independent
and is based on the information about the rank kof
the restaurants and their occupancy given by the num-
bersnk(t−1)...nk(0). For several stochastic strate-
gies, onlynk(t−1) is utilized and each agent chooses the
k-th ranked restaurant with probability pk(t) given by
Eq. (52). The utilization fraction fkof thek-th ranked
restaurants on every evening is studied and their aver-
age (overk) distributions D(f) are studied numerically,
as well as analytically, and one ﬁnds (Chakrabarti et al.(2009); Ghosh and Chakrabarti (2009); Ghosh et al.
(2010a)) their distributions to be Gaussianwith the most
probable utilization fraction ¯f≃0.63, 0.58 and 0.46 for
the caseswith α= 0,T→ ∞;α= 1,T→ ∞; andα= 0,
T→0 respectively. For the stochastic crowd-avoiding
strategy discussed in Ghosh et al.(2010b), where pk(t+
1) =1
nk(t)fork=k0the restaurant visited by the agent
last evening, and = 1 /(N−1) for all other restaurants
(k∝ne}ationslash=k0), one gets the best utilization fraction ¯f≃0.8,
and the analytical estimates for ¯fin these limits agree
verywell with the numerical observations. Also, the time
required to converge to the above value of ¯fis indepen-
dent ofN.
The KPR problem has similarity with the Minority
Game Problem (Arthur (1994); Challet et al.(2004)) as
in both the games, herding behaviour is punished and di-
versity’s encouraged. Also, both involves learning of the
agents from the past successes etc. Of course, KPR has
some simple exact solution limits, a few of which are dis-
cussed here. The real challenge is, of course, to design al-
gorithms of learning mixed strategies (e.g., from the pool
discussed here) by the agents so that the fair social norm
emerges eventually (in N0or lnNorder time) even when
every one decides on the basis of their own information
independently. As we have seen, some naive strategies
givebettervaluesof ¯fcomparedtomostofthe“smarter”
strategies like strict crowd-avoiding strategies, etc. This
observation in fact compares well with earlier observa-
tion in minority games (see e.g. Satinover and Sornette
(2007)).
Itmaybenoted thatallthestochasticstrategies,being
parallel in computational mode, have the advantage that
they converge to solution at smaller time steps ( ∼N0or
lnN) while for deterministic strategies the convergence
time is typically of orderof N, which renderssuch strate-
gies useless in the truly macroscopic ( N→ ∞) limits.
However, deterministic strategies are useful when Nis
small and rational agents can design appropriate punish-
ment schemes for the deviators (see Kandori (2008)).
The study of the KPR problem shows that while a
dictatedsolutionleadstooneofthebestpossiblesolution
to the problem, with each agent getting his dinner at the
best ranked restaurant with a period of Nevenings, and
with best possible value of ¯f(= 1) starting from the ﬁrst
eveningitself. Theparalleldecisionstrategies(employing
evolvingalgorithmsbytheagents,andpastinformations,
e.g., ofn(t)), which are necessarily parallel among the
agents and stochastic (as in democracy), are less eﬃcient
(¯f≪1; the best one discussed in Ghosh et al.(2010b),
giving¯f≃0.8 only). Note here that the time required
is not dependent on N. We also note that most of the
“smarter” strategies lead to much lower eﬃciency.
X. CONCLUSIONS AND OUTLOOK
Agent-based models of order books are a good ex-
ample of interactions between ideas and methods that",2009-09-10T15:25:45Z, for  b b trefore b b nce trefore naturally givi ifor eq t chara bart ghost chara bart ghost with for ghost also t minority game problem arthur chal  also of t orr as  satiocorvee it maybe noted is and ori t evenis t llel cisstrateghost note  agent
paper_qf_47.pdf,46,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","46
are usually linked either to Economics and Finance (mi-
crostructure of markets, agent interaction) or to Physics
(reaction-diﬀusionprocesses,deposition-evaporationpro-
cess, kinetic theory of gases). As of today, existing mod-
els exhibit a trade-oﬀ between “realism” and calibration
in its mechanisms and processes (empirical models such
as Mike and Farmer (2008)), and explanatory power of
simple observed behaviours (Cont and Bouchaud (2000);
Cont (2007) for example). In the ﬁrst case, some of the
“stylized facts” may be reproduced, but using empiri-
cal processes that may not be linked to any behaviour
observed on the market. In the second case, these are
only toy models that cannot be calibrated on data. The
mixing of many features, as in Lux and Marchesi (2000)
and as is usually the case in behavioural ﬁnance, leads
to poorly tractable models where the sensitivity to one
parameter is hardly understandable. Therefore, no em-
pirical model can tackle properly empirical facts such as
volatility clustering. Importing toy model features ex-
plaining volatility clustering or market interactions in or-
der book models is yet to be done. Finally, let us also
note that to our knowledge, no agent-based model of or-
der books deals with the multidimensional case. Imple-
menting agents trading simultaneously several assets in
a way that reproduces empirical observations on correla-
tion and dependence remains an open challenge.
We believe this type of modelling is crucial for future
developments in ﬁnance. The ﬁnancial crisis that oc-
curred in 2007-2008is expected to create a shock in clas-
sic modelling in Economics and Finance. Many scientists
haveexpressedtheirviewsonthissubject(e.g. Bouchaud
(2008); Lux and Westerhoﬀ (2009); Farmer and Foley
(2009)) and we believe as well that agent-based models
we have presented here will be at the core of future mod-
elling. As illustrations, let us mention Iori et al.(2006),
which models the interbank market and investigates sys-
temic risk, Thurner et al.(2009), which investigates the
eﬀects of use of leverage and margin calls on the stabil-
ity of a market and Yakovenko and Rosser (2009), which
provides a brief overview of the study of wealth distribu-
tions and inequalities. No doubt these will be followed
by many other contributions.
ACKNOWLEDGEMENTS
The authors would like to thank their collaborators
and two anonymous reviewers whose comments greatly
helped improving the review. AC is grateful to B.K.
Chakrabarti, K. Kaski, J. Kertesz, T. Lux, M. Marsili,
D. Stauﬀer and V. Yakovenko for invaluable suggestions
and criticisms.
Alﬁ, V., Cristelli, M., Pietronero, L. and Zaccaria, A., Min imal
agent based model for ﬁnancial markets I. The European Physi-
cal Journal B - Condensed Matter and Complex Systems , 2009a,
67, 385–397.
Alﬁ, V., Cristelli, M., Pietronero, L. and Zaccaria, A., Min imal
agent based model for ﬁnancial markets II. The European Physi-cal Journal B - Condensed Matter and Complex Systems , 2009b,
67, 399–417.
Angle, J., The surplus theory of social stratiﬁcation and th e size
distributionofpersonalwealth. In Proceedings of the Proceedings
of the American Social Statistical Association, Social Sta tistics
Section, p. 395, 1983 (Alexandria, VA).
Angle, J., The surplus theory of social stratiﬁcation and th e size
distribution of personal wealth. Social Forces , 1986,65, 293.
Angle, J., The Statistical signature of pervasive competit ion on
wage and salary incomes. J. Math. Sociol. , 2002,26, 217.
Angle, J., The Inequality Process as a wealth maximizing pro cess.
Physica A , 2006,367, 388.
Aoyama, H., Souma, W. and Fujiwara, Y., Growth and ﬂuctuatio ns
of personal and company’s income. Physica A , 2003,324, 352.
Arthur, W.B., Inductive Reasoning and Bounded Rationality .The
American Economic Review , 1994,84, 406–411.
Arthur, W., Complexity and the economy. Science, 1999,284, 107.
Ausloos, M. and Pekalski, A., Model of wealth and goods dynam ics
in a closed market. Physica A , 2007,373, 560.
Bachelier, L., Theorie de la speculation. Annales Scientiﬁques de
l’Ecole Normale Superieure , 1900,III-17, 21–86.
Bak, P., Paczuski, M. and Shubik, M., Price variations in a st ock
market with many agents. Physica A: Statistical and Theoretical
Physics, 1997,246, 430 – 453.
Barndorﬀ-Nielsen, O.E. and Shephard, N., Econometric Soci ety
Monograph, Variation, jumps, market frictions and high fre -
quency data in ﬁnancial econometrics. In Advances in Eco-
nomics and Econometrics: Theory and Applications, Ninth
World Congress , 2007, Cambridge University Press.
Benhabib, J. and Bisin, A., The distribution of wealth and ﬁs cal
policy in economies with ﬁnitely lived agents. National Bureau
of Economic Research Working Paper Series , 2009,No. 14730 .
Bennati, E., La simulazione statistica nell’analisi della dis-
tribuzione del reddito: modelli realistici e metodo di Mont e
Carlo1988a, ETS Editrice, Pisa.
Bennati, E., Un metodo di simulazione statistica nell’anal isi della
distribuzione del reddito. Rivista Internazionale di Scienze Eco-
nomiche e Commerciali , 1988b, 35, 735–756.
Bennati, E., Il metodo Monte Carlo nell’analisi economica. InPro-
ceedings of the Rassegna di lavori dell’ISCO X , Vol. 31, 1993.
Biais, B., Foucault, T. and Hillion, P., Microstructure des marches
ﬁnanciers : Institutions, modeles et tests empiriques 1997,
Presses Universitaires de France - PUF.
Biais, B., Hillion, P. and Spatt, C., An empirical analysis o f the
limit order book and the order ﬂow in the Paris Bourse. Journal
of Finance , 1995, pp. 1655–1689.
Black, F. and Scholes, M., The pricing of options and corpora te
liabilities. Journal of political economy , 1973,81, 637.
Blatt, M., Wiseman, S. and Domany, E., Superparamagnetic Cl us-
tering of Data. Physical Review Letters , 1996,76, 3251.
Bollerslev, T., Engle, R.F. and Nelson, D.B., Chapter 49 Arc h
models. In Handbook of Econometrics 4 , edited by R.F. Engle
and D.L. McFadden, pp. 2959 – 3038, 1994, Elsevier.
Bonanno, G., Lillo, F. and Mantegna, R.N., High-frequency c ross-
correlation in a set of stocks. Quantitative Finance , 2001,1, 96.
Boness, A.J., In The Random Character of Stock Market Prices ,
edited by P.H. Cootner, chap. English translation of: L. Bac he-
lier, Theorie de la Speculation, Annales scientiﬁques de l’ Ecole
Normale Superieure III-17, 1967, MIT Press.
Bouchaud, J.P. and Mezard, M., Wealth condensation in a simp le
model of economy. Physica A , 2000,282, 536.
Bouchaud, J.P., M´ ezard, M. and Potters, M., Statistical pr operties
of stock order books: empirical results and models. Quantitative
Finance, 2002,2, 251.
Bouchaud, J., Farmer, J.D., Lillo, F., Hens, T. and Schenk-H opp,
K.R., How Markets Slowly Digest Changes in Supply and De-
mand. In Handbook of Financial Markets: Dynamics and Evo-
lution, pp. 57–160, 2009 (North-Holland: San Diego).
Bouchaud, J., Economics needs a scientiﬁc revolution. Nature,
2008,455, 1181.",2009-09-10T15:25:45Z,economics nance psics as mike farmer cont  chacont iit lux marcs trefore imti nally imp le  t economics nance many  chalux sterho farmer foley as ori turer v e roster no t chara bart ka  kept lux mars li tau v e al is tell metro nero zac aria mit aph ys journal connsed maer lex tems al is tell metro nero zac aria mit aph ys journal connsed maer lex tems ale t iproceedis proceedis social statistical associatsocial st sealexandria ale t social forcale t statistical math soc ale t inequality process psics obama so uma fiwara groh psics arthur induive reasoni nd ratnality t economic review arthur lexity science us loss pek al  mpsics cac lit or ie annals cie nti ecole normale suieure bak pac zu  sh ubi price psics statistical toical psics bardor nielsesprd econometric soci monograph variatiadvanceco econometrics tory applicatns ninth world coress cambridge   behabit is it natnal bureau economic researworki pa serino beeat la mont  edit rice visa beeat uri vista internaznale sci enz eco coercial beeat    ipro pass eg na vbia is foucault mlm struure institutns  articial intellence re france bia is mlspat aparis course journal nance  schools t journal blast basemado many su  magnetic  data psical review ters roller lev eagle nelsochapter arc ihandbook econometrics eagle mc madelse vier bonanno lle antenna hh quantitative nance bonit random charaer stock market priccost ner eh bac t or ie speculatannals ecole normale suieure   chame hard alth psics  chaters statistical quantitative nance  chafarmer lle ns cck how markets slowly dest supply  ihandbook nancial markets dynamics evo north holland sadiego  chaeconomics nature
paper_qf_47.pdf,47,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","47
Brock, W.A. and Hommes, C.H., Heterogeneous beliefs and rou tes
to chaos in a simple asset pricing model. Journal of Economic
Dynamics and Control , 1998,22, 1235–1274.
Burda, Z., Jurkiewics, J. and Nowak, M.A., Is Econophysics a solid
science?. Acta Physica Polonica B , 2003,34, 87.
Burda, Z., Jurkiewicz, J., Nowak, M.A., Papp, G. and Zahed, I .,
Levy Matrices and Financial Covariances. cond-mat/0103108 ,
2001.
Case, K.E. and Fair, R.C., Principles of Economics , International
edition 2008, Pearson Education Limited.
Cavagna, A., Garrahan, J.P., Giardina, I. and Sherrington, D.,
Thermal Model for Adaptive Competition in a Market. Phys.
Rev. Lett. , 1999,21, 4429.
Chakrabarti, A.S. and Chakrabarti, B.K., Microeconomics o f the
ideal gas like market models. Physica A: Statistical Mechanics
and its Applications , 2009,388, 4151 – 4158.
Chakrabarti, A.S. and Chakrabarti, B.K., Statistical Theo ries of
Income and Wealth Distribution. Economics E-Journal (open
access), 2010,4.
Chakrabarti, A.S., Chakrabarti, B.K., Chatterjee, A. and M itra,
M., The Kolkata Paise Restaurant problem and resource uti-
lization. Physica A: Statistical Mechanics and its Applications ,
2009,388, 2420–2426.
Chakrabarti, B.K., Chakraborti, A. and Chatterjee, A. (Eds )
Econophysics and Sociophysics: Trends and Perspectives , 1st
2006, Wiley - VCH, Berlin.
Chakrabarti, B.K. and Chatterjee, A., Ideal Gas-Like Distr ibutions
in Economics: Eﬀects of Saving Propensity. In Proceedings of
the Application of Econophysics , edited by H. Takayasu, 2003
(Springer-Verlag: Tokyo).
Chakraborti, A., Distributions of money in model markets of econ-
omy.International Journal of Modern Physics C-Physics and
Computer , 2002,13, 1315–1322.
Chakraborti, A. and Chakrabarti, B., Statistical mechanic s of
money: how saving propensity aﬀects its distribution. The Eu-
ropean Physical Journal B - Condensed Matter and Complex
Systems, 2000,17, 167–170.
Chakraborti, A. and Patriarca, M., Gamma-distribution and
Wealth inequality. Pramana J. Phys , 2008,71, 233.
Chakraborti, A., Patriarca, M. and Santhanam, M.S., Financ ial
Time-seriesAnalysis: a BriefOverview.In Econophysics of Mar-
kets and Business Networks , 2007 (Springer: Milan).
Chakraborti, A. and Patriarca, M., Variational Principle f or the
Pareto Power Law. Physical Review Letters , 2009,103, 228701.
Challet, D., Marsili, M. and Zecchina, R., Statistical Mech anics
of Systems with Heterogeneous Agents: Minority Games. Phys.
Rev. Lett. , 2000,84, 1824.
Challet, D. and Stinchcombe, R., Analyzing and modeling 1+ 1 d
markets. Physica A: Statistical Mechanics and its Applications ,
2001,300, 285–299.
Challet, D. and Zhang, Y.C., Emergence of cooperation and or -
ganization in an evolutionary game. Physica A: Statistical and
Theoretical Physics , 1997,246, 407–418.
Challet, D., Marsili, M. and Zhang, Y., Minority Games 2004,
Oxford University Press.
Challet, D. and Zhang, Y.C., On the minority game: Analytica l
and numerical studies. Physica A , 1998,256, 514.
Champernowne, D.G., A Model of Income Distribution. The Eco-
nomic Journal , 1953,63, 318–351.
Chatterjee, A., Chakrabarti, B.K. and Stinchcombe, R.B., M aster
equation for a kinetic model of trading market and its analyt ic
solution. Phys. Rev. E , 2005a, 72, 026126.
Chatterjee, A. and Chakrabarti, B., Kinetic exchange model s for
income and wealth distributions. Eur. Phys. J. B , 2007,60, 135.
Chatterjee, A., Yarlagadda, S. and Chakrabarti, B.K.(Eds) Econo-
physics of Wealth Distributions 2005b, Springer.
Chatterjee, A., Chakrabarti, B.K. and Manna, S.S., Money in Gas-
Like Markets: Gibbs and Pareto Laws. Physica Scripta , 2003,
T106, 36–38.Chatterjee, A., Chakrabarti, B.K. and Manna, S.S., Pareto l aw in a
kinetic model of market with random saving propensity. Physica
A: Statistical Mechanics and its Applications , 2004,335, 155–
163.
Chiarella, C. and Iori, G., A simulation analysis of the micr ostruc-
ture of double auction markets. Quantitative Finance , 2002,2,
346–353.
Chiarella, C., He, X. and Hommes, C., A dynamic analysis of mo v-
ing average rules. Journal of Economic Dynamics and Control ,
2006,30, 1729–1753.
Cizeau, P., Potters, M. and Bouchaud, J., Correlation struc ture of
extreme stock returns. Quantitative Finance , 2001,1, 217.
Cizeau, P., Liu, Y., Meyer, M., Peng, C.K. and Stanley, H.E.,
Volatility distribution in the S& P500 stock index. Physica A:
Statistical and Theoretical Physics , 1997,245, 441 – 445.
Clark, P.K., A Subordinated Stochastic Process Model with F inite
Variance for Speculative Prices. Econometrica , 1973,41, 135–
55.
Cliﬀ, D. and Bruten, J., Zero is not enough: On the lower limit
of agent intelligence for continuous double auction market s.
1997, Technical report HPL-97-141, Hewlett-Packard Labor a-
tories, Bristol, UK.
Cohen, M., Report of the Special Study of the Securities Mark ets
of the Securities and Exchanges Commission. 1963a, Technic al
report U.S. Congress, 88th Cong., 1st sess., H. Document 95.
Cohen, M.H., Reﬂections on the Special Study of the Securiti es
Markets. 1963b, Technical report Speech at the Practising L aw
Intitute, New York.
Cont, R., Empirical properties of asset returns: stylized f acts and
statistical issues. Quantitative Finance , 2001,1, 223–236.
Cont, R. and Bouchaud, J.P., Herd behavior and aggregate ﬂuc tu-
ations in ﬁnancial markets. Macroeconomic dynamics , 2000,4,
170–196.
Cont, R., Potters, M. and Bouchaud, J.P., Scale Invariance a nd
Beyond. In Proceedings of the Scale Invariance and Beyond:
Les Houches Workshop 1997 , edited by F.G. B. Dubrulle and
D. Sornette, 1997, Springer.
Cont, R. and Tankov, P., Financial modelling with jump processes
2004, Chapman & Hall/CRC.
Cont, R., Volatility Clustering in Financial Markets: Empi rical
Facts and Agent-Based Models. In Long Memory in Economics ,
pp. 289–309, 2007.
Cont, R., Stoikov, S. and Talreja, R., A Stochastic Model for Order
Book Dynamics. SSRN eLibrary , 2008.
Coolen, A., The Mathematical Theory Of Minority Games: Statis-
tical Mechanics Of Interacting Agents 2005, Oxford University
Press.
Cordier, S., Pareschi, L. and Toscani, G., On a kinetic model for a
simple market economy. J. Stat. Phys. , 2005,120, 253.
Courtault, J., Kabanov, Y., Bru, B., Crepel, P., Lebon, I. an d
Le Marchand, A., Louis Bachelier on the centenary of Theorie
de la Speculation. Mathematical Finance , 2000,10, 339–353.
Das, A. and Yarlagadda, S., A distribution function analysi s of
wealth distribution. , 2003.
Das, A. and Yarlagadda, S., An analytic treatment of the Gibb s–
Pareto behavior in wealth distribution. Physica A , 2005,353,
529.
de Haan, L., Resnick, S. and Drees, H., How to make a Hill plot.
The Annals of Statistics , 2000,28, 254–274.
de Oliveira, S.M., de Oliveira, P.M.C. and Stauﬀer, D., Evolution,
Money, War and Computers 1999, B. G. Teubner, Stuttgart-
Leipzig.
di Ettore Majorana, N., Il valore delle leggi statistiche ne lla ﬁsica
e nelle scienze sociali. Scientia , 1942,36, 58–66.
Dragulescu, A. and Yakovenko, V., Statistical mechanics of money.
The European Physical Journal B , 2000,17, 7 pages.
Dragulescu, A. and Yakovenko, V., Evidence for the exponent ial
distribution of income in the USA. The European Physical Jour-
nal B, 2001a, 20, 5 pages.",2009-09-10T15:25:45Z,brock hoterogeneous journal economic dynamics contrbur da jur view ics iki is eco no psics a psics polo nica bur da jur view cz iki app zh ed levy matricnancial co variance case articial intellence principleconomics internatnal pearsoeducatlimited ce grarr thagi ard isrritotrmal madaptive etitmarket ph ys rev  chara bart chara bart maoeconomic psics statistical meics applicatns chara bart chara bart statistical t income alth distributeconomics journal chara bart chara bart terjee t lkata articial intellence se restaurant psics statistical meics applicatns chara bart chara bor ti terjee eds eco no psics soc psics trends speivrey berlichara bart terjee ial gas like is tr economics si prsity iproceedis applicateco no psics kaya su  verlag  chara bor ti distributns internatnal journal morpsics psics uter chara bor ti chara bart statistical t eu psical journal connsed maer lex tems chara bor ti patria rca gaa alth romana ph ys chara bor ti patria rca sathaam anc time analysis brief overview ieco no psics mar business networks  machara bor ti patria rca variatal principle party  law psical review ters chal  mars li ze  statistical me tems terogeneous  minority gamph ys rev  chal  stinalyzi psics statistical meics applicatns chal   emergence psics statistical toical psics chal  mars li  minority gamoxford   chal   oanalytic psics ham  ne mincome distributt eco journal terjee chara bart stinchph ys rev terjee chara bart etic eur ph ys terjee yar lag add chara bart eds eco no alth distributns  terjee chara bart maa money gas like markets gibbs party laws psics sipt terjee chara bart maa party psics statistical meics applicatns chi are lla ori quantitative nance chi are lla  hojournal economic dynamics contrsize au ters  chacorrelatquantitative nance size au  meyer pe stanley volatity psics statistical toical psics ark subordinated stochastic process mvariance speculative priceconometric i brute zero otechnical ro package labor bristcoret special study securitimark securitiexcoisstechnical coress co document core special study se curit markets technical speepraisi iit ute new york cont emical quantitative nance cont  chard maoeconomic cont ters  chascale ivariance beyond iproceedis scale ivariance beyond lhou c workshdub rul le corvee  cont tank ov nancial chapmahall cont volatity usteri nancial markets emp fas agent based mols ilo memory economics cont to v tal re stochastic morr book dynamics library coet matmatical tory of minority gamstat is meics of inti  oxford   cord er pagchi to caostat ph ys court all kabanov bru repel leb ole marand louis cac lit or ie speculatmatmatical nance das yar lag add das yar lag add agrbb party psics hasrnick free how hl t annals statistics oliolitau evolutmoney war uters te bner stugart leipz entire majori  sci etia drag ul escu v e statistical t apsical journal drag ul escu v e evince t apsical jour
paper_qf_47.pdf,48,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","48
Dragulescu, A. and Yakovenko, V.M., Exponential and power- law
probability distributions of wealth and income in the Unite d
Kingdom and the United States. Physica A: Statistical Mechan-
ics and its Applications , 2001b, 299, 213–221.
Dremin, I. and Leonidov, A., On distribution of number of tra des
in diﬀerent time windows in the stock market. Physica A: Sta-
tistical Mechanics and its Applications , 2005,353, 388 – 402.
Duﬃe, D., Dynamic asset pricing theory 1996, Princeton Univer-
sity Press Princeton, NJ.
D¨ uring, B., Matthes, D. and Toscani, G., Kinetic equations mod-
elling wealth redistribution: A comparison of approaches. Phys.
Rev. E, 2008,78, 056103.
D¨ uring, B. and Toscani, G., Hydrodynamics from kinetic mod els
of conservative economies. Physica A , 2007,384, 493.
Engle, R.F., The Econometrics of Ultra-High-Frequency Dat a.
Econometrica , 2000,68, 1–22.
Engle, R.F. and Russell, J.R., Forecasting the frequency of changes
in quoted foreign exchange prices with the autoregressive c ondi-
tional duration model. Journal of Empirical Finance , 1997,4,
187–212.
Epps, T.W., Comovements in Stock Prices in the Very Short Run .
Journal of the American Statistical Association , 1979,74, 291–
298.
Erdos, P. and Renyi, A., On the evolution of random graphs. Publ.
Math. Inst. Hung. Acad. Sci , 1960,5, 17–61.
Farmer, J.D. and Foley, D., The economy needs agent-based mo d-
elling.Nature, 2009,460, 685–686.
Feller, W., Introduction to the Theory of Probability and its Appli-
cations, Vol. 2 1968, Wiley, New York.
Ferrero, J.C., The statistical distribution of money and th e rate of
money transference. Physica A , 2004,341, 575.
Follmer, H. and Schied, A., Stochastic Finance: An Introduction In
Discrete Time 2 , 2nd Revised edition 2004, Walter de Gruyter
& Co.
Forfar, D. Louis Bachelier, In: The MacTutor history of math emat-
ics archive (published online), J. O’Connor and E. F. Robert son
eds. 2002.
Fujiwara, Y., Souma, W., Aoyama, H., Kaizoji, T. and Aoki, M. ,
Growth and ﬂuctuations of personal income. Physica A: Statis-
tical Mechanics and its Applications , 2003,321, 598–604.
Gabaix, X., Zipf’s Law For Cities: An Explanation*. Quarterly
Journal of Economics , 1999,114, 739–767.
Gabaix, X., Gopikrishnan, P., Plerou, V.and Stanley, H.E., Institu-
tional Investors and Stock Market Volatility. Quarterly Journal
of Economics , 2006,121, 461–504.
Gallegati, M. and Kirman, A.P. (Eds) Beyond the Representative
Agent, 1st 1999, Edward Elgar Publishing.
Garibaldi, U., Scalas, E.and Viarengo, P., Statistical equ ilibriumin
simple exchange games II. The redistribution game. Eur. Phys.
J. B, 2007,60, 241.
Garman, M.B., Market microstructure. Journal of Financial Eco-
nomics, 1976,3, 257 – 275.
Gatheral, J., The volatility surface: a practitioner’s guide 2006,
Wiley.
Ghosh, A., Chakrabarti, A.S.andChakrabarti, B.K.,Kolkat a Paise
Restaurant problem in some uniform learning strategy limit s. In
Econophysics & Economis of Games, Social Choices & Quanti-
tative Techniques , pp. 3–9, 2010a (Springer: Milan).
Ghosh, A. and Chakrabarti, B.K., Mathematica Demonstra-
tion of the Kolkata Paise Restaurant (KPR) problem.
http://demonstrations.wolfram.com/ 2009.
Ghosh, A., Chatterjee, A., Mitra, M. and Chakrabarti, B.K.,
Statistics of the Kolkata Paise Restaurant Problem. New J.
Phys., 2010b (in press).
Gibrat, R., Les in´ egalit´ es ´ economiques. , 1931.
Glosten, L.R., Is the Electronic Open Limit Order Book In-
evitable?. The Journal of Finance , 1994,49, 1127–1161.
Gode, D.K. and Sunder, S., Allocative Eﬃciency of Markets wi th
Zero-Intelligence Traders: Market as a Partial Substitute for
Individual Rationality. The Journal of Political Economy , 1993,101, 119–137.
Gopikrishnan, P., Plerou, V., Gabaix, X. and Stanley, H.E., Sta-
tistical properties of share volume traded in ﬁnancial mark ets.
Physical Review - Series E- , 2000a, 62, 4493–4496.
Gopikrishnan, P., Meyer, M., Amaral, L.A. and Stanley, H.E. , In-
verse Cubic Law for the Probability Distribution of Stock Pr ice
Variations. The European Physical Journal B , 1998,3, 139.
Gopikrishnan, P., Plerou, V., Amaral, L.A., Meyer, M. and St an-
ley, H.E., Scaling of the distribution of ﬂuctuations of ﬁna ncial
market indices. Phys. Rev. E , 1999,60, 5305–5316.
Gopikrishnan, P., Plerou, V., Gabaix, X. and Stanley, H.E., Sta-
tistical properties of share volume traded in ﬁnancial mark ets.
Physical Review E , 2000b, 62, R4493.
Gopikrishnan, P., Rosenow, B., Plerou, V. and Stanley, H.E. ,
Quantifying and interpreting collective behavior in ﬁnanc ial
markets. Physical Review E , 2001,64, 035106.
Griﬃn,J.E.and Oomen, R.C.A.,Samplingreturnsforrealize dvari-
ance calculations: tick time or transaction time?. Econometric
Reviews, 2008,27, 230–253.
Gu, G. and Zhou, W., Emergence of long memory in stock volatil -
ity from a modiﬁed Mike-Farmer model. EPL (Europhysics Let-
ters), 2009,86, 48002.
Guillaume, D., Dacorogna, M., Dav´ e, R., M¨ uller, U., Olsen , R. and
Pictet, O., From the bird’s eye to the microscope: A survey of
new stylized facts of the intra-daily foreign exchange mark ets.
Finance and Stochastics , 1997,1, 95–129.
Gupta, A.K., Money exchange model and a general outlook. Phys-
ica A, 2006,359, 634.
Gupta, A.K., Relaxation in the wealth exchange models. Physica
A, 2008,387, 6819.
Haberman, S. and Sibbett, T.A. (Eds), English translation o f:
Louis bachelier, Th´ eorie de la sp´ eculation, Annales scie ntiﬁques
de l’Ecole Normale Superieure. In History of Actuarial Science
7, 1995, Pickering and Chatto Publishers, London.
Hakansson, N., Beja, A. and Kale, J., On the feasibility of au to-
mated market making by a programmed specialist. Journal of
Finance, 1985, pp. 1–20.
Hart, M.L., Jeﬀeries, P., Hui, P.M. and Johnson, N.F., From m ar-
ket games to real-world markets. European Physical Journal B ,
2001,20, 547.
Hasbrouck, J., Empirical Market Microstructure: The Institutions,
Economics, and Econometrics of Securities Trading 2007, Ox-
ford University Press, USA.
Hautsch, N., Modelling irregularly spaced ﬁnancial data 2004,
Springer.
Hayashi, T. and Yoshida, N., On covariance estimation of non -
synchronously observed diﬀusion processes. Bernoulli , 2005,11,
359–379.
Hayes, B., Follow the Money. Am. Sci. , 2002,90.
Heston, S., A closed-form solution for options with stochas tic
volatility with applications to bond and currency options. Rev.
Financ. Stud. , 1993,6, 327–343.
Hill, B.M., A Simple General Approach to Inference About the Tail
of a Distribution. The Annals of Statistics , 1975,3, 1163–1174.
Huth, N. and Abergel, F., The Times Change: Multivariate Sub -
ordination, Empirical Facts. SSRN eLibrary , 2009.
Iglesias, J.R., Goncalves, S., Abramsonb, G. and Vega, J.L. , Cor-
relation between risk aversion and wealth distribution. Physica
A, 2004,342, 186.
Iglesias, J.R., Goncalves, S., Pianegonda, S., Vega, J.L. a nd
Abramson, G., Wealth redistribution in our small world. Physica
A, 2003,327, 12.
Iori, G., Jafarey, S. and Padilla, F.G., Systemic risk on the inter-
bank market. Journal of Economic Behavior & Organization ,
2006,61, 525–542.
Iori, G. and Precup, O.V., Weighted network analysis of high -
frequency cross-correlation measures. Physical Review E (Statis-
tical, Nonlinear, and Soft Matter Physics) , 2007,75, 036110–7.
Ispolatov, S., Krapivsky, P.L. and Redner, S., Wealth distr ibutions
in asset exchange models. Eur. Phys. J. B , 1998,2, 267.",2009-09-10T15:25:45Z,drag ul escu v e eonential unite kidom united statpsics statistical me chaapplicatns dr milogid ov opsics st meics applicatns du dynamic princetouo princetomat t to caetic ph ys rev to cadrodynamics psics eagle t econometrics ultra hh frequency dat econometric eagle russell forecasti journal emical nance apps co movements stock pricvery short rournal statistical associatordo re nyi opub math ist hu acad sci farmer foley t nature seller introdutory probabity app li vrey new york  t psics oll mer sc stochastic nance aintroduidisete time revised walter roster co format louis cac liit mac tutor connor robert fiwara so uma obama articial intellence zo ji aki groh psics stat is meics applicatns ga articial intellence zip law for citiaelanatquarterly journal economics ga articial intellence go pi krishna le rou stanley ist it investors stock market volatity quarterly journal economics gable gat kir maeds beyond representative agent edward edgar pubhi garibaldi scale vi are o statistical t eur ph ys germamarket journal nancial eco gatr al t rey ghost chara bart chara bart kkat articial intellence se restaurant ieco no psics eco nom is gamsocial choicquant techni  maghost chara bart matmatics mostra lkata articial intellence se restaurant ghost terjee mira chara bart statistics lkata articial intellence se restaurant problem new ph ys gi brat lgl ofteis eleronic opelimit orr book it journal nance co unr al locatmarkets zero intellence trars market partial substitute individual ratnality t journal political economy go pi krishna le rou ga articial intellence stanley st psical review sero pi krishna meyer moral stanley icubic law probabity distributstock pr variatns t apsical journal go pi krishna le rou moral meyer st scali ph ys rev go pi krishna le rou ga articial intellence stanley st psical review go pi krishna rose  le rou stanley qualyi psical review gri oo mesampli urns for realize econometric reviews gu hou emergence mike farmer euro psics  gulaume da core gd olsepic tet from nance stochastic gupta money ph ys gupta relaxatpsics haber masib be  eds eh louis th annals ecole normale suieure ihistory auarial science bickeri  to pubrs londoaka nssobe male ojournal nance hart je hui sofrom apsical journal harbor uk emical market m struure t institutns economics econometrics securititradi ox   hats molli  hayashi yeshiva obernoulli hayfollow money am sci storev anc stud hl simple genl approainference at articial intellence distributt annals statistics hu th aber gel t time multivariate sub emical fas library lesias goncalvas ovega cor psics lesias goncalvplagonna vega as oalth psics ori j ey vanla temic journal economic behr organizatori pre cup hted psical review stat is nonlinear soft maer psics is poll to rap iv  red ner alth eur ph ys
paper_qf_47.pdf,49,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","49
It¯ o, K. and McKean, H., Diﬀusion Processes and Their Sample
Paths1996 (Springer: Berlin).
Ivanov, P.C., Yuen, A., Podobnik, B. and Lee, Y., Common scal ing
patterns in intertrade times of U. S. stocks. Phys. Rev. E , 2004,
69, 056107.
Johnson, N.F., Hart, M.and Hui, P.M.,Crowd eﬀects and volat ility
in markets with competing agents. Physica A , 1999a, 269, 1.
Johnson, N.F., Hui, P.M., Johnson, R.and Lo, T.S., Self-Org anized
Segregation within an Evolving Population. Physical Review
Letter, 1999b, 82, 3360.
Kadanoﬀ, L., From simulation model to public policy: An exam i-
nation of Forrester’s” Urban Dynamics”. Simulation , 1971,16,
261.
Kaldor, N., Economic growth and capital accumulation. The The-
ory of Capital, Macmillan, London , 1961.
Kandori, M., Repeated Games. In The New Palgrave Dictionary
of Economics , 2008, Palgrave Macmillan.
Keynes, J.M., The general theory of employment, Interest and
Money1973, The Royal Economic Society, Macmillan Press,
London.
Kindleberger, C.P.and Aliber,R.Z., Manias, Panics, And Crashes:
A History Of Financial Crises , Fifth edition 2005, John Wiley
& Sons.
Kirman, A., In Individual Forecasting and Aggregate Outcomes:
Rational Expectations Examined , edited by R. Frydman and
E. Phelps, chap. On mistaken beliefs and resultant equilibr ia,
1983, Cambridge University Press.
Kirman, A., Ants, rationality, and recruitment. The Quarterly
Journal of Economics , 1993, pp. 137–156.
Kirman, A., Reﬂections on interaction and markets. Quantitative
Finance, 2002,2, 322–326.
Kirman, A., Whom or what does the representative individual rep-
resent?. The Journal of Economic Perspectives , 1992, pp. 117–
136.
Kullmann, L., Kertesz, J. and Mantegna, R.N., Identiﬁcatio n
of clusters of companies in stock indices via Potts super-
paramagnetic transitions. Physica A: Statistical Mechanics and
its Applications , 2000,287, 412419.
Kullmann, L., Toyli, J., Kertesz, J., Kanto, A. and Kaski, K. ,
Characteristic times in stock market indices. Physica A: Statis-
tical Mechanics and its Applications , 1999,269, 98 – 110.
Kyle, A.S., Continuous Auctions and Insider Trading. Economet-
rica, 1985,53, 1315–1335.
Lallouache, M., Jedidi, A. and Chakraborti, A., Wealth dist ri-
bution: To be or not to be a Gamma?. Science and Culture
(Kolkata, India), special issue on ”Econophysics” , 2010.
Laloux, L., Cizeau, P., Bouchaud, J. and Potters, M., Noise D ress-
ing of Financial Correlation Matrices. Physical Review Letters ,
1999,83, 1467.
Lamper, D., Howison, S.D. and Johnson, N.F., Predictabilit y of
Large Future Changes in a Competitive Evolving Population.
Phys. Rev. Lett. , 2002,88, 017902.
Landau, L.D., Statistical Physics, Vol. 5 of Theoretical Physics
1965, Pergamon Press, Oxford.
LeBaron, B., Agent-based computational ﬁnance. Handbook of
computational economics , 2006a, 2, 1187–1233.
LeBaron, B., Agent-Based Financial Markets: Matching Styl ized
Facts with Style. In Post Walrasian macroeconomics , edited by
D.C. Colander, p. 439, 2006b, Cambridge University Press.
Levy, M. and Solomon, S., Power laws are logarithmic Boltzma nn
laws.Int. J. Mod. Phys. C , 1996,7, 595.
Li, Y., Riolo, R. and Savit, R., Evolution in minority games. (I).
Games with a ﬁxed strategy space. Physica A: Statistical Me-
chanics and its Applications , 2000a, 276, 234–264.
Li, Y., Riolo, R. and Savit, R., Evolution in minority games. (II).
Games with variable strategy spaces. Physica A: Statistical Me-
chanics and its Applications , 2000b, 276, 265–283.
Lillo, F., Farmer, D. and Mantegna, R., Econophysics: Maste r
curve for price-impact function. Nature, 2003,421, 130, 129.Lillo, F. and Farmer, J.D., The Long Memory of the Eﬃcient Mar -
ket.Studies in Nonlinear Dynamics & Econometrics , 2004,8.
Liu, Y., Cizeau, P., Meyer, M., Peng, C.K. and Stanley, H.E.,
Correlations in economic time series. Physica A: Statistical and
Theoretical Physics , 1997,245, 437 – 440.
Lux, T.and Marchesi, M., Volatility clustering in ﬁnancial markets.
Int. J. Theo. Appl. Finance , 2000,3, 675–702.
Lux, T. and Westerhoﬀ, F., Economics crisis. Nature Physics , 2009,
5, 2–3.
Lux, T., Emergent Statistical Wealth Distributions in Simp le Mon-
etary Exchange Models: A CriticalReview. In Proceedings of the
Econophysics of Wealth Distributions , edited by A. Chatterjee,
S.Yarlagadda and B.K. Chakrabarti, p. 51, 2005, Springer.
Lux, T. and Sornette, D., On Rational Bubbles and Fat Tails.
Journal of Money, Credit, and Banking , 2002,34, 589–610.
Malliavin, P. and Mancino, M.E., Fourier series method for m ea-
surement of multivariate volatilities. Finance and Stochastics ,
2002,6, 49–61.
Mandelbrot, B., The Pareto-Levy law and the distribution of in-
come.International Economic Review , 1960, pp. 79–106.
Mandelbrot, B., The variation of certain speculative price s.Journal
of business , 1963,36, 394.
Mantegna, R., Levy walks and enhanced diﬀusion in Milan stoc k
exchange. Physica. A , 1991,179, 232–242.
Mantegna, R., Hierarchical structure in ﬁnancial markets. The Eu-
ropean Physical Journal B - Condensed Matter and Complex
Systems, 1999,11, 193–197.
Mantegna, R., Presentation of the English translation of Et tore
Majorana’s paper: The value of statistical laws in physics a nd
social sciences. Quantitative Finance , 2005,5, 133–140.
Mantegna, R., The Tenth Article of Ettore Majorana. Europhysics
News, 2006,37.
Martino, A.D., Giardina, I., Tedeschi, A. and Marsili, M., G eneral-
ized minority games with adaptive trend-followers and cont rar-
ians.Physical Review E , 2004,70, 025104.
Maslov, S., Simplemodel of a limitorder-drivenmarket. Physica A:
Statistical Mechanics and its Applications , 2000,278, 571–578.
Maslov, S. and Mills, M., Price ﬂuctuations from the order bo ok
perspective – empirical facts and a simple model. Physica A:
Statistical Mechanics and its Applications , 2001,299, 234–246.
Matthes, D. and Toscani, G., On steady distributions of kine tic
models of conservative economies. J. Stat. Phys. , 2007, 130,
1087.
McAleer, M. and Medeiros, M.C., Realized volatility: A revi ew.
Econometric Reviews , 2008,27, 10–45.
Merton, R., Theory of rational option pricing. The Bell Journal of
Economics and Management Science , 1973, pp. 141–183.
Mike, S. and Farmer, J.D., An empiricalbehavioral model of l iquid-
ity and volatility. Journal of Economic Dynamics and Control ,
2008,32, 200–234.
Mohanty, P.K., Generic features of the wealth distribution in ideal-
gas-like markets. arXiv:physics/0603141 , 2006.
Montroll, E. and Badger, W., Introduction to quantitative aspects
of social phenomena 1974, Gordon and Breach New York.
Muni Toke, I., ”Market making” in an order book model and its
impact on the bid-ask spread. In Econophysics of Order-Driven
Markets, 2010 (Springer: Milan).
Nirei, M. and Souma, W.., A Two Factor Model of Income Distri-
bution Dynamics. The Review of Income and Wealth , 2007,53,
440–459.
Noh, J.D., Model for correlations in stock markets. Physical Review
E, 2000,61, 5981.
O’Hara, M., Market Microstructure Theory 1995, Blackwell Pub-
lishers.
O’Hara, M., Market Microstructure Theory , Second edition 1997,
Blackwell Publishers.
Onnela, J.P., Chakraborti, A., Kaski, K. and Kertesz, J., Dy namic
asset trees and Black Monday. Physica A: Statistical Mechanics
and its Applications , 2003a, 324, 247–252.",2009-09-10T15:25:45Z,it mc ke adi processtir sample paths  berliivayepoor   cooph ys rev sohart hui owd psics sohui solo self org segregatevolvi populatpsical review ter ada no from aforester urbadynamics simulatkal dor economic t t capital macmlalondoand ori repeated gamit new pal gre dinary economics pal gre macmlaeyt interest money t royal economic society macmla londodly berger liner mania panics and ass history of nancial isfth  rey sons kir maiindividual forecasti aregate outcomratnal eeatns examined fry maplps ocambridge   kir maants t quarterly journal economics kir mare quantitative nance kir mawhom t journal economic speivullmankept antenna int posts psics statistical meics applicatns ullmantoy li kept canto ka  charaeristic psics stat is meics applicatns kyle continuous auns insir tradi eco no met lal lou ae jedi di chara bor ti alth to gaa science culture lkata india eco no psics halo ux size au  chaters noise nancial correlatmatricpsical review ters lam  how is osopredi b it large future etitive evolvi populatph ys rev  land statistical psics vtoical psics gamo oxford le baroagent handbook le baroagent based nancial markets matchi l fas style ipost wal  coanr cambridge   levy solomo bolt zm imod ph ys li r lo s it evolutgampsics statistical me applicatns li r lo s it evolutgampsics statistical me applicatns lle farmer antenna eco no psics mast nature lle farmer t lo memory mar studinonlinear dynamics econometrics  size au meyer pe stanley correlatns psics statistical toical psics lux marcs volatity it ppl nance lux sterho economics nature psics lux emergent statistical alth distributns imp moexe mols itical review iproceedis eco no psics alth distributns terjee yar lag add chara bart  lux corvee oratnal bubblfat articial intellence ls journal money edit banki alla imac courier nance stochastic manlbrot t party levy internatnal economic review manlbrot t journal antenna levy mapsics antenna hirchical t eu psical journal connsed maer lex tems antenna presentateh et majori t quantitative nance antenna t tenth artie entire majori euro psics news marti ard ited eamars li psical review mas lov simple mpsics statistical meics applicatns mas lov mls price psics statistical meics applicatns mat t to caostat ph ys mc peer ma ros realized econometric reviews mortotory t bell journal economics management science mike farmer ajournal economic dynamics contrmohanty generic  mont roll badger introdugordobreanew york muni take market ieco no psics orr drivemarkets  mani rei so uma two faor mincome is tri dynamics t review income alth not mpsical review hara market m struure tory ll pub hara market m struure tory second ll pubrs onel chara bor ti ka  kept dy  monday psics statistical meics applicatns
paper_qf_47.pdf,50,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","50
Onnela, J.P., Taxonomy of ﬁnancial assets. Master’s thesis ,
Helsinki University of Technology, Espoo, Finland 2000.
Onnela, J., Chakraborti, A.,Kaski, K.andKertesz, J., Dyna micas-
set trees and portfolio analysis. The European Physical Journal
B - Condensed Matter and Complex Systems , 2002,30, 285–288.
Onnela, J., Chakraborti, A., Kaski, K., Kertesz, J. and Kant o,
A., Dynamics of market correlations: Taxonomy and portfoli o
analysis. Physical Review E , 2003b, 68, 056110.
Osborne, M.F.M., Brownian Motion in the Stock Market. Opera-
tions Research , 1959,7, 145–173.
Pagan, A., The econometrics of ﬁnancial markets. Journal of em-
pirical ﬁnance , 1996,3, 15–102.
Pareto, V., Cours d’economie politique 1897a (Rouge: Lausanne),
Reprinted as a volume of Oeuvres Completes, G. Bousquet and
G. Busino Eds., Droz, Genve, 1964.
Pareto, V., Cours d’economie politique 1897b, Rouge, Lausanne.
Parisi, G., Complex systems: a physicist’s viewpoint. Physica A:
Statistical Mechanics and its Applications , 1999,263, 557 – 564
Proceedings of the 20th IUPAP International Conference on S ta-
tistical Physics.
Pathria, R.K., Statistical Mechanics , 2nd 1996, Butterworth-
Heinemann, Oxford.
Patriarca, M., Chakraborti, A. and Germano, G., Inﬂuence of sav-
ing propensity on the power law tail ofwealth distribution. Phys-
ica A, 2006,369, 723.
Patriarca, M., Chakraborti, A. and Kaski, K., Statistical m odel
with a standard gamma distribution. Phys. Rev. E , 2004a, 70,
016104.
Patriarca, M., Chakraborti, A., Kaski, K. and Germano, G., K i-
netic theory models for the distribution of wealth: Power la w
from overlap of exponentials. In Proceedings of the Econophysics
of Wealth Distributions , edited by A. Chatterjee, S.Yarlagadda
and B.K. Chakrabarti, p. 93, 2005, Springer.
Patriarca, M., Chakraborti, A., Heinsalu, E. and Germano, G .,
Relaxation in statistical many-agent economy models. Eur. J.
Phys. B, 2007,57, 219.
Patriarca, M., Chakraborti, A. and Kaski, K., Gibbs versus n on-
Gibbs distributions in money dynamics. Physica A: Statistical
Mechanics and its Applications , 2004b, 340, 334–339.
Patriarca, M., Heinsalu, E. and Chakraborti, A., The ABCD’s of
statistical many-agent economy models. , 2009.
Patriarca, M., Heinsalu, E. and Chakraborti, A., Basic kine tic
wealth-exchange models: common features and open problems .
Eur. J. Phys. B 73 , 2010, p. 145.
Plerou, V., Gopikrishnan, P., Nunes Amaral, L.A., Gabaix, X . and
Eugene Stanley, H., Economic ﬂuctuations and anomalous dif -
fusion.Phys. Rev. E , 2000,62, R3023–R3026.
Plerou, V., Gopikrishnan, P., Rosenow, B., Amaral, L.A.N., Guhr,
T. and Stanley, H.E., Random matrix approach to cross corre-
lations in ﬁnancial data. Physical Review E , 2002,65, 066126.
Plerou, V., Gopikrishnan, P., Rosenow, B., Amaral, L.A.N. a nd
Stanley, H.E., Universal and Nonuniversal Properties of Cr oss
Correlations in Financial Time Series. Physical Review Letters ,
1999,83, 1471.
Podobnik, B., Ivanov, P.C., Lee, Y., Chessa, A. and Stanley, H.E.,
Systems with correlations in the variance: Generating powe r
lawtailsinprobability distributions. EPL (Europhysics Letters) ,
2000,50, 711–717.
Politi, M. and Scalas, E., Fitting the empirical distributi on of in-
tertrade durations. Physica A: Statistical Mechanics and its Ap-
plications , 2008,387, 2025 – 2034.
Potters, M.and Bouchaud, J.P., Morestatistical propertie s oforder
books and price impact. Physica A: Statistical Mechanics and
its Applications , 2003,324, 133–140.
Preis, T., Golke, S., Paul, W. and Schneider, J.J., Statisti cal anal-
ysis of ﬁnancial returns for a multiagent order book model of
asset trading. Physical Review E , 2007,76.
Raberto, M., Cincotti, S., Focardi, S. and Marchesi, M., Age nt-
based simulation of a ﬁnancial market. Physica A: Statistical
Mechanics and its Applications , 2001,299, 319–327.Reif, F., Fundamentals of Statistical and Thermal Physics 1985,
Mc Grow-Hill, Singapore.
Reno, R., A closer look at the Epps eﬀect. International Journal
of Theoretical and Applied Finance , 2003,6, 87–102.
Repetowicz, P., Hutzler, S. and Richmond, P., Dynamics of Mo ney
and Income Distributions. Physica A , 2005,356, 641.
Roehner, B., Patterns of speculation: a study in observational
econophysics 2002, Cambridge University Press.
Saha, M.N., Srivastava, B.N. and Saha, M..S., A treatise on heat
1950 (Indian Press: Allahabad).
Sala-i Martin, X., The World Distribution Of Income. NBER
Working Paper Series , 2002.
Sala-i Martin, X. and Mohapatra, S., Poverty, Inequality an d the
Distribution of Income in the G20. Columbia University, De-
partment of Economics, Discussion Paper Series , 2002.
Samanidou, E., Zschischang, E., Stauﬀer, D. and Lux, T., Age nt-
based models of ﬁnancial markets. Reports on Progress in
Physics, 2007,70, 409–450.
Samuelson, P., Proof that properly anticipated prices ﬂuct uate ran-
domly.Management Review , 1965,6.
Samuelson, P., Economics 1998, Mc Grow Hill, Auckland.
Satinover, J.B. and Sornette, D., Illusion of control in Tim e-
Horizon Minority and Parrondo Games. The European Physical
Journal B - Condensed Matter and Complex Systems , 2007,60,
369–384.
Savit, R., Manuca, R. and Riolo, R., Adaptive Competition, M ar-
ket Eﬃciency, and Phase Transitions. Phys. Rev. Lett. , 1999,
82, 2203.
Scafetta, N., Picozzi, S. and West, B.J., An out-of-equilib rium
model of the distributions of wealth. 2004.
Scalas, E., Garibaldi, U. and Donadio, S., Statistical equi librium in
simple exchange games I - Methods of solution and applicatio n
to the Bennati-Dragulescu-Yakovenko (BDY) game. Eur. Phys.
J. B, 2006,53, 267.
Scalas, E., Garibaldi, U.andDonadio, S., Erratum.Statist ical equi-
librium in simple exchange games I. Eur. Phys. J. B , 2007,60,
271.
Sen, A.K., Collective Choice and Social Welfare 1971, Oliver &
Boyd.
Shostak, F., The Mystery of The Money Supply Deﬁnition. Quart.
J. Aust. Economics , 2000,3, 69.
Silva, A.C. and Yakovenko, V.M., Temporal evolution of the ‘ ther-
mal’ and ‘superthermal’ income classes in the USA during 198 3-
2001.Europhysics Letters , 2005,69, 304.
Silva, A.C. and Yakovenko, V.M., Stochastic volatility of ﬁ nan-
cial markets as the ﬂuctuating rate of trading: An empirical
study.Physica A: Statistical Mechanics and its Applications ,
2007,382, 278 – 285.
Sinha, S., Chatterjee, A., Chakraborti, A. and Chakrabarti , B.K.,
Econophysics: An Introduction , 1 2010, Wiley-VCH.
Slanina, F., Inelastically scattering particles and wealt h distribu-
tion in an open economy. Phys. Rev. E , 2004,69, 046102.
Slanina, F., Critical comparison of several order-book mod els for
stock-market ﬂuctuations. The European Physical Journal B-
Condensed Matter and Complex Systems , 2008,61, 225–240.
Slanina, F.c.v., Mean-ﬁeld approximation for a limit order driven
market model. Phys. Rev. E , 2001,64, 056136.
Smith, E., Farmer, J.D., Gillemot, L. and Krishnamurthy, S. , Sta-
tistical theory of the continuous double auction. Quantitative
Finance, 2003,3, 481–514.
Solomon, S. and Levy, M., Spontaneous Scaling Emergence in
Generic Stochastic Systems. , 1996.
Sornette, D., Multiplicative processes and power laws. Phys. Rev.
E, 1998,57, 4811.
Stauﬀer, D., de Oliveira, S.M., de Oliveira, P.M.C. and de Sa Mar-
tins, J.S., Biology, Sociology, Geology by Computational Physi-
cists2006, Elsevier Science.
Stigler, G., Public regulation of the securities markets. Journal of
Business , 1964, pp. 117–142.",2009-09-10T15:25:45Z,onel taxonomy master lsinki  technology s nland onel chara bor ti ka  kept dyt apsical journal connsed maer lex tems onel chara bor ti ka  kept want dynamics taxonomy psical review osborne brownish motstock market oa researpagat journal party cours rouge lausanne reprinted oe uvr que bus ieds dr oz geve party cours rouge lausanne parish lex psics statistical meics applicatns proceedis internatnal conference psics path ria statistical meics buerworth inemanoxford patria rca chara bor ti germany iph ys patria rca chara bor ti ka  statistical ph ys rev patria rca chara bor ti ka  germany  iproceedis eco no psics alth distributns terjee yar lag add chara bart  patria rca chara bor ti  ialu germany relaxateur ph ys patria rca chara bor ti ka  gibbs gibbs psics statistical meics applicatns patria rca  ialu chara bor ti t patria rca  ialu chara bor ti basic eur ph ys le rou go pi krishna tunmoral ga articial intellence eugene stanley economic ph ys rev le rou go pi krishna rose  moral gu hr stanley random psical review le rou go pi krishna rose  moral stanley universal nouniversal proti correlatns nancial time seripsical review ters poor  iva css stanley tems genti euro psics ters polite scale i psics statistical meics ap ters  cha statistical psics statistical meics applicatns  gke paul schneir status psical review racer to cinco  focal rdi marcs age psics statistical meics applicatns re  fundamentals statistical trmal psics mc grow hl siae reno apps internatnal journal toical applied nance re pet owicz hut ler richmond dynamics mo income distributns psics roe ner paerns cambridge   sha sri vast a sha india allahabad sala martit world distributof income worki pa serisala martimoh  poverty inequality distributincome columbia   economics discusspa serisatanic ou sis  tau lux age rets progress psics samuelsoproof management review samuelsoeconomics mc grow hl auckland satiocorvee lustim horizominority par mondo gamt apsical journal connsed maer lex tems s it manu ca r lo adaptive etitphase transitns ph ys rev  caf eta pico zz st ascale garibaldi dod statistical methods beeat drag ul escu v e eur ph ys scale garibaldi dod erratic status eur ph ys secolleive choice social lfare oliboyd host ak t mystery t money supply  quartz just economics sva v e temal euro psics ters sva v e stochastic apsics statistical meics applicatns sinh terjee chara bor ti chara bart eco no psics aintrodurey spaiidrastically ph ys rev spaiitical t apsical journal connsed maer lex tems spaimeaph ys rev smith farmer kled ot krishna mur t st quantitative nance solomolevy spontaneous scali emergence generic stochastic tems corvee multiplicative ph ys rev tau oliolimar blogy soclogy geology tnal ph ys else vier science ter public journal business
paper_qf_47.pdf,51,Econophysics: Empirical facts and agent-based models,"  This article aims at reviewing recent empirical and theoretical developments
usually grouped under the term Econophysics. Since its name was coined in 1995
by merging the words Economics and Physics, this new interdisciplinary field
has grown in various directions: theoretical macroeconomics (wealth
distributions), microstructure of financial markets (order book modelling),
econometrics of financial bubbles and crashes, etc. In the first part of the
review, we discuss on the emergence of Econophysics. Then we present empirical
studies revealing statistical properties of financial time series. We begin the
presentation with the widely acknowledged stylized facts which describe the
returns of financial assets- fat tails, volatility clustering, autocorrelation,
etc.- and recall that some of these properties are directly linked to the way
time is taken into account. We continue with the statistical properties
observed on order books in financial markets. For the sake of illustrating this
review, (nearly) all the stated facts are reproduced using our own
high-frequency financial database. Finally, contributions to the study of
correlations of assets such as random matrix theory and graph theory are
presented. In the second part of the review, we deal with models in
Econophysics through the point of view of agent-based modelling. Amongst a
large number of multi-agent-based models, we have identified three
representative areas. First, using previous work originally presented in the
fields of behavioural finance and market microstructure theory, econophysicists
have developed agent-based models of order-driven markets that are extensively
presented here. Second, kinetic theory models designed to explain some
empirical facts on wealth distribution are reviewed. Third, we briefly
summarize game theory models by reviewing the now classic minority game and
related problems.
","51
Sysi-Aho, M., Chakraborti, A. and Kaski, K., Adaptation usi ng
hybridized genetic crossover strategies. Physica A , 2003a, 322,
701.
Sysi-Aho, M., Chakraborti, A. and Kaski, K., Biology Helps Y ou
to Win a Game. Physica Scripta T , 2003b, 106, 32.
Sysi-Aho, M., Chakraborti, A. and Kaski, K., Intelligent mi nority
game with genetic crossover strategies. Eur. Phys. J. B , 2003c,
34, 373.
Sysi-Aho, M., Chakraborti, A. and Kaski, K., Searching for g ood
strategies in adaptive minority games. Phys. Rev. E , 2004,69,
036125.
Taqqu, M., Bachelier and his times: A conversation with Bern ard
Bru.Finance and Stochastics , 2001,5, 3–32.Thurner, S., Farmer, J.D. and Geanakoplos, J., Leverage Cau ses
Fat Tails and Clustered Volatility. Preprint , 2009.
Tsay, R., Analysis of ﬁnancial time series 2005, Wiley-Interscience.
Wyart, M.and Bouchaud, J.P.,Self-referentialbehaviour, overreac-
tion and conventions in ﬁnancial markets. Journal of Economic
Behavior & Organization , 2007,63, 1–24.
Yakovenko, V.M. and Rosser, J.B., Colloquium: Statistical me-
chanics of money, wealth, and income. Reviews of Modern
Physics, 2009,81, 1703.",2009-09-10T15:25:45Z,sy si ho chara bor ti ka  adaptatpsics sy si ho chara bor ti ka  blogy lps wame psics sipt sy si ho chara bor ti ka  intellent eur ph ys sy si ho chara bor ti ka  searchi ph ys rev aq qu cac liberbru nance stochastic turer farmer ge aklos levge au fat articial intellence ls ustered volatity pre print say analysis rey inter science wy art  chaself journal economic behr organizatv e roster colloquial statistical reviews morpsics
paper_qf_48.pdf,1,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","arXiv:0901.1945v1  [q-fin.ST]  14 Jan 2009Amathematicalproofoftheexistenceof
trendsinﬁnancialtimeseries
MichelFLIESS & C´ edricJOIN
INRIA-ALIEN – LIX (CNRS, UMR7161)
´Ecolepolytechnique,91128Palaiseau, France
Michel.Fliess@polytechnique.edu
&
INRIA-ALIEN – CRAN (CNRS, UMR7039)
Nancy-Universit´ e,BP239, 54506Vandœuvre-l` es-Nancy,F rance
Cedric.Join@cran.uhp-nancy.fr
Keywords: Financial time series, mathematical ﬁnance, technical ana lysis, trends,
random walks, efﬁcient markets, forecasting, volatility, heteroscedasticity, quickly
ﬂuctuatingfunctions,low-passﬁlters, nonstandardanaly sis,operationalcalculus.
Abstract
We are settling a longstanding quarrel in quantitative ﬁnan ce by proving the
existence of trends in ﬁnancial time series thanks to a theor em due to P. Cartier
and Y. Perrin, which is expressed in the language of nonstand ard analysis ( Inte-
gration over ﬁnite sets , F. & M. Diener (Eds): Nonstandard Analysis inPractice ,
Springer, 1995, pp. 195–204). Those trends, which might coe xist with some al-
tered random walk paradigm and efﬁcient market hypothesis, seem nevertheless
difﬁcult to reconcile with the celebrated Black-Scholes mo del. They are esti-
mated via recent techniques stemming from control and signa l theory. Several
quite convincing computer simulations on the forecast of va rious ﬁnancial quan-
titiesare depicted. We conclude bydiscussing the rˆ ole of p robability theory.",2009-01-14T07:47:18Z, jamatmatical proof of t existence of micl ecole polytechnique pal articial intellence eau france micl flinancy  and nancy metric joikeywords nancial abstra  carrier soite tiene eds nonstandard analysis praice  those  schools ty sevl 
paper_qf_48.pdf,2,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","1 Introduction
Ouraimistosettle a severeandlongstandingquarrelbetwee n
1. the paradigmof randomwalks1and the related efﬁcientmarket hypothesis [15]
whicharethebreadandbutterofmodernﬁnancialmathematic s,
2. theexistenceof trendswhichisthekeyassumptionin technicalanalysis .2
There are many publications questioning the existence eith er of trends (see, e.g.,
[15, 36, 47]), of random walks (see, e.g., [31, 55]), or of the market efﬁciency (see,
e.g.,[23, 51,55]).3
AtheoremduetoCartierandPerrin[9],whichisstatedinthe languageof nonstandard
analysis,4yieldstheexistenceoftrendsfortimeseriesunderaverywe akintegrability
assumption. Thetime series f(t)maythenbedecomposedasasum
f(t) =ftrend(t) +fﬂuctuation (t) (1)
where
•ftrend(t)isthetrend,
•fﬂuctuation (t)is a“quicklyﬂuctuating”functionaround 0.
Thevery“nature”ofthosequickﬂuctuationsisleftunknown andnothingpreventsus
from assuming that fﬂuctuation (t)is random and/or fractal. It implies the following
conclusionwhichseemstoberatherunexpectedinthe existi ngliterature:
The two above alternatives are not necessarily contradicto ry and may coexist
fora giventimeseries.5
We nevertheless show that it might be difﬁcult to reconcile w ith our setting the cele-
brated Black-Scholesmodel [8], which is in the heart of the a pproach to quantitative
ﬁnanceviastochasticdifferentialequations(see, e.g.,[52]andthereferencestherein).
Consider,as usual in signal,control,andin otherengineer ingsciences, fﬂuctuation (t)
inEq. (1)asanadditivecorruptingnoise. Weattenuateit, i.e.,weobtainanestimation
offtrend(t)byanappropriateﬁltering.6Theseﬁlters
1RandomwalksinﬁnancegobacktotheworkofBachelier[3]. Th eybecameamainstayintheacademic
worldsixtyyearsago(see, e.g.,[7,10,40]andthereferences therein) andgaverisetoahug eliterature (see,
e.g.,[52] and thereferences therein).
2Technical analysis (see, e.g., [4, 29, 30, 43, 44] and the references therein), or charting, is popular
among traders and ﬁnancial professionals. The notion of tre nds here and in the usual time series literature
(see,e.g.,[22,24]) do not coincide.
3An excellent book by Lowenstein [35] is giving ﬂesh and blood to those hot debates.
4See Sect. 2.1.
5One should then deﬁne random walks and/or market efﬁciency “ around” trends.
6Sometechnical analysts (see, e.g.,[4]) arealready advocating this standpoint.",2009-01-14T07:47:18Z,introduour articial intellence is to sele tre torem due to carrier and sot time t very it t   schools mconsir eq  aenuated it tse random walks icac lith technical t alo nstei se one some technical
paper_qf_48.pdf,3,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","•arededucedfromourapproachto noisesvianonstandardanal ysis[16],which
–isstronglyconnectedto thiswork,
–led recently to many successful results in signal and in cont rol (see the
referencesin[17]),
•yields excellent numerical differentiation [39], which is here again of utmost
importance(seealso[18,20]andthereferencesthereinfor applicationsincon-
trolandsignal).
A mathematical deﬁnition of trends and effective means for e stimating them, which
were missing until now, bear important consequences on the s tudy of ﬁnancial time
series,whichweresketchedin [19]:
•The forecast of the trend is possible on a “short” time interv al under the as-
sumption of a lack of abrupt changes, whereas the forecast of the “accurate”
numericalvalueatagiventimeinstantismeaninglessandsh ouldbeabandoned.
•The ﬂuctuationsof the numericalvaluesaroundthe trend lea d to new ways for
computingstandarddeviation,skewness,andkurtosis,whi chmaybeforecasted
to someextent.
•Thepositionofthenumericalvaluesaboveorunderthetrend maybeforecasted
to someextent.
ThequiteconvincingcomputersimulationsreportedinSect . 4showthatwe are
•offeringfortechnicalanalysisasoundtheoreticalbasis( see also[14,32]),
•on the verge of producing on-line indicators for short time t rading, which are
easily implementableoncomputers.7
Remark 1. We utilize as in [19] the differences between the actual pric es and the
trend for computing quantities like standard deviation, sk ewness, kurtosis. This is a
major departure from today’s literature where those quanti ties are obtained via re-
turns and/or logarithmic returns,8and where trends do not play any r ˆole. It might
yield a new understanding of “volatility”, and therefore a n ew model-free risk man-
agement.9
Our paper is organized as follows. Sect. 2 proves the existen ce of trends, which
seem to contradict the Black-Scholes model. Sect. 3 sketche s the trend estimation
by mimicking [20]. Several computer simulations are depict ed in Sect. 4. Sect. 5
concludesbyexaminingprobabilitytheoryin ﬁnance.
7The very same mathematical tools already provided successf ul computer programs in control and sig-
nal.
8See Sect. 2.4.
9The existing literature contains of course other attempts f or introducing nonparametric risk manage-
ment(see, e.g.,[1]).",2009-01-14T07:47:18Z,t t t positof t numerical valuabove or unr t trend t quite convinci uter simulatns reted ise remark   it our se  schools se sevl se se t  se t
paper_qf_48.pdf,4,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","2 Existence oftrends
2.1 Nonstandard analysis
Nonstandardanalysiswasdiscoveredintheearly60’sbyRob inson[50]. Itvindicates
Leibniz’s ideas on “inﬁnitely small” and “inﬁnitely large” numbers and is based on
deepconceptsandresultsfrommathematicallogic. Thereex istsanotherpresentation
due to Nelson [45], where the logical background is less dema nding (see, e.g., [12,
13,49]forexcellentintroductions). Nelson’sapproach[4 6]ofprobabilityalongthose
lines had a lasting inﬂuence.10As demonstrated by Harthong [25], Lobry [33], and
severalotherauthors,nonstandardanalysisisalso a marve loustoolforclarifyingin a
mostintuitivewayquestionsstemmingfromsomeappliedsid esofscience. Thiswork
isanotherstepinthat direction,like [16,17].
2.2 Sketch oftheCartier-Perrin theorem11
2.2.1 DiscreteLebesgue measureand S-integrability
LetIbe an interval of R, with extremities aandb. A sequence T={0 =t0<
t1<· · ·< tν= 1}is called an approximation ofI, or anear interval , ifti+1−ti
isinﬁnitesimal for0≤i < ν. TheLebesgue measure onTis the function mdeﬁned
onT\\\\{b}bym(ti) =ti+1−ti. The measure of any interval [c, d[⊂I,c≤d, is its
length d−c. Theintegralover [c, d[ofthefunction f:I→Risthesum
/integraldisplay
[c,d[fdm=/summationdisplay
t∈[c,d[f(t)m(t)
Thefunction f:T→Rissaidtobe S-integrable if,andonlyif,foranyinterval [c, d[
theintegral/integraltext
[c,d[|f|dmis limitedand,if d−cisinﬁnitesimal,alsoinﬁnitesimal.
2.2.2 Continuityand Lebesgue integrability
The function fis said to be S-continuous attι∈Tif, and only if, f(tι)≃f(τ)
whentι≃τ.12The function fis said to be almost continuous if, and only if, it is S-
continuouson T\\\\R,where Risararesubset.13Wesaythat fisLebesgueintegrable
if,andonlyif,it is S-integrableandalmostcontinuous.
10The following quotation of D. Laugwitz, which is extracted f rom [27], summarizes the power of non-
standard analysis: Mit ¨ ublicher Mathematik kann man zwar alles gerade so gut be weisen; mit der nicht-
standard Mathematik kann man es aber verstehen .
11The reference [34] contains a well written elementary prese ntation. Note also that the Cartier-Perrin
theorem is extending previous considerations in [26,48].
12x≃ymeans that x−yis inﬁnitesimal.
13The set Ris said to be rare [5] if, for any standard real number α >0, there exists an internal set
B⊃Asuch that m(B)≤α.",2009-01-14T07:47:18Z,existence nonstandard nonstandard analysis was divered it  rob it indicatleibniz tre ex nelsonelsoas hart ho obey  work sketcarrier sodisete le be gue  be t le be gue tis t t integral ois t sum t funis articial intellence to be continuity and le be gue t t t risk rare subset  say that le be gue ite gable t aug with mit ma tm tik ma tm tik t note carrier sot is such
paper_qf_48.pdf,5,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","2.2.3 Quicklyﬂuctuatingfunctions
A function h:T→Ris said to be quickly ﬂuctuating , oroscillating , if, and only if,
it isS-integrableand/integraltext
Ahdmisinﬁnitesimalforany quadrable subset.14
Theorem 2.1. Letf:T→Rbe an S-integrable function. Then the decomposition
(1)holdswhere
•ftrend(t)isLebesgueintegrable,
•fﬂuctuation (t)is quicklyﬂuctuating.
Thedecomposition (1)isuniqueupto aninﬁnitesimal.
ftrend(t)andfﬂuctuation (t)arerespectivelycalledthe trendandthequickﬂuctuations
off. Theyareuniqueupto aninﬁnitesimal.
2.3 The Black-Scholes model
ThewellknownBlack-Scholesmodel[8],whichdescribesthe priceevolutionofsome
stockoptions,istheItˆ ostochasticdifferentialequatio n
dSt=µSt+σStdWt (2)
where
•WtisastandardWienerprocess,
•thevolatility σandthedrift, ortrend,µare assumedto beconstant.
This model and its numerous generalizations are playing a ma jor rˆ ole in ﬁnancial
mathematicssince more thanthirty yearsalthoughEq. (2) is oftenseverelycriticized
(see,e.g.,[38, 54]andthereferencestherein).
ThesolutionofEq. (2)isthe geometricBrownianmotion whichreads
St=S0exp/parenleftbigg
(µ−σ2
2)t+σWt/parenrightbigg
where S0is the initial condition. It seems most natural to consider t he mean S0eµt
ofStas the trend of St. This choice unfortunatelydoes not agree with the followin g
fact:Ft=St−S0eµtis almost surely not a quickly ﬂuctuating function around 0,
i.e.,the probabilitythat |/integraltextT
0Fτdτ|> ǫ > 0,T >0,isnot“small”,when
•ǫis“small”,
•Tisneither“small”nor“large”.
14A setis quadrable [9] if its boundary is rare.",2009-01-14T07:47:18Z,quickly is hdi itorem  be tle be gue ite gable t ositty are unique up to t  schools t ll k schools mit st st std   is standard winner process  eq t solutof eq brownish motst  it st as st  ft st tis neitr
paper_qf_48.pdf,6,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","Remark2. Arigoroustreatment,whichwouldagreewith nonstandardan alysis(see,
e.g., [2, 6]), may be deduced from some inﬁnitesimal time-sampli ng of Eq. (2), like
theCox-Ross-Rubinsteinone[11].
Remark 3. Many assumptions concerning Eq. (2)are relaxed in the literature (see,
e.g.,[52]andthereferencestherein):
•µandσarenomoreconstantandmaybetime-dependentand/or St-dependent.
•Eq.(2)is no more driven by a Wiener process but by more complex rando m
processeswhichmightexhibitjumpsinorderto dealwith“ex treme events”.
Theconclusionreachedbeforeshouldnotbemodiﬁed, i.e.,thepriceisnotoscillating
arounditstrend.
2.4 Returns
Assumethatthefunction f:I→Rgivesthepricesofsomeﬁnancialasset. Itimplies
thatthevaluesof farepositive. Whatisusuallystudiedinquantitativeﬁnanc earethe
return
r(ti) =f(ti)−f(ti−1)
f(ti−1)(3)
andthelogarithmicreturn ,orlog-return ,
r(ti) = log( f(ti))−log(f(ti−1)) = log/parenleftbiggf(ti)
f(ti−1)/parenrightbigg
= log (1 + r(ti))(4)
whicharedeﬁnedfor ti∈T\\\\{a}. Thereisahugeliteratureinvestigatingthestatistical
propertiesofthe twoabovereturns, i.e.,ofthetime series(3)and(4).
Remark 4. Returns and log-returnsare less interesting for us since th e trends of the
originaltimeseriesaredifﬁculttodetectonthem. Notemor eoverthatthereturnsand
log-returnswhichareassociatedtotheBlack-Scholesequa tion(2)viasomeinﬁnites-
imal time-sampling [2, 6] are not S-integrable: Theorem 2.1 does not hold for the
correspondingtimeseries (3)and(4).
Assumethatthetrend ftrend:I→RisS-continuousat t=ti. ThenEq. (1) yields
f(ti)−f(ti−1)≃fﬂuctuation(ti)−fﬂuctuation(ti−1)
Thus
r(ti)≃fﬂuctuation(ti)−fﬂuctuation(ti−1)
f(ti−1)
Ityieldsthe followingcrucialconclusion:
The existence of trends does not preclude, but does not imply either, the possi-
bility of a fractal and/or random behavior for the returns (3)and(4)where the
fastoscillatingfunction fﬂuctuation(t)would befractaland/orrandom .",2009-01-14T07:47:18Z,remark rorous treatment eq cox ross rubinsteione remark many eq st eq winner t conusreacd before should not be modi urns  that t fungivt pricof some it impliwhat is usually studied iquantitative tre is huge re investati t statistical remark urns not em or  schools qua torem  that t trend is teq  it yields t t
paper_qf_48.pdf,7,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","3 Trend estimation
Consider the real-valued polynomialfunction xN(t) =/summationtextN
ν=0x(ν)(0)tν
ν!∈R[t],t≥
0, of degree N. Rewrite it in the well known notations of operational calcu lus (see,
e.g.,[56]):
XN(s) =N/summationdisplay
ν=0x(ν)(0)
sν+1
Introduced
ds, which is sometimes called the algebraicderivative [41, 42], andwhich
corresponds in the time domain to the multiplication by −t. Multiply both sides by
dα
dsαsN+1,α= 0,1, . . ., N. Thequantities x(ν)(0),ν= 0,1, . . ., N,whicharegiven
by the triangular system of linear equations, are said to be linearly identiﬁable (see,
e.g.,[17]):
dαsN+1XN
dsα=dα
dsα/parenleftBiggN/summationdisplay
ν=0x(ν)(0)sN−ν/parenrightBigg
(5)
The time derivatives, i.e.,sµdιXN
dsι,µ= 1, . . ., N,0≤ι≤N, are removed by
multiplying both sides of Eq. (5) by s−¯N,¯N > N, which are expressed in the time
domainbyiteratedtimeintegrals.
Consider now a real-valued analytic time function deﬁned by the convergent power
series x(t) =/summationtext∞
ν=0x(ν)(0)tν
ν!,0≤t < ρ. Approximating x(t)by its truncated
Taylorexpansion xN(t) =/summationtextN
ν=0x(ν)(0)tν
ν!yieldsasabovederivativesestimates.
Remark5. Theiteratedtimeintegralsarelow-passﬁlterswhichatten uatethenoises
whenviewedasin[16]asquicklyﬂuctuatingphenomena.15See[39]forfundamental
computationaldevelopments,whichgiveasabyproductmost efﬁcientestimations.
Remark 6. See [21] for other studies on ﬁlters and estimation in econom ics and
ﬁnance.
Remark7. See[55]foranotherviewpointonamodel-basedtrendestima tion.
4 Some illustrative computer simulations
Consider the Arcelor-Mittal daily stock prices from 7 July 1 997 until 27 October
2008.16
4.1 1 day forecast
Figures1and2 present
15See [34] for an introductory presentation.
16Thosedata are borrowed from http://finance.yahoo.com/ .",2009-01-14T07:47:18Z,trend consir rewrite introduced multiply t quantitib b t eq consir approximattaylor eansremark t oated time integrals are low  remark  remark  some consir arc el or mi july ober s  those data
paper_qf_48.pdf,8,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","0 500 1000 1500 2000 2500 3000020406080100120
 Samples
Figure 1: 1 day forecast – Prices (red –), ﬁltered signal (blu e –), forecasted signal
(black--)
300 400 500 600 700 800 90002468101214161820
 Samples
Figure2: 1dayforecast–Zoomofﬁgure1",2009-01-14T07:47:18Z,sampl pricsampl zoom of
paper_qf_48.pdf,9,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","•theestimationofthetrendthanksto themethodsofSect. 3,w ithN= 2;
•a1dayforecastofthe trendbyemployinga 2nd-orderTaylorexpansion. Itne-
cessitatestheestimationoftheﬁrsttwotrendderivatives whichisalsoachieved
viathe methodsofSect. 3.17
We now look at some properties of the quick ﬂuctuations fﬂuctuation (t)around the
trendftrend(t)of the price f(t)(see Eq. (1)) by computing moving averages which
correspondto variousmoments
MA k,M(t) =/summationtextM
τ=0(fﬂuctuation (τ−M)−¯fﬂuctuation )k
M+ 1
where
•k≥2,
•¯fﬂuctuation isthemeanof fﬂuctuation overthe M+ 1samples,18
•M= 100samples.
The standard deviation and its 1day forecast are displayed in Figure 3. Its het-
eroscedasticityisobvious.
The kurtosisMA4,100(t)
MA2,100(t)2, the skewnessMA3,100(t)
MA2,100(t)3/2, and their 1day forecasts are
respectivelydepictedinFigures4and5. Theyshowquitecle arlythatthepricesdonot
exhibitGaussianproperties19especiallywhentheyarecloseto someabruptchange.
4.2 5 daysforecast
Aslight degradationwith a 5daysforecastisvisibleontheFigures6to 10.
4.3 Aboveorunder the trend?
Estimating the ﬁrst two derivatives yields a forecast of the price position above or
under the trend. The results reported in Figures 11-12 show f or1day (resp. 5days)
ahead 75.69%(resp. 68.55%) for an exact prediction, 3.54%(resp. 3.69%) without
anydecision, 20.77%(resp. 27.76%)fora wrongprediction.
17Here, asin [19], forecasting is achieved without specifyin g amodel (seealso [18]).
18According to Sect. 2.2 ¯fﬂuctuation is “small”.
19Lack of spaces prevents usto look at returns and log-returns .",2009-01-14T07:47:18Z,se taylor eansit ne se  eq t  its t s ty show quite e protias lht s above or unr estimati t s re accordi se lack
paper_qf_48.pdf,10,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","0 500 1000 1500 2000 2500 3000024681012
 Samples
Figure 3: 1day forecast – Standarddeviationw.r.t. trend (blue –), pre dictedstandard
deviation(black--)
0 500 1000 1500 2000 2500 3000051015
 Samples
Figure4: 1dayforecast–Kurtosisw.r.t. trend(blue–),predictedkur tosis(black- -)",2009-01-14T07:47:18Z,sampl standard viatsampl kurt os is
paper_qf_48.pdf,11,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","0 500 1000 1500 2000 2500 3000−4−3−2−101234
 Samples
Figure5: 1dayforecast – Skewnessw.r.t. trend(blue–), predictedske wness (black-
-)
5 Conclusion: probability in quantitative ﬁnance
The following question may arise at the end of this prelimina ry study on trends in
ﬁnancialtime series:
Is it possible to improve the forecasts given here and in [19] by taking advantage
ofapreciseprobabilitylaw fortheﬂuctuationsaroundthet rend?
AlthoughMandelbrot[37]hasshowninamostconvincingwaym orethanfortyyears
agothattheGaussiancharacterofthepricevariationsshou ldbeatleast questioned,it
doesnotseemthatthenumerousinvestigationswhichhavebe encarriedonsincethen
forﬁndingotherprobabilitylawswithjumpsand/orwith “fa t tails” havebeenable to
produce clear-cut results, i.e., results which are exploitable in practice (see, e.g., the
enlighteningdiscussionsin[28,38,53]andthereferences therein). Thisshortcoming
maybe dueto an“ontologicalmistake”onuncertainty:
Let us base our argument on new advances in model-free control [18]. Engineers
know that obtaining the differential equations governinga concrete plant is always a
mostchallengingtask: itisquitedifﬁculttoincorporatei nthoseequationsfrictions,20
20Thosefrictions have nothing to do with what are called frictionsin market theory!",2009-01-14T07:47:18Z,sampl skew ness conust is although manlbrot charaer of t price variatns show  shortcomis  eineers those frin
paper_qf_48.pdf,12,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","0 500 1000 1500 2000 2500 3000020406080100120
 Samples
Figure 6: 5days forecast – Prices (red –), ﬁltered signal (blue –), fore casted signal
(black--)
300 400 500 600 700 800 90002468101214161820
 Samples
Figure7: 5daysforecast– Zoomofﬁgure6",2009-01-14T07:47:18Z,sampl pricsampl zoom of
paper_qf_48.pdf,13,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","0 500 1000 1500 2000 2500 3000024681012
 Samples
Figure8: 5daysforecast–Standarddeviationw.r.t. trend(blue–),pr edictedstandard
deviation(black--)
0 500 1000 1500 2000 2500 3000051015
 Samples
Figure9: 5daysforecast–Kurtosisw.r.t. trend(blue–),predictedku rtosis(black--)",2009-01-14T07:47:18Z,sampl standard viatsampl kurt os is
paper_qf_48.pdf,14,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","0 500 1000 1500 2000 2500 3000−4−3−2−101234
 Samples
(a) Skewness oferrortrend (blue–),predicted skewnessofe rrortrend (black--)(5dayahead)
Figure10: 5daysforecast–Skewnessw.r.t. trend(blue–),predictedsk ewness(black
--)
heating effects, ageing, etc, which might have a huge inﬂuen ce on the plant’s behav-
ior. The tools proposed in [18] for bypassing those equation s21got already in spite
of their youth a few impressive industrial applications. Th is is an important gap be-
tweenengineering’spracticeandtheoreticalphysicswher ethebasicprinciplesleadto
equationsdescribing “stylized” facts. The probabilityla ws stemming from statistical
and quantum physics can only be written down for “idealized” situations. Is it not
thereforequitena¨ ıvetowishtoexhibitwelldeﬁnedprobab ilitylawsinquantitativeﬁ-
nance,ineconomicsandmanagement,andinothersocialandp sychologicalsciences,
where the environmental world is much more involved than in a ny physical system?
Inotherwords amathematicaltheoryofuncertainsequencesofeventsshou ldnot
necessarily be confused with probability theory .22To ask if the uncertainty of a
“complex” system is of probabilistic nature23is an undecidable metaphysical ques-
tion which cannot be properly answered via experimentalmea ns. It should therefore
beignored.
21Theeffects oftheunknownpartoftheplantareestimated int hemodel-freeapproachandnotneglected
as in the traditional setting of robustcontrol (see,e.g.,[57] and thereferences therein).
22Itdoesnotimplyofcoursethatstatistical toolsshouldbea bandoned (rememberthatwecomputedhere
standard deviations, skewness, kurtosis).
23We understand by “probabilistic nature” a precise probabil istic description which satisﬁes some set of
axioms like Kolmogorov’s ones.",2009-01-14T07:47:18Z,samplskew ness  skew ness t th t is iotr words to it t effes it donot imply of course that statistical  lmogorov
paper_qf_48.pdf,15,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","0 500 1000 1500 2000 2500 3000020406080100120
 Samples
Figure 11: 1day forecast – Prices (red –), predicted trend (blue - -), pre dicted conﬁ-
denceinterval( 95%)(black–),price’sforecasthigherthanthe predictedtren d(green
△),price’sforecastlowerthanthe predictedtrend(blue ▽)
Remark8. Oneshouldnotmisunderstandtheauthors. Theyfullyrecogn izethemath-
ematicalbeautyof probabilitytheoryanditsnumerousande xciting connectionswith
physics. Theauthorsareonlyexpressingdoubtsaboutanymo delingatlargeinquan-
titativeﬁnance,withorwithoutprobabilities.
TheCartier-Perrintheorem[9]whichisdecomposingatimes eriesasasumofatrend
anda quicklyﬂuctuatingfunctionmightbe
•a possiblealternativetotheprobabilisticviewpoint,
•a usefultoolforanalyzing
–differenttime scales,
–complex behaviors, including abrupt changes, i.e., “rare” extreme events
like ﬁnancial crashes or booms, without having recourse to a model via
differentialordifferenceequations.
We hope to be able to show in a near future what are the beneﬁts n ot only in quanti-
tativeﬁnancebutalso fora newapproachtotime seriesingen eral(see[19] fora ﬁrst
draft).",2009-01-14T07:47:18Z,sampl pricremark one should not misunrstand t authors ty fully re co gt authors are only essi dous at any mo t carrier sotorem 
paper_qf_48.pdf,16,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","300 400 500 600 700 800 90002468101214161820
 Samples
Figure12: 1dayforecast–Zoomofﬁgure11
References
[1] A¨ ıt-SahaliaY.,LoA.W. Nonparametricrisk managementandimpliedriskaver-
sion.J. Econometrics, 9,(2000)9–51.
[2] AlbeverioS.,FenstadJ.E.,Hoegh-KrøhnR.,LindstrømT .NonstandardMethods
inStochasticAnalysisandMathematicalPhysics .AcademicPress,1986.
[3] BachelierL. Th´eoriedelasp ´eculation.Ann.Sci. ´EcoleNormaleSup.S´ er.3, 17,
(1900)21–86.
[4] B´ echu T., Bertrand E., Nebenzahl J. L’analyse technique (6e´ ed.). Economica,
2008.
[5] Benoˆ ıt E. Diffusions discr `etes et m´ecanique stochastique . Pr´ epubli. Lab. Math.
J.Dieudonn´ e,Universit´ ede Nice,1989.
[6] Benoˆ ıt E. Random walks and stochastic differential equations . F. & M. Diener
(Eds):NonstandardAnalysisinPractice .Springer,1995,pp.71–90.
[7] Bernstein P.L. Capital Ideas: The Improbable Origins of Modern Wall Street .
FreePress, 1992.
[8] Black F., Scholes M. The pricing of options and corporate liabilities . J. Polit.
Econ.,81,(1973)637–654.",2009-01-14T07:47:18Z,sampl zoom of referencsha lia lo nometric risk econometrics alb e fans tad hoe gh kr lindsay nonstandard methods stochastic analysis and matmatical psics acamic  cac lith ansci ecole normale sup bertrand ne  ahl economic be no dfuspr lab math died do nice be no random tiene eds nonstandard analysis ipraice   capital ias t improbable orins morwall street free   schools t polite ec on
paper_qf_48.pdf,17,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","0 500 1000 1500 2000 2500 3000020406080100120
 Samples
Figure13: 5daysforecast– Prices (red–), predictedtrend (blue- -), pr edictedconﬁ-
denceinterval( 95%)(black–),price’sforecasthigherthanthe predictedtren d(green
△),valueisforecastedaslowerthanpredictedtrend(blue ▽)
[9] Cartier P., Perrin Y. Integrationover ﬁnite sets . F. & M. Diener (Eds): Nonstan-
dardAnalysisin Practice .Springer,1995,pp.195–204.
[10] CootnerP.H. TheRandomCharactersofStockMarket Prices .MITPress, 1964.
[11] Cox J., Ross S., Rubinstein M. Option pricing: a simpliﬁed approach .J. Finan-
cialEcon., 7,(1979)229–263.
[12] Diener F., Diener M. Tutorial. F. & M. Diener (Eds): Nonstandard Analysis in
Practice.Springer,1995,pp.1–21.
[13] DienerF., ReebG., Analysenonstandard .Hermann,1989.
[14] DacorognaM.M.,Genc ¸ayR.,M¨ ullerU.,OlsenR.B.,Pic tetO.V.AnIntroduction
toHighFrequencyFinance .AcademicPress, 2001.
[15] Fama E.F. Foundations of Finance: Portfolio Decisions and Securitie s Prices.
Basic Books,1976.
[16] FliessM. Analysenonstandarddubruit .C.R.Acad.Sci.ParisSer.I, 342,(2006)
797–802.",2009-01-14T07:47:18Z,sampl priccarrier sointegratotiene eds nostaanalysis ipraice  cost ner t random charaers of stock market pric cox ross rubinsteioptaec otiene tiene tutorial tiene eds nonstandard analysis praice  tiene ree analyse nonstandard rmada core eolsepic aintroduhh frequency nance acamic  fam foundatns nance tfcisns sec uri tie pricbasic books flianalyse nonstandard du bru it acad sci paris ser
paper_qf_48.pdf,18,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","300 400 500 600 700 800 90002468101214161820
 Samples
Figure14: 5daysforecast– Zoomofﬁgure13
[17] Fliess M. Critique du rapport signal `a bruit en communica-
tions num ´eriques. ARIMA, 9, (2008) 419–429. Available at
http://hal.inria.fr/inria-00311719/en/ .
[18] Fliess M., Join C. Commande sans mod `ele et commande
`a mod`ele restreint , e-STA, 5, (2008) n◦4. Available at
http://hal.inria.fr/inria-00288107/en/ .
[19] Fliess M., Join C. Time series technical analysis via new fast estimation meth -
ods: a preliminary study in mathematical ﬁnance . Proc. 23rdIAR Work-
shop Advanced Control Diagnosis (IAR-ACD08), Coventry, 20 08. Available at
http://hal.inria.fr/inria-00338099/en/ .
[20] Fliess M., Join C., Sira-Ram´ ırez H. Non-linear estimation is easy ,
Int. J. Modelling Identiﬁcation Control, 4, (2008) 12–27. Available at
http://hal.inria.fr/inria-00158855/en/ .
[21] Genc ¸ay R., Selc ¸uk F., Whitcher B. An Introduction to Wavelets and Other Fil-
teringMethodsin FinanceandEconomics .AcademicPress, 2002.
[22] Gouri´ eroux C., Monfort A. S´eries temporelles et mod `eles dynamiques (2e´ ed.).
Economica,1995.English translation: Time Series and Dynamic Models . Cam-
bridgeUniversityPress, 1996.",2009-01-14T07:47:18Z,sampl zoom of fliitique  articial intellence  flijoicoand  articial intellence  flijoitime proc work advanced contrdiagnosis coventry  articial intellence  flijoisir ram noimolli int contr articial intellence  gesel whit cr aintroduwals otr l methods inance and economics acamic  go uri mofort economic eh time seridynamic mols cam  
paper_qf_48.pdf,19,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","[23] GrossmanS.,StiglitzJ. Ontheimpossibilityofinformationallyefﬁcientmarkets .
Amer.EconomicRev., 70,(1980)393–405.
[24] HamiltonJ.D. Time SeriesAnalysis .PrincetonUniversityPress, 1994.
[25] HarthongJ. Lemoir´e. Adv.Appl.Math., 2,(1981)21–75.
[26] HarthongJ. Lam´ethodedelamoyennisation .M.Diener&C.Lobry(Eds): Anal-
ysenonstandardetrepr ´esentationdur ´eel. OPU& CNRS, 1985,pp.301–308.
[27] Harthong J. Comment j’ai connu et compris Georges Reeb . L’ouvert (1994).
Availableat http://moire4.u-strasbg.fr/souv/Reeb.htm .
[28] JondeauE.,PoonS.-H.,RockingerM. FinancialModelingUnderNon-Gaussian
Distributions .Springer,2007.
[29] KaufmanP.J. NewTradingSystemsandMethods (4thed.).Wiley,2005.
[30] KirkpatrickC.D.,DahlquistJ.R. TechnicalAnalysis: TheCompleteResourcefor
FinancialMarket Technicians .FT Press,2006.
[31] Lo A.W., MacKinley A.C. A Non-Random Walk Down Wall Street . Princeton
UniversityPress,2001.
[32] Lo A.W., Mamaysky H., Wang J. (2000). Foundations of technical analysis:
computational algorithms, statistical inference, and emp irical implementation .
J.Finance, 55,(2000)1705–1765.
[33] LobryC. Lam´ethodedes ´elucidationssuccessives .ARIMA, 9,(2008)171–193.
[34] LobryC.,SariT. Nonstandardanalysisandrepresentationofreality .Int.J.Con-
trol,81,(2008)517–534.
[35] Lowenstein R. When Genius Failed: The Rise and Fall of Long-Term Capital
Management .RandomHouse,2000.
[36] MalkielB.G. A RandomWalk Down Wall Street (revisedand updateded.).Nor-
ton,2003.
[37] Mandelbrot B. The variation of certain speculative prices . J. Business, 36,
(1963)394–419.
[38] Mandelbrot B.B., Hudson R.L. The (Mis)Behavior of Markets . Basic Books,
2004.
[39] Mboup M., Join C., Fliess M. Numerical differentiation with an-
nihilators in noisy environment . Numer. Algorithm., (2009) DOI:
10.1007/s11075-008-9236-1 .",2009-01-14T07:47:18Z,gross mastlitz ot impossibity of informatally ef amer economic rev hamtotime serianalysis princeto  hart ho le noir adv ppl math hart ho lam tiene obey eds anal hart ho coent georgree  articial intellence  at ree jo au po orocki er nancial moli unr nodistributns  kaufmanew tradi tems and methods rey kirkpatrick dal qui st technical analysis t e resource for nancial market technicians  lo mac kled norandom walk dowwall street princeto  lo ma may  wa foundatns nance obey lam obey sari nonstandard analysis and representatof reality icolo nsteiwgenius articial intellence led t rise fall lo term capital management random house mal kiel random walk dowwall street nor manlbrot t business manlbrot hudsot mis behr markets basic books bo up joiflinumerical umer algorithm
paper_qf_48.pdf,20,A mathematical proof of the existence of trends in financial time series,"  We are settling a longstanding quarrel in quantitative finance by proving the
existence of trends in financial time series thanks to a theorem due to P.
Cartier and Y. Perrin, which is expressed in the language of nonstandard
analysis (Integration over finite sets, F. & M. Diener (Eds): Nonstandard
Analysis in Practice, Springer, 1995, pp. 195--204). Those trends, which might
coexist with some altered random walk paradigm and efficient market hypothesis,
seem nevertheless difficult to reconcile with the celebrated Black-Scholes
model. They are estimated via recent techniques stemming from control and
signal theory. Several quite convincing computer simulations on the forecast of
various financial quantities are depicted. We conclude by discussing the r\\\\^ole
of probability theory.
","[40] MertonR.C. Continuous-TimeFinance (reviseded.).Blackwell,1992.
[41] MikusinskiJ. OperationalCalculus (2nded.),Vol.1.PWN &Pergamon,1983.
[42] MikusinskiJ.,BoehmeT. OperationalCalculus (2nded.),Vol.2.PWN &Perg-
amon,1987.
[43] M¨ ullerT.,LindnerW. Dasgrosse BuchderTechnischenIndikatoren.Alles ¨uber
Oszillatoren, Trendfolger, Zyklentechnik (9. Auﬂage). TM B¨ orsenverlag AG,
2007.
[44] MurphyJ.J. TechnicalAnalysisoftheFinancialMarkets (3rdrev.ed.).NewYork
InstituteofFinance,1999.
[45] NelsonE. Internalset theory .Bull.Amer.Math.Soc., 83,(1977)1165–1198.
[46] NelsonE. RadicallyElementaryProbabilityTheory .PrincetonUniversityPress,
1987.
[47] PaulosJ.A. AMathematicianPlaystheStockMarket . Basic Books,2003.
[48] Reder C. Observation macroscopique de ph ´enom`enes microscopiques . M. Di-
ener&C. Lobry(Eds): Analyse nonstandardet repr ´esentationdur ´eel. OPU &
CNRS, 1985,pp.195–244.
[49] Robert A. Analyse non standard . Presses polytechniques romandes, 1985. En-
glishtranslation: NonstandardAnalysis .Wiley,1988.
[50] Robinson A. Non-standard Analysis (revised ed.). Princeton University Press,
1996.
[51] RosenbergB., Reid K., Lanstein R. (1985). Persuasive evidence of market inef-
ﬁciency.J.PortfolioManagement, 13,(1985)pp.9–17.
[52] ShreveS.J. StochasticCalculusforFinance,I&II .Springer,2005& 2004.
[53] Sornette D. Why Stock Markets Crash: Critical Events in Complex Financi al
Systems.PrincetonUniversityPress, 2003.
[54] Taleb N.N. Fooled by Randomness: The Hidden Role of Chance in Life and in
theMarkets . RandomHouse,2004.
[55] TaylorS.J. ModellingFinancialTime Series (2nded.).WorldScientiﬁc,2008.
[56] Yosida, K. Operational Calculus: A Theory of Hyperfunctions (translated from
theJapanese).Springer,1984.
[57] Zhou K., Doyle J.C., Glover K. Robust and Optimal Control . Prentice-Hall,
1996.",2009-01-14T07:47:18Z,mortocontinuous time nance ll mi us i oatnal calculus vgamomi us i both oatnal calculus v link ner das gross bur technisc idik to reallos vla to rend fger  lete au murp technical analysis of t nancial markets new york institute of nance nelsointernal set bull amer math soc nelsoradically elementary probabity tory princeto  paulo matmaticiaplays t stock market basic books re r observatdi obey eds analyse robert analyse enonstandard analysis rey robinsonoanalysis princeto  rstenberg reid lasteisuasive tfmanagement hr eve stochastic calculus for nance  corvee w stock markets ash itical events lex  nancy tems princeto  tale fooled randomness t hidrole ce le markets random house taylor molli nancial time seriworld cie nti yo sid oatnal calculus tory  funns ese  hou doyle glorobust optimal contrpraice hall
paper_qf_49.pdf,1,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
Exploring Classic Quantitative Strategies
Jun Lu
jun.lu.locky@gmail.com
Abstract
The goal of this paper is to debunk and dispel the magic behind the black-box quantitative
strategies. It aims to build a solid foundation on how and why the techniques work. This
manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics
behind the strategies. This tutorial doesn't shy away from addressing both the formal
and informal aspects of quantitative strategies. By doing so, it hopes to provide readers
with a deeper understanding of these techniques as well as the when, the how and the
why of applying these techniques. The strategies are presented in terms of both S&P500
and SH510300 data sets. However, the results from the tests are just examples of how the
methods work; no claim is made on the suggestion of real market positions.
Keywords: Quantitative strategies, Machine learning, Times series problem, Sharpe
ratio, Information ratio, RSI, Moving averages and adaptive moving averages, Aroon,
Bollinger bands, Keltner channels, MACD, Big drawdown due to the outbreak of COVID-
19.
Contents
1 Performance Measures 2
1.1 Rate of Return (RR) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Volatility of Returns (VOL) . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.3 Maximum Drawdown . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.4 Sharpe Ratio (SR) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.5 Information Ratio (IR) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.6 Fundamental Analysis vs Technical Analysis . . . . . . . . . . . . . . . . . . 5
1.7 The Kelly Criterion and Optimal Betting . . . . . . . . . . . . . . . . . . . 6
2 Two-Average Strategy 7
2.1 Simple Moving Average (SMA) . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.2 Exponential Moving Average (EMA) . . . . . . . . . . . . . . . . . . . . . . 8
2.3 Adaptive Moving Average (AMA) . . . . . . . . . . . . . . . . . . . . . . . 9
2.4 The Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.5 Data: S&P500 and SH510300 . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.6 Results with AMA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
2.7 Results with non-adaptive MA . . . . . . . . . . . . . . . . . . . . . . . . . 19
3 Keltner Strategy 20
3.1 Keltner Channels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
©2022 Jun Lu.arXiv:2202.11309v1  [q-fin.GN]  23 Feb 2022",2022-02-23T05:02:27Z,elori assic quantitative strategielori assic quantitative strategijulu abstra t it   by t keywords quantitative machine timshare informatmovi aro obollier keltner b contents formance measurrate urvolatity urns maximum draw dowshare rat informatrat fundamental analysis technical analysis t kelly iteroptimal bei two ge strategy simple movi ge eonential movi ge adaptive movi ge t strategy data results results keltner strategy keltner nels julu  
paper_qf_49.pdf,2,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
3.2 Results with non-adaptive MA . . . . . . . . . . . . . . . . . . . . . . . . . 21
3.3 Results with AMA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
4 RSI Overbought and Oversold Strategy 23
4.1 Relative Strength Index (RSI) . . . . . . . . . . . . . . . . . . . . . . . . . . 23
4.2 Result without constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
4.3 Result with constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
4.4 More to Go . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
5 Aroon Strategy 28
5.1 Aroon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
5.2 Results without and with condition . . . . . . . . . . . . . . . . . . . . . . . 29
6 Bollinger Bands Strategy 30
6.1 Bollinger Bands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
6.2 Results with non-adaptive MA . . . . . . . . . . . . . . . . . . . . . . . . . 31
6.3 Results with AMA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
7 MACD Strategy 34
7.1 Moving Average Convergence Divergence (MACD) . . . . . . . . . . . . . . 34
7.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
8 Machine Learning Strategy 35
1. Performance Measures
In this section, we shortly review measures for the performance that will be useful for
evaluating the strategies in the sequel. For more measurements, one can also refer to
(Investopedia, 2022).
1.1 Rate of Return (RR)
In nance, return is a prot on an investment, and a lossinstead of a prot is described as
anegative return , assuming the amount invested is greater than zero. Then the total prot
(TP) is dened to represent the protability of all the transactions; see Equation (1.1). We
note that when the loss is greater than the prot, TP can be negative. We use RR to express
the return or loss of investment in a given time period as a percentage of the investment
amount; see Equation (1.2) where \\\\INVEST"" is the initial amount of investment.
TP = all returns",2022-02-23T05:02:27Z,julu results results oght veryold strategy relative sth inx result result  go aro ostrategy aro oresults bollier bands strategy bollier bands results results strategy movi ge convergence divergence results machine learni strategy formance measurifor iveto pedia rate uritequat  equatn
paper_qf_49.pdf,3,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
1.2 Volatility of Returns (VOL)
In statistics, the dispersion of a sequence can be dened by its standard deviation. While,
in nance, the dispersion of returns for a given security or market index is known as the
volatility of returns and is dened as the standard deviations of returns in percentage
1(i.e., returns or losses divided by the amount of investment). In most cases, the higher
the volatility, the riskier the security. The volatility of returns measures the uctuation of
returns day over day, and it is taken as a risk measure generally. Volatility is sometimes
measured by the variance among the returns in percentage. However, we will only consider
the standard deviation version of it in the sequel.
1.3 Maximum Drawdown
A maximum drawdown (MDD) is the maximum observed loss from a peak to a trough of a
portfolio, before a new peak is attained. The MDD is an indicator of downside risk over a
specied time period and is usually expressed in percentage terms. The smaller the MDD,
the smaller the risk.
1.4 Sharpe Ratio (SR)
The Sharpe ratio, originally called the reward-to-variability ratio , measures the performance
of an investment such as a security or a portfolio compared to a risk-free asset , say, annual-
ized return with 5%2. The Sharpe ratio discounts the expected excess returns of a portfolio
by the volatility of the returns, i.e., measures the excess return per unit of deviation
in an investment asset or a trading strategy. The ex-ante Sharpe ratio is dened as follows
SR =E[Ra",2022-02-23T05:02:27Z,elori assic quantitative strategivolatity urns iw it volatity maximum draw do t share rat t share t share t share ra
paper_qf_49.pdf,4,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
The ex-post Sharpe ratio uses the same equation as the one above but with realized
returns of the asset and benchmark rather than the expected returns.
Remark 1.1: (Ex-Post Sharpe Ratio)
The Sharpe ratio can be recalculated at the end of the year to examine the actual return
rather than the expected return:
SR =ra",2022-02-23T05:02:27Z,julu t share remark ex post share rat t share
paper_qf_49.pdf,5,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
whereRfis a constant risk-free return throughout the time period. And the denition in
Equation (1.3) is due to Sharpe's revision in (Sharpe, 1994) where Sharpe acknowledged
that the basis of the comparison should be an applicable benchmark that changes with time,
i.e.,Rfis not a constant.
1.5 Information Ratio (IR)
The information ratio (IR) is a measurement of portfolio returns above the returns of a
benchmark, usually an index such as the S&P500, to the volatility of those returns. In
this sense, the information ratio is just the same as the revised version of the Sharpe ratio
(Equation (1.3)).
The information ratio is used to evaluate the skill of a portfolio manager at generating
returns in excess of a given benchmark. A higher IR result implies a better portfolio manager
who's achieving a higher return in excess of the benchmark, given the risk taken.
As mentioned above, the information ratio is similar to the Sharpe ratio, the main
dierence being that the Sharpe ratio uses a risk-free return as the benchmark (such as a
U.S. Treasury security) whereas the information ratio uses a risky index as the benchmark
(such as the S&P500). The Sharpe ratio is useful for an attribution of the absolute returns
of a portfolio, and the information ratio is useful for an attribution of the relative returns
of a portfolio.
The information ratio is a benchmark-relative statistic. It is entirely possible for a
manager to have a high information ratio, but still exhibit signicant losses if the benchmark
is down. The Python code for calculating the annualized Information ratio is shown as
follows:
def information_ratio_annual ( returns , benchmark ):
""""""
: param returns : DAILY returns in percentage
: param benchmark : DAILY benchmark return , e.g., S &P 500
: return : annualized Information ratio
""""""
diff = returns - benchmark # Calculate the difference
return np. mean ( diff ) / np. std( diff ) * np. sqrt ( 252)
As one can tell, the higher the information ratio, the better. If the information ratio is
less than zero, it means the active manager failed on the rst objective of outperforming
the benchmark. Of all the performance statistics, the information ratio is one of the most
dicult hurdles to clear. Generally speaking,
•An information ratio between 0.40 and 0.60 is considered quite good;
•An information ratio between 0.61 and 1 is considered a great investment;
•An information ratio of 1.00 for a long period of time is rare.
Typical values for information ratios vary by asset class.
1.6 Fundamental Analysis vs Technical Analysis
There are many dierent ways to assess the value of a company/security, and the methods
used to analyze securities and make investment decisions fall into two very broad categories:
5",2022-02-23T05:02:27Z,elori assic quantitative strategirf is and equatshare share share rf is informatrat t ishare equatt as share share treasury t share t it t pythoinformatinformatcalculate as  of genlly aaatypical fundamental analysis technical analysis tre
paper_qf_49.pdf,6,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
Figure 1: Demonstration of
where the quantitative strategies
lie in the assessment of the mar-
ket.
fundamental analysis and technical analysis. Fundamental analysis is a method of evalu-
ating a security that includes measuring its intrinsic value by examining related economic,
nancial, and other qualitative and quantitative factors. Fundamental analysts attempt
to study everything that can aect the company's value, including macroeconomic factors
(like the overall economy, economic period, and industry conditions) and company-specic
factors (like nancial condition, company size, and management). However, technical anal-
ysis takes a completely dierent approach. It is a method of evaluating assets by analyzing
statistics generated by market signals or indicators, such as past prices, volume, and liquid-
ity (total volumes in a specic time frame). Technical analysts do not attempt to measure
a company's intrinsic value, but instead, use algorithms and other methods to identify
patterns that can suggest future positions.
To be more specic, most of the technical strategies in the sequel lie in between the
deductive and inductive analysis as shown in Figure 1. While we shall shed light on how to
apply machine learning or deep learning techniques to nd new strategies.
1.7 The Kelly Criterion and Optimal Betting
Given a gambling game, for 1 unit of investment, there is a probability of pto obtain
additional positive return L, and there is a probability of q= 1",2022-02-23T05:02:27Z,julu  monstratfundamental fundamental it technical to  w t kelly iteroptimal bei given
paper_qf_49.pdf,7,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
0.0 0.2 0.4 0.6 0.8 1.00.000.050.100.150.200.250.300.350.400.0 0.2 0.4 0.6 0.8 1.00.000.050.100.150.200.250.300.350.40
Figure 2: Logarithmic returns of
dierentx's whenp= 0:9;q= 0:1,
L= 1:1;M= 1.
Sincef(x) is assumed to be positive, we haven
Lp
1+Lx",2022-02-23T05:02:27Z,elori assic quantitative strategi logarithmic since lp
paper_qf_49.pdf,8,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
upward trend is underway; and if the MA line is angled down, a downward trend is ongoing.
However, moving averages don't make predictions about the future value of a stock; they
simply reveal what the price is doing, on average, over a period of time.
The simple moving average (SMA) is literally the simplest form of a moving average.
Each output value is the average of the previous Nvalues where Nis known as the time
period to smooth the array. Given the time period N, thei-th element of the SMA is
dened as follows:
8
<
:SMA[i] =input[i",2022-02-23T05:02:27Z,julu t eavaluis given
paper_qf_49.pdf,9,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
where EMA[ i",2022-02-23T05:02:27Z,elori assic quantitative strategies
paper_qf_49.pdf,10,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
It is calculated with a simple formula:
ER[i] =Signal[i]
Noise[i]=Total price change for a period
Sum of absolute price change for each bar: (2.3)
To avoid confusion, we call the period in the above equation an \\\\adaptive window length""
(AdaWin) to dierentiate from the time period for the MAs. Given the AdaWin= M, we
have
•ER[i] is the current value of the eciency ratio;
•Signal[i] = input[i]",2022-02-23T05:02:27Z,julu it snal noise total sum to ada wias giveada wisnal
paper_qf_49.pdf,11,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
or after rearrangement:
AMA[i] = AMA[i",2022-02-23T05:02:27Z,elori assic quantitative strategies
paper_qf_49.pdf,12,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
/uni00000011 /uni00000016/uni00000011 /uni00000012/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011 /uni00000013/uni00000011/uni00000011 /uni00000013/uni00000016/uni00000011/uni00000012/uni00000012/uni00000011/uni00000011/uni00000012/uni00000012/uni00000016/uni00000011/uni00000012/uni00000013/uni00000011/uni00000011/uni00000012/uni00000013/uni00000016/uni00000011/uni00000012/uni00000014/uni00000011/uni00000011/uni00000012/uni00000014/uni00000016/uni00000011/uni00000034/uni00000031/uni00000016/uni00000011/uni00000011
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000034/uni0000002e/uni00000022
/uni00000026/uni0000002e/uni00000022
/uni00000022/uni0000002e/uni00000022/uni00000001/uni00000058/uni0000004a/uni00000055/uni00000049/uni00000001/uni00000034/uni0000002e/uni00000022
/uni00000022/uni0000002e/uni00000022/uni00000001/uni00000058/uni0000004a/uni00000055/uni00000049/uni00000001/uni00000026/uni0000002e/uni00000022/uni00000011 /uni00000016/uni00000011 /uni00000012/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011 /uni00000013/uni00000011/uni00000011 /uni00000013/uni00000016/uni00000011/uni00000012/uni00000012/uni00000011/uni00000011/uni00000012/uni00000012/uni00000016/uni00000011/uni00000012/uni00000013/uni00000011/uni00000011/uni00000012/uni00000013/uni00000016/uni00000011/uni00000012/uni00000014/uni00000011/uni00000011/uni00000012/uni00000014/uni00000016/uni00000011/uni00000034/uni00000031/uni00000016/uni00000011/uni00000011
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000034/uni0000002e/uni00000022
/uni00000026/uni0000002e/uni00000022
/uni00000022/uni0000002e/uni00000022/uni00000001/uni00000058/uni0000004a/uni00000055/uni00000049/uni00000001/uni00000034/uni0000002e/uni00000022
/uni00000022/uni0000002e/uni00000022/uni00000001/uni00000058/uni0000004a/uni00000055/uni00000049/uni00000001/uni00000026/uni0000002e/uni00000022
(a) MAs for S&P500.
/uni00000011 /uni00000016/uni00000011 /uni00000012/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011 /uni00000013/uni00000011/uni00000011 /uni00000013/uni00000016/uni00000011/uni00000013/uni0000000f/uni00000013/uni00000013/uni0000000f/uni00000014/uni00000013/uni0000000f/uni00000015/uni00000013/uni0000000f/uni00000016/uni00000013/uni0000000f/uni00000017/uni00000013/uni0000000f/uni00000018/uni00000013/uni0000000f/uni00000019/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000034/uni0000002e/uni00000022
/uni00000026/uni0000002e/uni00000022
/uni00000022/uni0000002e/uni00000022/uni00000001/uni00000058/uni0000004a/uni00000055/uni00000049/uni00000001/uni00000034/uni0000002e/uni00000022
/uni00000022/uni0000002e/uni00000022/uni00000001/uni00000058/uni0000004a/uni00000055/uni00000049/uni00000001/uni00000026/uni0000002e/uni00000022/uni00000011 /uni00000016/uni00000011 /uni00000012/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011 /uni00000013/uni00000011/uni00000011 /uni00000013/uni00000016/uni00000011/uni00000013/uni0000000f/uni00000013/uni00000013/uni0000000f/uni00000014/uni00000013/uni0000000f/uni00000015/uni00000013/uni0000000f/uni00000016/uni00000013/uni0000000f/uni00000017/uni00000013/uni0000000f/uni00000018/uni00000013/uni0000000f/uni00000019/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000034/uni0000002e/uni00000022
/uni00000026/uni0000002e/uni00000022
/uni00000022/uni0000002e/uni00000022/uni00000001/uni00000058/uni0000004a/uni00000055/uni00000049/uni00000001/uni00000034/uni0000002e/uni00000022
/uni00000022/uni0000002e/uni00000022/uni00000001/uni00000058/uni0000004a/uni00000055/uni00000049/uni00000001/uni00000026/uni0000002e/uni00000022 (b) MAs for SH510300.
Figure 3: MAs for S&P500 and SH510300 where the time periods for SMA and EMA are
both 50. TimeperiodLong, TimeperiodShort, AdaWin for AMA are 50, 5, 12 respectively.
for i, closeData in enumerate ( array [1:])]
noise = [0. 0001 ]* AdaWin + [sum( absshift [i- AdaWin :i+1])\\\\
for i in range (len ( absshift )) if i>= AdaWin ]
# relative moving momentum - signal
signal = [0]* AdaWin + [( array [i] - array [i- AdaWin ]) \\\\
for i in range (len ( array )) if i>= AdaWin ]
# ERs \\\\in [-1,1]
ER = [ signal [i]/ noise [i] for i in range ( len( signal ))]
if matype == 1: # AMA by EMA
slowSC = 2*1./( TimeperiodLong +1) # e.g., 2/31
fastSC = 2*1./( TimeperiodShort +1) # e.g., 2/3
diffSC = fastSC - slowSC
for i, closeData in enumerate ( array ):
if i==0:
res [i] = array [i]
continue
er_this = abs (ER[i])
# mimicking EMA
scaledSC = pow( slowSC + er_this *diffSC , 2)
res [i] = res [i-1] + scaledSC * ( array [i] - res [i-1])
elif matype == 2: # AMA by SMA
for i, closeData in enumerate ( array ):
if i< TimeperiodLong :
res [i] = array [i]
continue
finalperiod = TimeperiodShort + \\\\
abs(ER[i]) * ( TimeperiodLong - TimeperiodShort )
finalperiod = int( finalperiod )
res [i] = np. mean ( array [i- finalperiod :i+1])
else :
pass
return res
12",2022-02-23T05:02:27Z,julu as as  as time d lo time d short ada widata ada wiada wiada wiada wiada wiada wirs time d lo time d short data data time d lo time d short time d lo time d short
paper_qf_49.pdf,13,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
2.4 The Strategy
The Two-Average strategy, also known as the Crossover strategy , is one of the main moving
average strategies. The rst type is a price crossover, which is when the (closing) price
crosses above or below a MA to signal a potential change in trend (buy and sell respectively).
The further idea on this strategy is to have two sets of MAs: one longer and one
shorter. When the shorter-term MA crosses above the longer-term MA, it's a buy signal,
as it indicates that the trend is shifting up. This is known as a \\\\ golden cross "".
Meanwhile, when the shorter-term MA crosses below the longer-term MA, it's a sell
signal, as it indicates that the trend is shifting down. This is known as a \\\\ dead/death
cross "".
Problem One major problem is that, if the price action becomes choppy, the price may
swing back and forth, generating multiple trend reversals or trade signals. When this
occurs, it's best to step aside or utilize another indicator to help clarify the trend. The
same thing can occur with MA crossovers when the MAs get \\\\tangled up"" for a period of
time, triggering multiple losing trades.
Moving averages work quite well in strong trending conditions but poorly in choppy
or ranging conditions. Adjusting the time period can remedy this problem temporarily,
although at some point, these issues are likely to occur regardless of the time period chosen
for the MAs.
2.5 Data: S&P500 and SH510300
A typical method for obtaining measurements about quantitative/trading strategies is to
run a simulation (that is, backtest) and measure characteristics of the result, such as the
Sharpe ratio. To evaluate the strategy, we then obtain the S&P500 index data, which
is a market-capitalization-weighted index of 500 leading publicly traded companies in the
U.S. from Yahoo Finance6with a time period of 11 years (between Jan. 14, 2011 and
Jan. 14, 2022). The S&P500 index uses a market-cap weighting method, giving a higher
percentage allocation to companies with the larger market capitalizations. The weighting of
each company in the index is calculated by taking the company's market cap and dividing
it by the total market cap of the index:
Company weighting in S&P500 =Company market cap
Total market caps;
where the market cap of a company is calculated by taking the current stock price and
multiplying it by the company's outstanding shares.7The daily closing price of the data
and its daily rate of returns are shown in Figure 4 where we observe that the distribution
of return in percentage is close to a Gaussian distribution, N(5:3e",2022-02-23T05:02:27Z,elori assic quantitative strategit strategy t two ge ossot t as w meanw  problem one  as movi adjusti as data share to yahoo nance jajat t any any total t  n
paper_qf_49.pdf,14,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011 /uni00000013/uni00000016/uni00000011/uni00000011/uni00000012/uni00000011/uni00000011/uni00000011/uni00000012/uni00000016/uni00000011/uni00000011/uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000016/uni00000011/uni00000011/uni00000014/uni00000011/uni00000011/uni00000011/uni00000014/uni00000016/uni00000011/uni00000011/uni00000015/uni00000011/uni00000011/uni00000011/uni00000015/uni00000016/uni00000011/uni00000011/uni00000034/uni00000031/uni00000016/uni00000011/uni00000011/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011 /uni00000013/uni00000016/uni00000011/uni00000011/uni00000012/uni00000011/uni00000011/uni00000011/uni00000012/uni00000016/uni00000011/uni00000011/uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000016/uni00000011/uni00000011/uni00000014/uni00000011/uni00000011/uni00000011/uni00000014/uni00000016/uni00000011/uni00000011/uni00000015/uni00000011/uni00000011/uni00000011/uni00000015/uni00000016/uni00000011/uni00000011/uni00000034/uni00000031/uni00000016/uni00000011/uni00000011
(a) Closing price of S&P500 in the time period of
11 years where the horizontal axis is the order
of the dates and vertical axis is the stock price.
 /uni00000011/uni0000000f/uni00000011/uni00000011 /uni00000011/uni0000000f/uni00000011/uni00000016 /uni00000011/uni0000000f/uni00000012/uni00000011/uni00000011/uni00000012/uni00000011/uni00000013/uni00000011/uni00000014/uni00000011/uni00000015/uni00000011/uni00000016/uni00000011/uni00000017/uni00000011/uni00000018/uni00000011/uni00000031/uni00000053/uni00000050/uni00000043/uni00000042/uni00000043/uni0000004a/uni0000004d/uni0000004a/uni00000055/uni0000005a/uni00000034/uni00000031/uni00000016/uni00000011/uni00000011/uni00000001/uni00000023/uni0000004a/uni0000004f/uni00000001/uni00000031/uni0000004d/uni00000050/uni00000055/uni00000011/uni0000000f/uni00000012/uni00000011
 /uni00000011/uni0000000f/uni00000011/uni00000016
 /uni00000011/uni0000000f/uni00000011/uni00000011 /uni00000011/uni0000000f/uni00000011/uni00000016 /uni00000011/uni0000000f/uni00000012/uni00000011/uni00000011/uni00000012/uni00000011/uni00000013/uni00000011/uni00000014/uni00000011/uni00000015/uni00000011/uni00000016/uni00000011/uni00000017/uni00000011/uni00000018/uni00000011/uni00000031/uni00000053/uni00000050/uni00000043/uni00000042/uni00000043/uni0000004a/uni0000004d/uni0000004a/uni00000055/uni0000005a/uni00000034/uni00000031/uni00000016/uni00000011/uni00000011/uni00000001/uni00000023/uni0000004a/uni0000004f/uni00000001/uni00000031/uni0000004d/uni00000050/uni00000055(b) Bin plot of the returns in percentage (range from
-1 to 1) where the dotted line is the tted Gaus-
sian distribution: N(5:3e",2022-02-23T05:02:27Z,julu osi ba
paper_qf_49.pdf,15,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni0000000f/uni00000011/uni00000013/uni0000000f/uni00000016/uni00000014/uni0000000f/uni00000011/uni00000014/uni0000000f/uni00000016/uni00000015/uni0000000f/uni00000011/uni00000015/uni0000000f/uni00000016/uni00000016/uni0000000f/uni00000011/uni00000016/uni0000000f/uni00000016/uni00000017/uni0000000f/uni00000011/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni0000000f/uni00000011/uni00000013/uni0000000f/uni00000016/uni00000014/uni0000000f/uni00000011/uni00000014/uni0000000f/uni00000016/uni00000015/uni0000000f/uni00000011/uni00000015/uni0000000f/uni00000016/uni00000016/uni0000000f/uni00000011/uni00000016/uni0000000f/uni00000016/uni00000017/uni0000000f/uni00000011/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011
(a) Closing price of SH510300 in the time period of
9 years where the horizontal axis is the order of
the dates and vertical axis is the stock price.
 /uni00000011/uni0000000f/uni00000011/uni00000011/uni00000011 /uni00000011/uni0000000f/uni00000011/uni00000013/uni00000016 /uni00000011/uni0000000f/uni00000011/uni00000016/uni00000011 /uni00000011/uni0000000f/uni00000011/uni00000018/uni00000016 /uni00000011/uni0000000f/uni00000012/uni00000011/uni00000011/uni00000011/uni00000012/uni00000011/uni00000013/uni00000011/uni00000014/uni00000011/uni00000015/uni00000011/uni00000016/uni00000011/uni00000031/uni00000053/uni00000050/uni00000043/uni00000042/uni00000043/uni0000004a/uni0000004d/uni0000004a/uni00000055/uni0000005a/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni00000001/uni00000023/uni0000004a/uni0000004f/uni00000001/uni00000031/uni0000004d/uni00000050/uni00000055/uni00000011/uni0000000f/uni00000012/uni00000011/uni00000011
 /uni00000011/uni0000000f/uni00000011/uni00000018/uni00000016
 /uni00000011/uni0000000f/uni00000011/uni00000016/uni00000011
 /uni00000011/uni0000000f/uni00000011/uni00000013/uni00000016
 /uni00000011/uni0000000f/uni00000011/uni00000011/uni00000011 /uni00000011/uni0000000f/uni00000011/uni00000013/uni00000016 /uni00000011/uni0000000f/uni00000011/uni00000016/uni00000011 /uni00000011/uni0000000f/uni00000011/uni00000018/uni00000016 /uni00000011/uni0000000f/uni00000012/uni00000011/uni00000011/uni00000011/uni00000012/uni00000011/uni00000013/uni00000011/uni00000014/uni00000011/uni00000015/uni00000011/uni00000016/uni00000011/uni00000031/uni00000053/uni00000050/uni00000043/uni00000042/uni00000043/uni0000004a/uni0000004d/uni0000004a/uni00000055/uni0000005a/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni00000001/uni00000023/uni0000004a/uni0000004f/uni00000001/uni00000031/uni0000004d/uni00000050/uni00000055(b) Bin plot of the returns in percentage (range from
-1 to 1) where the dotted line is the tted Gaus-
sian distribution: N(3:8e",2022-02-23T05:02:27Z,elori assic quantitative strategiosi ba
paper_qf_49.pdf,16,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
Figure 6: Demonstration on the
idea of sector neutral.
drops sharply. See (Schumaker and Chen, 2009) for a discussion on this sector/industry
classication.
And all in all, innovative and functional methods are discovered from data by quantita-
tive researchers.
2.6 Results with AMA
The result of the Two-Average strategy with AMA on the S&P500 is shown in Figure 7
where the horizontal axis is the order of the dates and the vertical axis is the stock price.
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011 /uni00000013/uni00000016/uni00000011/uni00000011/uni00000012/uni00000011/uni00000011/uni00000011/uni00000012/uni00000016/uni00000011/uni00000011/uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000016/uni00000011/uni00000011/uni00000014/uni00000011/uni00000011/uni00000011/uni00000014/uni00000016/uni00000011/uni00000011/uni00000015/uni00000011/uni00000011/uni00000011/uni00000015/uni00000016/uni00000011/uni00000011/uni00000016/uni00000011/uni00000011/uni00000011/uni00000034/uni00000031/uni00000016/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000013/uni0000001a/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000012/uni00000013/uni00000015/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000016/uni00000011/uni00000012/uni00000019/uni0000000f/uni00000011
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000016/uni00000012/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000016/uni0000000d/uni00000001/uni00000022/uni00000045/uni00000042/uni00000038/uni0000004a/uni0000004f/uni0000001e/uni00000012/uni00000013/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000013
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni0000002e/uni00000022
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011 /uni00000013/uni00000016/uni00000011/uni00000011/uni00000012/uni00000011/uni00000011/uni00000011/uni00000012/uni00000016/uni00000011/uni00000011/uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000016/uni00000011/uni00000011/uni00000014/uni00000011/uni00000011/uni00000011/uni00000014/uni00000016/uni00000011/uni00000011/uni00000015/uni00000011/uni00000011/uni00000011/uni00000015/uni00000016/uni00000011/uni00000011/uni00000016/uni00000011/uni00000011/uni00000011/uni00000034/uni00000031/uni00000016/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000013/uni0000001a/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000012/uni00000013/uni00000015/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000016/uni00000011/uni00000012/uni00000019/uni0000000f/uni00000011
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000016/uni00000012/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000016/uni0000000d/uni00000001/uni00000022/uni00000045/uni00000042/uni00000038/uni0000004a/uni0000004f/uni0000001e/uni00000012/uni00000013/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000013
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni0000002e/uni00000022
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d
(a) TimeperiodLong=51, TimeperiodShort=5,
AdaWin=12, Matype=2.
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011 /uni00000013/uni00000016/uni00000011/uni00000011/uni00000012/uni00000011/uni00000011/uni00000011/uni00000012/uni00000016/uni00000011/uni00000011/uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000016/uni00000011/uni00000011/uni00000014/uni00000011/uni00000011/uni00000011/uni00000014/uni00000016/uni00000011/uni00000011/uni00000015/uni00000011/uni00000011/uni00000011/uni00000015/uni00000016/uni00000011/uni00000011/uni00000016/uni00000011/uni00000011/uni00000011/uni00000034/uni00000031/uni00000016/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000014/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000012/uni00000014/uni00000017/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000016/uni00000012/uni00000012/uni00000016/uni0000000f/uni00000016
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000016/uni00000014/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000014/uni0000000d/uni00000001/uni00000022/uni00000045/uni00000042/uni00000038/uni0000004a/uni0000004f/uni0000001e/uni00000012/uni00000013/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000013
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni0000002e/uni00000022
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011 /uni00000013/uni00000016/uni00000011/uni00000011/uni00000012/uni00000011/uni00000011/uni00000011/uni00000012/uni00000016/uni00000011/uni00000011/uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000016/uni00000011/uni00000011/uni00000014/uni00000011/uni00000011/uni00000011/uni00000014/uni00000016/uni00000011/uni00000011/uni00000015/uni00000011/uni00000011/uni00000011/uni00000015/uni00000016/uni00000011/uni00000011/uni00000016/uni00000011/uni00000011/uni00000011/uni00000034/uni00000031/uni00000016/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000014/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000012/uni00000014/uni00000017/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000016/uni00000012/uni00000012/uni00000016/uni0000000f/uni00000016
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000016/uni00000014/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000014/uni0000000d/uni00000001/uni00000022/uni00000045/uni00000042/uni00000038/uni0000004a/uni0000004f/uni0000001e/uni00000012/uni00000013/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000013
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni0000002e/uni00000022
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d(b) TimeperiodLong=53, TimeperiodShort=3,
AdaWin=12, Matype=2.
Figure 7: Two-Average strategy on S&P500 with AMA where the blue line is the \\\\stock
price"" of the strategy, the green line is the closing price of S&P500 on each day, and the
yellow line is a specic moving average whose parameter is shown in the title of each gure.
The red dots indicate when to buy and the cyan dots indicate when to sell. \\\\FinalPrice"" in
the title represents the nal \\\\stock price"" of the strategy; and \\\\BuyCNT"" in the title counts
the number of buying. \\\\Matype=2"" means the AMA is based on SMA; while, \\\\Matype=1""
means the AMA is based on EMA (here, we only observe good results when Matype=2).
16",2022-02-23T05:02:27Z,julu  monstrat smaker cand results t two ge  time d lo time d short ada wima  time d lo time d short ada wima   two ge t nal price buy ma  ma  ma 
paper_qf_49.pdf,17,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
The detailed measures in Figure 7(a) are given as follows where \\\\MDD"" is the max
drawdown, \\\\SR"" is the Sharpe ratio, and \\\\IR"" is the information ratio whose benchmark
is set to be the S&P500 itself. The \\\\Initial Price"" is the stock price of the rst buy and
the \\\\Final Price"" represents the nal price of the strategy. \\\\RR"" is short for rate of return;
and minimal and maximal annualized RR are provided as a reference.
The \\\\Total number of buy count"" (i.e., \\\\BuyCNT"" in the titles of Figure 7(a) and 7(b))
can be understood as the turnover of the strategy. The higher the turnover, the higher
the trading costs and the market impact. However, high turnover strategies usually have
stronger trading signals that can help follow the upward trend of the markets. Therefore,
turnover should not be too high or too low. And dierent traders may not easily agree with
which turnover is the best and we shall not discuss this issue where the \\\\BuyCNT"" is just
served as a reference.
We notice that, in this case, the IR is negative; the reason is partly from that the strategy
does not work well from the 2400-th day (i.e., the outbreak of the COVID-19). The \\\\big
drawdown"" of the S&P500 series due to the COVID-19 issue also causes interesting results
as we shall see in the Keltner strategy (Section 3, p. 20). Therefore, dierent strategies may
be applied before and after the cuto.
Initial Price : 1271 .87
Final Price : 5017 . 952801940561
RR of whole period : 3. 945334666232053
RR/ year : 1. 132538418927603
RR of year -1: 1. 2052468859894903
RR of year -2: 1. 209821429404758
RR of year -3: 1. 0925606788248348
RR of year -4: 1. 1625258120500706
RR of year -5: 1. 1196006019227314
RR of year -6: 1. 150334996114205
RR of year -7: 1. 0566055909800327
RR of year -8: 1. 02915151309352
RR of year -9: 1. 1501585844135516
RR of year -10: 1. 153891406484533
RR of year -11: 1. 2548521976045632
Total number of buy count : 124
MAX rate : 1. 2548521976045632 MIN rate : 1. 02915151309352
MDD: 0. 1782640232818626
SR: 1. 289569812149183
IR: -0. 021788463873900706
Moreover, the result of the Two-Average strategy with AMA on the SH510300 is shown in
Figure 8. As we mentioned previously, there are more drawdowns in the SH510300 data
set, the results on this data set may provide more information, e.g., whether the strategy
can avoid losing prot and follows the upward trend when it's coming.
17",2022-02-23T05:02:27Z,elori assic quantitative strategit  share t initial price nal price t total buy  t trefore and buy  t keltner setrefore initial price nal price total otwo ge  as
paper_qf_49.pdf,18,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000015/uni00000017/uni00000019/uni00000012/uni00000011/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000012/uni0000001a/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000015/uni0000001a/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000012/uni00000012/uni0000000f/uni00000013
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000012/uni0000001a/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000014/uni0000000d/uni00000001/uni00000022/uni00000045/uni00000042/uni00000038/uni0000004a/uni0000004f/uni0000001e/uni00000012/uni00000015/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000013
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni0000002e/uni00000022
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000015/uni00000017/uni00000019/uni00000012/uni00000011/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000012/uni0000001a/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000015/uni0000001a/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000012/uni00000012/uni0000000f/uni00000013
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000012/uni0000001a/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000014/uni0000000d/uni00000001/uni00000022/uni00000045/uni00000042/uni00000038/uni0000004a/uni0000004f/uni0000001e/uni00000012/uni00000015/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000013
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni0000002e/uni00000022
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d
(a) TimeperiodLong=19, TimeperiodShort=3,
AdaWin=14, Matype=2.
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni00000012/uni00000011/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000012/uni00000015/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000017/uni00000016/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni0000001a/uni0000000f/uni00000013
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000017/uni00000012/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000016/uni0000000d/uni00000001/uni00000022/uni00000045/uni00000042/uni00000038/uni0000004a/uni0000004f/uni0000001e/uni00000015/uni00000017/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000013
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni0000002e/uni00000022
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni00000012/uni00000011/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000012/uni00000015/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000017/uni00000016/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni0000001a/uni0000000f/uni00000013
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000017/uni00000012/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000016/uni0000000d/uni00000001/uni00000022/uni00000045/uni00000042/uni00000038/uni0000004a/uni0000004f/uni0000001e/uni00000015/uni00000017/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000013
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni0000002e/uni00000022
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d(b) TimeperiodLong=61, TimeperiodShort=5,
AdaWin=46, Matype=2.
Figure 8: Two-Average strategy with AMA on the SH510300 where the green line is the
closing price of SH510300 on each day.
The detailed measures in Figure 8(a) are given as follows where again the \\\\IR"" is the
information ratio whose benchmark is set to be the SH510300 itself. Figure 8(a) shows a
promising result as the strategy can shy away from the drawdown periods, e.g., the period
around 1500-th day; and more importantly, it chases the upward trends.
Initial Price : 2.604
Final Price : 11. 246482441341051
RR of whole period : 4. 318925668717761
RR/ year : 1. 1699381785623744
RR of year -1: 1. 0044238376549013
RR of year -2: 0. 9454947007322866
RR of year -3: 1. 8294921696975432
RR of year -4: 1. 1414544503541364
RR of year -5: 1. 074509559922823
RR of year -6: 1. 1744891266255308
RR of year -7: 1. 1231252139480294
RR of year -8: 1. 262463201126989
RR of year -9: 1. 2130668826994588
Total number of buy count : 49
MAX rate : 1. 8294921696975432 MIN rate : 0. 9454947007322866
MDD: 0. 18872919818456887
SR: 1. 1888577990184008
IR: 0. 36914729565256765
One thing we need to keep in mind is the dierence between the in-sample results and the
out-of-sample results. Before applying the strategy into live trading, careful monitoring of
the results should be applied for a certain period of time. And here, we use a long period of
time to evaluate the performance of the strategy(11 years for the S&P500 data and 9 years
for the SH510300 data). In practice, evaluation on a 3-year period should be acceptable
since the \\\\old data"" may not reect the markets suciently and induce an overtting on
the data.
Key takeaway Comparing the results on S&P500 and SH510300 with AMA, we nd the
adaptive version of SMA works better than the adaptive version of EMA. This is partly
18",2022-02-23T05:02:27Z,julu time d lo time d short ada wima  time d lo time d short ada wima   two ge t   initial price nal price total one before and ikey ari 
paper_qf_49.pdf,19,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
because the AMA from EMA has been extensively used in the industry, and the signal
might disappear in the recent markets; while, AMA from SMA is not that famous at the
moment and shows new ways to do the strategy. Readers are recommended to apply the
AMA idea for other moving averages, such as the DEMA, T3, and so on. To simply put,
the nal time period in the adaptive version of an MA is something like the Equation (2.8).
And we shall not give the details for simplicity.
2.7 Results with non-adaptive MA
For the Two-Average strategy with non-adaptive MA on the S&P500 data, the strategy
works poor and we shall not give the details. However, it still gives promising results on
the SH510300 data as shown in Figure 9.
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000015/uni00000017/uni00000019/uni00000012/uni00000011/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000011/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000014/uni00000019/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000012/uni00000011/uni0000000f/uni00000011
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000001e/uni00000012/uni00000014/uni00000017/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000012/uni00000012
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni0000002e/uni00000022
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000015/uni00000017/uni00000019/uni00000012/uni00000011/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000011/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000014/uni00000019/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000012/uni00000011/uni0000000f/uni00000011
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000001e/uni00000012/uni00000014/uni00000017/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000012/uni00000012
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni0000002e/uni00000022
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d
(a) Timeperiod=136.
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000018/uni00000019/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000016/uni00000019/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000017/uni0000000f/uni00000017
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000001e/uni00000012/uni00000011/uni00000018/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000012/uni00000016
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni0000002e/uni00000022
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000018/uni00000019/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000016/uni00000019/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000017/uni0000000f/uni00000017
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000001e/uni00000012/uni00000011/uni00000018/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000012/uni00000016
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni0000002e/uni00000022
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d (b) Timeperiod=107.
Figure 9: Two-Average strategy with non-adaptive MA on the SH510300. Here the
\\\\Matype="" in the title represents one of the moving average methods (e.g., SMA, EMA,
T3, TEMA, and so on) and we will not give the details for simplicity.
The detailed measures in Figure 9(a) are given as follows where again the \\\\IR"" is the
information ratio whose benchmark is set to be the SH510300 itself.
Initial Price : 2.604
Final Price : 10. 049906506216752
RR of whole period : 3. 8594111006976775
RR/ year : 1. 1559040824113815
RR of year -1: 0. 9744485970971373
RR of year -2: 0. 9972268008802848
RR of year -3: 2. 161406247343376
RR of year -4: 0. 8644925474432263
RR of year -5: 1. 165505303355888
RR of year -6: 1. 1184080266316072
RR of year -7: 1. 1212847717423118
RR of year -8: 1. 3077094873031774
RR of year -9: 1. 0867892863143571
Total number of buy count : 38
MAX rate : 2. 161406247343376 MIN rate : 0. 8644925474432263
MDD: 0. 23630255563820962
SR: 1. 0021273764071357
19",2022-02-23T05:02:27Z,elori assic quantitative strategirears to equatand results for two ge  time d time d  two ge re ma  t  initial price nal price total
paper_qf_49.pdf,20,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
IR: 0. 34673115120162523
Drawbacks Moving averages are calculated based on historical data, and nothing about
the calculation is predictive in nature. Therefore, results using moving averages can be
random. At times, the market seems to respect MA support, and at other times, it shows
these indicators no respect.
3. Keltner Strategy
3.1 Keltner Channels
Keltner channels are volatility-based bands that are placed on either side of an asset's price
and can aid in determining the direction of a trend. The Keltner channel was rst introduced
by Chester Keltner in the 1960s (Keltner, 1960). The original formula used SMA and the
high-low price range to calculate the bands. In the 1980s, a new formula was introduced,
the TR and ATR, that are commonly used today.
ATR The Keltner channel uses the average true range (ATR). The ATR is a technical
analysis indicator, introduced by market technician J. Welles Wilder in his book (Wilder,
1978), that measures market volatility by decomposing the entire range of an asset price
for that period N:
8
>><
>>:TR[i] = Max
high[i]",2022-02-23T05:02:27Z,julu drawbacks movi trefore at keltner strategy keltner nels keltner t keltner cster keltner keltner t it keltner t lls wir wir max
paper_qf_49.pdf,21,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
should consider initiating long/buy positions while liquidating short/sell positions. If the
price action breaks below the band, the trader should consider initiating short/sell positions
while exiting long/buy positions.
3.2 Results with non-adaptive MA
The result of the Keltner strategy with non-adaptive MA on the S&P500 is shown in
Figure 10.
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011 /uni00000013/uni00000016/uni00000011/uni00000011/uni00000012/uni00000011/uni00000011/uni00000011/uni00000013/uni00000011/uni00000011/uni00000011/uni00000014/uni00000011/uni00000011/uni00000011/uni00000015/uni00000011/uni00000011/uni00000011/uni00000016/uni00000011/uni00000011/uni00000011/uni00000034/uni00000031/uni00000016/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000014/uni00000012/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000012/uni00000012/uni00000016/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000016/uni00000013/uni00000018/uni00000018/uni0000000f/uni00000015
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000001e/uni00000019/uni0000000d/uni00000001/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni0000001a
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011 /uni00000013/uni00000016/uni00000011/uni00000011/uni00000012/uni00000011/uni00000011/uni00000011/uni00000013/uni00000011/uni00000011/uni00000011/uni00000014/uni00000011/uni00000011/uni00000011/uni00000015/uni00000011/uni00000011/uni00000011/uni00000016/uni00000011/uni00000011/uni00000011/uni00000034/uni00000031/uni00000016/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000014/uni00000012/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000012/uni00000012/uni00000016/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000016/uni00000013/uni00000018/uni00000018/uni0000000f/uni00000015
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000001e/uni00000019/uni0000000d/uni00000001/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni0000001a
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d
(a) Timeperiod=8.
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011 /uni00000013/uni00000016/uni00000011/uni00000011/uni00000012/uni00000011/uni00000011/uni00000011/uni00000012/uni00000016/uni00000011/uni00000011/uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000016/uni00000011/uni00000011/uni00000014/uni00000011/uni00000011/uni00000011/uni00000014/uni00000016/uni00000011/uni00000011/uni00000015/uni00000011/uni00000011/uni00000011/uni00000015/uni00000016/uni00000011/uni00000011/uni00000016/uni00000011/uni00000011/uni00000011/uni00000034/uni00000031/uni00000016/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000012/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000012/uni00000013/uni00000015/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000015/uni00000018/uni00000019/uni00000016/uni0000000f/uni00000013
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000001e/uni00000017/uni0000000d/uni00000001/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni0000001a
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011 /uni00000013/uni00000016/uni00000011/uni00000011/uni00000012/uni00000011/uni00000011/uni00000011/uni00000012/uni00000016/uni00000011/uni00000011/uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000016/uni00000011/uni00000011/uni00000014/uni00000011/uni00000011/uni00000011/uni00000014/uni00000016/uni00000011/uni00000011/uni00000015/uni00000011/uni00000011/uni00000011/uni00000015/uni00000016/uni00000011/uni00000011/uni00000016/uni00000011/uni00000011/uni00000011/uni00000034/uni00000031/uni00000016/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000012/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000012/uni00000013/uni00000015/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000015/uni00000018/uni00000019/uni00000016/uni0000000f/uni00000013
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000001e/uni00000017/uni0000000d/uni00000001/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni0000001a
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d (b) Timeperiod=6.
Figure 10: Keltner strategy on S&P500 with non-adaptive MA. Here the \\\\Matype="" in
the title represents one of the moving average methods (e.g., SMA, EMA, T3, TEMA, and
so on) and we shall not give the details for simplicity.
The detailed measures in Figure 10(a) is given as follows where again the \\\\IR"" is the
information ratio whose benchmark is set to be the S&P500 itself. Though both the results
in Figure 10(a) and Figure 10(b) are acceptable, we observe that the rst one works better
after the \\\\big drawdown"" due to the outbreak of the COVID-19; and the second one works
better before the \\\\big drawdown"". Therefore, dierent strategies may be applied before
and after the cuto (in theory).
Initial Price : 1271 .87
Final Price : 5277 . 378092162411
RR of whole period : 4. 149306212240568
RR/ year : 1. 1365625260709509
RR of year -1: 1. 1209917702418386
RR of year -2: 1. 0590707254487217
RR of year -3: 1. 241576085383799
RR of year -4: 1. 1943157812476106
RR of year -5: 1. 0157534362807321
RR of year -6: 1. 0560342266720353
RR of year -7: 1. 122138960294929
RR of year -8: 1. 1645196768443067
RR of year -9: 1. 1528355526922454
RR of year -10: 1. 3422459085639766
RR of year -11: 1. 143119783030126
Total number of buy count : 115
MAX rate : 1. 3422459085639766 MIN rate : 1. 0157534362807321
21",2022-02-23T05:02:27Z,elori assic quantitative strategi results t keltner  time d time d  keltner re ma  t  though   trefore initial price nal price total
paper_qf_49.pdf,22,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
MDD: 0. 10047291580736482
SR: 1. 312384211147428
IR: 0. 013201664414390562
The result of the Keltner strategy with non-adaptive MA on the SH510300 is shown in
Figure 11.
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000015/uni00000017/uni00000019/uni00000012/uni00000011/uni00000012/uni00000013/uni00000012/uni00000015/uni00000012/uni00000017/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000013/uni0000001a/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000012/uni00000011/uni00000015/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000012/uni00000016/uni0000000f/uni00000016
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000001e/uni00000017/uni0000000d/uni00000001/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni0000001a
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000015/uni00000017/uni00000019/uni00000012/uni00000011/uni00000012/uni00000013/uni00000012/uni00000015/uni00000012/uni00000017/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000013/uni0000001a/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000012/uni00000011/uni00000015/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000012/uni00000016/uni0000000f/uni00000016
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000001e/uni00000017/uni0000000d/uni00000001/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni0000001a
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d
(a) Timeperiod=6.
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000015/uni00000017/uni00000019/uni00000012/uni00000011/uni00000012/uni00000013/uni00000012/uni00000015/uni00000012/uni00000017/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000013/uni00000012/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000017/uni0000001a/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000012/uni00000015/uni0000000f/uni00000014
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000001e/uni00000012/uni00000013/uni0000000d/uni00000001/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni0000001a
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000015/uni00000017/uni00000019/uni00000012/uni00000011/uni00000012/uni00000013/uni00000012/uni00000015/uni00000012/uni00000017/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000013/uni00000012/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000017/uni0000001a/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000012/uni00000015/uni0000000f/uni00000014
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000001e/uni00000012/uni00000013/uni0000000d/uni00000001/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni0000001a
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d (b) Timeperiod=12.
Figure 11: Keltner strategy on SH510300 with non-adaptive MA.
The detailed measures in Figure 11(a) are given as follows where again the \\\\IR"" is the
information ratio whose benchmark is set to be the SH510300 itself.
Initial Price : 2.604
Final Price : 15. 481375580472795
RR of whole period : 5. 945228717539476
RR/ year : 1. 2089084666981096
RR of year -1: 0. 9591281739149894
RR of year -2: 1. 1556931564748096
RR of year -3: 1. 9551016478949061
RR of year -4: 1. 0178450078822818
RR of year -5: 1. 1858471067498066
RR of year -6: 1. 1532814626720256
RR of year -7: 1. 240845339361789
RR of year -8: 1. 4062147816871258
RR of year -9: 1. 1399219015987205
Total number of buy count : 104
MAX rate : 1. 9551016478949061 MIN rate : 0. 9591281739149894
MDD: 0. 15582450832072614
SR: 1. 2876574619127727
IR: 0. 6059468181557052
3.3 Results with AMA
The result of the Keltner strategy with AMA on the SH510300 is shown in Figure 12.
22",2022-02-23T05:02:27Z,julu t keltner  time d time d  keltner t  initial price nal price total results t keltner 
paper_qf_49.pdf,23,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000012/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000014/uni00000012/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000018/uni0000000f/uni00000019
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000016/uni00000015/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000013/uni0000000d/uni00000001/uni00000022/uni00000045/uni00000042/uni00000038/uni0000004a/uni0000004f/uni0000001e/uni00000012/uni00000019/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000013
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000012/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000014/uni00000012/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000018/uni0000000f/uni00000019
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000016/uni00000015/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000013/uni0000000d/uni00000001/uni00000022/uni00000045/uni00000042/uni00000038/uni0000004a/uni0000004f/uni0000001e/uni00000012/uni00000019/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000013
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d
(a) TimeperiodLong=54, TimeperiodShort=2,
AdaWin=18, Matype=2.
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000011/uni00000018/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000014/uni00000014/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000018/uni0000000f/uni00000015
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000015/uni00000017/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000013/uni0000000d/uni00000001/uni00000022/uni00000045/uni00000042/uni00000038/uni0000004a/uni0000004f/uni0000001e/uni00000012/uni00000017/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000013
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000011/uni00000018/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000014/uni00000014/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000018/uni0000000f/uni00000015
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000015/uni00000017/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000013/uni0000000d/uni00000001/uni00000022/uni00000045/uni00000042/uni00000038/uni0000004a/uni0000004f/uni0000001e/uni00000012/uni00000017/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000013
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d(b) TimeperiodLong=46, TimeperiodShort=2,
AdaWin=16, Matype=2.
Figure 12: Keltner strategy on SH510300 with AMA where \\\\Matype=2"" in the title
indicates the AMA is from SMA; see the \\\\adaptiveMovAvg"" code on p. 12.
The detailed measures in Figure 12(a) are given as follows where again the \\\\IR"" is the
information ratio whose benchmark is set to be the SH510300 itself.
Initial Price : 2.604
Final Price : 7. 83078199364342
RR of whole period : 3. 007212747174892
RR/ year : 1. 124320637334687
RR of year -1: 0. 9974343484257681
RR of year -2: 0. 9892031706228506
RR of year -3: 1. 812199489656705
RR of year -4: 1. 04416668679696
RR of year -5: 1. 0104317126809472
RR of year -6: 1. 0494680691869664
RR of year -7: 1. 2081837140293505
RR of year -8: 1. 18035738803435
RR of year -9: 1. 064563078825207
Total number of buy count : 31
MAX rate : 1. 812199489656705 MIN rate : 0. 9892031706228506
MDD: 0. 18872919818456887
SR: 1. 1016017674998995
IR: 0. 1333340167631022
4. RSI Overbought and Oversold Strategy
4.1 Relative Strength Index (RSI)
The relative strength index (RSI) calculates a ratio of the recent upward price movements
to the absolute price movement such that the RSI ranges from 0 to 100. The RSI is a
momentum indicator used in technical analysis that measures the magnitude of recent price
changes to evaluate overbought or oversold conditions in the price of a stock or other asset.
Specically, the RSI is interpreted as an overbought/oversold indicator when the value is
over 70/below 30. You can also look for divergence with price. If the price is making
new highs/lows, and the RSI is not, it indicates a reversal. The RSI was developed by J.
23",2022-02-23T05:02:27Z,elori assic quantitative strategitime d lo time d short ada wima  time d lo time d short ada wima   keltner ma  ov g t  initial price nal price total oght veryold strategy relative sth inx t t spec you  t
paper_qf_49.pdf,24,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
Welles Wilder and was rst introduced in his article in the June, 1978 issue of Commodities
magazine, now known as the Futures magazine, and is detailed in his book (Wilder, 1978).
Given the time frame N, the denition of the RSI is shown in the following Algorithm 1.
Algorithm 1 Compute the i-th element of the RSI sequence
Require: For thei-th element of RSI sequence, \\\\upavg, dnavg"" are from last element.
1:Given the time period N;
2:ifclose[i]>close[i",2022-02-23T05:02:27Z,julu lls wir june cooditifuturwir givealgorithm algorithm ute require for given
paper_qf_49.pdf,25,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
: param SMArate : band parameter for the second condition on overbought /
oversold , in 0. 001~0.02
: return :
""""""
for i in range (60 , len ( data ['close '])-1):
if rsitype == 1: # RAW RSI
condition1 = True
condition2 = True
elif rsitype == 2: # RSI+ SMArate
condition1 = data ['close '][i] < ((1- SMArate )*SMA[i])
condition2 = data ['close '][i] > ((1+ SMArate )*SMA[i])
# Oversold - BUY
if rsi[i]< downThres and condition1 :
downrate = ( data ['close '][i-1] - data ['close '][i])/ data ['close '][i-1]
if downrate <= DiffRate and downrate >=0:
if not position_buy :
# TODO : BUY
position_buy = True
# Overbought - SELL
elif rsi [i]> upperThres and condition2 :
uprate = ( data ['close '][i] - data ['close '][i-1])/ data ['close '][i-1]
if uprate <= DiffRate and uprate >=0:
if position_buy :
# TODO : SELL
position_buy = False
4.2 Result without constraints
The result of the RSI Strategy without constraints on the SH510300 is shown in Figure 13.
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000018/uni00000013/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000014/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000018/uni0000000f/uni00000018
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000033/uni00000034/uni0000002a/uni0000001e/uni00000017/uni0000000d/uni00000001/uni00000025/uni0000004a/uni00000047/uni00000047/uni00000033/uni00000042/uni00000055/uni00000046/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000011/uni00000015/uni00000014/uni00000018/uni0000001a/uni00000014
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000043/uni00000056/uni0000005a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000018/uni00000013/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000014/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000018/uni0000000f/uni00000018
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000033/uni00000034/uni0000002a/uni0000001e/uni00000017/uni0000000d/uni00000001/uni00000025/uni0000004a/uni00000047/uni00000047/uni00000033/uni00000042/uni00000055/uni00000046/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000011/uni00000015/uni00000014/uni00000018/uni0000001a/uni00000014
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000043/uni00000056/uni0000005a
/uni00000034/uni00000026/uni0000002d/uni0000002d
(a) TimeperiodRSI=6, DiRate=0.00043793.
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000017/uni00000017/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000015/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000017/uni0000000f/uni00000017
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000033/uni00000034/uni0000002a/uni0000001e/uni00000017/uni0000000d/uni00000001/uni00000025/uni0000004a/uni00000047/uni00000047/uni00000033/uni00000042/uni00000055/uni00000046/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000011/uni00000017/uni00000011/uni00000017/uni0000001a
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000043/uni00000056/uni0000005a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000017/uni00000017/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000015/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000017/uni0000000f/uni00000017
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000033/uni00000034/uni0000002a/uni0000001e/uni00000017/uni0000000d/uni00000001/uni00000025/uni0000004a/uni00000047/uni00000047/uni00000033/uni00000042/uni00000055/uni00000046/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000011/uni00000017/uni00000011/uni00000017/uni0000001a
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000043/uni00000056/uni0000005a
/uni00000034/uni00000026/uni0000002d/uni0000002d (b) TimeperiodRSI=6, DiRate=0.0006069.
Figure 13: RSI strategy without constraints on SH510300.
The detailed measures in Figure 13(a) are given as follows where again the \\\\IR"" is the
information ratio whose benchmark is set to be the SH510300 itself. Though SR=0.72 is
acceptable in some sense, the result in Figure 13(a) is still risky as it only nds the rst
drawdown, and totally follows the second drawdown (around 1300-th day). While the result
in Figure 13(b) is relatively poor in SR, it actually avoids the second drawdown. On the
25",2022-02-23T05:02:27Z,elori assic quantitative strategirate true true rate rate rate veryold th rdf rate true oght th rdf rate false result t strategy  time d di rate time d di rate  t  though  w  on
paper_qf_49.pdf,26,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
other hand, the BuyCNT for both of the above two results are low, which may lose some
trading signals.
Initial Price : 2.604
Final Price : 7. 6612689111278485
RR of whole period : 2. 942115557268759
RR/ year : 1. 1217049662146583
RR of year -1: 0. 9577572964669739
RR of year -2: 0. 8672124701729041
RR of year -3: 2. 384579870729455
RR of year -4: 1.0
RR of year -5: 1. 0939617083946982
RR of year -6: 0. 9492399565689466
RR of year -7: 1. 1102585961920999
RR of year -8: 1. 2471590909090908
RR of year -9: 1. 052435233160622
Total number of buy count : 3
MAX rate : 2. 384579870729455 MIN rate : 0. 8672124701729041
MDD: 0. 31291277613299934
SR: 0. 7181964777285531
IR: 0. 24693874680366934
4.3 Result with constraints
The result of the RSI Strategy with constraints on the SH510300 is shown in Figure 14.
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000015/uni00000017/uni00000019/uni00000012/uni00000011/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000019/uni00000019/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000016/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni0000001a/uni0000000f/uni00000015
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000033/uni00000034/uni0000002a/uni0000001e/uni00000013/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002e/uni00000022/uni0000001e/uni00000014/uni00000019/uni0000000d/uni00000001/uni00000025/uni0000004a/uni00000047/uni00000047/uni00000033/uni00000042/uni00000055/uni00000046/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000011/uni00000012/uni0000000d/uni00000001/uni00000034/uni0000002e/uni00000022/uni00000053/uni00000042/uni00000055/uni00000046/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000013
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000043/uni00000056/uni0000005a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000015/uni00000017/uni00000019/uni00000012/uni00000011/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000019/uni00000019/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000016/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni0000001a/uni0000000f/uni00000015
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000033/uni00000034/uni0000002a/uni0000001e/uni00000013/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002e/uni00000022/uni0000001e/uni00000014/uni00000019/uni0000000d/uni00000001/uni00000025/uni0000004a/uni00000047/uni00000047/uni00000033/uni00000042/uni00000055/uni00000046/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000011/uni00000012/uni0000000d/uni00000001/uni00000034/uni0000002e/uni00000022/uni00000053/uni00000042/uni00000055/uni00000046/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000013
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000043/uni00000056/uni0000005a
/uni00000034/uni00000026/uni0000002d/uni0000002d
(a) TimeperiodRSI=2, TimeperiodMA=38,
DiRate=0.0001, SMArate=0.02.
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000019/uni00000012/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000016/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000018/uni0000000f/uni00000018
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000033/uni00000034/uni0000002a/uni0000001e/uni00000013/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002e/uni00000022/uni0000001e/uni00000017/uni0000000d/uni00000001/uni00000025/uni0000004a/uni00000047/uni00000047/uni00000033/uni00000042/uni00000055/uni00000046/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000011/uni00000017/uni00000015/uni00000015/uni0000000d/uni00000001/uni00000034/uni0000002e/uni00000022/uni00000053/uni00000042/uni00000055/uni00000046/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000012/uni00000014/uni00000017/uni00000017
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000043/uni00000056/uni0000005a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000019/uni00000012/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000016/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000018/uni0000000f/uni00000018
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000033/uni00000034/uni0000002a/uni0000001e/uni00000013/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002e/uni00000022/uni0000001e/uni00000017/uni0000000d/uni00000001/uni00000025/uni0000004a/uni00000047/uni00000047/uni00000033/uni00000042/uni00000055/uni00000046/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000011/uni00000011/uni00000017/uni00000015/uni00000015/uni0000000d/uni00000001/uni00000034/uni0000002e/uni00000022/uni00000053/uni00000042/uni00000055/uni00000046/uni0000001e/uni00000011/uni0000000f/uni00000011/uni00000012/uni00000014/uni00000017/uni00000017
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000043/uni00000056/uni0000005a
/uni00000034/uni00000026/uni0000002d/uni0000002d(b) TimeperiodRSI=2, TimeperiodMA=6,
DiRate=0.000644, SMArate=0.0136.
Figure 14: RSI strategy with constraints on SH510300 where \\\\TimeperiodMA"" in the
title is the time frame for the SMA.
The detailed measures in Figure 14(a) are given as follows where again the \\\\IR"" is
the information ratio whose benchmark is set to be the SH510300 itself. Interestingly,
though the turnover is small (low \\\\BuyCNT""), every sell signal from the strategies in both
Figure 14(a) and 14(b) tells a lot about the \\\\big"" downtrend.
Initial Price : 2.604
Final Price : 9. 355288065508722
RR of whole period : 3. 5926605474303845
26",2022-02-23T05:02:27Z,julu buy initial price nal price total result t strategy  time d time d di rate rate time d time d di rate rate  time d t  interesti buy  initial price nal price
paper_qf_49.pdf,27,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
RR/ year : 1. 1458083483941202
RR of year -1: 1. 078502286731385
RR of year -2: 0. 9151307329618517
RR of year -3: 2. 3845798707294548
RR of year -4: 1.0
RR of year -5: 1. 0939617083946982
RR of year -6: 0. 9934511326283224
RR of year -7: 1. 1102585961920999
RR of year -8: 1. 2471590909090908
RR of year -9: 1. 052435233160622
Total number of buy count : 5
MAX rate : 2. 3845798707294548 MIN rate : 0. 9151307329618517
MDD: 0. 1744446357843981
SR: 0. 8804066887285559
IR: 0. 3434084355517314
4.4 More to Go
The relative momentum index (RMI) is a variation on the RSI. To determine up and down
days, the RSI uses the closing price compared to the previous closing price. While, the RMI
uses the closing price compared to the closing price Mdays ago (known as the \\\\look back
frame""). Apparently, an RMI with a time period of 1 is equal to the RSI. Likewise, the
RMI ranges from 0 to 100 and the RMI can also be interpreted as an overbought/oversold
indicator when the value is over 70/below 30 respectively. We can also look for divergence
with price. If the price is making new highs/lows, and the RMI is not, it indicates a reversal
signal. The RMI provides a more exible algorithm on the above strategy and we shall not
provide more tests for simplicity.
Given the time frame Nand look back frame M, the denition of the RMI is shown in
the following Algorithm 2.
Algorithm 2 Compute the i-th element of the RMI sequence
Require: For thei-th element of RMI sequence, \\\\upavg, dnavg"" are from last element.
1:Given the time period N, and look back frame M;
2:ifclose[i]>close[i",2022-02-23T05:02:27Z,elori assic quantitative strategitotal  go t to w days apparently likewise   t giveand algorithm algorithm ute require for given
paper_qf_49.pdf,28,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
5. Aroon Strategy
5.1 Aroon
The word \\\\Aroon"" is Sanskrit for \\\\dawn's early light"". The Aroon indicator attempts to
show when a new trend is dawning. Similar to the Keltner channel, the indicator consists of
two lines (Up and Down) that measure how long it has been since the highest high/lowest
low has occurred within an Nperiod frame.
The Aroon indicator was developed by Tushar S. Chande and rst described in the
September 1995 issue of Technical Analysis of Stocks & Commodities magazine (Chande,
1995). Given the time period N, the Aroon is dened as follows:
8
>>>>><
>>>>>:Aroon Up = 100N",2022-02-23T05:02:27Z,julu aro ostrategy aro ot aro osanskrit t aro osimar keltner up dowd t aro otu sha cha september technical analysis stocks cooditicha givearo oaro oup
paper_qf_49.pdf,29,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
The following \\\\algAroon"" code is written for Python 3.7. When \\\\aroonType=1"", the
algorithm computes the \\\\unconditioned Aroon strategy""; while \\\\aroonType=2"", the algo-
rithm calculates the \\\\conditioned Aroon strategy"".
def algAroon (data , position_buy , aroon_up , aroon_down , aroonType =1):
""""""
: param data : data dictionary contains 'close ' as data [' close ']
: param position_buy : in the buy position if True , and False otherwise
: param aroon_up : Aroon Up line
: param aroon_down : Aroon Down line
: param aroonType : 1 for RAW Aroon ; 2 for conditioned Aroon
: return :
""""""
for i in range (60 , len ( data ['close '])-1):
if aroonType ==1:
condition1 = True
condition2 = True
elif aroonType ==2:
condition1 = ( aroon_down [i]<45)
condition2 = ( aroon_up [i]<45)
# aroon_up cross above down - BUY
if ( aroon_up [i-1]< aroon_down [i-1]) \\\\
and ( aroon_up [i]> aroon_down [i]) \\\\
and condition1 \\\\
and (not position_buy ):
# TODO : BUY
position_buy = True
# aroon_down cross above up - SELL
elif ( aroon_up [i-1]> aroon_down [i-1]) \\\\
and ( aroon_up [i]< aroon_down [i]) \\\\
and condition2
and position_buy :
# TODO : SELL
position_buy = False
5.2 Results without and with condition
The Aroon strategy does not give many promising results and the result of the Aroon
strategy on the SH510300 is shown in Figure 15.
29",2022-02-23T05:02:27Z,elori assic quantitative strategit aro opythow aro o aro oaro o true false aro oup aro odow aro oaro o true true  true false results t aro oaro o
paper_qf_49.pdf,30,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000017/uni00000012/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000015/uni00000019/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000016/uni0000000f/uni0000001a
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000001e/uni00000013/uni00000016
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000017/uni00000012/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000015/uni00000019/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000016/uni0000000f/uni0000001a
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000001e/uni00000013/uni00000016
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d
(a) Timeperiod=25, unconditioned Aroon.
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni0000000f/uni00000011/uni00000013/uni0000000f/uni00000016/uni00000014/uni0000000f/uni00000011/uni00000014/uni0000000f/uni00000016/uni00000015/uni0000000f/uni00000011/uni00000015/uni0000000f/uni00000016/uni00000016/uni0000000f/uni00000011/uni00000016/uni0000000f/uni00000016/uni00000017/uni0000000f/uni00000011/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000017/uni00000014/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000016/uni00000011/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000016/uni0000000f/uni00000012
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000001e/uni00000017
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni0000000f/uni00000011/uni00000013/uni0000000f/uni00000016/uni00000014/uni0000000f/uni00000011/uni00000014/uni0000000f/uni00000016/uni00000015/uni0000000f/uni00000011/uni00000015/uni0000000f/uni00000016/uni00000016/uni0000000f/uni00000011/uni00000016/uni0000000f/uni00000016/uni00000017/uni0000000f/uni00000011/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000017/uni00000014/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000016/uni00000011/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000016/uni0000000f/uni00000012
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000001e/uni00000017
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d (b) Timeperiod=6, conditioned Aroon.
Figure 15: Aroon overbought and oversold strategy on SH510300 without and with con-
straints.
The detailed measures in Figure 15(a) are given as follows where again the \\\\IR"" is the
information ratio whose benchmark is set to be the SH510300 itself.
Initial Price : 2.604
Final Price : 5. 887990726795886
RR of whole period : 2. 261133151611323
RR/ year : 1. 090712566635823
RR of year -1: 0. 9556453450783325
RR of year -2: 0. 9082324091273398
RR of year -3: 1. 9710427015497842
RR of year -4: 1. 0297552354919766
RR of year -5: 1. 1289329700532444
RR of year -6: 0. 9714393615034962
RR of year -7: 1. 0325934356179702
RR of year -8: 1. 101722320850014
RR of year -9: 1. 0265693427413187
Total number of buy count : 48
MAX rate : 1. 9710427015497842 MIN rate : 0. 9082324091273398
MDD: 0. 3068100965375994
SR: 0. 614348285853909
IR: 0. 03005677887500972
6. Bollinger Bands Strategy
6.1 Bollinger Bands
Bollinger bands consist of three lines. The middle band is an SMA (generally 20 periods)
of the typical price (TP).9The upper and lower bands are devtimes standard deviations
(generallydev=2) above and below the middle band. The bands widen and narrow when
the volatility of the price becomes higher or lower, respectively.
Bollinger bands do not, in themselves, generate buy or sell signals; they are an indicator
of overbought or oversold conditions. When the price is near the upper or lower band
9. Again, one can also explore other moving average methods in this sense.
30",2022-02-23T05:02:27Z,julu time d aro otime d aro o aro ot  initial price nal price total bollier bands strategy bollier bands bollier t t t bollier wag articial intellence
paper_qf_49.pdf,31,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
it indicates that a reversal may be imminent. The middle band becomes a support or
resistance level. The upper and lower bands can also be interpreted as price targets. When
the price bounces o of the lower band and crosses the middle band, then the upper band
becomes the price target.
Bollinger bands were developed and copyrighted by John Bollinger, a famous technical
trader (Bollinger, 1992, 2002). See also the empirical study on the Bollinger band in various
regions (Leung and Chong, 2003). The formula to compute the band is given by:
8
>>>>><
>>>>>:TP =high+low+close
3;
Middle Band = SMA(TP, N);
Upper Band = Middle + dev(TP;N);
Lower Band = Middle ",2022-02-23T05:02:27Z,elori assic quantitative strategit t wbollier  bollier bollier  bollier lu ho t middle band up band middle lor band middle
paper_qf_49.pdf,32,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000018/uni00000012/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000012/uni00000018/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000017/uni0000000f/uni00000016
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000028/uni00000042/uni00000051/uni0000001e/uni00000016/uni0000000d/uni00000001/uni00000045/uni00000046/uni00000057/uni0000001e/uni00000012/uni0000000f/uni00000012/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000015
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000018/uni00000012/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000012/uni00000018/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000017/uni0000000f/uni00000016
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000028/uni00000042/uni00000051/uni0000001e/uni00000016/uni0000000d/uni00000001/uni00000045/uni00000046/uni00000057/uni0000001e/uni00000012/uni0000000f/uni00000012/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000015
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d
(a) Timeperiod=5, dev=1.1.
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000016/uni0000001a/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000013/uni00000011/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000016/uni0000000f/uni00000015
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000028/uni00000042/uni00000051/uni0000001e/uni00000013/uni0000000d/uni00000001/uni00000045/uni00000046/uni00000057/uni0000001e/uni00000014/uni0000000f/uni00000011/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000014
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000016/uni0000001a/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000013/uni00000011/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000016/uni0000000f/uni00000015
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000028/uni00000042/uni00000051/uni0000001e/uni00000013/uni0000000d/uni00000001/uni00000045/uni00000046/uni00000057/uni0000001e/uni00000014/uni0000000f/uni00000011/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000014
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d (b) Timeperiod=2, dev=3.0.
Figure 16: Bollinger band strategy on SH510300 with non-adaptive MA.
The detailed measures in Figure 16(a) are given as follows where again the \\\\IR"" is the
information ratio whose benchmark is set to be the SH510300 itself.
Initial Price : 2.604
Final Price : 6. 451714254225132
RR of whole period : 2. 477616841100281
RR/ year : 1. 101377934198916
RR of year -1: 0. 9167068001328132
RR of year -2: 0. 7972897922117375
RR of year -3: 1. 6750900346654767
RR of year -4: 1. 2276852370378257
RR of year -5: 1. 1238603905387452
RR of year -6: 1. 0586927715683763
RR of year -7: 0. 9832706903117197
RR of year -8: 1. 1940637458782737
RR of year -9: 1. 1859827767807352
Total number of buy count : 17
MAX rate : 1. 6750900346654767 MIN rate : 0. 7972897922117375
MDD: 0. 2999781605155163
SR: 0. 7058010527061538
IR: 0. 07404386090219181
6.3 Results with AMA
The result of the Bollinger bands strategy with AMA on the SH510300 is shown in Figure 17.
32",2022-02-23T05:02:27Z,julu time d time d  bollier t  initial price nal price total results t bollier 
paper_qf_49.pdf,33,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000018/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000016/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000018/uni0000000f/uni00000019
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000013/uni00000015/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000019/uni0000000d/uni00000001/uni00000022/uni00000045/uni00000042/uni00000038/uni0000004a/uni0000004f/uni0000001e/uni00000012/uni00000019/uni0000000d/uni00000001/uni00000045/uni00000046/uni00000057/uni0000001e/uni00000013/uni0000000f/uni00000017/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000012
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000018/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000016/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000018/uni0000000f/uni00000019
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000013/uni00000015/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000019/uni0000000d/uni00000001/uni00000022/uni00000045/uni00000042/uni00000038/uni0000004a/uni0000004f/uni0000001e/uni00000012/uni00000019/uni0000000d/uni00000001/uni00000045/uni00000046/uni00000057/uni0000001e/uni00000013/uni0000000f/uni00000017/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000012
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d
(a) TimeperiodLong=24, TimeperiodShort=8,
AdaWin=18, dev=2.6, Matype=1.
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000017/uni00000018/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000015/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000018/uni0000000f/uni00000017
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000013/uni00000013/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000019/uni0000000d/uni00000001/uni00000022/uni00000045/uni00000042/uni00000038/uni0000004a/uni0000004f/uni0000001e/uni00000012/uni00000017/uni0000000d/uni00000001/uni00000045/uni00000046/uni00000057/uni0000001e/uni00000013/uni0000000f/uni00000017/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000012
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000017/uni00000018/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000015/uni0000000d/uni00000001/uni00000027/uni0000004a/uni0000004f/uni00000042/uni0000004d/uni00000031/uni00000053/uni0000004a/uni00000044/uni00000046/uni0000001e/uni00000018/uni0000000f/uni00000017
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000013/uni00000013/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000019/uni0000000d/uni00000001/uni00000022/uni00000045/uni00000042/uni00000038/uni0000004a/uni0000004f/uni0000001e/uni00000012/uni00000017/uni0000000d/uni00000001/uni00000045/uni00000046/uni00000057/uni0000001e/uni00000013/uni0000000f/uni00000017/uni0000000d/uni00000001/uni0000002e/uni00000042/uni00000055/uni0000005a/uni00000051/uni00000046/uni0000001e/uni00000012
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000036/uni00000051/uni00000051/uni00000046/uni00000053
/uni0000002d/uni00000050/uni00000058/uni00000046/uni00000053
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d(b) TimeperiodLong=22, TimeperiodShort=8,
AdaWin=16, dev=2.6, Matype=1.
Figure 17: Bollinger band strategy on SH510300 with AMA. \\\\Matype=1"" means the
AMA is based on EMA; while, \\\\Matype=2"" means the AMA is based on SMA (here, we
only observe good results when Matype=1); see the \\\\adaptiveMovAvg"" code on p. 12.
The detailed measures in Figure 17(a) are given as follows where again the \\\\IR"" is the
information ratio whose benchmark is set to be the SH510300 itself.
Initial Price : 2.604
Final Price : 7. 817242080105559
RR of whole period : 3. 0020130875981406
RR/ year : 1. 1241135785454597
RR of year -1: 0. 957757296466974
RR of year -2: 0. 968041542757221
RR of year -3: 2. 349213742419991
RR of year -4: 0. 9352175867671978
RR of year -5: 1. 0816596789654878
RR of year -6: 0. 8618010877268545
RR of year -7: 1. 1102585961920999
RR of year -8: 1. 2471590909090908
RR of year -9: 1. 1091469535133192
Total number of buy count : 5
MAX rate : 2. 349213742419991 MIN rate : 0. 8618010877268545
MDD: 0. 3526473187294988
SR: 0. 7003306913660954
IR: 0. 3098106077858669
Dierent from the results of the Two-Average strategy or the Keltner strategy, we nd
the AMA's for the EMA (Matype=1 in the titles of Figure 17) work better than that for
the SMA (Matype=2). However, the AMA from SMA seems promising since it outputs
more acceptable results. This is again partly because the AMA from SMA has not been
extensively used in quantitative strategies till now.
33",2022-02-23T05:02:27Z,elori assic quantitative strategitime d lo time d short ada wima  time d lo time d short ada wima   bollier ma  ma  ma  ov g t  initial price nal price total di two ge keltner ma   ma  
paper_qf_49.pdf,34,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
7. MACD Strategy
7.1 Moving Average Convergence Divergence (MACD)
The moving average convergence divergence (MACD) is the dierence between two EMAs
(one with a short time period, and one with a long time period). The signal line is simply
an SMA (or EMA) of the MACD. The MACD was developed by Gerald Appel (Appel and
Dobson, 2007; Appel and Appel, 2008) and the formula is given by:
8
><
>:MACD = EMA(TimeperiodShort) ",2022-02-23T05:02:27Z,julu strategy movi ge convergence divergence t as t t gld appeal appeal doesappeal appeal time d short
paper_qf_49.pdf,35,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000015/uni00000017/uni00000019/uni00000012/uni00000011/uni00000012/uni00000013/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000012/uni00000016/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000015/uni00000011/uni00000013/uni0000000d/uni00000001/uni0000002e/uni00000050/uni0000004f/uni00000046/uni0000005a/uni0000001e/uni00000012/uni00000013/uni0000000f/uni00000015
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000013/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000015/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni0000004a/uni00000048/uni0000004f/uni00000042/uni0000004d/uni0000001e/uni00000013
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000015/uni00000017/uni00000019/uni00000012/uni00000011/uni00000012/uni00000013/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000012/uni0000000f/uni00000012/uni00000016/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000015/uni00000011/uni00000013/uni0000000d/uni00000001/uni0000002e/uni00000050/uni0000004f/uni00000046/uni0000005a/uni0000001e/uni00000012/uni00000013/uni0000000f/uni00000015
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000013/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000015/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni0000004a/uni00000048/uni0000004f/uni00000042/uni0000004d/uni0000001e/uni00000013
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d
(a) TimeperiodLong=4, TimeperiodShort=2,
TimeperiodSignal=18.
/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000017/uni00000019/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000013/uni00000019/uni00000019/uni0000000d/uni00000001/uni0000002e/uni00000050/uni0000004f/uni00000046/uni0000005a/uni0000001e/uni00000017/uni0000000f/uni00000014
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000015/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000012/uni00000011/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni0000004a/uni00000048/uni0000004f/uni00000042/uni0000004d/uni0000001e/uni00000013
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d/uni00000011 /uni00000016/uni00000011/uni00000011 /uni00000012/uni00000011/uni00000011/uni00000011 /uni00000012/uni00000016/uni00000011/uni00000011 /uni00000013/uni00000011/uni00000011/uni00000011/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000034/uni00000029/uni00000016/uni00000012/uni00000011/uni00000014/uni00000011/uni00000011/uni0000000d/uni00000001/uni00000034/uni00000033/uni0000001e/uni00000011/uni0000000f/uni00000017/uni00000019/uni0000000d/uni00000001/uni00000023/uni00000056/uni0000005a/uni00000024/uni0000002f/uni00000035/uni0000001e/uni00000013/uni00000019/uni00000019/uni0000000d/uni00000001/uni0000002e/uni00000050/uni0000004f/uni00000046/uni0000005a/uni0000001e/uni00000017/uni0000000f/uni00000014
/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni00000049/uni00000050/uni00000053/uni00000055/uni0000001e/uni00000015/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni0000002d/uni00000050/uni0000004f/uni00000048/uni0000001e/uni00000012/uni00000011/uni0000000d/uni00000001/uni00000035/uni0000004a/uni0000004e/uni00000046/uni00000051/uni00000046/uni00000053/uni0000004a/uni00000050/uni00000045/uni00000034/uni0000004a/uni00000048/uni0000004f/uni00000042/uni0000004d/uni0000001e/uni00000013
/uni00000032/uni00000056/uni00000042/uni0000004f/uni00000055/uni00000001/uni00000034/uni00000055/uni00000053/uni00000042/uni00000055/uni00000046/uni00000048/uni0000005a
/uni00000024/uni0000004d/uni00000050/uni00000054/uni00000046
/uni00000023/uni00000036/uni0000003a
/uni00000034/uni00000026/uni0000002d/uni0000002d(b) TimeperiodLong=10, TimeperiodShort=4,
TimeperiodSignal=18.
Figure 19: MACD strategy on SH510300.
The detailed measures in Figure 19(a) are given as follows where again the \\\\IR"" is the
information ratio whose benchmark is set to be the SH510300 itself. The result shows the
MACD work well with a small MDD and an acceptable SR or IR. However, the BuyCNT
is large, approximately 4 times per month, which says the turnover is large. It may cause
a large fee payment.
Initial Price : 2.604
Final Price : 12. 371646201432752
RR of whole period : 4. 751016206387385
RR/ year : 1. 1804000407529112
RR of year -1: 1. 0237116887501392
RR of year -2: 1. 0612853834574043
RR of year -3: 1. 4779805119148437
RR of year -4: 1. 2079752530396133
RR of year -5: 1. 1402069617344712
RR of year -6: 1. 0224306409466652
RR of year -7: 1. 1683210530919812
RR of year -8: 1. 4766918267971532
RR of year -9: 1. 230244980326647
Total number of buy count : 402
MAX rate : 1. 4779805119148437 MIN rate : 1. 0224306409466652
MDD: 0. 18557388402199687
SR: 1. 1494491803824245
IR: 0. 46413451717265125
8. Machine Learning Strategy
One thing to notice it that, although the Aroon strategy alone is not working well, machine
learning techniques such as uniform/linear blending can be employed together with other
strategies to develop new algorithms (Sill et al., 2009; Lu, 2017).
Machine learning techniques for time series problems are explored in the eld of airplane
ticker prediction (Lu, 2017). We here only briey discuss potential methods, though we do
not observe any promising results from this simple idea. However, an extensive test is not
35",2022-02-23T05:02:27Z,elori assic quantitative strategitime d lo time d short time d snal time d lo time d short time d snal  t  t buy it initial price nal price total machine learni strategy one aro osl lu machine lu  ver
paper_qf_49.pdf,36,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
applied, and readers are recommended to explore methods in (Lu, 2017), e.g., for both
regression or classication ideas, for outlier removal. From the above strategies, several
features can be extracted, e.g., the SMA, EMA, AMA, RSI, Aroon of each day, and even
the volume or liquidity (total volumes in a specic time frame) on that day.10These features
can be treated as inputs of the machine learning \\\\blackbox"". For the output, considering
the classication problem, it is reasonable to set the output of that day as 1 (i.e., a buy
signal) if the price can achieve a rate of return of 1% (just an example here) in one of the
next 5 trading days (again, 5 is just an example); and set the output to be 0 if otherwise
(i.e., a sell signal). Then dierent from the above strategies, the training set can be selected
as the rst 10 years for the S&P500 data; and the validation set can be chosen as the last
year. In practice, other features or indicators are required to obtain a promising result and
we shall not give further details.
10. More features are discussed in (Kakushadze and Tulchinsky, 2016; Kakushadze, 2016; Tulchinsky, 2019;
Kakushadze et al., 2018) and references therein.
36",2022-02-23T05:02:27Z,julu lu from aro otse for ti u sha tul chi u sha tul chi u sha
paper_qf_49.pdf,37,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Exploring Classic Quantitative Strategies
References
Gerald Appel and Marvin Appel. A quick tutorial in macd: Basic concepts. Technical
report, Working Paper, 2008. 34
Gerald Appel and Edward Dobson. Understanding MACD . Traders Press, 2007. 34
Daniel Alexandre Bloch. A practical guide to quantitative portfolio trading. Available at
SSRN 2543802 , 2014. 3
John Bollinger. Using bollinger bands. Stocks & Commodities , 10(2):47{51, 1992. 31
John Bollinger. Bollinger on Bollinger bands . McGraw-Hill New York, 2002. 31
Charles Bram Cadsby et al. Performance hypothesis testing with the sharpe and treynor
measures: a comment. Journal of Finance , 41(5):1175{1176, 1986. 4
Tushar S Chande. A time price oscillator. Technical Analysis of Stocks & Commodities , 13
(9):369{374, 1995. 28
FMLabs. Fm labs online: https://www.fmlabs.com/. 2020. 7
Joseph E Granville. Granville's New Key to Stock Market Prots . Pickle Partners Publish-
ing, 2018. 8
Investopedia. Investopedia online: https://www.investopedia.com/. 2022. 2
J Dave Jobson and Bob M Korkie. Performance hypothesis testing with the sharpe and
treynor measures. Journal of Finance , pages 889{908, 1981. 4
Zura Kakushadze. 101 formulaic alphas. Wilmott , 2016(84):72{81, 2016. 36
Zura Kakushadze and Igor Tulchinsky. Performance v. turnover: A story by 4,000 alphas.
The Journal of Investment Strategies , 5(2):75{89, 2016. 36
Zura Kakushadze, Juan Andr es Serur, et al. 151 Trading Strategies . Springer, 2018. 36
Perry J Kaufman. Smarter trading, 1995. 9
Perry J Kaufman. Trading Systems and Methods,+ Website , volume 591. John Wiley &
Sons, 2013. 9
John L Kelly Jr. A new interpretation of information rate. In The Kelly capital growth
investment criterion: theory and practice , pages 25{34. World Scientic, 2011. 7
Chester W Keltner. How to make money in commodities . Keltner Statistical Service, 1960.
20
Joseph Man-Joe Leung and Terence Tai-Leung Chong. An empirical comparison of moving
average envelopes and bollinger bands. Applied Economics Letters , 10(6):339{341, 2003.
31
37",2022-02-23T05:02:27Z,elori assic quantitative strategireferencgld appeal marviappeal basic technical worki pa gld appeal edward doesunrstandi trars  daniel alexanr block  articial intellence   bollier usi stocks cooditi bollier bollier bollier mc raw hl new york charl caps by formance journal nance tu sha cha technical analysis stocks cooditilabs fm joseph granvle granvle new key stock market pro pickle partners pubh iveto pedia iveto pedia jobs obob  rki formance journal nance  u sha pot  u sha or tul chi formance t journal investment strategi u sha juaand se ru tradi strategi ry kaufmasmarter ry kaufmatradi tems methods bsite  rey sons  kelly jr it kelly world cie nti cster keltner how keltner statistical service joseph majoe lu fence articial intellence lu ho aapplied economics ters
paper_qf_49.pdf,38,Exploring Classic Quantitative Strategies,"  The goal of this paper is to debunk and dispel the magic behind the black-box
quantitative strategies. It aims to build a solid foundation on how and why the
techniques work. This manuscript crystallizes this knowledge by deriving from
simple intuitions, the mathematics behind the strategies. This tutorial doesn't
shy away from addressing both the formal and informal aspects of quantitative
strategies. By doing so, it hopes to provide readers with a deeper
understanding of these techniques as well as the when, the how and the why of
applying these techniques. The strategies are presented in terms of both
S\\\\&P500 and SH510300 data sets. However, the results from the tests are just
examples of how the methods work; no claim is made on the suggestion of real
market positions.
","Jun Lu
Jun Lu. Machine learning modeling for time series problem: Predicting ight ticket prices.
arXiv preprint arXiv:1705.07205 , 2017. 35, 36
Jun Lu. Numerical matrix decomposition and its modern applications: A rigorous rst
course. arXiv preprint arXiv:2107.02579 , 2021a. 15
Jun Lu. A rigorous introduction for linear models. arXiv preprint arXiv:2105.04240 , 2021b.
14, 15
Jun Lu. Matrix decomposition and applications. arXiv preprint arXiv:2201.00145 , 2022.
15
Robert P Schumaker and Hsinchun Chen. A quantitative stock prediction system based on
nancial news. Information Processing & Management , 45(5):571{583, 2009. 16
William F Sharpe. Mutual fund performance. The Journal of business , 39(1):119{138, 1966.
4
William F Sharpe. The sharpe ratio. Journal of portfolio management , 21(1):49{58, 1994.
5
Joseph Sill, G abor Tak acs, Lester Mackey, and David Lin. Feature-weighted linear stacking.
arXiv preprint arXiv:0911.0460 , 2009. 35
Igor Tulchinsky. Finding Alphas: A quantitative approach to building trading strategies .
John Wiley & Sons, 2019. 36
J Welles Wilder. New concepts in technical trading systems . Trend Research, 1978. 20, 24
38",2022-02-23T05:02:27Z,julu julu machine predii   julu numerical   julu   julu matrix   robert smaker hsinchu cinformatprocessi management wliam share mutual t journal wliam share t journal joseph sl tak ter mac key did lifeature   or tul chi ndi alpha  rey sons lls wir new trend research
paper_qf_50.pdf,1,"Probability distribution of returns in the Heston model with stochastic
  volatility","  We study the Heston model, where the stock price dynamics is governed by a
geometrical (multiplicative) Brownian motion with stochastic variance. We solve
the corresponding Fokker-Planck equation exactly and, after integrating out the
variance, find an analytic formula for the time-dependent probability
distribution of stock price changes (returns). The formula is in excellent
agreement with the Dow-Jones index for the time lags from 1 to 250 trading
days. For large returns, the distribution is exponential in log-returns with a
time-dependent exponent, whereas for small returns it is Gaussian. For time
lags longer than the relaxation time of variance, the probability distribution
can be expressed in a scaling form using a Bessel function. The Dow-Jones data
for 1982-2001 follow the scaling function for seven orders of magnitude.
","arXiv:cond-mat/0203046v3  [cond-mat.stat-mech]  5 Nov 2002Probability distribution of returns in the Heston model wit h stochastic volatility
Adrian A. Dr˘ agulescu∗and Victor M. Yakovenko†
Department of Physics, University of Maryland, College Par k, MD 20742-4111, USA
(Dated: cond-mat/0203046 , v.1: 3 March 2002, v.2: 21 October 2002, v.3: 5 November 2002 )
We study the Heston model, where the stock price dynamics is g overned by a geometrical (mul-
tiplicative) Brownian motion with stochastic variance. We solve the corresponding Fokker-Planck
equation exactly and, after integrating out the variance, ﬁ nd an analytic formula for the time-
dependent probability distribution of stock price changes (returns). The formula is in excellent
agreement with the Dow-Jones index for the time lags from 1 to 250 trading days. For large returns,
the distribution is exponential in log-returns with a time- dependent exponent, whereas for small
returns it is Gaussian. For time lags longer than the relaxat ion time of variance, the probability
distribution can be expressed in a scaling form using a Besse l function. The Dow-Jones data for
1982–2001 follow the scaling function for seven orders of ma gnitude.
I. INTRODUCTION
Stochastic dynamics of stock prices is commonly de-
scribed by a geometric (multiplicative) Brownian motion,
which gives a log-normal probability distribution func-
tion (PDF) for stock price changes (returns) [1]. How-
ever, numerous observations show that the tails of the
PDF decay slower than the log-normal distribution pre-
dicts (the so-called “fat-tails” eﬀect) [2, 3, 4]. Particu-
larly, much attention was devoted to the power-law tails
[5, 6]. The geometric Brownian motion model has two
parameters: the drift µ, which characterizes the aver-
age growth rate, and the volatility σ, which characterizes
the noisiness of the process. There is empirical evidence
and a set of stylized facts indicating that volatility, in-
stead of being a constant parameter, is driven by a mean-
reverting stochastic process [7, 8]. Various mathematical
models with stochastic volatility have been discussed in
literature [9, 10, 11, 12, 13, 14, 15].
In this paper, we study a particular stochastic volatil-
ity model, called the Heston model [11], where the square
of the stock-price volatility, called the variance v, follows
a random process known in ﬁnancial literature as the
Cox-Ingersoll-Ross process and in mathematical statis-
tics as the Feller process [8, 16]. Using the Fourier and
Laplace transforms [14, 16], we solve the Fokker-Planck
equation for this model exactly and ﬁnd the joint PDF
of returns and variance as a function of time, conditional
on the initial value of variance. While returns are read-
ily known from a ﬁnancial time-series data, variance is
not given directly, so it acts as a hidden stochastic vari-
able. Thus, we integrate the joint PDF over variance and
obtain the marginal probability distribution function of
returns unconditional on variance. The latter PDF can
be directly compared with ﬁnancial data. We ﬁnd an
excellent agreement between our results and the Dow-
∗Now at the Constellation Energy Group, Baltimore; Electron ic
address: adrian.dragulescu@constellation.com
†URL:http://www2.physics.umd.edu/~yakovenk ; Electronic ad-
dress: yakovenk@physics.umd.eduJones data for the 20-years period of 1982–2001. Us-
ing only four ﬁtting parameters, our equations very well
reproduce the PDF of returns for time lags between 1
and 250 trading days. In contrast, in ARCH, GARCH,
EGARCH, TARCH, and similar models, the number of
ﬁtting parameters can easily go to a few tens [17].
Our result for the PDF of returns has the form of a
one-dimensional Fourier integral, which is easily calcu-
lated numerically or, in certain asymptotical limits, an-
alytically. For large returns, we ﬁnd that the PDF is
exponential in log-returns, which implies a power-law dis-
tribution for returns, and we calculate the time depen-
dence of the corresponding exponents. In the limit of
long times, the PDF exhibits scaling, i.e. it becomes a
function of a single combination of return and time, with
the scaling function expressed in terms of a Bessel func-
tion. The Dow-Jones data follow the predicted scaling
function for seven orders of magnitude.
The original paper [11] solved the problem of option
pricing for the Heston model. Numerous subsequent
studies [13, 14, 15, 18] compared option pricing derived
from this model and its extensions with the empirical
data on option pricing. They found that the Heston
model describes the empirical option prices much better
than the Black-Scholes theory, and modiﬁcations of the
Heston model, such as adding discontinuous jumps, fur-
ther improve the agreement. However, these papers did
not address the fundamental question whether the stock
market actually follows the Heston stochastic process or
not. Obviously, if the answer is negative, then using the
Heston model for option pricing would not make much
sense. The stock-market time series was studied in Ref.
[15] jointly with option prices, but the focus was just on
extracting the eﬀective parameters of the Heston model.
In contrast, we present a comprehensive comparison of
the stock market returns distribution with the predic-
tions of the Heston model. Using a single set of four
parameters, we ﬁt the whole family of PDF curves for
a wide variety of time lags. In order to keep the model
as simple as possible with the minimal number of ﬁtting
parameters, we use the original Heston model and do not
include later modiﬁcations proposed in literature, such as
jumps, multiple relaxation time, etc. [13, 14, 15]. Inter-",2002-03-03T23:59:06Z, nov probabity stoadriadr vior v e partment psics  maryland college par dated marober november  stobrownish  former  t dow jonfor for jesse t dow jonstochastic brownish how part icu t brownish tre varus istocox inoll ross seller usi courier place former  w  t  dow  constellatenergy group balti eleroeleronic jonus iour courier for ivessel t dow jont stonumerous ty sto schools stostoobvusly stot ref stoistousi istointer
paper_qf_50.pdf,2,"Probability distribution of returns in the Heston model with stochastic
  volatility","  We study the Heston model, where the stock price dynamics is governed by a
geometrical (multiplicative) Brownian motion with stochastic variance. We solve
the corresponding Fokker-Planck equation exactly and, after integrating out the
variance, find an analytic formula for the time-dependent probability
distribution of stock price changes (returns). The formula is in excellent
agreement with the Dow-Jones index for the time lags from 1 to 250 trading
days. For large returns, the distribution is exponential in log-returns with a
time-dependent exponent, whereas for small returns it is Gaussian. For time
lags longer than the relaxation time of variance, the probability distribution
can be expressed in a scaling form using a Bessel function. The Dow-Jones data
for 1982-2001 follow the scaling function for seven orders of magnitude.
","2
estingly, the parameters of the model that we ﬁnd from
our ﬁts of the stock market data are of the same order of
magnitude as the parameters extracted from the ﬁts of
option prices in Refs. [13, 14, 15].
II. THE MODEL
We consider a stock, whose price St, as a function of
timet, obeys the stochastic diﬀerential equation of a geo-
metric (multiplicative) Brownian motion in the Itˆ o form
[1, 19]:
dSt=µStdt+σtStdW(1)
t. (1)
Here the subscript tindicates time dependence, µis the
drift parameter, W(1)
tis a standard random Wiener pro-
cess1, and σtis the time-dependent volatility.
Since any solution of (1) depends only on σ2
t, it is con-
venient to introduce the new variable vt=σ2
t, which is
called the variance. We assume that vtobeys the follow-
ing mean-reverting stochastic diﬀerential equation:
dvt=−γ(vt−θ)dt+κ√vtdW(2)
t. (2)
Hereθis the long-time mean of v,γis the rate of relax-
ation to this mean, W(2)
tis a standard Wiener process,
andκis a parameter that we call the variance noise. Eq.
(2) is known in ﬁnancial literature as the Cox-Ingersoll-
Ross (CIR) process and in mathematical statistics as the
Feller process [8, 16]. Alternative equations for vt, with
the last term in (2) replaced by κ dW(2)
torκvtdW(2)
t,
have been also discussed in literature [9]. However, in
our paper, we study only the case given by Eq. (2).
We take the Wiener process appearing in (2) to be
correlated with the Wiener process in (1):
dW(2)
t=ρ dW(1)
t+/radicalbig
1−ρ2dZt, (3)
where Ztis a Wiener process independent of W(1)
t, and
ρ∈[−1,1] is the correlation coeﬃcient. A negative cor-
relation ( ρ <0) between W(1)
tandW(2)
tis known as the
leverage eﬀect [8, p. 41].
It is convenient to change the variable in (1) from price
Stto log-return rt= ln(St/S0). Using Itˆ o’s formula [19],
we obtain the equation satisﬁed by rt:
drt=/parenleftBig
µ−vt
2/parenrightBig
dt+√vtdW(1)
t. (4)
The parameter µcan be eliminated from (4) by changing
the variable to xt=rt−µt, which measures log-returns
relative to the growth rate µ:
dxt=−vt
2dt+√vtdW(1)
t. (5)
1The inﬁnitesimal increments of the Wiener process dWtare
normally-distributed (Gaussian) random variables with ze ro
mean and the variance equal to dt.0 1 2 3 4 500.10.20.30.40.50.60.70.8
v/θStationary distribution Π*(v/θ)
0 1 2 300.20.40.60.81
σ/θ1/2Π*(σ)(σ/θ1/2)
FIG. 1: The stationary probability distribution Π ∗(v) of vari-
ancev, given by Eq. (9) and shown for α= 1.3 from Table I.
The vertical line indicates the average value of v. Inset: The
corresponding stationary probability distribution Π(σ)
∗(v) of
volatility σgiven by Eq. (10).
Where it does not cause confusion with rt, we use the
term “log-return” also for the variable xt.
Equations (5) and (2) deﬁne a two-dimensional
stochastic process for the variables xtandvt[11, 14].
This process is characterized by the transition probabil-
ityPt(x, v|vi) to have log-return xand variance vat
timetgiven the initial log-return x= 0 and variance vi
att= 0. Time evolution of Pt(x, v|vi) is governed by the
Fokker-Planck (or forward Kolmogorov) equation [19]
∂
∂tP=γ∂
∂v[(v−θ)P] +1
2∂
∂x(vP) (6)
+ρκ∂2
∂x∂v(vP) +1
2∂2
∂x2(vP) +κ2
2∂2
∂v2(vP).
The initial condition for (6) is a product of two delta
functions
Pt=0(x, v|vi) =δ(x)δ(v−vi). (7)
The probability distribution of the variance itself,
Πt(v) =/integraltext
dxP t(x, v), satisﬁes the equation
∂
∂tΠt(v) =∂
∂v[γ(v−θ)Πt(v)] +κ2
2∂2
∂2v[vΠt(v)],(8)
which is obtained from (6) by integration over x. Feller
[16] has shown that this equation is well-deﬁned on the
interval v∈[0,+∞) as long as θ >0. Eq. (8) has the
stationary solution
Π∗(v) =αα
Γ(α)vα−1
θαe−αv/θ, α =2γθ
κ2,(9)
which is the Gamma distribution. The parameter αin
(9) is the ratio of the average variance θto the character-
istic ﬂuctuation of variance κ2/2γduring the relaxation",2002-03-03T23:59:06Z,refs  st brownish it st std std re winner since  re winner eq cox inoll ross seller alternative eq  winner winner tis winner it st to st usi it b b t t winner  are statnary t eq table t iset t eq wre equatns  pt time pt former  lmogorov t pt t seller eq gaa t
paper_qf_50.pdf,3,"Probability distribution of returns in the Heston model with stochastic
  volatility","  We study the Heston model, where the stock price dynamics is governed by a
geometrical (multiplicative) Brownian motion with stochastic variance. We solve
the corresponding Fokker-Planck equation exactly and, after integrating out the
variance, find an analytic formula for the time-dependent probability
distribution of stock price changes (returns). The formula is in excellent
agreement with the Dow-Jones index for the time lags from 1 to 250 trading
days. For large returns, the distribution is exponential in log-returns with a
time-dependent exponent, whereas for small returns it is Gaussian. For time
lags longer than the relaxation time of variance, the probability distribution
can be expressed in a scaling form using a Bessel function. The Dow-Jones data
for 1982-2001 follow the scaling function for seven orders of magnitude.
","3
time 1 /γ. When α→ ∞, Π∗(v)→δ(v−θ). The corre-
sponding stationary PDF of volatility σis
Π(σ)
∗(σ) =2αα
Γ(α)σ2α−1
θαe−ασ2/θ. (10)
Functions (9) and (10) are integrable as long as α >0.
The distributions Π ∗(v) and Π(σ)
∗(σ) are shown in Fig. 1
for the value α= 1.3 deduced from the ﬁt of the Dow-
Jones time series and given in Table I in Sec. VIII.
III. SOLUTION OF THE FOKKER-PLANCK
EQUATION
Since xappears in (6) only in the derivative operator
∂/∂x, it is convenient to take the Fourier transform
Pt(x, v|vi) =/integraldisplay+∞
−∞dpx
2πeipxxPt,px(v|vi). (11)
Inserting (11) into (6), we ﬁnd
∂
∂tP=γ∂
∂v/bracketleftbig
(v−θ)P/bracketrightbig
(12)
−/bracketleftbiggp2
x−ipx
2v−iρκp x∂
∂vv−κ2
2∂2
∂v2v/bracketrightbigg
P.
Eq. (12) is simpler than (6), because the number of vari-
ables has been reduced to two, vandt, whereas pxonly
plays the role of a parameter.
Since Eq. (12) is linear in vand quadratic in ∂/∂v, it
can be simpliﬁed by taking the Laplace transform over v
/tildewidePt,px(pv|vi) =/integraldisplay+∞
0dv e−pvvPt,px(v|vi). (13)
The partial diﬀerential equation satisﬁed by /tildewidePt,px(pv|vi)
is of the ﬁrst order
/bracketleftbigg∂
∂t+/parenleftbigg
Γpv+κ2
2p2
v−p2
x−ipx
2/parenrightbigg∂
∂pv/bracketrightbigg
/tildewideP=−γθpv/tildewideP,
(14)
where we introduced the notation
Γ =γ+iρκp x. (15)
Eq. (14) has to be solved with the initial condition
/tildewidePt=0,px(pv|vi) = exp( −pvvi). (16)
The solution of (14) is given by the method of charac-
teristics [20]:
/tildewidePt,px(pv|vi) = exp/parenleftbigg
−˜pv(0)vi−γθ/integraldisplayt
0dτ˜pv(τ)/parenrightbigg
,(17)
where the function ˜ pv(τ) is the solution of the character-
istic (ordinary) diﬀerential equation
d˜pv(τ)
dτ= Γ˜pv(τ) +κ2
2˜p2
v(τ)−p2
x−ipx
2(18)with the boundary condition ˜ pv(t) =pvspeciﬁed at τ=
t. The diﬀerential equation (18) is of the Riccati type
with constant coeﬃcients [21], and its solution is
˜pv(τ) =2Ω
κ21
ζeΩ(t−τ)−1−Γ−Ω
κ2, (19)
where we introduced the frequency
Ω =/radicalbig
Γ2+κ2(p2x−ipx). (20)
and the coeﬃcient
ζ= 1 +2Ω
κ2pv+ (Γ−Ω). (21)
Substituting (19) into (17), we ﬁnd
/tildewidePt,px(pv|vi) (22)
= exp/braceleftbigg
−˜pv(0)vi+γθ(Γ−Ω)t
κ2−2γθ
κ2lnζ−e−Ωt
ζ−1/bracerightbigg
.
IV. AVERAGING OVER VARIANCE
Normally we are interested only in log-returns xand
do not care about variance v. Moreover, whereas log-
returns are directly known from ﬁnancial data, variance
is a hidden stochastic variable that has to be estimated.
Inevitably, such an estimation is done with some degree
of uncertainty, which precludes a clear-cut direct com-
parison between Pt(x, v|vi) and ﬁnancial data. Thus we
introduce the reduced probability distribution
Pt(x|vi) =+∞/integraldisplay
0dv P t(x, v|vi) =/integraldisplaydpx
2πeipxx/tildewidePt,px(0|vi),
(23)
where the hidden variable vis integrated out, so pv= 0.
Substituting ζfrom (21) with pv= 0 into (22), we ﬁnd
Pt(x|vi) =/integraldisplay+∞
−∞dpx
2πeipxx−vip2
x−ipx
Γ+Ω coth (Ω t/2)
×e−2γθ
κ2ln(coshΩt
2+Γ
ΩsinhΩt
2)+γΓθt
κ2. (24)
To check the validity of (24), let us consider the limit-
ing case κ= 0. In this case, the stochastic term in (2) is
absent, so the time evolution of variance is deterministic:
vt=θ+ (vi−θ)e−γt. (25)
Then process (5) gives a Gaussian distribution for x,
P(κ=0)
t(x|vi) =1√2πtvtexp/parenleftbigg
−(x+vtt/2)2
2vtt/parenrightbigg
,(26)
with the time-averaged variance vt=1
t/integraltextt
0dτ vτ. On the
other hand, by taking the limit κ→0 and integrating
overpxin (24), we reproduce the same expression (26).",2002-03-03T23:59:06Z, funns t  dow jontable sec since courier pt pt inserti eq since eq place pt pt t pt eq pt t pt t ric cat substituti pt normally oinevitably pt  pt pt substituti pt to iton
paper_qf_50.pdf,4,"Probability distribution of returns in the Heston model with stochastic
  volatility","  We study the Heston model, where the stock price dynamics is governed by a
geometrical (multiplicative) Brownian motion with stochastic variance. We solve
the corresponding Fokker-Planck equation exactly and, after integrating out the
variance, find an analytic formula for the time-dependent probability
distribution of stock price changes (returns). The formula is in excellent
agreement with the Dow-Jones index for the time lags from 1 to 250 trading
days. For large returns, the distribution is exponential in log-returns with a
time-dependent exponent, whereas for small returns it is Gaussian. For time
lags longer than the relaxation time of variance, the probability distribution
can be expressed in a scaling form using a Bessel function. The Dow-Jones data
for 1982-2001 follow the scaling function for seven orders of magnitude.
","4
−0.4−0.3−0.2−0.1 00.10.20.3100101102103104
Log−return, xProbability density, Pt(x)Dow Jones data, 1982−2001 and 1990−2001
1 day5 days20 days40 days250 days
−0.2 00.2
FIG. 2: Probability distribution Pt(x) of log-return xfor
diﬀerent time lags t. Points: The 1982–2001 Dow-Jones data
fort= 1, 5, 20, 40, and 250 trading days. Solid lines: Fit of
the data with Eqs. (28) and (29). For clarity, the data points
and the curves for successive tare shifted up by the factor
of 10 each. Inset: The 1990–2001 Dow-Jones data points
compared with the same theoretical curves.
Eq. (24) cannot be directly compared with ﬁnancial
time-series data, because it depends on the unknown ini-
tial variance vi. In order to resolve this problem, we
assume that vihas the stationary probability distribu-
tion Π ∗(vi), which is given by (9). Thus we introduce
the probability distribution function Pt(x) by averaging
(24) over viwith the weight Π ∗(vi):
Pt(x) =/integraldisplay∞
0dviΠ∗(vi)Pt(x|vi). (27)
The integral over viis similar to the one of the Gamma
function and can be taken explicitly. The ﬁnal result is
the Fourier integral
Pt(x) =1
2π/integraldisplay+∞
−∞dpxeipxx+Ft(px)(28)
with
Ft(px) =γθ
κ2Γt (29)
−2γθ
κ2ln/bracketleftbigg
coshΩt
2+Ω2−Γ2+ 2γΓ
2γΩsinhΩt
2/bracketrightbigg
.
The variable pxenters (29) via the variables Γ from
(15) and Ω from (20). It is easy to check that Pt(x) is
real, because Re Fis an even function of pxand Im Fis
an odd one. One can also check that Ft(px= 0) = 0,
which implies that Pt(x) is correctly normalized at all
times:/integraltext
dxP t(x) = 1. The simpliﬁed version of Eq. (29)
for the case ρ= 0 is given in Appendix A.
Eqs. (28) and (29) for the probability distribution
Pt(x) of log-return xat time tare the central analyti-cal result of the paper. The integral in (28) can be cal-
culated numerically or, in certain regimes discussed in
Secs. V, VI, and VII, analytically. In Fig. 2, the calcu-
lated function Pt(x), shown by solid lines, is compared
with the Dow-Jones data, shown by dots. (Technical de-
tails of the data analysis are discussed in Sec. VIII.) Fig.
2 demonstrates that, with a ﬁxed set of the parameters
γ,θ,κ,µ, and ρ, Eqs. (28) and (29) very well reproduce
the distribution of log-returns xof the Dow-Jones index
foralltimes t. In the log-linear scale of Fig. 2, the tails
of lnPt(x) vs.xare straight lines, which means that that
tails of Pt(x) are exponential in x. For short times t,
the distribution is narrow, and the slopes of the tails are
nearly vertical. As the time tprogresses, the distribution
broadens and ﬂattens.
V. ASYMPTOTIC BEHAVIOR FOR LONG
TIME t
Eq. (2) implies that variance reverts to the equilibrium
value θwithin the characteristic relaxation time 1 /γ. In
this section, we consider the asymptotic limit where time
tis much longer than the relaxation time: γt≫2. Ac-
cording to (15) and (20), this condition also implies that
Ωt≫2. Then Eq. (29) reduces to
Ft(px)≈γθt
κ2(Γ−Ω). (30)
Let us change of the variable of integration in (28) to
px=ω0
κ/radicalbig
1−ρ2˜px+ip0, (31)
where
p0=κ−2ργ
2κ(1−ρ2), ω 0=/radicalBig
γ2+κ2(1−ρ2)p2
0.(32)
Substituting (31) into (15), (20), and (30), we transform
(28) to the following form
Pt(x) =ω0e−p0x+Λt
πκ/radicalbig
1−ρ2/integraldisplay∞
0d˜pxcos(A˜px)e−B√
1+˜p2x,(33)
where
A=ω0
κ/radicalbig
1−ρ2/parenleftbigg
x+ργθt
κ/parenrightbigg
, B =γθω0t
κ2,(34)
and
Λ =γθ
2κ22γ−ρκ
1−ρ2. (35)
According to formula 3.914 from [22], the integral in (33)
is equal to BK1(√
A2+B2)/√
A2+B2, where K1is the
ﬁrst-order modiﬁed Bessel function.
Thus, Eq. (28) in the limit γt≫2 can be represented
in the scaling form
Pt(x) =Nte−p0xP∗(z), P ∗(z) =K1(z)/z, (36)",2002-03-03T23:59:06Z,log probabity pt dow jonprobabity pt points t dow jonsolid t for iset t dow joneq i pt pt pt t gaa t courier pt ft ft t it pt re is im is one ft pt t eq  pt t secs i pt dow jontechnical sec  dow joni pt pt for as eq iac teq ft  b substituti pt accordi vessel  eq pt te
paper_qf_50.pdf,5,"Probability distribution of returns in the Heston model with stochastic
  volatility","  We study the Heston model, where the stock price dynamics is governed by a
geometrical (multiplicative) Brownian motion with stochastic variance. We solve
the corresponding Fokker-Planck equation exactly and, after integrating out the
variance, find an analytic formula for the time-dependent probability
distribution of stock price changes (returns). The formula is in excellent
agreement with the Dow-Jones index for the time lags from 1 to 250 trading
days. For large returns, the distribution is exponential in log-returns with a
time-dependent exponent, whereas for small returns it is Gaussian. For time
lags longer than the relaxation time of variance, the probability distribution
can be expressed in a scaling form using a Bessel function. The Dow-Jones data
for 1982-2001 follow the scaling function for seven orders of magnitude.
","5
−10−8−6−4−2024681010−810−610−410−2100102
zRenormalized probability, Pt(x)exp(p0x)/NtDow−Jones data, 1982−2001 and 1990−2001
10 days
20 days
30 days
40 days
80 days
100 days
150 days
200 days
250 days
FIG. 3: Renormalized probability density Pt(x)ep0x/Ntplot-
ted as a function of the scaling argument zgiven by (37).
Solid lines: The scaling function P∗(z) =K1(z)/zfrom (36),
where K1is the ﬁrst-order modiﬁed Bessel function. Upper
and lower sets of points: The 1982–2001 and 1990–2001 Dow-
Jones data for diﬀerent time lags t. For clarity, the lower data
set and the curve are shifted by the factor of 10−2.
where the argument z=√
A2+B2is
z=ω0
κ/radicalBigg
(x+ργθt/κ )2
1−ρ2+/parenleftbiggγθt
κ/parenrightbigg2
, (37)
and the time-dependent normalization factor Ntis
Nt=ω2
0γθt
πκ3/radicalbig
1−ρ2eΛt, (38)
Eq. (36) demonstrates that, up to the factors Ntand
e−p0x, the dependence of Pt(x) on the two arguments x
andtis given by the function P∗(z) of the single scaling
argument zin (37). Thus, when plotted as a function
ofz, the data for diﬀerent xandtshould collapse on
the single universal curve P∗(z). This is beautifully illus-
trated by Fig. 3, where the Dow-Jones data for diﬀerent
time lags tfollows the curve P∗(z) for seven orders of
magnitude.
In the limit z≫1, we can use the asymptotic expres-
sion [22] K1(z)≈e−z/radicalbig
π/2zin (36) and take the loga-
rithm of P. Keeping only the leading term proportional
tozand omitting the subleading term proportional to
lnz, we ﬁnd that ln Pt(x) has the hyperbolic distribution
[2, p. 14]
lnPt(x)
Nt≈ −p0x−zforz≫1. (39)
Let us examine Eq. (39) for large and small |x|.
In the ﬁrst case |x| ≫ γθt/κ , Eq. (37) gives z≈
ω0|x|/κ/radicalbig
1−ρ2, so Eq. (39) becomes
lnPt(x)
Nt≈ −p0x−ω0
κ/radicalbig
1−ρ2|x|. (40)0 40 80 120 160 200 24000.10.20.30.40.50.60.70.80.91
Time lag, t (days)Gaussian weight, Gt01234567891011γ t
1 10 1001 10 100 
Time lag, t (days)Pt(xm)
FIG. 4: The fraction Gtof the total probability contained
in the Gaussian part of Pt(x) vs. time lag t. Inset: Time
dependence of the probability density at maximum Pt(xm)
(points), compared with the Gaussian t−1/2behavior (solid
line).
Thus, the PDF Pt(x) has the exponential tails (40) for
large log-returns |x|. Notice that, in the considered limit
γt≫2, the slopes dlnP/dx of the exponential tails (40)
do not depend on time t. Because of p0, the slopes (40)
for positive and negative xare not equal, thus the dis-
tribution Pt(x) is not symmetric with respect to posi-
tive and negative price changes. According to (32), this
asymmetry is enhanced by a negative correlation ρ <0
between stock price and variance.
In the second case |x+ργθt/κ | ≪γθt/κ , by Taylor-
expanding zin (37) near its minimum in xand substi-
tuting the result into (39), we get
lnPt(x)
N′
t≈ −p0x−ω0(x+ργθt/κ )2
2(1−ρ2)γθt, (41)
where N′
t=Ntexp(−ω0γθt/κ2). Thus, for small log-
returns |x|, the PDF Pt(x) is Gaussian with the width
increasing linearly in time. The maximum of Pt(x) in
(41) is achieved at
xm(t) =−γθt
2ω0/parenleftbigg
1 + 2ρ(ω0−γ)
κ/parenrightbigg
. (42)
Eq. (42) gives the most probable log-return xm(t) at time
t, and the coeﬃcient in front of tconstitutes a correction
to the average growth rate µ, so that the actual growth
rate is ¯ µ=µ+dxm/dt.
As Fig. 2 illustrates, ln Pt(x) is indeed linear in xfor
large|x|and quadratic for small |x|, in agreement with
(40) and (41). As time progresses, the distribution, which
has the scaling form (36) and (37), broadens. Thus,
the fraction Gtof the total probability contained in the
parabolic (Gaussian) portion of the curve increases, as",2002-03-03T23:59:06Z,re normalized pt nt dow jonre normalized pt ntp lot solid t vessel up t dow jonfor b nt is nt eq nt and pt    dow joeepi pt pt nt  eq ieq eq pt nt time gt time pt t gt of pt iset time pt  pt notice because pt accordi itaylor pt nt e  pt t pt eq as  pt as  gt of n
paper_qf_50.pdf,6,"Probability distribution of returns in the Heston model with stochastic
  volatility","  We study the Heston model, where the stock price dynamics is governed by a
geometrical (multiplicative) Brownian motion with stochastic variance. We solve
the corresponding Fokker-Planck equation exactly and, after integrating out the
variance, find an analytic formula for the time-dependent probability
distribution of stock price changes (returns). The formula is in excellent
agreement with the Dow-Jones index for the time lags from 1 to 250 trading
days. For large returns, the distribution is exponential in log-returns with a
time-dependent exponent, whereas for small returns it is Gaussian. For time
lags longer than the relaxation time of variance, the probability distribution
can be expressed in a scaling form using a Bessel function. The Dow-Jones data
for 1982-2001 follow the scaling function for seven orders of magnitude.
","6
psp1+p2+p3+
p1−
p2−
Re(px)Im(px)
0
−iq−iq+
FIG. 5: Complex plane of px. Dots: The singularities of
Ft(px). Circled crosses: The accumulation points ±iq±
∗of
the singularities in the limit γt≫2. Cross: The saddle point
ps, which is located in the upper half-plane for x >0. Dashed
line: The contour of integration displaced from the real axi s
in order to pass through the saddle point ps.
illustrated in Fig. 4. (The procedure of calculating Gtis
explained in Appendix B.) Fig. 4 shows that, at suﬃ-
ciently long times, the total probability contained in the
non-Gaussian tails becomes negligible, which is known
in literature [2]. The inset in Fig. 4 illustrates that the
time dependence of the probability density at maximum,
Pt(xm), is close to t−1/2, which is characteristic for a
Gaussian evolution.
VI. ASYMPTOTIC BEHAVIOR FOR LARGE
LOG-RETURN x
In the complex plane of px, function F(px) becomes
singular at the points pxwhere the argument of the log-
arithm in (29) vanishes. These points are located on the
imaginary axis of pxand are shown by dots in Fig. 5.
The singularity closest to the real axis is located on the
positive (negative) imaginary axis at the point p+
1(p−
1).
Because the argument of the logarithm in (29) vanishes at
these two points, we can approximate F(px) by the dom-
inant, singular term: F(px)≈ −(2γθ/κ2)ln(px−p±
1).
For large |x|, the integrand of (28) oscillates very fast
as a function of px. Thus, we can evaluate the integral
using the method of stationary phase [21] by shifting the
contour of integration so that is passes through a saddle
point of the argument ipxx+F(px) of the exponent in
(28). The saddle point position ps, shown in Fig. 5 by
the cross, is determined by the equation
ix=−dF(px)
dpx/vextendsingle/vextendsingle/vextendsingle/vextendsingle
px=ps≈2γθ
κ2×/braceleftBigg1
ps−p+
1, x > 0,
1
ps−p−
1, x < 0.(43)
For a large |x|, such that |xp±
1| ≫2γθ/κ2, the saddle01020304050607080020406080100120140160180The slope, qt+=−d ln Pt(x)/dx
Time lag, t (days)0 1 2 3γ t
FIG. 6: Solid line: The slope q+
t=−dlnP/dx of the
exponential tail for x >0 as a function of time. Points:
The asymptotic approximation (46) for the slope in the limit
γt≪2. Dashed line: The saturation value q+
∗forγt≫2,
Eq. (45).
point psis very close to the singularity point: ps≈p+
1
forx >0 and ps≈p−
1forx <0. Then the asymptotic
expression for the probability distribution is
Pt(x)∼/braceleftBigg
e−xq+
t, x > 0,
exq−
t, x < 0,(44)
where q±
t=∓ip±
1(t) are real and positive. Eq. (44) shows
that, for all times t, the tails of the probability distribu-
tionPt(x) for large |x|are exponential. The slopes of
the exponential tails, q±=∓dlnP/dx, are determined
by the positions p±
1of the singularities closest to the real
axis.
These positions p±
1(t) and, thus, the slopes q±
tdepend
on time t. For times much shorter than the relaxation
time ( γt≪2), the singularities lie far away from the real
axis. As time increases, the singularities move along the
imaginary axis toward the real axis. Finally, for times
much longer than the relaxation time ( γt≫2), the sin-
gularities approach limiting points: p±
1→ ±iq±
∗, which
are shown in Fig. 5 by circled crosses. Thus, as illus-
trated in Fig. 6, the slopes q±
tmonotonously decrease in
time and saturate at long times:
q±
t→q±
∗=ω0
κ/radicalbig
1−ρ2±p0forγt≫2. (45)
The slopes (45) are in agreement with Eq. (40) valid for
γt≫2. The time dependence q±
tat short times can be
also found analytically:
q±
t≈2
κ/radicalbiggγ
tforγt≪2. (46)
The dotted curve in Fig. 6 shows that Eq. (46) works
very well for short times t, where the slope diverges at
t→0.",2002-03-03T23:59:06Z,re im lex dots t ft cired t oss t dasd t  t gt is   t  pt itse  t because for  t  b for t pt time solid t points t dasd t eq tpt b eq pt t tse for as nally    t eq t t  eq
paper_qf_50.pdf,7,"Probability distribution of returns in the Heston model with stochastic
  volatility","  We study the Heston model, where the stock price dynamics is governed by a
geometrical (multiplicative) Brownian motion with stochastic variance. We solve
the corresponding Fokker-Planck equation exactly and, after integrating out the
variance, find an analytic formula for the time-dependent probability
distribution of stock price changes (returns). The formula is in excellent
agreement with the Dow-Jones index for the time lags from 1 to 250 trading
days. For large returns, the distribution is exponential in log-returns with a
time-dependent exponent, whereas for small returns it is Gaussian. For time
lags longer than the relaxation time of variance, the probability distribution
can be expressed in a scaling form using a Bessel function. The Dow-Jones data
for 1982-2001 follow the scaling function for seven orders of magnitude.
","7
VII. ASYMPTOTIC BEHAVIOR FOR SHORT
TIME t
For a short time t, we expand the equations of Sec.
III to the ﬁrst order in tand set pv= 0. The last term
in Eq. (22) cancels the penultimate term, and Eq. (18)
gives ˜ pv(0) = t(p2
x−ipx)/2. Substituting this formula
into (22) and taking the integral (23) over px, we ﬁnd
Pt(x|vi) =1√2πvite−(x+vit/2)2
2vit. (47)
Eq. (47) shows that, for a short t, the probability distri-
bution of xevolves in a Gaussian manner with the initial
variance vi, because variance has no time to change.
Substituting (47) and (9) into (27), we ﬁnd
Pt(x) =αα
Γ(α)e−x/2
√
2πθt/integraldisplay∞
0d˜vi˜vC−1
ie−A˜vi−B/˜vi(48)
where ˜ vi=vi/θ,A=α+θt/8,B=x2/2θt, and C=
α−1/2. According to formula 3.471.9 from [22], the
integral in (48) is 2( B/A)C/2KC(2√
AB) for Re A >0
and Re B >0, where KCis the modiﬁed Bessel function
of the order C. Taking into account that A≈α(because
t≪16γ/κ2for short t), we obtain the ﬁnal expression
Pt(x) =21−αe−x/2
Γ(α)/radicalbiggα
πθtyα−1/2Kα−1/2(y),(49)
where we introduced the scaling variable
y=/radicalbigg
2αx2
θt=2√γ
κ|x|√
t(50)
In the limit y≫1, using the formula Kν(y)≈
e−y/radicalbig
π/2yin (49), we ﬁnd
Pt(x)≈21/2−α
Γ(α)/radicalbiggα
θtyα−1e−y. (51)
Eqs. (50) and (51) show that the tails of the distribu-
tion are exponential in x, and the slopes dlnP/dx are in
agreement with Eq. (46).
In the opposite limit y≪1, the small argument ex-
pansion of the Bessel function can be found from the
following equations [23]:
Kν(y) =π
2I−ν(y)−Iν(y)
sin(νπ),π
sin(πν)= Γ(ν)Γ(1−ν),
(52)
and
Iν(y)≈/parenleftBigy
2/parenrightBigν∞/summationdisplay
k=0(y2/4)k
k! Γ(ν+k+ 1). (53)
Substituting (53) into (52), we ﬁnd in the case 1 /2≤α <
3/2
Kα−1/2(y)≈Γ(α−1/2)
2/parenleftBigy
2/parenrightBig−α+1/2
+Γ(−α+ 1/2)
2/parenleftBigy
2/parenrightBigα−1/2
.(54)Substituting (54) into (49), we obtain
Pt(x)≈Γ(α−1/2)
Γ(α)/radicalbiggα
2πθt/bracketleftbigg
1−λ/parenleftBigy
2/parenrightBig2α−1/bracketrightbigg
,(55)
where we introduced the coeﬃcient
λ=|Γ(−α+ 1/2)|
Γ(α−1/2). (56)
Eq. (55) can be written in the form
lnPt(x)−lnPt(0)≈ −λ/parenleftBigy
2/parenrightBig2α−1
. (57)
We see that ln Pt(x) approaches x= 0 as a power of x
lower than 2 (for 1 /2≤α <3/2). The slope dlnP/dx
atx→0 is zero for α >1 and inﬁnite for α <1.
VIII. COMPARISON WITH THE DOW-JONES
TIME SERIES
To test the model against ﬁnancial data, we down-
loaded daily closing values of the Dow-Jones industrial
index for the period of 20 years from 1 January 1982 to
31 December 2001 from the Web site of Yahoo [24]. The
data set contains 5049 points, which form the time series
{Sτ}, where the integer time variable τis the trading day
number. We do not ﬁlter the data for short days, such
as those before holidays.
Given {Sτ}, we use the following procedure to extract
the probability density P(DJ)
t(r) of log-return rfor a
given time lag t. For the ﬁxed t, we calculate the set
of log-returns {rτ= lnSτ+t/Sτ}for all possible times
τ. Then we partition the r-axis into equally spaced bins
of the width ∆ rand count the number of log-returns rτ
belonging to each bin. In this process, we omit the bins
with occupation numbers less than ﬁve, because we con-
sider such a small statistics unreliable. Only less than
1% of the entire data set is omitted in this procedure.
Dividing the occupation number of each bin by ∆ rand
by the total occupation number of all bins, we obtain the
probability density P(DJ)
t(r) for a given time lag t. To
ﬁndP(DJ)
t(x), we replace r→x+µt.
Assuming that the system is ergodic, so that ensem-
ble averaging is equivalent to time averaging, we com-
pareP(DJ)
t(x) extracted from the time-series data and
Pt(x) calculated in previous sections, which describes en-
semble distribution. In the language of mathematical
statistics, we compare our theoretically derived popula-
tion distribution with the sample distribution extracted
from the time series data. We determine parameters
of the model by minimizing the mean-square deviation/summationtext
x,t|lnP(DJ)
t(x)−lnPt(x)|2, where the sum is taken
over all available xandt= 1, 5, 20, 40, and 250 days.
These values of tare selected because they represent dif-
ferent regimes: γt≪1 fort= 1 and 5 days, γt≈1 for
t= 20 days, and γt≫1 for t= 40 and 250 days. As",2002-03-03T23:59:06Z,for sec t eq eq substituti pt eq substituti pt accordi re re is vessel taki pt ipt eq ivessel b b substituti b b b b substituti pt b b eq pt pt b b  pt t to dow jonuary cember b yahoo t  givefor tionly dividi to assumi pt i pt tse as
paper_qf_50.pdf,8,"Probability distribution of returns in the Heston model with stochastic
  volatility","  We study the Heston model, where the stock price dynamics is governed by a
geometrical (multiplicative) Brownian motion with stochastic variance. We solve
the corresponding Fokker-Planck equation exactly and, after integrating out the
variance, find an analytic formula for the time-dependent probability
distribution of stock price changes (returns). The formula is in excellent
agreement with the Dow-Jones index for the time lags from 1 to 250 trading
days. For large returns, the distribution is exponential in log-returns with a
time-dependent exponent, whereas for small returns it is Gaussian. For time
lags longer than the relaxation time of variance, the probability distribution
can be expressed in a scaling form using a Bessel function. The Dow-Jones data
for 1982-2001 follow the scaling function for seven orders of magnitude.
","8
TABLE I: Parameters of the Heston model obtained from the
ﬁt of the Dow-Jones data using ρ= 0 for the correlation coeﬃ-
cient. We also ﬁnd 1 /γ= 22.2 trading days for the relaxation
time of variance, α= 2γθ/κ2= 1.3 for the parameter in the
variance distribution function (9), and x0=κ/γ= 5.4% for
the characteristic scale (A4) of x.
Units γ θ κ µ
1/day 4.50×10−28.62×10−52.45×10−35.67×10−4
1/year 11.35 0.022 0.618 0.143
Figs. 2 and 3 illustrate, our expression (28) and (29) for
the probability density Pt(x) agrees with the data very
well, not only for the selected ﬁve values of time t, but
for the whole time interval from 1 to 250 trading days.
However, we cannot extend this comparison to tlonger
than 250 days, which is approximately 1/20 of the entire
range of the data set, because we cannot reliably extract
P(DJ)
t(x) from the data when tis too long.
The values obtained for the four ﬁtting parameters ( γ,
θ,κ,µ) are given in Table I. We ﬁnd that our ﬁts are
not very sensitive to the value of ρ, so we cannot reliably
determine it. Thus, we use ρ= 0 for simplicity, which
gives a good ﬁt of the data. On the other hand, a nonzero
value of ρwas found in [25] by ﬁtting the leverage corre-
lation function introduced in [26] and in [13, 14, 15] by
ﬁtting the option prices.
All four parameters ( γ,θ,κ,µ) shown in Table I have
the dimensionality of 1/time. The ﬁrst line of the Table
gives their values in the units of 1/day, as originally de-
termined in our ﬁt. The second line shows the annualized
values of the parameters in the units of 1/year, where we
utilize the average number of 252.5 trading days per cal-
endar year to make the conversion. The relaxation time
of variance is equal to 1 /γ= 22.2 trading days = 4.4
weeks ≈1 month, where we took into account that 1
week = 5 trading days. Thus, we ﬁnd that variance has
a rather long relaxation time, of the order of one month,
which is in agreement with the conclusion of Ref. [25].
The eﬀective growth rate of stock prices is determined
by the coordinate rm(t) where the probability density
Pt(rm) is maximal. Using the relation rm=xm+µt
and Eq. (42), we ﬁnd that the actual growth rate is
¯µ=µ−γθ/2ω0≈µ−θ/2 = 13% per year. [Here we
took into account that ω0≈γ, because γ≫κ/2 in Eq.
(32).] This number coincides with the average growth
rate of the Dow-Jones index obtained by a simple ﬁt of
the time series {Sτ}with an exponential function of τ, as
shown in Fig. 7. The eﬀective stock growth rate ¯ µis com-
parable with the average stock volatility after one year
σ=√
θ= 14.7%. Moreover, as Fig. 1 shows, the distri-
bution of variance is broad, and the variation of variance
is comparable to its average value θ. Thus, even though
the average growth rate of stock index is positive, there
is a substantial probability/integraltext0
−∞dr P t(r) = 17 .7% to have
negative return for t= 1 year.
According to (45), the asymmetry between the slopes8284868890929496980002103104
YearDow−Jones IndexDow−Jones data, 1982−2001
FIG. 7: Time dependence of the Dow-Jones index shown in
the log-linear scale. The straight line represents the aver age
exponential growth in time.
of exponential tails for positive and negative xis given
by the parameter p0, which is equal to 1/2 when ρ=
0 (see also the discussion of Eq. (A1) in Appendix A).
The origin of this asymmetry can be traced back to the
transformation from (1) to (4) using Itˆ o’s formula. It
produces the term 0 .5vtdtin the r.h.s. of (4), which then
generates the second term in the r.h.s. of (6). The latter
term is the only source of asymmetry in xofPt(x) when
ρ= 0. However, in practice, the asymmetry of the slopes
p0= 1/2 is quite small (about 2.7%) compared with the
average slope q±
∗≈ω0/κ≈1/x0= 18.4.
By ﬁtting the Dow-Jones data to our formula, we im-
plicitly assumed that the parameters of the stochastic
process ( γ,θ,κ,µ) do not change in time. While this as-
sumption may be reasonable for a limited time interval,
the parameters generally could change in time. The time
interval of our ﬁt, 1982–2001, includes the crash of 1987,
so one might expect that the parameters of the ﬁt would
change if we use a diﬀerent interval. To verify this conjec-
ture, in Figs. 2 and 3, we also compare the data points for
the time interval 1990–2001 with the theoretical curves
produced using the same values for the parameters as
shown in Table I. Although the empirical data points
in the tails for long time lags decrease somewhat faster
than the theory predicts, the overall agreement is quite
reasonable. We ﬁnd that changing the values of the ﬁt-
ting parameters does not visibly improve the agreement.
Thus, we conclude that the parameters of the Heston
stochastic process are essentially the same for 1980s and
1990s. Apparently, the crash of 1987 produced little ef-
fect on the probability distribution of returns, because
the stock market quickly resumed its overall growth. On
the other hand, the study [27] indicates that the data for
2000s do not follow our theoretical curves with the same
ﬁtting parameters. The main diﬀerence appears to be",2002-03-03T23:59:06Z,meters stodow jon units s pt t table   oall table t table t t  ref t pt usi eq re eq  dow jon t o  accordi year dow joninx dow jontime dow jont eq  t it it t pt by dow jonw t to s table although   stoapparently ot
paper_qf_50.pdf,9,"Probability distribution of returns in the Heston model with stochastic
  volatility","  We study the Heston model, where the stock price dynamics is governed by a
geometrical (multiplicative) Brownian motion with stochastic variance. We solve
the corresponding Fokker-Planck equation exactly and, after integrating out the
variance, find an analytic formula for the time-dependent probability
distribution of stock price changes (returns). The formula is in excellent
agreement with the Dow-Jones index for the time lags from 1 to 250 trading
days. For large returns, the distribution is exponential in log-returns with a
time-dependent exponent, whereas for small returns it is Gaussian. For time
lags longer than the relaxation time of variance, the probability distribution
can be expressed in a scaling form using a Bessel function. The Dow-Jones data
for 1982-2001 follow the scaling function for seven orders of magnitude.
","9
in the average growth rate µ, which became negative in
2000s, as opposed to +13% per year in 1980s and 1990s.
Unfortunately, the statistics for 2000s is limited, becaus e
we have only few years. Nevertheless, it does seem to
indicate that in 2000s the stock market switched to a
diﬀerent regime compared with 1980s and 1990s.
IX. DISCUSSION AND CONCLUSIONS
We derived an analytical solution for the PDF Pt(x)
of log-returns xas a function of time tfor the Heston
model of a geometrical Brownian motion with stochas-
tic variance. The ﬁnal result has the form of a one-
dimensional Fourier integral (28) and (29). (In the case
ρ= 0, the equations have the simpler form presented
in Appendix A.) Our result agrees very well with the
Dow-Jones data, as shown in Fig. 2. Comparing the
theory and the data, we determine the four (non-zero)
ﬁtting parameters of the model, particularly the vari-
ance relaxation time 1 /γ= 22.2 days. For time longer
than 1 /γ, our theory predicts the scaling behavior (36)
and (37), which the Dow-Jones data indeed exhibits over
seven orders of magnitude, as shown in Fig. 3. The scal-
ing function P∗(z) =K1(z)/zis expressed in terms of
the ﬁrst-order modiﬁed Bessel function K1. Previous es-
timates in literature of the relaxation time of volatility
using various indirect indicators range from 1.5 days [8,
p. 80] to 73 days for the half-life of the Dow-Jones index
[7]. Since we have a good ﬁt of the entire family of PDFs
for time lags from 1 to 250 trading days, we believe that
our estimate, 22.2 days, is more reliable. A close value
of 19.6 days was found in Ref. [25].
An alternative point of view in literature is that the
time evolution of volatility is not characterized by a sin-
gle relaxation rate. As shown in Appendix C, the vari-
ance correlation function C(v)
t(C4) in the Heston model
has a simple exponential decay in time. However, the
analysis of ﬁnancial data [2, p. 70] indicates that the
correlation function has a power-law dependence or su-
perposition of two (or more) exponentials with the re-
laxation times of less than one day and more than few
tens of days. (Large amount of noise in the data makes
it diﬃcult to give a precise statement.) Ref. [28] argues
that volatility relaxation is multifractal and has no char-
acteristic time. However, one should keep in mind that
the total range (C2) of variation of C(v)
tis only about
77% of its saturation value, not many orders of magni-
tude. As Figs. 2 of Refs. [29] and [28] shows, the main
drop of C(v)
ttakes place within a reasonably well-deﬁned
and relatively short time, whereas residual relaxation is
stretched over a very long time. In this situation, a sim-
ple exponential time dependence, while not exact, may
account for the main part of relaxation and give a rea-
sonable approximation for the purposes of our study. Al-
ternatively, it is possible to generalize the Heston model
by incorporating more than one relaxation times [14].As Fig. 2 shows, the probability distribution Pt(x) is
exponential in xfor large |x|, where it is characterized
by the time-dependent slopes dlnP/dx. The theoretical
analysis presented in Sec. VI shows that the slopes are de-
termined by the singularities of the function Ft(px) from
(29) in the complex plane of pxthat are closest to the
real axis. The calculated time dependence of the slopes
dlnP/dx, shown in Fig. 6, agrees with the data very well,
which further supports our statement that 1 /γ= 22.2
days. Exponential tails in the probability distribution of
stock log-returns have been noticed in literature before
[2, p. 61], [30], however time dependence of the slopes
has not been recognized and analyzed theoretically. As
shown in Fig. 2, our equations give the parabolic depen-
dence of ln Pt(x) onxfor small xand linear dependence
for large x, in agreement with the data. Qualitatively
similar results were found in Ref. [31] for a diﬀerent
model with stochastic volatility and in agreement with
the NYSE index daily data. It suggests that the linear
and parabolic behavior is a generic feature of the mod-
els with stochastic volatility. In Ref. [6], the power-law
dependence on xof the tails of Pt(x) was emphasized.
However, the data for S&P 500 were analyzed in Ref. [6]
only for short time lags t, typically shorter than one day.
On the other hand, our data analysis is performed for
the time lags longer than one day, so the results cannot
be directly compared.
Deriving Pt(x) in Sec. IV, we assumed that variance v
has the stationary gamma-distribution Π ∗(v) (9). This
assumption should be compared with the data. There
were numerous attempts in literature to reconstruct the
probability distribution of volatility from the time-seri es
data [32, 33]. Generally, these papers agree that the cen-
tral part of the distribution is well described by a log-
normal distribution, but opinions vary on the ﬁtting of
the tails. Particularly, Ref. [33] performed a ﬁt with an
alternative probability distribution of volatility descr ibed
in Ref. [2, p. 88]. Unfortunately, none of these papers at-
tempted to ﬁt the data using Eq. (10), so we do not have
a quantitative comparison. Taking into account that we
only need the integral (27), the exact shape of Π ∗(v) may
be not so important, and Eq. (9) may give a reasonably
good approximation for our purposes, even if it does not
ﬁt the tails very precisely.
Although we tested our model for the Dow-Jones in-
dex, there is nothing speciﬁc in the model which indicates
that it applies only to stock market data. It would be
interesting to see how the model performs when applied
to other time series, for example, the foreign exchange
data [34], which also seem to exhibit exponential tails.
The study [27] indicates that our Pt(x) also works very
well for the S&P 500 and Nasdaq indices for 1980s and
1990s. However, in the 2000s the average growth rate µ
of the stock market changed to a negative value, which
complicates separation of ﬂuctuations from the overall
trend.",2002-03-03T23:59:06Z,unfortunately nevertless  pt stobrownish t courier i our dow jon ari for dow jon t vessel us dow jonsince fs ref aas  stolarge ref as s refs ial stoas  pt t sec ft t  eonential as  pt qualitative ref it iref pt ref orivi pt sec  tre genlly particularly ref ref unfortunately eq taki eq although dow jonit t pt nasdaq ver
paper_qf_50.pdf,10,"Probability distribution of returns in the Heston model with stochastic
  volatility","  We study the Heston model, where the stock price dynamics is governed by a
geometrical (multiplicative) Brownian motion with stochastic variance. We solve
the corresponding Fokker-Planck equation exactly and, after integrating out the
variance, find an analytic formula for the time-dependent probability
distribution of stock price changes (returns). The formula is in excellent
agreement with the Dow-Jones index for the time lags from 1 to 250 trading
days. For large returns, the distribution is exponential in log-returns with a
time-dependent exponent, whereas for small returns it is Gaussian. For time
lags longer than the relaxation time of variance, the probability distribution
can be expressed in a scaling form using a Bessel function. The Dow-Jones data
for 1982-2001 follow the scaling function for seven orders of magnitude.
","10
Acknowledgments
We thank J.-P. Bouchaud and R. E. Prange for de-
tailed discussions of the paper and numerous valuable
comments, and A. T. Zheleznyak and A. C. Silva for care-
ful reading of the manuscript and ﬁnding minor errors.
APPENDIX A: THE CASE ρ= 0
As explained in Sec. VIII, we ﬁt the data using ρ= 0
for simplicity. In this case, by shifting the variable of
integration in (28) px→px+i/2, we ﬁnd
Pt(x) =e−x/2/integraldisplay+∞
−∞dpx
2πeipxx+Ft(px), (A1)
where α= 2γθ/κ2,
Ft(px) =αγt
2−αln/bracketleftbigg
coshΩt
2+Ω2+γ2
2γΩsinhΩt
2/bracketrightbigg
,
(A2)
and
Ω =/radicalbig
γ2+κ2(p2x+ 1/4)≈γ/radicalbig
1 +p2x(κ2/γ2).(A3)
Now the function Ft(px) is real and symmetric: Ft(px) =
Ft(−px). Thus, the integral in (A1) is a symmetric func-
tion of x, and the only source of asymmetry of Pt(x) inx
is the exponential prefactor in (A1), as discussed at the
end of Sec. VIII.
In the second equation (A3), we took into account that,
according to values shown in Table I, κ2/4γ2≪1. In-
troducing the dimensionless variables
˜t=γt,˜x=x/x0,˜px=pxx0, x0=κ/γ, (A4)
Eqs. (A1), (A2), and (A3) can be rewritten as follows:
Pt(x) =e−x/2
x0/integraldisplay+∞
−∞d˜px
2πei˜px˜x+F˜t(˜px), (A5)
where ˜Ω =/radicalbig
1 + ˜p2xand
F˜t(˜px) =α˜t
2−αln/bracketleftBigg
cosh˜Ω˜t
2+˜Ω2+ 1
2˜Ωsinh˜Ω˜t
2/bracketrightBigg
.(A6)
It is clear from (A4), (A5), and (A6) that the parameter
αdetermines the shape of the function Pt(x), whereas
1/γandx0set the scales of tandx.
In the limit ˜t≫2, the scaling function (36) for ρ= 0
can be written as
Pt(x) =Nte−x/2K1(z)/z, z =/radicalbig
˜x2+¯t2,(A7)
where ¯t=α˜t/2 =tθ/x2
0andNt=¯te¯t/πx0. Notice
that Eq. (A7) has only two ﬁtting parameters, x0and
θ, whereas the general formula (A5) and (A6) has three
ﬁtting parameters. As follows from (39), for ˜ x≫¯tand
˜x≫1,Pt(x)∝exp(−|x|/x0), so 1 /x0is the slope of the
exponential tails in x.APPENDIX B: GAUSSIAN WEIGHT
Let us expand the integral in (A1) for small x:
Pt(x)≈e−x/2/parenleftbigg
µ0−1
2µ2x2/parenrightbigg
, (B1)
where the coeﬃcients are the ﬁrst and the second mo-
ments of exp[ Ft(px)]
µ0(t) =+∞/integraldisplay
−∞dpx
2πeFt(px), µ 2(t) =+∞/integraldisplay
−∞dpx
2πp2
xeFt(px).
(B2)
On the other hand, we know that Pt(x) is Gaussian for
small x. So, we can write
Pt(x)≈µ0e−x/2e−µ2x2/2µ0, (B3)
with the same coeﬃcients as in (B1). If we ignore the ex-
istence of fat tails and extrapolate (B3) to x∈(−∞,∞),
the total probability contained in such a Gaussian ex-
trapolation will be
Gt=+∞/integraldisplay
−∞dxµ0e−x/2−µ2x2/2µ0=/radicalBigg
2πµ3
0
µ2eµ0/8µ2.(B4)
Obviously, Gt<1, because the integral (B4) does not
take into account the probability contained in the fat
tails. Thus, the diﬀerence 1 −Gtcan be taken as a mea-
sure of how much the actual distribution Pt(x) deviates
from a Gaussian function.
We calculated the moments (B2) numerically for the
function Fgiven by (A2), then determined the Gaussian
weight Gtfrom (B4) and plotted it in Fig. 4 as a function
of time. For t→ ∞,Gt→1, i.e. Pt(x) becomes Gaussian
for very long time lags, which is known in literature [2].
In the opposite limit t→0,Ft(px) becomes a very broad
function of px, so we cannot calculate the moments µ0
andµ2numerically. The singular limit t→0 is studied
analytically in Sec. VII.
APPENDIX C: CORRELATION FUNCTION OF
VARIANCE
The correlation function of variance is deﬁned as
C(v)
t=∝an}bracketle{tvt+τvτ∝an}bracketri}ht=/integraldisplay∞
0dvi/integraldisplay∞
0dv vΠt(v|vi)viΠ∗(vi).
(C1)
It depends only on the relative time tand does not de-
pend on the initial time τ. The averaging ∝an}bracketle{t. . .∝an}bracketri}htis per-
formed over the ensemble probability distribution, as
written in (C1), or over the initial time τfor time-series
data. Eq. (C1) has the same structure as in the inﬂuence-
functional formalism of Feynman and Vernon [35], where",2002-03-03T23:59:06Z,ackledgment   charae t lez yak sva as sec ipt ft ft  ft ft ft  pt sec itable ipt b b it pt ipt te nt notice eq as pt  pt ft ft ft opt so pt  gt b obvusly gt  gt capt  givegt from  for gt pt ift t sec t it t eq leymavernon
paper_qf_50.pdf,11,"Probability distribution of returns in the Heston model with stochastic
  volatility","  We study the Heston model, where the stock price dynamics is governed by a
geometrical (multiplicative) Brownian motion with stochastic variance. We solve
the corresponding Fokker-Planck equation exactly and, after integrating out the
variance, find an analytic formula for the time-dependent probability
distribution of stock price changes (returns). The formula is in excellent
agreement with the Dow-Jones index for the time lags from 1 to 250 trading
days. For large returns, the distribution is exponential in log-returns with a
time-dependent exponent, whereas for small returns it is Gaussian. For time
lags longer than the relaxation time of variance, the probability distribution
can be expressed in a scaling form using a Bessel function. The Dow-Jones data
for 1982-2001 follow the scaling function for seven orders of magnitude.
","11
Πt(v|vi) represents the conditional probability propaga-
tor from the initial value vito the ﬁnal value vover the
timet, and Π ∗(vi) represents the stationary, equilibrium
probability distribution of vi.
Using Eqs. (C1) and (9), it is easy to ﬁnd the limiting
values of C(v)
t:
C(v)
∞=∝an}bracketle{tv∝an}bracketri}ht2=θ2, (C2)
C(v)
0=∝an}bracketle{tv2∝an}bracketri}ht=θ2/parenleftbigg
1 +1
α/parenrightbigg
=θ2(1 + 0 .77),
where we used the numerical value from Table I.
Diﬀerentiating Eq. (C1) with respect to tand using Eq.(8), we ﬁnd that C(v)
tsatisﬁes the following diﬀerential
equation:
dC(v)
t
dt=−γ(C(v)
t−θ2). (C3)
Thus, C(v)
tchanges in time exponentially with the relax-
ation rate γ:
C(v)
t=θ2/parenleftbigg
1 +e−γt
α/parenrightbigg
. (C4)
[1] P. Wilmott, Derivatives (John Willey & Sons, New York,
1998).
[2] J. P. Bouchaud and M. Potters, Theory of Financial
Risks (Cambridge University Press, Cambridge, 2001).
[3] R. N. Mantegna and H. E. Stanley, An Introduction to
Econophysics (Cambridge University Press, Cambridge,
2000).
[4] J. Voit, The Statistical Mechanics of Financial Markets
(Springer, Berlin, 2001).
[5] B. B. Mandelbrot, Journal of Business 36, 393 (1963).
[6] R. N. Mantegna and H. E. Stanley, Nature 376, 46
(1995); P. Gopikrishnan, M. Meyer, L. A. N. Amaral,
and H. E. Stanley, European Physical Journal B 3, 139
(1998); P. Gopikrishnan, V. Plerou, L. A. N. Amaral,
M. Meyer, and H. E. Stanley, Physical Review E 60,
5305 (1999).
[7] R. F. Engle and A. J. Patton, Quantitative Finance 1,
237 (2001).
[8] J. P. Fouque, G. Papanicolaou, and K. R. Sircar, Deriva-
tives in Financial Markets with Stochastic Volatility
(Cambridge University Press, Cambridge, 2000); Inter-
national Journal of Theoretical and Applied Finance, 3,
101 (2000).
[9] J. Hull and A. White, Journal of Finance 42, 281 (1987);
C. A. Ball and A. Roma, Journal of Financial and Quan-
titative Analysis 29, 589 (1994); R. Sch¨ obel and J. Zhu,
European Finance Review 3, 23 (1999).
[10] E. M. Stein and J. C. Stein, Review of Financial Studies
4, 727 (1991).
[11] S. L. Heston, Review of Financial Studies 6, 327 (1993).
[12] B. E. Baaquie, Journal de Physique I (France) 7, 1733
(1997).
[13] G. Bakshi, C. Cao, and K. Singleton, The Journal of
Finance 52, 2002 (1997).
[14] D. Duﬃe, J. Pan, and K. Singleton, Econometrica 68,
1343 (2000).
[15] J. Pan, Journal of Financial Economics 63, 3 (2002).
[16] W. Feller, Annals of Mathematics 54, 173 (1951).
[17] D. G. McMillan, Applied Economics Letters 8, 605
(2001).
[18] R. G. Tompkins, Journal of Futures Markets 21, 43
(2001); Y.-N. Lin, N. Strong, and X. Xu, Journal of Fu-
tures Markets 21, 197 (2001); G. Fiorentini, A. Leon, and
G. Rubio, Journal of Empirical Finance, to be published.
[19] C. W. Gardiner, Handbook of Stochastic Methods forPhysics, Chemistry, and the Natural Sciences (Springer,
Berlin, 1993).
[20] R. Courant and D. Hilbert, Methods of Mathematical
Physics, vol. 2 (John Willey & Sons, New York, 1962).
[21] C. M. Bender and S. A. Orszag, Advanced Mathematical
Methods for Scientists and Engineers (Springer-Verlag,
New York, 1999).
[22] I. S. Gradshteyn and I. R. Ryzhik, Table of Integrals,
Series, and Products (Academic Press, New York, 1996).
[23] M. Abramowitz and I. A. Stegun, Handbook of Mathe-
matical Functions (Dover, New York, 1972), page 375.
[24] Yahoo Finance http://ﬁnance.yahoo.com/. To download
data, type in the symbol box: “ˆDJI”, and then click on
the link: “Download Spreadsheet”.
[25] J. Masoliver and J. Perell´ o, International Journal of The-
oretical and Applied Finance 5, 541 (2002); preprint
http://lanl.arXiv.org/abs/cond-mat/0202203.
[26] J.-P. Bouchaud, A. Matacz, and M. Potters, Physical
Review Letters 87, 228701 (2001).
[27] A. C. Silva and V. M. Yakovenko, preprint
http://lanl.arXiv.org/abs/cond-mat/0211050.
[28] J. F. Muzy, J. Delour, and E. Bacry, European Physical
Journal B 17, 537 (2000).
[29] M. Potters, R. Cont, and J.-P. Bouchaud, Europhysics
Letters 41, 239 (1998).
[30] L. C. Miranda and R. Riera, Physica A 297, 509 (2001).
[31] M. Serva, U. L. Fulco, M. L. Lyra,
and G. M. Viswanathan, preprint
http://lanl.arXiv.org/abs/cond-mat/0209103.
[32] P. Cizeau, Y. Liu, M. Meyer, C.-K. Peng, and H. E. Stan-
ley, Physica A 245, 441 (1997); Y. Liu, P. Gopikrish-
nan, P. Cizeau, M. Meyer, C.-K. Peng, and H. E. Stan-
ley, Physical Review E 60, 1390 (1999); M. Raberto,
E. Scalas, G. Cuniberti, and M. Riani, Physica A 269,
148 (1999); M. Pasquini and M. Serva, European Physi-
cal Journal B 16, 195 (2000).
[33] S. Miccich` e, G. Bonanno, F. Lillo, R. N. Mantegna,
preprint http://lanl.arXiv.org/abs/cond-mat/0202527.
[34] R. Friedrich, J. Peinke and C. Renner, Physical Re-
view Letters 84, 5224 (2000); C. Renner, J. Peinke and
R. Friedrich, Physica A 298, 499 (2001).
[35] R. P. Feynman and A. R. Hibbs, Quantum Mechanics
and Path Integrals (McGraw-Hill, New York, 1965).",2002-03-03T23:59:06Z,usi table di eq eq  pot rivativ wl ey sons new york  chaters tory nancial risks cambridge   cambridge antenna stanley aintrodueco no psics cambridge   cambridge vo it t statistical meics nancial markets  berlimanlbrot journal business antenna stanley nature go pi krishna meyer moral stanley apsical journal go pi krishna le rou moral meyer stanley psical review eagle patroquantitative nance found papa nicola ou sir car  rival nancial markets stochastic volatity cambridge   cambridge inter journal toical applied nance hull white journal nance ball roma journal nancial quaanalysis szhu anance review steisteireview nancial studistoreview nancial studibaa qui journal psique france bash  set t journal nance du paset econometric pajournal nancial economics seller annals matmatics mc ml aapplied economics ters hops journal futurmarkets listro xu journal fu markets re nt ileorub journal emical nance garner handbook stochastic methods psics cmistry natural scienc berlicour ant gbert methods matmatical psics  wl ey sons new york benr or zag advanced matmatical methods scientists eineers  verlag new york gra ty ry zhi table integrals seriprodus acamic  new york  with ste guhandbook ma t funns donew york yahoo nance to download spreadset mas oli ell internatnal journal t applied nance   chamata cz ters psical review ters sva v e  mu  l our ba y apsical journal ters cont  chaeuro psics ters miranda rie ra psics serve ful co lyr viswanatha size au  meyer pe stapsics  go pi kris size au meyer pe stapsical review racer to scale uni berth ria ni psics pas quit serve aph ys journal icc ibonanno lle antenna  friedripe ike re nner psical re ters re nner pe ike friedripsics leymagibbs quantum meics path integrals mc raw hl new york
paper_qf_51.pdf,1,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","arXiv:cond-mat/0403022v2  [cond-mat.other]  26 Mar 2004A Non-Gaussian Option Pricing Model with
Skew
Lisa Borland∗and Jean-Philippe Bouchaud+
∗Evnine-Vaughan Associates, Inc.
456 Montgomery Street, Suite 800, San Francisco, CA 94104, U SA
lisa@evafunds.com
+Science & Finance/Capital Fund Management
6-8 Bd Haussmann, 75009 Paris, France
February 2, 2008
Abstract
Closed form option pricing formulae explaining skew and smi le are
obtained within a parsimonious non-Gaussian framework. We extend
the non-Gaussian option pricing model of L. Borland (Quanti tative
Finance, 2, 415-431, 2002) to include volatility-stock correlations con-
sistent with the leverage eﬀect. A generalized Black-Schol es partial
diﬀerential equation for this model is obtained, together w ith closed-
form approximate solutions for the fair price of a European c all op-
tion. In certain limits, the standard Black-Scholes model i s recovered,
as is the Constant Elasticity of Variance (CEV) model of Cox a nd
Ross. Alternative methods of solution to that model are ther eby also
discussed. The model parameters are partially ﬁt from empir ical ob-
servations of the distribution of the underlying. The optio n pricing
model then predicts European call prices which ﬁt well to emp irical
market data over several maturities.
1",2004-02-29T22:06:05Z, mar nooptprici mskew a brand  phippe  chaev nine afghaassociatinc montgomery street suite safranci science nance capital fund management bd haussmanparis france ruary abstra osed  brand quant nance  sai schools constant elasticity variance cox ross alternative t t an
paper_qf_51.pdf,2,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","1 Introduction
Over the past three decades, since the seminal works of Black , Scholes and
Merton [1, 2], the basic ideas of arbitrage-free option pric ing have become
quite well-known. The Black and Scholes model is such a stand ard bench-
mark for calculating option prices, that is used by traders w orld-wide in
spite of the fact that the prices it yields do no match ones obs erved on the
market place. This diﬀerence is clearly a signature of the fa ct that, after
all, the Black-Scholes formula is just a model of reality, ba sed on a set of
assumptions which obviously only approximate (often quite remotely) the
true dynamics of ﬁnancial markets. Most importantly, real m arkets are in-
complete and risk cannot in general be fully hedged away, as i t is the case
in the Black-Scholes world [3]. Nevertheless, prices are qu oted in terms of
the Black-Scholes model, and in order to obtain the market pr ices from this
model, it is commonplace to adjust the volatility parameter σwhich enters
the model. In other words, traders use a diﬀerent value of σfor each value
of the option strike price K, as well as for each value of the option expira-
tion time T. This is tantamount to having a continuum of diﬀerent models
(corresponding to each value of σ) for the diﬀerent options on the very same
underlying instrument. A plot of σover strike and maturity then yields a
surface, often convex and sloping, which is referred to as th e smile or the
skew, or more generally the smile or skew surface. Given a smi le surface, one
can plug those values of σinto a Black-Scholes model and obtain prices for
each strike and time to expiration.
Many attempts have been made to modify or extend the Black-Sc holes
model in order to accommodate for the skew observed on option markets.
A large class of those models is based on modeling the skew sur face itself.
For example, so-called local volatility models [4, 5] aim to ﬁt the observed
skew surface by calibrating a function σloc(S, t) (where Sis the stock price
andtthe time) such that it reproduces actual market prices for ea chK
andT. This function σloc(S, t) is then used in conjunction with the Black-
Scholes model to perform other important operations such as the pricing of
exotic options, hedging, and so on. The problem is however, t hat while σloc
by construction allows one to reproduce the set of prices to w hich it was
calibrated, it does not contain any information about the tr ue dynamics of
the underlying asset; therefore it can actually lead to wors e hedging strategies
and erroneous exotic prices than would have been obtained wi th simply using
2",2004-02-29T22:06:05Z,introduo schools mortot  schools   schools most  schools nevertless  schools i give schools many  sc for is   schools t
paper_qf_51.pdf,3,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","Black-Scholes without any attempts to account for the smile [6]. Nonetheless,
local volatility models have been widely used by many banks a nd trading
desks.
L´ evy processes [7, 8, 9, 10], stochastic volatility models [11, 12, 13, 6, 14,
15] or cumulant expansions around the Black-Scholes case [1 6, 17, 18, 19, 10]
constitute approaches which have been successful in captur ing some of the
features of real option prices. For example the recent ‘stoc hastic alpha, beta,
rho’ (SABR) model ([6], and see Appendix C) can be well-ﬁt to e mpirical
skew surfaces. It also provides a better model of the dynamic s of the smile
over time. Nevertheless, in all these models the focus is sti ll on ﬁtting or
somehow calibrating the parameters of the model to match obs erved option
prices. The diﬀerence in our current approach will instead b e to introduce a
stock price model capturing some important features of the e mpirical distri-
bution of stock returns. Our model will be intrinsically mor e parsimonious
than stochastic volatility models in the sense that we have j ust one source of
randomness, which also allows us to remain within the framew ork of complete
markets. The option pricing methodology yields closed form approximate for-
mulae based on such a model, thereby predicting option price s rather than
ﬁtting parameters to match observed market prices. We ﬁnd go od agree-
ment between theoretical and traded prices, lending suppor t to the possible
validity and potential applicability of the model.
2 Non-Gaussian Stock Price Model with Skew
The standard Black-Scholes stock price model reads
dS=µSdt+σSdω (1)
where dωrepresents a zero mean Brownian random noise correlated in t ime
tas
∝an}bracketle{tdω(t)dω(t′)∝an}bracketri}htF=dtdt′δ(t−t′), (2)
Here, µrepresents the rate of return and σthe volatility of log stock returns.
This model implies that stock returns follow a log-normal di stribution, which
is only a very rough approximate description of the actual si tuation. In fact,
real returns have strong power-law tails which are not at all accounted for
within the standard theory. Furthermore, there is a skew in t he distribution
3",2004-02-29T22:06:05Z, schools nonetless  schools for  it nevertless t our t  nostock price mskew t  schools dt sd brownish re  ifurtr
paper_qf_51.pdf,4,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","such that there is a higher probability of large negative ret urns than large
positive ones. Both the tails and the skew depend on the time o ver which
the returns are calculated. In general, it is observed that t he power-law
statistics of the distributions are very stable, exhibitin g tails decaying as −3
in the cumulative distribution for returns taken over time- scales ranging from
minutes to weeks, only slowly converging to Gaussian statis tics for very long
time-scales [20, 10]. Similarly, the skew of the distributi on varies with the
time-scale of returns such that it is largest for intermedia te time-scales [10].
The deviations of the statistics of real returns to those of t he log-normal
model of Eq. (1) become particularly important when it comes to calculating
the fair price of options on the underlying stock. For exampl e, the simplest
‘European’ call option, which is the right to buy the stock Sat the strike price
Kat a given expiration time T. The call will expire worthless if S(T)< K,
and proﬁtably otherwise. The fair price of the call thus depe nds on the
probability that the stock price S(T) exceeds K, and if one uses the wrong
statistics in the model then the theoretical price will diﬀe r quite a bit from
the empirically traded price. (Interestingly enough, mark et players intuitively
adjust the price of options to be much more consistent with th e true statistics
of stock returns [19], even though the log-normal Black-Sch oles model is
widely used by traders themselves to get an estimate of what t he price should
be.)
In this paper, we develop a model of the underlying stock whic h is con-
sistent with both fat-tails and skewness in the returns dist ribution. We then
derive option prices for this model, obtaining (approximat e) closed-form so-
lutions for European call options.
Our model bases on the non-Gaussian model [21, 22], where it w as pro-
posed that the ﬂuctuations driving stock returns could be mo deled by a
‘statistical feedback’ process [23], namely
dS=µSdt+σSdΩ (3)
where
dΩ =P(Ω)1−q
2dω. (4)
In this equation, Pcorresponds to the probability distribution of Ω, which si-
multaneously evolves according to the corresponding nonli near Fokker-Planck
equation [24]
∂P
∂t=1
2∂P2−q
∂Ω2. (5)
4",2004-02-29T22:06:05Z,both isimarly t eq for aat kat t t interesti  si aour dt sd icorresponds former 
paper_qf_51.pdf,5,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","The index qwill be taken 3 ≥q≥1. In that case, Eq. (5) is known also
as the fast diﬀusion equation [26]. Note also that in the abov e equation, the
timetmust be a-dimensional. In the following, the unit of time has been
chosen to be one year. Correspondingly, σis also a-dimensional.
Equation (5) can be solved exactly, leading, when the initia l condition on
Pis aP(Ω, t= 0) = δ(Ω), to a Student-t (or Tsallis [25]) distribution:
P=1
Z(t)/parenleftBig
1 + (q−1)β(t)Ω2(t)/parenrightBig−1
q−1(6)
with
β(t) =c1−q
3−qq((2−q)(3−q)t)−2
3−q (7)
and
Z(t) = ((2 −q)(3−q)cqt)1
3−q (8)
where the q-dependent constant cqis given by
cq=/bracketleftbigg/integraldisplay∞
−∞(1 + (q−1)u2)−1
q−1du/bracketrightbigg2
≡π
q−1Γ2(1
q−1−1
2)
Γ2(1
q−1). (9)
Eq. (6) recovers a Gaussian in the limit q→1 while exhibiting power
law tails for all q >1.
The statistical feedback term Pcan also be seen as a price-dependent
volatility that captures the market sentiment. Intuitivel y, this means that
if the market players observe unusually large deviations of Ω (which - after
removing a noise induced drift term which could equivalentl y have been ab-
sorbed in the dynamics of Eq. (3) [21] - is essentially equal t o the detrended
and normalized log stock price) from its mean, then the eﬀect ive volatility
will be high because in such cases P(Ω) is small, and the exponent qis larger
than unity. Conversely, traders will react more moderately if Ω is close to its
more typical values. As a result, the model exhibits intermi ttent behaviour
consistent with that observed in the eﬀective volatility of markets – but see
the discussion below.
Option pricing based on the price dynamics elucidated above was solved
in [21], and it was seen that those prices agreed very well wit h traded prices
for instruments which have a symmetric underlying distribu tion, such as
certain foreign exchange currency markets. However, the qu estion of skew
was not discussed in that paper and is instead the topic of the current work.
5",2004-02-29T22:06:05Z,t ieq note icorrespondy equatis stunt ts all is b b eq t caintuitive eq conversely as optver
paper_qf_51.pdf,6,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","We extend the stock price model to include an eﬀective volati lity that is
consistent with the leverage correlation eﬀect, namely
dS=µSdt+σS1−α
0SαdΩ (10)
with Ω evolving according to Eq. (4). The parameter αintroduces an asym-
metric skew into the distribution of log stock returns. More precisely, when
α < 1, the relative volatility can be seen to increase when Sdecreases,
and vice-versa, an eﬀect known as the leverage correlation ( see [35]). For
α=q= 1 the standard Black-Scholes model is recovered. For α= 1 but
q >1 the model reduces to that discussed in [21], while for q= 1 but general
αit becomes the constant elasticity of variance (CEV) model o f Cox and
Ross [27].
There are two possible interpretations to model Eq. (10), an d some limi-
tations which we elucidate here. In the context of option pri cing, the relevant
question concerns the forward probability, estimated from now (t= 0), with
the current price S0corresponding to the reference price around which de-
viations are measured. In this case, the fact that t= 0 and S=S0(or
Ω = 0) play a special role makes perfect sense. If, on the other hand, one
wants to interpret Eq. (10) as a model for the real returns, th en the choice
ofS0=S(t= 0) as the reference price is somewhat arbitrary and therefo re
problematic. Still, this model produces returns which have many features
consistent with real stock returns. This is illustrated in F igure 1 where we
have plotted a time-series of simulated returns as well as, i n Figure 2, a
plot of the distribution of returns between times tandt+τ, for diﬀerent
time lags τ, averaged over all possible starting times t. Clearly, the model
reproduces volatility clustering. Also, the returns distr ibution exhibits fat
tails becoming Gaussian over larger time scales. (Note that this is not in
contradiction with the fact that returns counted from t= 0 have a Student-t
distribution for all times t). The skew in the distribution is also apparent.
For visual comparison, we show the same data for the SP500. Th e qualitative
behaviour of the two data sets is very similar. However, in or der to have a
consistent model of real stock returns, one should allow the reference price
to be itself time dependent. A possibility we are presently i nvestigating is to
write the statistical feedback term as P(Ω−¯Ω)(1−q)/2, where ¯Ω is a moving
average of past values of Ω. (The current model corresponds t o¯Ω = 0.)
One can also easily alter the super-diﬀusive behaviour (as g iven by Eq(7))
of the distribution if a mean reversion of the ﬂuctuations is included. Both
6",2004-02-29T22:06:05Z, dt eq t  easfor  schools for cox ross tre eq ii eq stl    also note stunt t for th t one eq both
paper_qf_51.pdf,7,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","of these features make the model a more realistic one of real r eturns (where
distributions are non-Gaussian, yet volatilities are norm ally diﬀusive across
all time-scales). Incidentally, this super-diﬀusive vola tility scaling can alter-
natively be modiﬁed by a redeﬁnition of time (see also the dis cussion later
in this paper).
One point necessary to comment on regarding Eq. (10), is that depending
on the value of α, stock prices may go negative. We do not allow this to
happen: rather, we absorb the stock if the price hits zero. Ba sically, this
means that the company has gone bankrupt, or that the stock is trading
at such low prices (penny stocks) that it is practically wort hless and we so
not consider it part of the trading universe. It is interesti ng to look at the
probability of bankruptcy Pas a function of α. This is shown in Figure 3.
An analytical treatment [28], expected to be valid for η=σ(1−α)≪1,
predicts that
P=aT1
q−1η3−q
q−1 (11)
(where ais a computable constant), in good agreement with our data fo r
small P. In fact, this point suggests a connection with credit risk m arkets,
where the probability of default is an important quantity. O ne could imagine
using a variant of our model to model default risk; conversel y, one could
imagine using data of probability of default to help choose t he correct value
ofαof a particular stock. The consequence of this default proba bility for
option pricing will be discussed in Section 5.
3 Fair Price of Options
Our model contains only one source of randomness, ω, which is a standard
Brownian noise. Therefore, the market is complete and usual hedging argu-
ments are valid, as was shown in [21] for the case α= 1. It is however trivial
to see that these results are also valid for arbitrary α. This means that we
can immediately adopt many of the arguments usually associa ted with the
Black-Scholes log-normal world - even though we here have a p rocess with
non-Gaussian statistics. In particular, it is possible to d eﬁne a unique equiv-
alent martingale measure Qto the process, related to the original measure
Fvia the Radon-Nikodym derivative [29, 30], such that the dis counted stock
priceG=Se−rtis a martingale, where rcorresponds to the risk-free rate.
7",2004-02-29T22:06:05Z,incintally one eq  ba it pas   ait searticial intellence price optns our brownish trefore it   schools ito via radody se
paper_qf_51.pdf,8,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","Figure 1: A typical path of the non-Gaussian model with q= 1.5 is shown. Here α=−3
andσ= 30%.
With respect to Qthe process of Eq. (10) reads
dS=rSdt+σS1−α
0SαdΩ (12)
with
dΩ =P1−q
2dz (13)
where the noise zsatisﬁes
∝an}bracketle{tdz(t′)dz(t)∝an}bracketri}htQ=dtdt′δ(t′−t) (14)
Essentially then, we have replaced µ=rjust as in the standard theory. Our
task now is to solve for the option prices based on the model Eq . (12).
The dynamics for Gread
dG=σS1−α
0Gαe(α−1)rtdΩ (15)
=σS1−α
0GαP1−q
2e(α−1)rtdz (16)
8",2004-02-29T22:06:05Z, re with t eq dt essentially our eq t read
paper_qf_51.pdf,9,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","Figure 2: This plot exhibits the distribution of log returns ( Y= lnS) for the S&P500
over diﬀerent time lags τranging from 1 day to 16, together with corresponding histog rams
obtained by our model with q= 1.5 and α= 1.
We redeﬁne time by introducing the new variable ˆtsuch that
ˆt=e2(α−1)rt−1
2(α−1)r. (17)
Note that ˆt→twhen ( α−1)rt→0. With respect to ˆt, the Brownian noise
zsatisﬁes
∝an}bracketle{tdz(ˆt′)dz(ˆt)∝an}bracketri}htQ=e2(α−1)rt∝an}bracketle{tdz(t′)dz(t)∝an}bracketri}htQ (18)
resulting in
dG=σS1−α
0GαdΩ (19)
dΩ(ˆt) = P(Ω(ˆt))(1−q)
2dz(ˆt) (20)
Next, make the variable transformation
x=(G/S 0)1−α−1
1−α(21)
9",2004-02-29T22:06:05Z,   note with brownish next
paper_qf_51.pdf,10,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","Figure 3: a) The probability of bankruptcy (default) Pas a function of αfor our model
withq= 1.5, for times ranging from 0 .25 to 2 years. The inset shows the probability of
bankruptcy as a funtion of time for a stock with α= 0, compared to our prediction that
P ∝T2forq= 1.5. These results are based on Monte-Carlo simulations with σ= 30%,
r= 6% and S(0) = $50. b) The probability of bankruptcy is sensitive to th e values of
qandσas shown here, for S(0) = $50, α= 0.2,r= 4% and T= 1. For T <1 these
probabilities are quickly depressed according to Eq(11).
(which simply becomes x= lnG/S 0forα= 1), so that
dx(ˆt) =−α
2σ2 P1−q
1 + (1 −α)x(ˆt)dˆt+σdΩ(ˆt) (22)
where the initial conditions read
x0≡x(0) = 0 (23)
and where, at the real ﬁnal time T, we have
S(T) =erT/parenleftBig
1 + (1 −α)x(ˆT)/parenrightBig1
1−α(24)
10",2004-02-29T22:06:05Z, t pas t tse   t for eq b b
paper_qf_51.pdf,11,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","Given a representation of the stock price with respect to the risk-neutral
world in which the discounted price is a martingale, the fair value of a deriva-
tive of the underlying stock can be calculated as the expecta tion of the payoﬀ
of the option. In this paper we shall focus on this approach. H owever we
want to point out that one can equivalently obtain the option price by solv-
ing the generalized Black-Scholes partial diﬀerential equ ation for the current
problem, which can readily be written down following the lin es of [21]. It
reads
df
dt+rSdf
dS+1
2d2f
dS2σ2S2(1−α)
0S2αP1−q
q=rf (25)
where Pqevolves according to Eq. (6). In the limit q→1 and α→1, we
recover the standard Black-Scholes diﬀerential equation. In the limit α→1
we recover the case of [21].
In the following we proceed to study closed form option prici ng formulas
obtained via expectations. The most immediate and simplest path is to
utilize our knowledge of the statistical properties of the r andom variable Ω( ˆt).
By invoking approximations valid if σ2T≪1, which is certainly true for
stock returns for reasonable maturities, we can obtain clos ed form solutions
for European calls, much along the lines followed in [21]. Ho wever, we allude
in Appendix B to a more complicated path which is based on mapp ing the
process onto a higher dimensional process.
4 Solutions via a Generalized Feynman-Kac
Approach
Ifσ2T≪1, which is valid even for relatively high volatility stocks , (e.g.
typical volatilities of 10% to 30% yield σ2Tvalues of .01 to .09 for T= 1
year), then we can insert the approximation x(ˆt)≈σΩ(ˆt) into the right hand
side of Eq. (22) yielding,
x(ˆT) =σΩ(ˆT)−α
2σ2/integraldisplayˆT
0P(Ω(ˆt))1−q
1 + (1 −α)σΩ(ˆt)dˆt, (26)
plus order σ4corrections. For α= 1 and general q, the problem reduces to
that discussed in [21], which we shall revisit with a slightl y diﬀerent evalua-
tion technique below.
11",2004-02-29T22:06:05Z,givei schools it df pq evolveq i schools iit by aho  solutns genlized leymaka approa valueq for
paper_qf_51.pdf,12,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","The integral in Eq. (26) contains terms of the type
/integraldisplayT
0F(Ω(t))dt, (27)
in other words integrals of a function Fof the random path Ω( t), conditioned
on ending at a particular value Ω( T) = Ω T. One way to evaluate the integral,
which is the approach taken in [21], is to invoke the followin g (which is
only valid approximately [31]): Replace the actual paths Ω( t) with other
paths, such that the ensemble average of the two sets of paths are the same.
Such paths can be obtained by exploiting the scaling propert ies of the time-
dependent probability distribution P(Ω(t)). We know that the variable Ω( t)
at time tis distributed according to Eq. (6) above. Due to scaling, we know
that we can map Ω( t) onto the terminal value Ω Tin the following way:
Ω(t) =/radicaltp/radicalvertex/radicalvertex/radicalbtβ(T)
β(t)ΩT (28)
where Ω( T) is distributed according to Eq. (6) evaluated at time T. There-
fore, as we illustrate in Appendix A, the ensemble statistic s of this replace-
ment path and the original path are equivalent. This approac h was shown
in [21] to be quite precise numerically. However, as we discu ss next, there is
another more exact approximation which can be used to evalua te Eq. (27).
This entails solving a Feynman-Kac equation exactly up to ﬁrst order in σ2.
Deﬁne the quantity
W(ΩT, T) =/summationdisplay
i|ΩTwiexp{ǫ/integraldisplayT
0F(Ω(t))dt} (29)
which is the expectation calculated as the sum over all paths iending at Ω T
at time T, of the exponential of quantity which we wish to calculate. T he
coeﬃcients widenote the weights of the paths, and ǫis a small parameter.
The function Fcan be a general one of Ω( t), but in our current problem we
shall be only interested in polynomial expansion up to the se cond order in
Ω. We can write
W(ΩT, T) =/summationdisplay
i|ΩTwiexp{ǫ/integraldisplayT
0(2/summationdisplay
j=1fj(t)Ω(t)j)dt} (30)
12",2004-02-29T22:06:05Z,t eq of one replace su eq due tieq tre   eq  leymaka  two e t ca two e
paper_qf_51.pdf,13,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","In the case where the paths evolve according to the nonlinear Fokker-Planck
equation, one can establish a generalized Feynman-Kac equa tion for W(ΩT, T)
that reads:
∂W
∂T=1
2∂2W1−q
0W
∂Ω2+ǫ/summationdisplay
jfj(T)Ωj
T (31)
where W0=Pq(ΩT, T) represents the solution to the fast diﬀusion problem,
corresponding to ǫ= 0. It is relatively straightforward to insert the Ansatz
W=W0/bracketleftBig
1 +ǫ(g0(T) +g1(T)ΩT+g2(T)Ω2
T)/bracketrightBig
(32)
into Eq. (31) and obtain an exact set of diﬀerential equation s for the time
dependent coeﬃcients gj, given in Appendix A (Eq. (52) - Eq. (54)).
First, let us illustrate this approach on the example of α= 1 which cor-
responds to the problem discussed in [21]. The expression fo rS(T) becomes
(from Eq. (24) and Eq. (26) in the limit α→1)
S(T) =S(0) exp {rT+σΩT−σ2
2/integraldisplayT
0Z(t)1−q(1 + (q−1)β(t)Ω(t)2)dt}(33)
Using the Feynman-Kac formula to evaluate
/integraldisplayT
0Z(t)1−qβ(t)Ω(t)2dt (34)
(see Appendix A, Eq.(56) with ǫ=σ2/2 and h2=Z1−qβ), we obtain an
expression of the form:
∝an}bracketle{t/integraldisplayT
0Z(t)1−qβ(t)Ω(t)2dt∝an}bracketri}ht=g0(T) +g2(T)Ω2
T (35)
(g1is zero by symmetry in this case). For the stock price, we thus obtain
S(T) =S(0) exp {rT+σΩT−σ2
2/bracketleftbigg
γ(T)3−q
2+ (q−1)(g0(T) +g2(T)Ω2
T)/bracketrightbigg
}
(36)
withγandgjare given by Eq. (57) - Eq. (59). The above expres-
sion is exact to order σ2; to that order, the path to path ﬂuctuations of/integraltextT
0Z(t)1−qβ(t)Ω(t)2dtconditioned on a given value of Ω Tcan be neglected.
The coeﬃcients gjfound here are slightly diﬀerent than those in [21],
where instead of Eq. (34), the approximation Eq. (28) was use d. However,
13",2004-02-29T22:06:05Z,iformer  leymaka pq it aat b b eq  eq eq rst t eq eq usi leymaka  eq for eq eq t cat eq eq ver
paper_qf_51.pdf,14,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","the results are numerically very close, so that in practice e ither evaluation
method can be used; both give option price which match well to Monte-Carlo
simulations. In fact, why this is so becomes clear from the pl ots shown in
Appendix A, where Monte-Carlo simulations of the quantity E q. (34) are
compared with the Feynman-Kac approximation and the one of [ 21]. In
the general case where αis not restricted to unity, the challenge now is to
evaluate Eq. (26) which can be rewritten as:
x(ˆT) =σΩˆT−ασ2
2/integraldisplayˆT
0Z(t)1−q1 + (q−1)β(t)Ω(t)2
1 + (1 −α)σΩ(t)dt (37)
We shall focus our discussion on the last term. A solution can be found
using a two-step approach: First of all, we make the assumpti on that the
expression, when integrated to abitrary time u, can be well-described by a
Pad´ e approximation, namely a ratio of polynomials, which h as the correct
asymptotic behaviour for Ω →0 and Ω → ∞, resulting in
x(ˆT) =σΩˆT−ασ2
2A(ˆT) +B(ˆT)ΩˆT+C(ˆT)Ω2
ˆT
1 +D(ˆT)ΩˆT(38)
Secondly, the coeﬃcients can be determined by equating the P ad´ e expansion
in the small Ω ˆTand large Ω ˆTlimits with the corresponding Feynman-Kac
expectations of simple polynomials. The details of this cal culation are in
Appendix A, and yields values of the coeﬃcients A, B, C andDas given
in Eq. (68) below. Again note that even in this general case, t he naive
approximation of Eq. (28) yields a slightly diﬀerent result , yet numerically
the two are practically indistinguishable. This can again b e understood by
the plots shown in Appendix A, comparing the two approximati ons with
Monte-Carlo simulations. Note also that the Pad´ e approxim ation of order
2:1 which we use in Eq. (37) is already extremely close to the M onte-Carlo
results (see Appendix A, Figure 12b); yet if desired, conver gence can easily
be further improved simply by including higher order terms.
5 European Call
The ﬁnal expression for the stock price at time Twith respect to the mar-
tingale noise is thus given by Eq. (24) together with Eq. (38) . With this
14",2004-02-29T22:06:05Z,  i   leymaka ieq  rst pad secondly and limits leymaka t  das eq ag articial intellence eq     note pad eq    acall t with eq eq with
paper_qf_51.pdf,15,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","result, it is straightforward to obtain a general expressio n for the price of a
European call in the current framework. The price of a Europe an claim c
can formally be written as
c=e−rT∝an}bracketle{th(ST)∝an}bracketri}htQ (39)
where his the payoﬀ of the option. For a European call the payoﬀ is h=
max(ST−K,0), allowing us to evaluate the fair call price as
c=e−rT∝an}bracketle{tST−K)∝an}bracketri}htD
Q (40)
where Drepresents the domain of non-zero payoﬀ, namely
ST> K (41)
In our current framework there must be an additional constra int on the paths
expiring in the money, namely that they never crossed S(t) = 0 at any t < T.
The probability ˜ p=P(S(T)> K|S(t)≤0) that any path would cross 0 and
then return to expire greater than Kis intuitively the order of the probability
of default squared. More exactly though, it can be computed i n the limit
where η=σ(1−α) is small, corresponding to small default probabilities P.
Using most probable path methods [28], we ﬁnd
˜p≈ P × exp/parenleftBigg
−b
Tη2/parenrightBigg
, (42)
where bis a positive constant of order unity. For practical purpose s, this
diﬀerence is extremely small when compared to the accuracy o f our solution,
and can be neglected in most cases.
The condition ST=Kimplies a quadratic equation for Ω Twhich can
easily be solved, resulting in two roots d1andd2given in Appendix A, Eq.
(69). These deﬁne a domain of integration for which the inequ alityST> K
is satisﬁed. It can be seen that d1always corresponds to the relevant lower
root, while the upper bound is dmax=d2ifd2> d1, anddmax=∞otherwise.
(Note that the strictness of this upper bound is however in pr inciple irrelevant
because of the smallness of the probability distribution in that region, and
also because our approximation scheme breaks down in that re gion).
We are now ready to state one of our main results, namely the cl osed-
form expression for the price of a European call option withi n this framework.
15",2004-02-29T22:06:05Z,at  for arepresents it is  usi b b for t impliwhi eq tse it note  an
paper_qf_51.pdf,16,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","Following Eq. (40), it is given as
c=S0/integraldisplaydmax
d1(1 + (1 −α)x(ˆT))1
1−αPq(ΩˆT)dΩˆT
−e−rTK/integraldisplaydmax
d1Pq(ΩˆT)dΩˆT (43)
withxˆTa function of Ω ˆTgiven by Eq. (38) evaluated at u=ˆT,ˆTis given
by Eq; (17) with t=T, andPq(ΩˆT) is the probability of the ﬁnal value of Ω,
given by:
Pq=1
Z(ˆT)(1 + (q−1)β(ˆT)Ω2
ˆT)−1
q−1. (44)
Note that it is at this point that we are in fact overcounting t hose paths
which crossed zero yet expired above K. To account for this, Pqshould be
replaced with Pq−˜pof Eq. (42). However, as mentioned above, ˜ pcan readily
be dropped as it leads to negligibly small contributions, an d the above result
for the option price can be seen as exact to order σ2.
6 Special Cases
As already mentioned, several special cases are recovered f or certain values
ofqandα. In practice, occasions could arise in which it might be usef ul to
work with these simpler solutions. Therefore, we spend a few lines discussing
them here.
6.1 CEV model q=1
The case of q= 1 and general αcorresponds to the CEV model of Cox and
Ross [27]. One obtains for the call price
c=S0/integraldisplay∞
d1(1 + (1 −α)x(ˆT))1
1−αP(zˆT)dzˆT−e−rTK/integraldisplay∞
d1P(zˆT)dzˆT(45)
with notation zt= Ω q=1(t) and where
x(ˆT) =−α
2σ2ˆT+ Γ (46)
16",2004-02-29T22:06:05Z,followi eq pq pq ta giveeq tis eq pq pq note to pq should pq eq special casas itrefore t cox ross one
paper_qf_51.pdf,17,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","with
Γ = 1 +σ2α(1−α)
2γˆT (47)
and
d1=1
σΓ/parenleftBigg(Ke−rT/S0)1−α−1
1−α+ασ2ˆT
2/parenrightBigg
(48)
Of course, ztis a Gaussian variable so
P(zt) =1√
2πtexp−z2
t
2t(49)
These equations look diﬀerent to the solutions presented in [27], which are
expressed in terms of Bessel functions. Numerically, howev er, our solution
and theirs are the same. The reason that they look diﬀerent is – apart from
the small σ2approximation we have made – is that we have made a point
of explicitly averaging with respect to the distribution of the random noise
variable zrather than with respect to the distribution of the stock pri ce itself.
The reason for this is because as we saw above in the general ca se, we know
something about the distribution of the noise Ω, whereas the distribution of
Sitself is more complicated – but see Appendix B.
6.2 Non-Gaussian Additive or q-Normal Model α= 0
The CEV model with α= 0 is known as the normal model . Here we present
the solution of the corresponding model for general q, which we term the
q-normal model. In this particular case, it is easy to solve f or the option
price in terms of the distribution of S, since the noise is additive. The call
price reads:
c=e−rT
Z(T)σ/integraldisplay∞
KS/bracketleftBigg
1 + (q−1)β(T)
σ2(S−S0erT)2/bracketrightBigg−1
q−1
dS
−Ke−rT
Z(T)σ/integraldisplay∞
K/bracketleftBigg
1 + (q−1)β(T)
σ2(S−S0erT)2/bracketrightBigg−1
q−1
dS (50)
17",2004-02-29T22:06:05Z,b ke b of tse vessel numerically t t itself  noadditive normal mt re it b b ke b b
paper_qf_51.pdf,18,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","6.3 Non-Gaussian Multiplicative α= 1
Finally, the case studied in [21] is recovered for general q, and α= 1. For
completeness we cite this result, albeit using the more prec ise Feynman-Kac
evaluation of the path integral which resulted in Eq. (36):
c=S0/integraldisplay∞
d1exp/braceleftBigg
σΩT−σ2
2[γ(T)3−q
2−(1−q)(g0(T) +g2(T)Ω2
T)]/bracerightBigg
Pq(ΩT)dΩT−e−rTK/integraldisplay∞
d1Pq(ΩT)dΩT (51)
withPqas in Eq. (44) and d1is the smallest root of the equation Eq. (36)
such that S(ΩT) =K.
7 Numerical Results
Now that we have pricing formulas for a non-Gaussian model wi th skew, it
is interesting to look at the option prices which result from the model, and
to see how they compare with a standard Black-Scholes formal ism. Using
Eq. (43) with q= 1.5,α=−1.5,σ= 30%, r= 6% and S0= $50, we
calculated call option prices as a function of diﬀerent stri kesKand times
to expiration T. We then backed out implied volatilities, i.e. the value of
σwhich would have been needed in conjunction with the standar d Black-
Scholes model ( q=α= 1) to reproduce the same option prices. A plot
of those implied volatilities as a function of KandTconstitutes the skew
surface which we show in Figure 4. The general shape of this su rface is very
similar to what is observed on the marketplace. For small T, the proﬁle
across strikes is smile-like, becoming more and more of a dow nward sloping
smirk as Tincreases. In Figure 5 we show a similar plot for q= 1 (i.e. the
CEV model). Note that this skew surface does not capture the s hape one
observes empirically. This goes to show that both fat tails ( q >1) and skew
(α <1) are necessary for a realistic description.
It is also interesting to look at the variation of the option p rice with
respect to the new parameter α(the variations to q, denoted by Upsilon Υ,
were studied in [21]). We denote this new ‘Greek’ by the Hebre w letter Aleph
ℵ, such that ℵ=∂c/∂α . Figure 6 shows ℵas a function of stock price S0,
using K= $50, T= 0.5 years, σ= 30%, and r= 6%, for q= 1.5 and q= 1.
18",2004-02-29T22:06:05Z,nomultiplicative nally for leymaka eq b b pq pq pq as eq eq numerical results   schools usi eq and   schools and constitut t for ineasi note  it epso greek  bre aleph 
paper_qf_51.pdf,19,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","Figure 4: A plot of the skew surface, i.e. Black-Scholes implied volat ilities across strikes
(K) and time to expiration ( T), backed out from our non-Gaussian model with q= 1.5
andα=−1.5. Other parameters were S0= $50 , r= 6% and σ= 30%.
The asymmetric nature of the sensitivity to variations in Al eph is apparent
forq= 1.5. Another way of depicting much the same information is show n
in Figure 7, where volatilities implied by comparing standa rd Black-Scholes
with our model Eq. (43) for diﬀerent values of αare shown. In all cases
we used q= 1.5,S0= 50, T= 0.5 years, σ= 30% and r= 6%. One can
clearly see that the implied volatility curve is like a smile forα= 1 (no skew)
becoming more and more asymmetric about K=S0asαdecreases.
In all of these results, we used the closed form pricing formu la, Eq. (43),
to generate option prices. However, since we used some appro ximations along
the way, it is a good check to see how the closed form price comp ares with that
obtained from pricing via Monte-Carlo simulations of the pr ocess. Indeed,
we saw that the two values are indistinguishable within the l imits of accuracy
of the Monte-Carlo simulations.
19",2004-02-29T22:06:05Z,  schools otr t al anotr   schools eq ione ieq   ined  
paper_qf_51.pdf,20,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","Figure 5: The skew surface implied from a CEV model ( q= 1) with α=−1.5. Other
parameters were S0= $50 , r= 6% and σ= 30%. The kink at short times may just be a
numerical artefact.
8 Empirical Results
To illustrate the qualitative agreement between our model a nd market data,
we show in Figure 8 a plot of the empirical skew surface for OEX options, as
well as the skew surface backed out of our model with q= 1.5 and α=−1.2.
We have not at all tried to calibrate the model to match the emp irical data
in any way, the plot is only intending to show that our model pr oduces a
surface with similar features to the empirical one across se veral time scales,
with just one set of parameters α, σandq. In other words while with respect
to the Black-Scholes model, the entire skew surface of Figur e 8b is needed
to describe the option prices, with respect to the q= 1.5 and α=−1.2
model the skew surface reduces to a single point. Consequent ly, we have
the hope that real market smiles and skews might be captured b y our model
with parameters q, αandσvarying only slightly across strikes and times to
expiration.
Clearly, to test this with any statistical signiﬁcance requ ires a large study
20",2004-02-29T22:06:05Z, t otr t emical results to   i schools  ur consequent 
paper_qf_51.pdf,21,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","Figure 6: Aleph the Greek, ℵ: The partial derivative of the call price with respect to α
is plotted as a function of the stock price S(0), for q= 1.5 and q= 1.. The asymmetric
nature is apparent for q= 1.5. We used K= $50 , r= 6%, σ= 30% and T= 0.5 years.
on many options, which we leave for a future work. However, we do present,
again for the purpose of illustration, results based on an an alysis of one set
of options, namely call options on Microsoft (MSFT) traded o n November
19, 2003. A popular methodology that market makers and trade rs follow
is to vary the parameters of whatever model they are using suc h that the
theoretical smile matches the market at each time to expirat ionT. We could
also follow such an approach: vary q,αandσfor each value of Tsuch that
the smiles and skews are reproduced. But if the model is good i n the sense
that the parameters do not change much over time, then perhap s the most
parsimonious treatment would be to choose one set of those pa rameters such
that the entire skew surface across both strikes and expirat ion times is well-
ﬁt. While interesting, both of these approaches would be way s ofimplying
the model parameters from the options data.
An alternative methodology which is well-suited for our cur rent approach,
is to instead try to relate at least one or two of the model para meters to prop-
21",2004-02-29T22:06:05Z, aleph greek t t  msoft november  subut w an
paper_qf_51.pdf,22,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","Figure 7: Implied volatilities from the q= 1.5 model as a function of α, with S0=
$50, r= 6%, σ= 30%, and T= 0.5 years. As αdecreases, the skew increases. Intuitively
this make sense: αcontributes to larger negative tails in the distribution of the underlying,
and therefore there will be higher probability of expiring o ut of the money relative to a
Black-Scholes process with lognormal noise. For extreme ou t of the money values however,
the noise in the fat tails can again increase the probability that an out-of-the money option
can expire in the money, so we expect the smile to increase aga in in this regime.
erties of the underlying asset, and then use these to predict market smiles.
In particular, the parameter qcan be readily determined from the empirical
distribution of the returns of the underlying asset. It has b een found in pre-
vious studies [33, 21, 34] that a value of q≈1.4 captures well the distribution
of daily stock returns, so this value could be adopted in the o ption pricing
formula. The parameter αcould in principle also be determined from the
distribution of underlying returns, or from measuring a lev erage correlation
function [35, 10]. Alternatively, it could perhaps be deter mined (as men-
tioned earlier in this paper) form empirical probabilities of default. For the
present study we ﬁx only qfrom the underlying distribution, and imply α
andσfrom the market smiles. We shall then look at how good the impl ied
smiles ﬁt the data, in conjunction with how the implied param eters vary with
22",2004-02-29T22:06:05Z, implied as intuitively  schools for iit t alternatively for 
paper_qf_51.pdf,23,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","OEX SP100q =1.5, alpha = -1.2
Figure 8: A purely qualitative comparison between the empirical skew surface of a set
of OEX options on S&P100 futures traded on June 6, 2001, and th e implied skew surface
from our model with q= 1.5,α=−1.2,σ= 30%, r= 4.5% and S0= 660. We have not
tried to calibrate to the OEX data in any way, we simply wish to show that the general
behaviour of the surfaces across strikes and time to expirat ion is similar. From top to
bottom: T= 0.03,T= 0.12,T= 0.20,T= 0.29 and T= 0.55.
T. If they are somewhat stable then we can conclude that the mod el is quite
good.
In Figure 9 we show the results for our model. As Tranges from the
order of a month to a year, we choose αandσsuch that the best ﬁt in
terms of least square pricing error is obtained between mode l and empirical
call prices, for ﬁxed q. The plots show actual implied volatility and the
implied volatility of our model. It is clear to see that the q= 1.4 model
provides a good ﬁt of observed smiles at each time to expirati on. However
for the longest maturity T= 1.17 one sees that the away-from-the money
23",2004-02-29T22:06:05Z, june  from  i as rat it ver
paper_qf_51.pdf,24,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","Figure 9: A quantitative comparison between the empirical skews obta ined from a set of
MSFT options traded on November 19, 2003, and our model with q= 1.4, which well-ﬁts
the returns distribution of the underlying stock. We varied αand the volatility parameter
σof the q= 1.4 model for each time to expiration T(see Table 1). We used r= 4.5% and
S0= $25 .55. AIV stands for the actual average (of put and call) implie d volatility.
strikes are slightly over-valued by our model. The valid que stion remains as
to whether this discrepancy is a true market mispricing or an artifact of the
model, which does not lead to log-normal statistics for larg eT. (Note that
we can easily match the market exactly even at T= 1.17 simply by reducing
qappropriately, but this is not what we are attempting to do he re.)
In Table 1 we show how the parameters of the model vary as a func tion
ofT. The parameter αgoes from 0 .1 at early times to ﬂuctuate around 0 .2.
Note that this order of magnitude of α, when related back to the distribution
of stock returns, is entirely consistent with empirical obs ervations of the
leverage eﬀect [35]. The volatility parameter σexhibits a negative term
structure, ranging from 32% to 25%. Such a negative term stru cture has
been consistently observed in our empirical studies (for ex ample it is seen in
ref. [21] where options on FX futures are analyzed). The reas on for such
an eﬀect is, as mentioned earlier, that the q-model considered here predicts
24",2004-02-29T22:06:05Z, november  table  t note itable t note t sut
paper_qf_51.pdf,25,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","T [Years] 0.082 0.159 0.41 1.17
σ[%] 32 31 27 25
α 0.1 0.2 0.3 0.2
Table 1: Variation of the ﬁtted parameters σandαwith time to expiration T, forq= 1.4.
an anomalous growth of the volatility with time, as T1/(3−q), instead of the
empirically observed standard diﬀusive scaling,√
T. A way to correct for
this is to allow the volatility σto be maturity dependent, which amounts
to a mere redeﬁnition of time in the nonlinear Fokker-Planck equation that
deﬁnes the model. For q= 1.4,σshould scale as T−1/8to reproduce the
correct time dependence of the width of the return distribut ion with time.
This scaling appears indeed to be satisﬁed as shown in Figure 10, where
we plot σ(Table 1) of the MSFT data against time on a log-log scale. To
illustrate the regularity of this temporal behaviour, we al so show in Figure
10 the corresponding plot for the σparameter of the options on FX futures
initially shown in [21]. Based on these results, we see that t he total variation
of the model parameters αand the maturity rescaled σare only very slight.
Furthermore, by construction qis kept constant.
Another comment, which should be taken loosely since we have not done
a systematic study of this point, concerns the probabilitie s of default implied
by the parameters of this MSFT example, when reinserted in ou r stock price
model. Assuming that the real drift of MSFT is the risk free ra te (which is
certainly an underestimate), one obtains <0.005% probability of bankruptcy
forT=.082, 0 .01% for T= 0.159, 0 .07% for T= 0.41 and 0 .35% for T=
1.17. These estimates were obtained from Monte Carlo simulati ons, and do
not seem unreasonable. However, the above probability of de fault at 1 year is
probably higher that typical credit risk ratings of MSFT. Th is overestimation
of the probability of default at longer timescales is intima tely related to the
smile shown in Figure 9, for T= 1.17: the away-from-the money strikes are
slightly over-valued by our model as Tincreases. As mentioned earlier, this
is because we choose to keep q= 1.4 ﬁxed at all timescales. If we instead
decided to vary qto ﬁt the smile exactly, it would be closer to q= 1 and
the default probabilities would drop much closer to 0 again ( for example, see
Figure 3 b). Also note that in reality, default probabilitie s might depend on
25",2004-02-29T22:06:05Z,years table variatformer  for   table to  based furtr anotr assumi tse   th  ineasas   also
paper_qf_51.pdf,26,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","Figure 10: Deterministic term-structure in the volatility parameter σof the q= 1.4
model, shown (top) for the MSFT options example of Figure 9 an d Table 1; shown (bot-
tom) for the JY Futures options example discussed in [21].
the real rate of return of the stock, which for MSFT is substan tially higher
than the risk free rate.
It could be interesting to place our results in the context of a comparison
with a popular stochastic volatility model (namely the SABR model, which
for the sake of this discussion is brieﬂy summarized in Appen dix C). The
ﬁt of the SABR model to the MSFT options discussed above was pe rformed
by an external source [32] so we only brieﬂy report the result s here. Three
parameters were varied at each T, and consequently the smiles could be ﬁt
much as in our example. The volatility parameter σvaried with a slightly
positive term structure around a value of 29%. However the pa rameters ρ
(related to the correlations between stock ﬂuctuations and volatility correla-
tions) and λ(which describes the volatility of the volatility) did vary quite
a bit. We saw that ρincreased from 0 .1 to 0 .6. Perhaps more signiﬁcantly,
λwent from around 9 .15 to 1 .14 as maturity increased from 0 .082 to 1 .17.
This decrease makes sense because at large times, the stocha stic volatility in
26",2004-02-29T22:06:05Z, terministic  table futurit ppet three t  haps 
paper_qf_51.pdf,27,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","the SABR model is unbounded; consequently the implied optio n smile would
be way too pronounced if λdid not suppress this feature, forcing the model
into the log-normal limit as Tincreases. On the whole, we saw that while the
SABR model certainly ﬁt the market smiles well, the number of parameters
(3 ifβis ﬁxed) and their variation is larger than in the non-Gaussi anq= 1.4
model (with 2 free parameters).
Consequently, we can conclude (at least for the options stud ied in this
example), that our non-Gaussian model with ﬁxed qindeed yields a rela-
tively parsimonious description of empirically observed o ption prices, cap-
turing both the smile and the skew with a few seemingly robust parameters.
9 Conclusions
In this paper we have extended the non-Gaussian option prici ng theory of
[21] (which recovers the standard Black-Scholes case in the limitq= 1)
to include asymmetries in the underlying process. These wer e introduced
so as to incorporate the leverage correlation eﬀect [10] in a fashion such
that the CEV model of Cox and Ross [27] is recovered in appropr iate limits
(q=α= 1). A theoretical treatment of the problem is possible much along
standard lines of mathematical ﬁnance. Using the fact that t he volatility
of the process is a deterministic function of the stock value , the zero-risk
property of the Black-Scholes hold, and one can set up a gener alized Black-
Scholes PDE as well as deﬁne a unique martingale measure allo wing us to
evaluate option prices via risk-neutral expectations. We h ave introduced
a generalized Feynman-Kac formula for the family of stochas tic processes
involved in our model, namely statistical feedback equatio ns of the type [23]
which evolve according to a nonlinear Fokker-Planck equati on [24],[26]. This
formula in conjunction with Pad´ e expansions could then be u sed to evaluate
closed-form option pricing equations for European call opt ions.
Numerical results allow us to back out Black-Scholes implie d volatilities.
A plot of those across strikes Kand time to expiration Tconstitute a skew
surface. We found that the skew surfaces resulting from our m odel exhibit
many properties seen in real markets. In particular the shap e of the surface
tends to go from a pronounced smile (across strikes) to a slop ing line as T
increases. A comparison of the model to a restricted set of MS FT options
was discussed. We found that the entire skew surface could be well-explained
27",2004-02-29T22:06:05Z,ineasogamconsequently conusns i schools tse cox ross usi  schools  schools  leymaka former   pad anumerical  schools and constitute  i
paper_qf_51.pdf,28,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","with a ﬁxed value of q= 1.4, which also ﬁts to the distribution of the under-
lying. The remaining parameters αandσvary only slightly, especially after
we factored out a deterministic term structure in the volati lity parameter.
While the philosophy of many market participants is in gener al to tune the
parameters of their models to ﬁt market smiles, we try to rela te our param-
eter to the underlying distribution in the hope that we can at tain a more
parsimonious (and therefore more stable) description of bo th underlying and
option. (Along this line of thought, see also [36] and refs. t herein). Indeed,
our results – though not yet statistically signiﬁcant – do in dicate that it might
be possible to explain the entire skew surface with one set of constant (or
slowly varying) parameters. We hope to strengthen this stat ement through
more empirical studies.
Acknowledgements: We wish to thank Jeremy Evnine, Roberto Os-
orio and Peter Carr for interesting and useful comments. Ben oit Pochart is
acknowledged for careful reading of the manuscript.
10 Appendix A: Feynman-Kac Expectations
and Pad´ e Coeﬃcients
The coeﬃcients of the Feynman-Kac Ansatz Eq. (32) when inser ted into Eq.
(31) must satisfy
dg0
du=Z(u)1−qg2(u) (52)
dg1
du= 2(q−2)Z(u)1−qβ(u)g1+h1(u) (53)
dg2
du= (5q−9)Z(u)1−qβ(u)g2(u) +h2(u) (54)
From these equations follows that for path integrals of type exp{/integraltextu
0h1(t)Ω(t)dt}
(with h2= 0), then only the g1coeﬃcient is relevant and the expected value
of the integral yields the approximation
/integraldisplayu
0h1(t)Ω(t)dt=g1(u)Ωu (55)
28",2004-02-29T22:06:05Z,t w alo ined  ackledgement   ev nine roberto os peter carr bepo chart  leymaka eeatns pad coe t leymaka aat eq eq from
paper_qf_51.pdf,29,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","Similarly, if we are only looking at path integrals of the typ e exp{/integraltextu
0h2(t)Ω(t)2dt}
(with h1= 0), coeﬃcients g0andg2are coupled together, so that the
Feynman-Kac approximation of the integral implies
/integraldisplayu
0h2(t)Ω(t)2=g0(u) +g2(u)Ω2
u (56)
In the option pricing problem without skew, namely the case α= 1 of
Eq. (33), we have h1= 0 and h2=Z1−qβ. Integration yields
g0(u) = γ(u)3−q
2(9−5q)(57)
g2(u) =1
9−5q(58)
with
γ(u) = ((3 −q)(2−q)cq)q−1
3−qu2
3−q (59)
For the general case of option pricing including skew (gener alαas in Eq.
(37)), we expand the Pad´ e Ansatz of Eq. (38) for both small an d large Ω,
and equate with the same expansions of the actual quantity we are interested
in, which we then evaluate using Feynman-Kac expectations. In the small Ω
case:
A+ (B−AD)Ωu+ (C−BD+AD2)Ω2
u (60)
=/integraldisplayu
0Z(t)1−q/bracketleftBig
1−ηΩ(t) + (1 −q)β(t)Ω(t)2/bracketrightBig
dt
=/integraldisplayu
0/bracketleftBig
Z(t)1−q−ηh1(t)Ω(t) + (1 −q)h2(t)Ω(t)2/bracketrightBig
dt
=γ(u)(3−q)
2−ηg1(u)Ωu+ (1−q)(g0(u) +g2(u)Ω2
u)
with
η=σ(1−α) (61)
andγas in Eq. (59).
The coeﬃcients gjare calculated from Eq. (52)-Eq. (54) with h1=Z1−q
andh2=βZ1−qand result in:
g0=γ(u)3−q
2(9−5q)(62)
29",2004-02-29T22:06:05Z,simarly leymaka ieq integratfor eq pad aat eq leymaka ib b b b eq t eq eq
paper_qf_51.pdf,30,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","g1=γ(u)(3−q)
4(63)
g2=1
9−5q(64)
Similarly, the large Ω expansion yields
C
DΩu=1−q
η/integraldisplayu
0˜h1(t)Ω(t)dt (65)
=1−q
η˜g1(u)Ωu (66)
with ˜g1calculated from Eq. (53) using ˜h1=β(t)Z(t)1−q, yielding
˜g1=1
2(2−q)(67)
Through standard coeﬃcient comparison, we have enough info rmation to
solve for the Pad´ e coeﬃcients A,B,C and D, resulting in
A=g0(q−1) +3−q
2γ (68)
B=AD−η˜g1
C= (q−1)˜g1
ηD
D=g2(q−1)
q−1
η˜g1+ηg1
In the case of a general skew, the condition ST=Kyields a quadratic
equation with the roots
d1,2=N∓√
N2−4MR
2M(69)
with
N=−D(Ke−rT/S0)1−α−1
1−α+σ−Bασ2
2(70)
M=Cασ2
2−σD (71)
R=(Ke−rT/S0)1−α−1
1−α+Aασ2
2(72)
30",2004-02-29T22:06:05Z,simarly eq through pad iyields ke ke
paper_qf_51.pdf,31,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","where A, B, C andDare evaluated from Eq. (68) at the time u=ˆTwithˆT
of Eq. (17).
These results are all based on evaluating the terms of type Eq . (27) using
the Feynman-Kac Ansatz Eq. (32). We want to compare in detail s these
results to the ones obtained in [21] using a diﬀerent evaluat ion technique,
namely replacing the path Ω( t) by another path deﬁned such that the en-
semble distribution at each time would be the same as for that of the original
paths, namely Ω( t) =/radicalBig
β(T)/β(t)ΩTas in Eq. (28). The numerical results
obtained by that approximations are extremely close to thos e obtained by the
current evaluation method, and by Monte-Carlo simulations . To elucidate
this point we show in Figure 11a a plot of the quantity
/integraldisplayT
0β(t)Z1−qΩ(t)2dt (73)
versus Ω Tevaluated for q= 1.5 and T= 0.5 via i) Monte-Carlo simulations,
ii) as in Eq. (28) and iii) using the Feynman-Kac evaluation E q. (35) with
Eq. (57) and Eq. (58). It is clear that the approach Eq. (28) sl ightly under-
values the expectation of the true paths for small Ω( t), and over-values for
large Ω( t), in such a way that the average value over all Ω( t) yields the correct
result. The Feynman-Kac approximation, on the other hand, i s expected to
be exact in this case.
In the more general case with a skew, we need to evaluate an exp ression
of form/integraldisplayT
01
1 + (1 −α)σΩ(t)β(t)Z1−qΩ(t)2dt (74)
Again this can be done for q= 1.5,α= 0.5 and T= 0.5 using the i) Monte-
Carlo simulations, ii) Eq. (28), iii) Feynman-Kac equation together with a
Pad´ e expansion. The results are shown in Figure 11b. Simila r behaviour as
that of Figure 11a is exhibited: the eﬀective path approxima tion of Eq. (28)
under-values and over-values the true result in such a way th at when averaged
over all Ω Ta good approximation is obtained, whereas the Feynman-Kac
approach is uniformly better.
The close agreement between both approximation techniques allows us in
practice to use either one for option price evaluation. The n umerical results
are extremely close. Nevertheless, the Feynman-Kac approa ch should be
preferred as the more exact one.
31",2004-02-29T22:06:05Z,dare eq with eq tse eq leymaka aat eq  b as eq t   to  evaluated   eq leymaka eq eq it eq t leymaka iag articial intellence   eq leymaka pad t  semi la  eq ta leymaka t t nevertless leymaka
paper_qf_51.pdf,32,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","Figure 11: Evaluation of path dependent integrals as a function of the t erminal value Ω T,
using i) Monte-Carlo simulations, ii) the approximation of Eq. (28), and iii) Feynman-Kac
techniques. a) The quantity Eq. (73) relevant for q= 1.5 and α= 1.0 (no skew) and b)
the quantity Eq. (74) relevant for the general skew case, her e with q= 1.5 and α= 0.5.
In both cases σ= 30%.
11 Appendix B: Exact Solutions via Hyper-
Geometric Functions - A Proposal
As an addition to the theoretical part of our paper we would li ke to brieﬂy
discuss a possible alternative path to solving for the optio n prices of the
non-Gaussian model with skew Eq. (10). The solutions will in volve the fact
that one can map the transformed problem of Eq. (22) onto a fre e-particle
problem in higher dimension as described below.
In the standard case q= 1 the CEV model of Cox and Ross admits an
explicit solution in terms of Bessel functions for arbitrar y values of α. In
this appendix, we show how this result can be obtained by mapp ing to CEV
32",2004-02-29T22:06:05Z, evaluat  eq leymaka t eq eq i exa solutns  geometric funns proposal as eq t eq icox ross vessel in
paper_qf_51.pdf,33,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","process to a standard Brownian motion in higher dimensions, and how this
method generalizes, although incompletely, to the case q >1.
Starting from Eq (22), the change of variable y= 1 + (1 −α)xleads to:
dy=ηdΩ−α
2(1−α)η2P1−qy−1dt (75)
withη=σ(1−α), in this appendix we drop the hat on the time variable.
To order η2, this corresponds to a Fokker-Planck equation of form
∂P
∂t=a∂
∂yP2−q
y+∂2P2−q
∂y2(76)
where the time has been rescaled by η2/2, and a≡α/(1−α).
Now, inserting the Ansatz P=f(y, t)Φ0(y) one obtains
Φ0∂f
∂t=a(−1
y2Φν
0fν+1
y(νΦν−1
0∂Φ0
∂yfν+νΦν
0fν−1∂f
∂y)) +
+ (ν(ν−1)Φν−2
0(∂Φ0
∂y)2fν+ 2ν2Φν−1
0fν−1∂Φ0
∂y∂f
∂y
+νΦν
0(ν−1)fν−2(∂f
∂y)2+νΦν−1
0fν∂2Φ0
∂y2+νΦν
0fν−1∂2f
∂y2) (77)
withν≡2−q.
Grouping together all terms which do not contain derivative s off, and
imposing that the coeﬃcient vanishes leads to an ordinary di ﬀerential equa-
tion for Φ 0:
ν∂
∂y(Φν−1
0∂Φ0
∂y)−aΦν
0
y2+aν
yΦν−1
0∂Φ0
∂y= 0 (78)
which is solved by
Φ0=yλ(79)
provided λ(ν, α) satisﬁes the following quadratic equation:
aν2λ2+νλ(a−1)−a= 0. (80)
The correct root is the one that vanishes when a→0. All remaining terms
can be regrouped as
∂f
∂t=yλ(ν−1)/parenleftBigg
(a+ 2λν)1
y∂
∂yfν+∂2
∂y2fν/parenrightBigg
≡yλ(ν−1)∆dfν(81)
33",2004-02-29T22:06:05Z,brownish starti eq to former   aat groupi t all b b
paper_qf_51.pdf,34,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","where
∆d=∂2
∂y2+d−1
y∂
∂y(82)
is the radial Laplacian operator in d(ﬁctitious) dimensions, where dis given
by:d−1 =a+2λν. The yλ(ν−1)term can ﬁnally be eliminated by introducing
a new coordinate g=g(y) deﬁned as:
g=y1−λ(ν−1)/2
1−λ(ν−1)/2. (83)
In terms of this new coordinate, fobeys a non linear radial diﬀusion equation
in dimensions d′
∂f
∂t= ∆ d′fν(g, t) (84)
where the eﬀective dimension d′is ﬁnally given by:
d′−1 =2(d−1)−λ(ν−1)
2−λ(ν−1)(85)
.
Let us ﬁrst focus on the case q=ν= 1, where the diﬀusion equation is
linear. The initial condition on x,x0= 0, translates into an initial condition
onf(g) which is a δ-function over the hyper-sphere in dimension d′=d, of
radius g0=g(0). The solution of the radial diﬀusion equation in ddimension
for an isotropic initial condition is obviously constructe d as the superposition
of point source solutions of the standard ddimensional diﬀusion equation (i.e.
addimensional Gaussian), averaged over the position of the st arting points,
here sitting on the hyper-sphere S0of radius g0. More explicitly, introducing
ddimensional vectors /vector g, one has:
f(/vector g) =/integraldisplay
S01
(4πt)d/2exp/parenleftBigg
−(/vector g−/vector g0)2
4t/parenrightBigg
(86)
Introducing the angle θbetween /vector gand/vector g0, one ﬁnds:
f(/vector g) =Ωd−2
(4πt)d/2exp/parenleftBigg
−g2+g2
0
4t/parenrightBigg/integraldisplayπ
0dθsind−2θexp/parenleftBigggg0cosθ
2t/parenrightBigg
.(87)
34",2004-02-29T22:06:05Z,la place iat i t t  b b introduci b b b  b
paper_qf_51.pdf,35,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","Using the following identity:
/integraldisplayπ
0dθsind−2θexp/parenleftBigggg0cosθ
2t/parenrightBigg
=√πΓ(d−1
2)/parenleftBigg4t
gg0/parenrightBiggd/2−1
Id/2−1(gg0
2t),
(88)
where Iis the Bessel function, we ﬁnally recover the solution of Cox and
Ross (see also [37]).
The case q >1,ν <1 leads to the so-called ‘fast’ diﬀusion equation in
d′dimensions. In this case, an explicit point source solution can be easily
constructed for an arbitrary position of the point source, a nd is similar to the
d′= 1, Student-like solution discussed in the main text. Unfor tunately, the
general solution for an arbitrary distribution of point sou rces can no longer
be constructed since the equation is non-linear. The case wh ere these points
are on an hyper-sphere is, to the best of our knowledge, unkno wn, although
approximate solutions could perhaps be constructed both fo r short and long
times. We leave the investigation of this path for future wor k.
12 Appendix C: The SABR Model
The SABR model [6] is a stochastic volatility model of the fol lowing form
dS=Sβ¯σdω1 (89)
d¯σ=λ¯σdω2 (90)
with
< dω 1dω2>=ρdt (91)
and
¯σ(0) = σ (92)
It contains four parameters, βandρboth contribute to the skew ( βis in fact
what we call αin the present paper), while λis related to the curvature of
the smile and σis the volatility parameter. Because the log-volatility fo llows
a purely diﬀusive process it can become arbitrarily large at large times. In
our study we assume βheld ﬁxed, seeing as varying ρcan already change
the slope of the skew curve.
35",2004-02-29T22:06:05Z,usi b  b b b gd id is vessel cox ross t istunt ufor t   t mt it because in
paper_qf_51.pdf,36,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","References
[1] F. Black and M. Scholes, The Pricing of Options and Corporate Liabili-
ties, Journal of Political Economy 81, 637-659, (1973)
[2] R.C. Merton, Theory of Rational Option Pricing , Bell Journal of Eco-
nomics and Management Science 4, 143-182, (1973)
[3] For a critique of the Black-Scholes model, see J.P. Bouch aud, M. Potters,
Welcome to a non Black-Scholes world , Quantitative Finance, 1, 482
(2001), and [10].
[4] B. Dupire, Pricing with a Smile , Risk7, 18-20, Jan 1994
[5] E. Derman and I. Kani, Riding on a Smile , Risk7, 32-39, Feb 1994
[6] P.S. Hagan, D. Kumar, A.S. Lesniewski and D.E. Woodward, Managing
Smile Risk , Wilmott, p. 84-108, Sept. 2002
[7] E. Eberlein, U. Keller, K. Prause,, New insights into smile, mispricing
and Value at Risk: the hyperbolic model , Journal of Business, 71, 371
(1998).
[8] S. I. Boyarchenko, and S. Z. Levendorskii, Non-gaussian Merton-Black-
Scholes Theory , World Scientiﬁc (2002)
[9] R. Cont, P. Tankov, Financial modelling with jump processes , CRC
Press, 2004.
[10] J.-P. Bouchaud and M. Potters, Theory of Financial Risks and Deriva-
tive Pricing (Cambridge: Cambridge University Press), 2nd Edition
2004
[11] S.L. Heston, A closed-form solution for options with stochastic volatil ity
with applications to bond and currency options , Rev. of Fin. Studies, 6,
327-343, 1993
[12] J.-P. Fouque, G. Papanicolaou, G. and K. R. Sircar, Derivatives in ﬁ-
nancial markets with stochastic volatility , Cambridge University Press,
2000
36",2004-02-29T22:06:05Z,referenc schools t prici optns corate lia b journal political economy mortotory ratnal optprici bell journal eco management science for  schools suters lcome  schools quantitative nance dup ire prici sme risk jar maani ridi sme risk  pagakumar lnie  woodland managi sme risk pot sept belle ikler pra use new value risk journal business boar c le vendors ki nomorto schools tory world cie nti cont tank ov nancial   chaters tory nancial risks  rival prici cambridge cambridge   editstorev studifound papa nicola ou sir car rivativcambridge  
paper_qf_51.pdf,37,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","[13] P. Carr, H. Geman, D. Madan, M. Yor, Stochastic Volatility for Levy
Processes , Mathematical Finance, 13, 345 (2003).
[14] B. Pochart, and J.-P. Bouchaud, The skewed multifractal random walk
with applications to option smiles , Quantitative Finance, 2, 303 (2002).
[15] J. Perello, J. Masoliver and J.-P. Bouchaud, Multiple time scales in
volatility and leverage correlations: a stochastic volati lity model , Applied
Mathematical Finance, 11, 1-24 (2004).
[16] R. Jarrow, and A. Rudd,, Approximate option valuation for arbitrary
stochastic processes , Journal of Financial Economics, 10, 347 (1982)
[17] C. J. Corrado, and T. Su,, Implied volatility skews and stock index skew-
ness and kurtosis implied by S&P 500 index option prices , The Journal
of Derivatives, XIX, 8 (1997).
[18] D. Backus, S. Foresi, K. Lai, and L. Wu, Accounting for biases in Black-
Scholes , Working paper of NYU Stern School of Business, 1997
[19] M. Potters, R. Cont, and J.-P. Bouchaud, Financial markets as adaptive
systems , Europhysics letters, 41, 239 (1998)
[20] P. Gopikrishnan, V. Plerou, L.A. Nunes Amaral, M. Meyer and H.E.
Stanley, Scaling of the distribution of ﬂuctuations of ﬁnancial mark et
indices , Phys. Rev. E 60, 5305, (1999)
[21] L. Borland, A Theory of non-Gaussian Option Pricing , Quantitative
Finance 2, 415-431, (2002)
[22] L. Borland, Option Pricing Formulas based on a non-Gaussian Stock
Price Model , Phys. Rev. Lett. 89N9, 098701, (2002)
[23] L. Borland, Microscopic dynamics of the nonlinear Fokker-Planck equa-
tion: a phenomenological model , Phys. Rev. E 57, 6634, (1998)
[24] C. Tsallis and D.J. Bukman, Anomalous diﬀusion in the presence of ex-
ternal forces: Exact time-dependent solutions and their th ermostatistical
basis, Phys. Rev. E 54, R2197, (1996)
37",2004-02-29T22:06:05Z,carr gem amadmaor stochastic volatity levy processmatmatical nance po chart  chat quantitative nance  ello mas oli chamultiple applied matmatical nance narrow rudd approximate journal nancial economics tornado su implied t journal rivativbacks forcarticial intellence wu accounti  schools worki sterschobusiness ters cont  ancial euro psics go pi krishna le rou tunmoral meyer stanley scali ph ys rev brand tory optprici quantitative nance brand optprici s stock price mph ys rev  brand mpic former  ph ys rev ts all is but maanomalous exa ph ys rev
paper_qf_51.pdf,38,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","[25] A.M.C. de Souza and C. Tsallis, Student’s t- and r-distributions: Uniﬁed
derivation from an entropic variational principle , Phyisca A 236, 52-57,
(1997)
[26] E. Chasseigne and J. L. Vazquez, Theory of extended solutions for Fast
Diﬀusion Equations in Optimal Classes of Data. Radiation fr om Singu-
larities , Archive Rat. Mech. Anal. 164, 133-187, (2002)
[27] J. C. Cox, unpublished working paper (1974); J. C. Cox an d S. A. Ross,
The Evaluation of Options for Alternative Stochastic Proce sses, Journal
of Financial Economics 3, 145-166, (1976)
[28] L. Borland, J.P. Bouchaud, unpublished.
[29] M. Musiela and M. Rutkowski Martingale Methods in Financial Mod-
elling (Berlin:Springer) 1997
[30] S. Shreve Lectures on Finance prepared by P. Chalasani and S. Jha,
www.cs.cmu.edu/ ˜chal/shreve.html
[31] Eq. (71) of [21] is an approximation.
[32] Market-makers (anonymous), New York, 2003
[33] R. Osorio, L. Borland and C. Tsallis, Distributions of High-Frequency
Stock-Market Observables , inNonextensive Entropy: Interdisciplinary
Applications ed C. Tsallis and M. Gell-Mann (Oxford:Santa Fe Institute
Studies in the Science of Complexity, (2003)
[34] F. Michael and M.D. Johnson, Financial Market Dynamics , Physica A
320, 525, (2003)
[35] J.-P. Bouchaud, A. Matacz, A. and M. Potters, Leverage eﬀect in ﬁnan-
cial markets: the retarded volatility model , Physical Review Letters, 87,
228701, (2001)
[36] M. Potters, J.-P. Bouchaud, and D. Sestovic, Hedged Monte-Carlo: low
variance derivative pricing with objective probabilities , Physica A, 289
517 (2001)
38",2004-02-29T22:06:05Z,souza ts all is stunt uni ph sica chase va zquez tory fast di equatns optimal assdata radiatsi archive rat me anal cox cox ross t evaluatoptns alternative stochastic pro ce journal nancial economics brand  chamus ie la rut   martale methods nancial mod berli hr eve leurnance chal sans ha eq market new york soc brand ts all is distributns hh frequency stock market observable noextensive entropy interdisciplinary applicatns ts all is gel manoxford santa fe institute studiscience lexity michael sonancial market dynamics psics  chamata cz ters levge psical review ters ters  chatesto vic edged   psics
paper_qf_51.pdf,39,A Non-Gaussian Option Pricing Model with Skew,"  Closed form option pricing formulae explaining skew and smile are obtained
within a parsimonious non-Gaussian framework. We extend the non-Gaussian option
pricing model of L. Borland (Quantitative Finance, {\\\\bf 2}, 415-431, 2002) to
include volatility-stock correlations consistent with the leverage effect. A
generalized Black-Scholes partial differential equation for this model is
obtained, together with closed-form approximate solutions for the fair price of
a European call option. In certain limits, the standard Black-Scholes model is
recovered, as is the Constant Elasticity of Variance (CEV) model of Cox and
Ross. Alternative methods of solution to that model are thereby also discussed.
The model parameters are partially fit from empirical observations of the
distribution of the underlying. The option pricing model then predicts European
call prices which fit well to empirical market data over several maturities.
","[37] see also C. Albanese, G. Campolieti, Extensions of the Black-Scholes
formula , working paper (2001).
39",2004-02-29T22:06:05Z,alaese campo et extensns  schools
paper_qf_52.pdf,1,Detecting intraday financial market states using temporal clustering,"  We propose the application of a high-speed maximum likelihood clustering
algorithm to detect temporal financial market states, using correlation
matrices estimated from intraday market microstructure features. We first
determine the ex-ante intraday temporal cluster configurations to identify
market states, and then study the identified temporal state features to extract
state signature vectors which enable online state detection. The state
signature vectors serve as low-dimensional state descriptors which can be used
in learning algorithms for optimal planning in the high-frequency trading
domain. We present a feasible scheme for real-time intraday state detection
from streaming market data feeds. This study identifies an interesting
hierarchy of system behaviour which motivates the need for time-scale-specific
state space reduction for participating agents.
","
Detecting intraday ﬁnancial market states using
temporal clustering
D. HENDRICKS∗, T. GEBBIE and D. WILCOX
School of Computer Science and Applied Mathematics, University of the Witwatersrand
Johannesburg, WITS 2050, South Africa
(v1.1 released December 2015 )
We propose the application of a high-speed maximum likelihood clustering algorithm to detect temporal
ﬁnancial market states, using correlation matrices estimated fro m intraday market microstructure fea-
tures. We ﬁrst determine the ex-ante intraday temporal cluster conﬁgurations to identify market stat es,
and then study the identiﬁed temporal state features to extract state signature vectors which enable
online state detection. The state signature vectors serve as low-dimensional state descriptors which can
be used in learning algorithms for optimal planning in the high- frequency trading domain. We present
a feasible scheme for real-time intraday state detection from strea ming market data feeds. This study
identiﬁes an interesting hierarchy of system behaviour which m otivates the need for time-scale-speciﬁc
state space reduction for participating agents.
Keywords : market microstructure; temporal clustering; ﬁnancial market state s; state space reduction
JEL Classiﬁcation : C61, C63, D81, G10
1. Introduction
The ﬁnancial market represents a prime example of an observable comple x adaptive system. Many
heterogeneous adaptive agents, such as traders, portfolio managers, market makers and regulatory
authorities, interact non-linearly over time with each other and the electronic exchange, allowing for
the emergence of complex behaviours beyond that expected based on int rinsic agent characteristics.
Many authors have viewed ﬁnancial markets through this lens, consider ing analogues with physical
systems to formulate models which aid our understanding of observed system characteristics (see
Arthur (1995), Arthur et al. (1997), Brock (1993), Hommes (2001), Wilcox and Gebbie (2014) and
the references therein). Recent technological advances, accelerat ed by a highly competitive industry,
have allowed for the eﬃcient generation, storage and retrieval of ﬁnancial data at micro time scales,
providing a rich record of the price formation process as a laboratory for intensive study. The
ﬁeld of market microstructure developed to study the characterist ics and behaviours of ﬁnancial
system dynamics at this scale (see O’Hara (1998), Madhavan (2000), Biais et al. ( 2005), Hinton
(2007), Gen¸ cay et al. (2010), Baldovin et al. (2015) for a comprehensive discus sion). In particular,
as intraday trading and investment processes become increasingly au tomated, understanding the
system dynamics at varying intraday time scales is critical for an eﬃc ient trajectory through the
system to be mapped by participating agents.
This paper aims to use a physical analogy to the ferromagnetic Potts model at thermal equilib-
rium to describe object interactions, before deriving an unsuper vised clustering algorithm, where
∗Corresponding author. Email: dieter.hendricks@students .wits.ac.za
1
arXiv:1508.04900v3  [q-fin.TR]  24 Feb 2017",2015-08-20T07:22:55Z,tei schouter science applied matmatics  witwatersrand johannesburg south africa cember   t   keywords ass introdut many many arthur arthur brock howsonewbie recent t hara ada vabia is htogebald ov ii posts correspondi em articial intellence  
paper_qf_52.pdf,2,Detecting intraday financial market states using temporal clustering,"  We propose the application of a high-speed maximum likelihood clustering
algorithm to detect temporal financial market states, using correlation
matrices estimated from intraday market microstructure features. We first
determine the ex-ante intraday temporal cluster configurations to identify
market states, and then study the identified temporal state features to extract
state signature vectors which enable online state detection. The state
signature vectors serve as low-dimensional state descriptors which can be used
in learning algorithms for optimal planning in the high-frequency trading
domain. We present a feasible scheme for real-time intraday state detection
from streaming market data feeds. This study identifies an interesting
hierarchy of system behaviour which motivates the need for time-scale-specific
state space reduction for participating agents.
","
both the number of clusters and conﬁguration emerges from the data (Blatt et al. (1996, 1997),
Wiseman et al. (1998), Giada and Marsili (2001)). Treating intraday time peri ods as objects, the
algorithm will be used to identify intraday market states from observ ed market microstructure
features. Although Marsili (2002) used a similar approach to classify days as states, the authors
are unaware of another study which applies this technique to intraday period clustering using
multiple features. In addition, a high-speed Parallel Genetic Algorithm (PGA) w ill be used for
eﬃcient computation of the cluster conﬁgurations, with absolute comput ation speeds conducive to
overnight or even intraday recalibration of identiﬁed states (Hendri cks et al. (2015)).
The results reveal an interesting hierarchy of system behaviour at diﬀerent time scales. Statis-
tically signiﬁcant power-law ﬁts to conﬁguration characteristics su ggest scale-invariant behaviour
which may translate to persistent features in market states. In add ition, the power-law ﬁts yield
diﬀerent scaling exponents at the diﬀerent time scales, suggestin g that the system may be at criti-
cality at each scale, possibly with diﬀerent universality classes c haracterising behaviour (Dacorogna
et al. (1996), Gabaix et al. (2003), Emmert-Streib and Dehmer (2010), Mastromatteo and Marsili
(2011)). This motivates the importance of time-scale speciﬁc information when planning in this
domain. Here we are considering a particular case of calendar time when investigating scale-related
phenomena, however we note that there are alternative scales used to m easure the ﬁnancial system.
There is a rich history in the literature which has aimed to direct ly model the event time founda-
tions of market microstructure processes. The seminal work of Garman ( 1976), which used point
processes to model order book events, forms the basis of many subseq uent event time approaches
to modelling transaction and quote data. An important extension of this vi ew is the vector autore-
gressive model for trades and quotes developed by Hasbrouck (1988, 1991) and Engle and Russell
(1998). A complementary approach introduces the concept of intrinsic time , which aims to measure
trading opportunities in reference to speciﬁc features of traded s tocks, for example, using the rate
of trading to modify calendar or chronological time. These are discussed by M¨ uller et al. (1995)
and Derman (2002). The more recent use of Hawkes processes to model mutu ally-exciting order
book events is an important return to the idea of viewing events as a f oundational concept when
modelling transactions and order book dynamics (Large (2000), Toke and Pomponio (2012), Bacry
et al. (2015), Abergel and Jedidi (2015)).
Easley et al. (2012) introduce the volume time paradigm for high-frequency trading, with the
clock ticking according to the number of events (proxied by trade volume) ﬂowing through the
system. This is a pragmatic attempt to reconcile the foundational even t-based paradigm introduced
by Garman (1976) with the wide use of chronological or calendar time. They argue t hat machines
operate on a clock which is not chronological, but rather related to the nu mber of cycles per
instruction initiated by an event (Easley et al. (2012), Patterson and Henn essy (2013)). This
allows one to measure time in terms of frequency of changes in informati on, as measured by
trading volumes. When one considers the complex event processing paradigm which underpins
many automated trading systems in ﬁnancial markets (Adi et al. (2006)), one can appreciate the
suitability of the event-based clock and the view that the calendar t ime clock is a legacy convenience
from the low-frequency, human-trader-driven world. As the shift from human-driven to machine-
driven trading dominates ﬁnancial markets, the study of event-time -scale phenomena has become
increasingly important and warrants further exploration.
While the identiﬁed market states reveal many interesting insigh ts, trading agents would beneﬁt
from being able to detect online (or in real-time) which state they ar ecurrently in. We develop
a novel technique which extracts the characteristic signature of m arket activity from each of the
identiﬁed states, and uses this as the basis for an online state detect ion algorithm. In one appli-
cation, this is used to construct 1-step transition probability matri ces, which can be reﬁned online
and used in optimal planning algorithms.
This paper proceeds as follows: Section 2 describes a non-parametri c clustering approach using
a physical Potts model analogy. Section 3 uses the Potts model analogy to de rive a maximum
likelihood estimator for the optimal cluster conﬁguration. Section 4 d escribes the idea of clustering
time periods as objects in order to identify market states. Section 5 describes the parallel genetic
2",2015-08-20T07:22:55Z,blast basemagra mars li treati although mars li illel genetic algorithm  ndr t stat is ida core a articial intellence meme rt st rei  mer astro maer mars li  re tre t germaaharbor uk eagle russell tse r mat hawks large take coo ba y aber gel jedi di   germaty  paerso wadi as w  i seposts seposts sesen
paper_qf_52.pdf,3,Detecting intraday financial market states using temporal clustering,"  We propose the application of a high-speed maximum likelihood clustering
algorithm to detect temporal financial market states, using correlation
matrices estimated from intraday market microstructure features. We first
determine the ex-ante intraday temporal cluster configurations to identify
market states, and then study the identified temporal state features to extract
state signature vectors which enable online state detection. The state
signature vectors serve as low-dimensional state descriptors which can be used
in learning algorithms for optimal planning in the high-frequency trading
domain. We present a feasible scheme for real-time intraday state detection
from streaming market data feeds. This study identifies an interesting
hierarchy of system behaviour which motivates the need for time-scale-specific
state space reduction for participating agents.
","
algorithm for high-speed detection of the temporal cluster conﬁguration u sing the maximum like-
lihood estimator. Section 6 describes an approach for online state dete ction. Section 7 discusses
scale-invariant properties of the cluster conﬁgurations and how these may be exploited for eﬃcient
online state detection. Section 8 discusses the data used, workﬂow and results. Section 9 provides
some concluding remarks and suggestions for further research.
2. Super-paramagnetic clustering
Blatt et al. (1996, 1997) and Wiseman et al. (1998) proposed a novel non-parametric clu stering ap-
proach, based on an analogy to the ferromagnetic Potts model at thermal equili brium. By assigning
a Potts spin variable to each object and introducing a short-range dist ance-dependent ferromag-
netic interaction ﬁeld, regions of aligned spins emerge, which are analogou s to groups of objects in
the same cluster, where spin alignment suggests object homogeneity (Wang and Swendsen (1990)).
More formally, consider a q-state Potts model with spins si= 1, ..., q fori= 1, ..., N , where Nis
the total number of objects in the system. The cost function is given b y the following Hamiltonian:
H=−/summationdisplay
si,sj∈SJijδ(si, sj) (1)
where the spins sican take on q-states and the coupling of the ithandjthobject are governed
byJij. In the case of object clustering for a data sample, a candidate conﬁgurat ion is given by
the set S={si}n
i=1, where sirepresents the cluster group index to which the ithobject belongs.
One can consider the coupling parameters Jijas being a function of the correlation coeﬃcient Cij
(Kullmann et al. (2000), Giada and Marsili (2001)). This is used to specify a distance function that
is decreasing with distance between objects. If all the spins are r elated in this way, then each pair
of spins is connected by some non-vanishing coupling Jij=Jij(Cij). This allows one to interpret
sias a Potts spin in the Potts model Hamiltonian with Jijdecreasing with the distance between
objects (Blatt et al. (1996), Kullmann et al. (2000)). The case where there is only one cluster
can be thought of as a ground state. As the system becomes more excited, it c ould break up into
additional clusters. Each cluster would have speciﬁc Potts magnetis ations, even though the nett
magnetisation can be zero for the complete system. Generically, the c orrelation would then be both
a function of time and temperature in order to encode both the evoluti on of clusters, as well as
the hierarchy of clusters as a function of temperature. In the basic ap proach, one is looking for the
lowest energy state that ﬁts the data.
3. A maximum likelihood approach
In order to parameterise the model eﬃciently, one can choose to make an ansatz for the data
generative function (Noh (2000)) and use this to develop a maximum-likeli hood approach (Giada
and Marsili (2001)), rather than explicitly solving the Potts Hamiltonian n umerically (Blatt et al.
(1996), Kullmann et al. (2000)). A number of authors have considered this appr oach for object clus-
tering (McLachlan et al. (1996), Giada and Marsili (2001), Mungan and Ramasco (2010)), h owever
we follow the proposition by Giada and Marsili (2001). A summary exposition w ill be presented
here (as shown in Hendricks et al. (2015)), with a full derivation availab le in the Appendices.
According to the Noh (2000) ansatz, the generative model of the time series as sociated with the
ithobject can then be written as
xi(t) =gsiηsi+/radicalBig
1−g2siǫi (2)
where the cluster-related inﬂuences are driven by ηsiand the object-speciﬁc eﬀects by ǫi, both
3",2015-08-20T07:22:55Z,sesesesesu blast basemaposts by posts wa ends e posts is t hamtoniaji ji ione ji as ci ullmangra mars li   ji ji ci  posts posts hamtoniaji easi blast ullmant as eaposts generically iinot gra mars li posts hamtoniablast ullmanmc lachlagra mars li su aram asc gra mars li kendrick appendicaccordi not b
paper_qf_52.pdf,4,Detecting intraday financial market states using temporal clustering,"  We propose the application of a high-speed maximum likelihood clustering
algorithm to detect temporal financial market states, using correlation
matrices estimated from intraday market microstructure features. We first
determine the ex-ante intraday temporal cluster configurations to identify
market states, and then study the identified temporal state features to extract
state signature vectors which enable online state detection. The state
signature vectors serve as low-dimensional state descriptors which can be used
in learning algorithms for optimal planning in the high-frequency trading
domain. We present a feasible scheme for real-time intraday state detection
from streaming market data feeds. This study identifies an interesting
hierarchy of system behaviour which motivates the need for time-scale-specific
state space reduction for participating agents.
","
treated as Gaussian random variables with unit variance and zero mean1. The relative contribution
is controlled by the intra-cluster coupling parameter gsi. The Noh-Giada-Marsili model encodes the
idea that objects which have something in common belong in the same cl uster, object membership
in a particular cluster is mutually exclusive and intra-cluster c orrelations are positive.
If one takes Equation 2 as a statistical hypothesis, it is possible to com pute the probability
density P({¯xi}|G,S) for any given set of parameters ( G,S) = ({gs},{si}) by observing the data
set{xi}, i, s= 1, ..., N as a realisation of the common component of Equation 2 as follows Giada
and Marsili (2001):
P({¯xi}|G,S) =D/productdisplay
d=1/angbracketleftBiggN/productdisplay
i=1δ/parenleftBig
xi(t)−(gsi¯ηsi+/radicalBig
1−g2si¯ǫi)/parenrightBig/angbracketrightBigg
. (5)
In Equation 5, Nis the number of objects and Dis the number of feature measurements for each
object. The variable δis the Dirac delta function and ∝an}bracketle{t...∝an}bracketri}htdenotes the mathematical expectation.
For a given cluster structure S, the likelihood is maximal when the parameter gstakes the values
g∗
s=/braceleftBigg/radicalBig
cs−ns
n2
s−nsforns>1,
0 for ns≤1.(6)
nsin Equation 6 denotes the number of objects in cluster s, i.e.
ns=N/summationdisplay
i=1δsi,s. (7)
The variable csis the internal correlation of the sthcluster, denoted by the following equation:
cs=N/summationdisplay
i=1N/summationdisplay
j=1Cijδsi,sδsj,s. (8)
The variable Cijis the Pearson correlation coeﬃcient of the data, denoted by the following
equation:
Cij=¯xi¯xj/radicalbig
∝bardbl¯xi2∝bardbl∝bardbl¯xj2∝bardbl. (9)
The maximum likelihood of structure Scan be written as P(G∗,S|¯xi)∝expDL(S), where the
resulting likelihood function per feature Lcis denoted by
Lc(S) =1
2/summationdisplay
s:ns>1/parenleftbigg
logns
cs+ (ns−1) logn2
s−ns
n2s−cs/parenrightbigg
. (10)
1This form of the price model ensures that the self correlatio n of a stock is one and independent of the cluster coupling. Th is
can be seen by computing the self correlation E[x2
i] and using that clusters and stock unique process are unit va riance zero
mean processes
E[(gsiηsi+/radicalBig
1−g2siǫi)2] =g2
si+ (1−g2
si) = 1 . (3)
This is not a unique choice, another possible choice often us ed is
E[(√gsi/radicalbig1 +gsiηsi+1/radicalbig1 +gsiǫi)2] =1 +gsi
1 +gsi= 1. (4)
4",2015-08-20T07:22:55Z,t t not gra mars li  equatequatgra mars li b b b b b iequatis is t dfrac for b b equatt ci t ci is pearsoci t calc is lc  th b 
paper_qf_52.pdf,5,Detecting intraday financial market states using temporal clustering,"  We propose the application of a high-speed maximum likelihood clustering
algorithm to detect temporal financial market states, using correlation
matrices estimated from intraday market microstructure features. We first
determine the ex-ante intraday temporal cluster configurations to identify
market states, and then study the identified temporal state features to extract
state signature vectors which enable online state detection. The state
signature vectors serve as low-dimensional state descriptors which can be used
in learning algorithms for optimal planning in the high-frequency trading
domain. We present a feasible scheme for real-time intraday state detection
from streaming market data feeds. This study identifies an interesting
hierarchy of system behaviour which motivates the need for time-scale-specific
state space reduction for participating agents.
","
From Equation 10, it follows that Lc= 0 for clusters of objects that are uncorrelated, i.e. where
g∗
s= 0 or cs=nsor when the objects are grouped in singleton clusters for all the cluste r indexes
(ns= 1). Equations 8 and 10 illustrate that the resulting maximum likelih ood function for S
depends on the Pearson correlation coeﬃcient Cijand hence exhibits the following advantages in
comparison to conventional clustering methods:
•It isunsupervised : The optimal number of clusters is unknown a priori and not ﬁxed at the
outset
•The interpretation of results is transparent in terms of the model, namely Equation 2.
Giada and Marsili state that max sLc(S) provides a measure of structure inherent in the cluster
conﬁguration represented by the set S={s1, ..., s n}Giada and Marsili (2001). The higher the
value, the more pronounced the structure.
We note that the particular choice of Gaussian innovations in Equation 2 is convenient, since the
Pearson correlation coeﬃcient then completely characterises pairwise interactions amongst objects
in the system (Giada and Marsili (2001)). This is a necessary condition, given the physical analogy
and link to the motivating Hamiltonian given in Equation 1. The application of this technique to
high-frequency ﬁnancial time series may motivate a more prudent ass umption for the underlying
object and cluster dynamics, incorporating jumps to better model t he price formation process
at this scale. However, the use of, say, jump diﬀusion innovations wou ld require an alternative
dependency metric, such as L´ evy copulas, to completely capture ob ject interactions (Cont and
Tankov (2004), McNeil et al. (2015)), requiring a careful re-derivation of th e appropriate likelihood
function. This will be explored in further research.
4. Detecting temporal states using clustering
The data generative model speciﬁed by Equation 2 is suﬃciently gene ric that it can be applied
to a diverse set of problem domains, where object and cluster innovat ions can be assumed to be
Gaussian. In the ﬁnancial domain, initial applications focused on clust ering stocks based on price
changes (Giada and Marsili (2001), Hendricks et al. (2015)), however Marsili ( 2002) proposed that
this technique could be used to cluster time periods in order to identify temporal market states .
Days were grouped into clusters based on the closing price performan ce of the chosen universe of
stocks, demonstrating a meaningful classiﬁcation of market-wide acti vity which persists through
time (Marsili (2002)). We propose that a similar approach can be applied to di scover intraday
temporal states, clustering time periods based on the performance of multiple observable market
microstructure features. A practical trading system often has acces s to a real-time market data
feed, from which multiple features can be extracted to describe v arious aspects of the evolving
limit order book. In addition, examining temporal cluster conﬁguration s at varying time scales
can suggest a hierarchy of system behaviour, providing insights int o exogenous and endogenous
market activity. This can also assist trading agents in developing opti mal trajectories for varying
objectives, such as stock acquisition or liquidation at minimal cost. I n particular, for an agent
tasked to learn an optimal policy (state-action mapping), the grouping of te mporal periods into
market states based on market microstructure feature performance pro vides a novel scheme to
reduce the dimensionality of the state space and promote eﬃcient learn ing.
In this paper, we will focus on the emergent hierarchy of system beh aviour at diﬀerent time scales
and explore a scheme for online state detection. In one application, thi s leads to a system of 1-step
state transition probability matrices at varying scales, which can be r eﬁned online in real-time.
These can be used in optimal planning schemes where Markovian dynami cs are assumed and state
persistence can be exploited.
5",2015-08-20T07:22:55Z,from equatlc equatns pearsocija nd it t t equatgra mars li lc gra mars li t  equatpearsogra mars li  hamtoniaequatt cont tank ov mc ne  tei t equatra mars li kendrick mars li days mars li  i iitse mark ian
paper_qf_52.pdf,6,Detecting intraday financial market states using temporal clustering,"  We propose the application of a high-speed maximum likelihood clustering
algorithm to detect temporal financial market states, using correlation
matrices estimated from intraday market microstructure features. We first
determine the ex-ante intraday temporal cluster configurations to identify
market states, and then study the identified temporal state features to extract
state signature vectors which enable online state detection. The state
signature vectors serve as low-dimensional state descriptors which can be used
in learning algorithms for optimal planning in the high-frequency trading
domain. We present a feasible scheme for real-time intraday state detection
from streaming market data feeds. This study identifies an interesting
hierarchy of system behaviour which motivates the need for time-scale-specific
state space reduction for participating agents.
","
5. A high-speed Parallel Genetic Algorithm implementation
The likelihood function speciﬁed in Equation 10 serves as the objec tive function in a metaheuristic
optimisation routine, where candidate cluster conﬁgurations are evaluat ed and successively im-
proved until a conﬁguration best explains the inherent structure in a given correlation matrix.
Giada and Marsili (2001) used simulated annealing and deterministic maxi misation to approxi-
mate the maximum likelihood structure. While appropriate for thei r study, these techniques are
inherently computationally intensive and may require a signiﬁcant amount of time to converge for
large-scale problems. In addition, it is unclear whether such traje ctory-based methods are appro-
priate for the multi-featured clustering problem considered in t his paper, since Giada and Marsili
(2001) clustered objects (stocks) based on a single feature (price ret urns). Hendricks et al. (2015)
and Cieslakiewicz (2014) propose the use of a high-speed Parallel Genetic Algorithm (PGA), lever-
aging the Streaming Multiprocessors (SMs) of a Graphics Processin g Unit (GPU), where Equation
10 is used as a ﬁtness function to ﬁnd the cluster conﬁguration which b est approximates the
maximum likelihood structure. They implemented a C-based mast er-slave PGA using the Nvidia
Compute Uniﬁed Device Architecture (CUDA) development environmen t, using the Single Pro-
gram Multiple Data (SPMD) architecture to enumerate the GPU thread hierarchy with population
members for concurrent application of genetic operators.
Consider the problem of ﬁnding the cluster conﬁguration of nobjects. Then, given Ncandidate
cluster conﬁguration structures making up the population,
S1={s1
1, ..., s1
n}
S2={s2
1, ..., s2
n}
...
SN={sN
1, ..., sN
n}
would be mapped to the GPU thread hierarchy using a 2-dimensional grid , as shown in Table 1.
CUDA thread block grid
S1S2. . . SN
object1s1
1s2
1. . . sN
1
object2s1
2s2
2. . . sN
2......... . . ....
objectns1
ns2
n. . . sN
n
Table 1.: Mapping of population to CUDA thread hierarchy
The PGA was applied to the relatively small problem of ﬁnding the clu ster conﬁguration of
18 objects, however demonstrated fast absolute computation time compare d to state-of-the-art
methods, with the promise of scalability within the constraints of t he GPU architecture used
(Hendricks et al. (2015), Cieslakiewicz (2014)). We have restricted our an alysis to intraday temporal
periods within one month, however this still yields up to 2208 objec ts in the 5-minute case. Table
2 shows the speciﬁcations and capabilities of the two candidate GPUs and Table 3 shows the PGA
parameter values and number of objects for each of the time scales invest igated. The mapping of
candidate conﬁgurations to the GPU thread hierarchy under the SPMD par adigm results in an
upper bound on the permissible number of objects and population size. Hendricks et al. (2015)
further recognised the importance of ensuring that the population siz e is large enough relative
to the number of objects, to ensure suﬃcient population diversity for convergence to the best
approximation of the maximum likelihood structure within a ﬁnite number of generations. Smaller
6",2015-08-20T07:22:55Z,llel genetic algorithm t equatgra mars li w ra mars li kendrick cie sla view cz llel genetic algorithm streami multiprocessors ms graphics process iunit equatty india ute uni vice architeure se pro multiple data consir tcandidate table table mappi t kendrick cie sla view cz  table us table t kendrick smaller
paper_qf_52.pdf,7,Detecting intraday financial market states using temporal clustering,"  We propose the application of a high-speed maximum likelihood clustering
algorithm to detect temporal financial market states, using correlation
matrices estimated from intraday market microstructure features. We first
determine the ex-ante intraday temporal cluster configurations to identify
market states, and then study the identified temporal state features to extract
state signature vectors which enable online state detection. The state
signature vectors serve as low-dimensional state descriptors which can be used
in learning algorithms for optimal planning in the high-frequency trading
domain. We present a feasible scheme for real-time intraday state detection
from streaming market data feeds. This study identifies an interesting
hierarchy of system behaviour which motivates the need for time-scale-specific
state space reduction for participating agents.
","
populations often lead to sub-optimal algorithm terminations and inconsis tent results. For the 60-
minute, 30-minute and 15-minute cases, the Nvidia Geforce GTX765m notebook GPU had suﬃcient
capability to determine the optimal cluster conﬁgurations from suﬃci ently large populations. The
5-minute case demanded a larger capacity GPU, and the Nvidia Geforce GTX T itan X provided
the necessary additional SMs, CUDA cores and global memory to facilitate eﬃ cient computation.
Graphics Processing Unit (GPU)
Feature Nvidia Geforce GTX 765m Nvidia Geforce GTX Titan X
Compute capability 3.5 5.2
CUDA cores 768 3072
Memory 2048MB 12228MB
Number of streaming multiprocessors 16 96
Max threads / thread block 1024 1024
Thread block dimension 32 32
Max thread blocks / multiprocessor 16 32
Table 2.: Graphics Processing Unit speciﬁcation and capabilities
Time Number of Population Generations Stall Mutation Crossover Computati on
scale periods (objects) size generations probability probability Ti me (sec) ∗
5-minute 2208 4000 4000 1000 0.09 0.9 603 ( D)
15-minute 736 1000 4000 500 0.09 0.9 382 ( N)
30-minute 368 800 4000 500 0.09 0.9 215 ( N)
60-minute 184 600 4000 500 0.09 0.9 132 ( N)
Table 3.: Parameter values and computation times for Parallel Geneti c Algorithm
∗Average from 20 independent runs; Nrefers to the GTX765m Notebook GPU and Drefers to the GTX Titan X Desktop GPU.
We note that the number of generations and stall generations indicated in T able 3 are higher than
one would typically specify for a genetic algorithm, since these promot e potential over-ﬁtting to the
prescribed dataset. Recall that our application is to ﬁnd the candidate cluster conﬁguration which
best explains the structure inherent in a given correlation matrix . Thus we are not concerned with
out-of-sample validity, but would rather prefer to ﬁnd a conﬁguration w ith the highest likelihood
value. The higher number of generations and stall generations, together wi th the mutation operator,
promotes convergence to a higher likelihood structure. The average computation times indicated
in Table 3 are not overly onerous, suggesting that for practical application , overnight or even
intraday estimation of cluster conﬁgurations to capture recent dynami cs is feasible. The proposed
PGA thus oﬀers an eﬃcient, scalable alternative for ﬁnding the best ap proximation of the optimal
cluster conﬁguration, suitable for clustering objects on multiple ob servable features.
6. State Signature Vectors for online state detection
The clustering procedure described thus far can be used as an unsu pervised algorithm to group
temporal periods into states according to feature similarity, howe ver this can only reveal the ex-
ante temporal states and is not suitable for online detection. Upon examination of the resulting
cluster conﬁgurations, we noted that each node refers to a particular t ime period, with an associated
signature of market activity. Furthermore, if two time periods appe ar in the same cluster, given the
data generative model assumed in Equation 2, we conjecture that it is th e relative similarity of their
characteristic signatures of market activity which resulted in the ir assignment to the same cluster.
Using this idea, given a cluster conﬁguration of temporal periods into m arket states, it is possible
to extract a state signature vector (SSV) which summarises the signature of market activity across
stocks and time periods for each state. Then, if one is faced with a ne w candidate feature vector
7",2015-08-20T07:22:55Z,for india before t india before ms graphics processi unit feature india before india before titaute memory number max thread max table graphics processi unit time number populatgentns stall mtossocom put at ti table meter llel gene algorithm ge refers notebook refers titas recall  t t table t state snature veors t upofurtr equatusi tn
paper_qf_52.pdf,8,Detecting intraday financial market states using temporal clustering,"  We propose the application of a high-speed maximum likelihood clustering
algorithm to detect temporal financial market states, using correlation
matrices estimated from intraday market microstructure features. We first
determine the ex-ante intraday temporal cluster configurations to identify
market states, and then study the identified temporal state features to extract
state signature vectors which enable online state detection. The state
signature vectors serve as low-dimensional state descriptors which can be used
in learning algorithms for optimal planning in the high-frequency trading
domain. We present a feasible scheme for real-time intraday state detection
from streaming market data feeds. This study identifies an interesting
hierarchy of system behaviour which motivates the need for time-scale-specific
state space reduction for participating agents.
","
(FV), the market state assignment can be determined by using the clos est match within the set of
pre-determined SSVs computed oﬄine. FVs are easy to compute online fr om a streaming datafeed
and state assignment can be achieved using a simple Euclidean distanc e computation. To make
these ideas concrete, consider the example illustrated in Figure 1.
 
STATE 1 
STATE 2 
New Feature 
Vector  
->  
STATE 1 
Detect temporal 
clusters / states Compute state signature vectors  for each 
state A new feature vector arrives Calculate distance between new feature 
vector and existing state signature vectors Assign to state 
based on closest 
match 
Figure 1.: Illustration of online state assignment based on identiﬁed s tate signature vectors.
Here, we compute two SSVs from the identiﬁed states, and use these as a basis for assigning a
new FV to a market state. This is based on a simple Euclidean distance metric,
argminp||F V−SSV p||,
where pis the index of the identiﬁed states.
In this paper, we have used four features to characterise market acti vity at intraday scale. These
include: trade price, trade volume, spread andquote volume imbalance . In particular, we consider
therelative change in each of these features. For example, based on a set of feature measurem ents
F5minat 5-minute scale, we would compute
△f5min
t =f5min
t−f5min
t−1
f5min
t−1
for all f5min
t∈ F5min. For the initial temporal cluster detection stage, these “feature ret urns” are
calculated for each stock and concatenated before computing the time pe riod correlation matrix.
For the extraction of SSVs from signiﬁcant states, we compute average featu re returns across
member periods and stocks. For example, consider the case of 15-minut e period clustering. If one
state (cluster) consisted of 2 periods (09:15 - 09:30 and 15:15 - 15:30), then we w ould ﬁnd the
average trade price, trade volume, spread andquote volume imbalance returns across stocks in each
period (i.e. two 4-element vectors), then average across these two vectors to get a single 4-element
vector, which would be the representative SSV for that state.
Although this results in a loss of information, we conjecture that the av erage signature of feature
returns broadly captures the state of market activity. The SSVs for each time-scale conﬁguration
are illustrated in Figures 4, 6, 8 and 10. Following this approach, the FVs cal culated in the on-
line environment would constitute the same averages of feature retur ns, before matching to the
appropriate SSV. We note that this is merely one candidate scheme for ext racting SSVs which are
conducive to online matching for state assignment, however alternati ve schemes for extraction of
SSVs which preserve state-speciﬁc information will be explored in future work. The chosen features
8",2015-08-20T07:22:55Z,vs vs euiato  new feature veor te ute calculate ass lustratre vs  euiaitse ifor for for vs for  although t vs s followi vs  vs vs t
paper_qf_52.pdf,9,Detecting intraday financial market states using temporal clustering,"  We propose the application of a high-speed maximum likelihood clustering
algorithm to detect temporal financial market states, using correlation
matrices estimated from intraday market microstructure features. We first
determine the ex-ante intraday temporal cluster configurations to identify
market states, and then study the identified temporal state features to extract
state signature vectors which enable online state detection. The state
signature vectors serve as low-dimensional state descriptors which can be used
in learning algorithms for optimal planning in the high-frequency trading
domain. We present a feasible scheme for real-time intraday state detection
from streaming market data feeds. This study identifies an interesting
hierarchy of system behaviour which motivates the need for time-scale-specific
state space reduction for participating agents.
","
do not represent an exhaustive set of possible explanatory factors for in traday market activity, but
rather were chosen based on the relative ease of their online construct ion from streaming Level-1
market data feeds JSE (2015). Additional features can be considered in fut ure work.
7. Scale-invariant characteristics of states
The detected temporal cluster conﬁgurations can be further analysed t o determine whether any
characteristics exhibit scale-invariant behaviour. In particular , a visual inspection of the cluster
conﬁgurations shown in Section 8.4 led us to conjecture a possible pow er-law ﬁt for cluster sizes.
Many physical and man-made systems exhibit characteristics which f ollow a power-law functional
form, and its unique mathematical properties sometimes lead to surpr ising physical insights (Gabaix
et al. (2003), Clauset et al. (2009)). Many authors have investigated the nature of information and
forecasting at diﬀerent time scales in ﬁnancial markets (see Dacorogna e t al. (1996), Zhang et al.
(2005), Emmert-Streib and Dehmer (2010) as examples). For our application, th e existence of
diﬀerent critical exponents for the best power-law ﬁts at diﬀere nt time scales may suggest diﬀerent
universality classes which characterise the system activity at eac h scale. In fact, Mastromatteo and
Marsili (2011) discuss the notion that, for a complex adaptive system, dis tinguishable models can
only be gleaned when the system is near criticality. Thus, if ﬁnanci al markets truly are a complex
adaptive system, measurable quantities from the dynamics at each scale should yield a statistically
signiﬁcant power-law ﬁt. Although it is diﬃcult to quantify the exac t nature of these scale-speciﬁc
behaviours or universality classes, their apparent existence sugge sts that investment and trading
decisions would beneﬁt from time-scale-speciﬁc state space inform ation. This would enhance the
eﬃcacy of intraday policies which aim to ﬁnd optimal trajectories thr ough the system.
Given the diﬃculties of identifying statistically signiﬁcant po wer-law ﬁts to empirical quantities
Bauke (2007), we incorporated the maximum likelihood ﬁtting procedur e provided by Clauset et al.
(2009). Outputs from their functions include the scaling parameter of t he proposed power-law ﬁt,
a Kolmogorov-Smirnov test for the goodness-of-ﬁt of the proposed model to the data, the lower-
bound for the ﬁt if the tail distribution follows a power-law and the log-likelihood of the data under
the power-law ﬁt.
We note that a detected temporal cluster conﬁguration results in a set of homogeneous market
states, although it is not clear which states are signiﬁcant, i.e. likel y to persist, or merely transient.
Using allidentiﬁed states may result in spurious state assignments if one us es the online algorithm
described in Section 6. This leads to the need for some selection cri teria for signiﬁcant states, before
extracting SSVs. Candidate criteria include using intra-cluste r connectedness ( cs) or cluster size
with some form of thresholding procedure, however these heurist ics are inherently subjective. The
power-law ﬁt to cluster size provides one candidate objective app roach for state selection. Under
the assumption that the system is near criticality when we ﬁnd a stab le parameter calibration,
choosing the states which best ﬁt the power-law functional form ma y aid in isolating those states
which best capture the system behaviour at that scale, i.e. ﬁlter t he stable, persistent states from
the noise. This provides an objective mechanism for selecting sign iﬁcant states, reducing the set of
SSVs which form the basis for the online state detection algorithm.
8. Data and results
8.1. Data description
The data for this study constituted tick-level trades and top-of-bo ok quotes for 42 stocks on the
Johannesburg Stock Exchange (JSE) from 1 November 2012 to 30 November 2012. Th is data was
sourced from the Thomson Reuters Tick History (TRTH) database. The raw d ata was aggregated
according to the time-scale considered (5-minute, 15-minute, 30-min ute and 60-minute), before cal-
culating the required features (change in trade price, trade volume , spread and volume imbalance).
9",2015-08-20T07:22:55Z,level additnal scale t isemany ga articial intellence ause many da core g meme rt st rei  mer for iastro maer mars li  although  givebau ke ause outputs lmogorov mir nov  usi se vs candidate t unr  vs data data t johannesburg stock exe november november th thomsoreuters tick history t
paper_qf_52.pdf,10,Detecting intraday financial market states using temporal clustering,"  We propose the application of a high-speed maximum likelihood clustering
algorithm to detect temporal financial market states, using correlation
matrices estimated from intraday market microstructure features. We first
determine the ex-ante intraday temporal cluster configurations to identify
market states, and then study the identified temporal state features to extract
state signature vectors which enable online state detection. The state
signature vectors serve as low-dimensional state descriptors which can be used
in learning algorithms for optimal planning in the high-frequency trading
domain. We present a feasible scheme for real-time intraday state detection
from streaming market data feeds. This study identifies an interesting
hierarchy of system behaviour which motivates the need for time-scale-specific
state space reduction for participating agents.
","
The 42 stocks considered represent the prevailing constituents of the FTSE/JSE Top40 headline
index, which contains the 42 largest stocks by market capitalisation in the main board’s FTSE/JSE
All-Share index.
The objects of interest for the cluster analysis are the time periods . Table 4 provides an example
of the required data returns matrix, from which a correlation matrix i s computed for time period
similarity. This is the only required input for the clustering al gorithm.
Feature Times
01-Nov-2012 09:00 01-Nov-2012 09:15 01-Nov-2012 09:30 ... 30-Nov-2012 16:30 30-Nov-2012 16:45Trade
PriceAGL trade price return 0.35 0.60 0.85 ... 0.39 0.22
AMS trade price return 0.94 0.71 0.73 ... 0.63 0.78
SBK trade price return 0.70 0.38 0.58 ... 0.38 0.81
.....................
WHL trade price return 0.90 0.49 0.05 ... 0.65 0.53SpreadAGL spread return 0.64 0.49 0.68 ... 0.05 0.95
AMS spread return 0.33 0.09 0.76 ... 0.44 0.97
SBK spread return 0.09 0.73 0.54 ... 0.80 0.48
.....................
WHL spread return 0.41 0.61 0.11 ... 0.40 0.69Trade
VolumeAGL trade volume return 0.61 0.59 0.96 ... 0.65 0.50
AMS trade volume return 0.16 0.09 0.47 ... 0.86 0.57
SBK trade volume return 0.98 0.05 0.67 ... 0.72 0.12
.....................
WHL trade volume return 0.38 0.49 0.36 ... 0.27 0.81Volume
ImbalanceAGL volume imb return 0.01 0.45 0.78 ... 0.69 0.77
AMS volume imb return 0.54 0.17 0.87 ... 0.47 0.44
SBK volume imb return 0.20 0.42 0.91 ... 0.88 0.58
.....................
WHL volume imb return 0.20 0.09 0.38 ... 0.90 0.12
Table 4.: Illustration of data returns matrix as an input for estimati on of 15-minute period correlations
8.2. Workﬂow
Figure 2 illustrates the process workﬂow and tools used for performi ng the temporal cluster analysis.
The TRTH tick data is stored in a MongoDB noSQL database, with optimised q uery indexes
for eﬃcient data retrieval. A bespoke Application Programming Interfac e (API) was written to
transport data from MongoDB to our primary scientiﬁc computing platform, MATLAB. The data
is used to instantiate a High Frequency Time Series (HFTS) object in MATLAB, which allows for
eﬃcient merging, resampling and aggregation of large-scale irregularly-spac ed tick data. Based on
a chosen time-scale, the data is aggregated, features are extracted and re turns calculated, before
computing the time period correlation matrix. The PGA was implemen ted in CUDA-C using
Nvidia Nsight and the Microsoft Visual Studio development environment . The compiled PGA was
called from the MATLAB environment to run the temporal cluster analysi s. The resulting cluster
conﬁguration is transported to the MATLAB workspace, from which we can det ermine the power-
law ﬁts, extract SSVs, estimate online clusters and compute transit ion probability matrices. Using
the stock, time period, cluster conﬁguration and correlation data, a MA TLAB script was written to
generate an XML ﬁle containing the required node and edge metadata for an un directed graph to
import into Gephi. Gephi was used for cluster conﬁguration visualisat ion, as described in Section
8.3.
10",2015-08-20T07:22:55Z,t tall share t table  feature timnov nov nov nov nov tra price spread tra volume volume imbalance table lustratwork  t mono applicatprograi inter fac mono t hh frequency time seribased t india sht msoft visual stud t t vs usi ge phi ge phi sen
